<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 3 Probability and Statistics | Supplementary Notes and References for ECON 4750</title>
  <meta name="description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 3 Probability and Statistics | Supplementary Notes and References for ECON 4750" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 3 Probability and Statistics | Supplementary Notes and References for ECON 4750" />
  
  <meta name="twitter:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

<meta name="author" content="Brantly Callaway" />


<meta name="date" content="2021-12-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-programming.html"/>
<link rel="next" href="linear-regression.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Outline/Notes for ECON 4750</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-is-this"><i class="fa fa-check"></i><b>1.1</b> What is this?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#what-is-this-not"><i class="fa fa-check"></i><b>1.2</b> What is this not?</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#why-did-i-write-this"><i class="fa fa-check"></i><b>1.3</b> Why did I write this?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#additional-references"><i class="fa fa-check"></i><b>1.4</b> Additional References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#goals-for-the-course"><i class="fa fa-check"></i><b>1.5</b> Goals for the Course</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#studying-for-the-class"><i class="fa fa-check"></i><b>1.6</b> Studying for the Class</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#data-used-in-the-course"><i class="fa fa-check"></i><b>1.7</b> Data Used in the Course</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#first-week-of-class"><i class="fa fa-check"></i><b>1.8</b> First Week of Class</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-programming.html"><a href="statistical-programming.html"><i class="fa fa-check"></i><b>2</b> Statistical Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up R</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-programming.html"><a href="statistical-programming.html#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-programming.html"><a href="statistical-programming.html#downloading-r"><i class="fa fa-check"></i><b>2.1.2</b> Downloading R</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio"><i class="fa fa-check"></i><b>2.1.3</b> RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio-development-environment"><i class="fa fa-check"></i><b>2.1.4</b> RStudio Development Environment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-programming.html"><a href="statistical-programming.html#installing-r-packages"><i class="fa fa-check"></i><b>2.2</b> Installing R Packages</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#a-list-of-useful-r-packages"><i class="fa fa-check"></i><b>2.2.1</b> A list of useful R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-programming.html"><a href="statistical-programming.html#r-basics"><i class="fa fa-check"></i><b>2.3</b> R Basics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-programming.html"><a href="statistical-programming.html#objects"><i class="fa fa-check"></i><b>2.3.1</b> Objects</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-programming.html"><a href="statistical-programming.html#workspace"><i class="fa fa-check"></i><b>2.3.2</b> Workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-programming.html"><a href="statistical-programming.html#functions-in-r"><i class="fa fa-check"></i><b>2.4</b> Functions in R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-types"><i class="fa fa-check"></i><b>2.5</b> Data types</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-programming.html"><a href="statistical-programming.html#numeric-vectors"><i class="fa fa-check"></i><b>2.5.1</b> Numeric Vectors</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-programming.html"><a href="statistical-programming.html#vector-arithmetic"><i class="fa fa-check"></i><b>2.5.2</b> Vector arithmetic</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-programming.html"><a href="statistical-programming.html#more-helpful-functions-in-r"><i class="fa fa-check"></i><b>2.5.3</b> More helpful functions in R</a></li>
<li class="chapter" data-level="2.5.4" data-path="statistical-programming.html"><a href="statistical-programming.html#other-types-of-vectors"><i class="fa fa-check"></i><b>2.5.4</b> Other types of vectors</a></li>
<li class="chapter" data-level="2.5.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-frames"><i class="fa fa-check"></i><b>2.5.5</b> Data Frames</a></li>
<li class="chapter" data-level="2.5.6" data-path="statistical-programming.html"><a href="statistical-programming.html#lists"><i class="fa fa-check"></i><b>2.5.6</b> Lists</a></li>
<li class="chapter" data-level="2.5.7" data-path="statistical-programming.html"><a href="statistical-programming.html#matrices"><i class="fa fa-check"></i><b>2.5.7</b> Matrices</a></li>
<li class="chapter" data-level="2.5.8" data-path="statistical-programming.html"><a href="statistical-programming.html#factors"><i class="fa fa-check"></i><b>2.5.8</b> Factors</a></li>
<li class="chapter" data-level="2.5.9" data-path="statistical-programming.html"><a href="statistical-programming.html#understanding-an-object-in-r"><i class="fa fa-check"></i><b>2.5.9</b> Understanding an object in R</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-programming.html"><a href="statistical-programming.html#logicals"><i class="fa fa-check"></i><b>2.6</b> Logicals</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-programming.html"><a href="statistical-programming.html#additional-logical-operators"><i class="fa fa-check"></i><b>2.6.1</b> Additional Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="statistical-programming.html"><a href="statistical-programming.html#programming-basics"><i class="fa fa-check"></i><b>2.7</b> Programming basics</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="statistical-programming.html"><a href="statistical-programming.html#writing-functions"><i class="fa fa-check"></i><b>2.7.1</b> Writing functions</a></li>
<li class="chapter" data-level="2.7.2" data-path="statistical-programming.html"><a href="statistical-programming.html#ifelse"><i class="fa fa-check"></i><b>2.7.2</b> if/else</a></li>
<li class="chapter" data-level="2.7.3" data-path="statistical-programming.html"><a href="statistical-programming.html#for-loops"><i class="fa fa-check"></i><b>2.7.3</b> for loops</a></li>
<li class="chapter" data-level="2.7.4" data-path="statistical-programming.html"><a href="statistical-programming.html#vectorization"><i class="fa fa-check"></i><b>2.7.4</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="statistical-programming.html"><a href="statistical-programming.html#advanced-topics"><i class="fa fa-check"></i><b>2.8</b> Advanced Topics</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="statistical-programming.html"><a href="statistical-programming.html#tidyverse"><i class="fa fa-check"></i><b>2.8.1</b> Tidyverse</a></li>
<li class="chapter" data-level="2.8.2" data-path="statistical-programming.html"><a href="statistical-programming.html#data-visualization"><i class="fa fa-check"></i><b>2.8.2</b> Data Visualization</a></li>
<li class="chapter" data-level="2.8.3" data-path="statistical-programming.html"><a href="statistical-programming.html#reproducible-research"><i class="fa fa-check"></i><b>2.8.3</b> Reproducible Research</a></li>
<li class="chapter" data-level="2.8.4" data-path="statistical-programming.html"><a href="statistical-programming.html#technical-writing-tools"><i class="fa fa-check"></i><b>2.8.4</b> Technical Writing Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-introduction-to-r-programming"><i class="fa fa-check"></i><b>2.9</b> Lab 1: Introduction to R Programming</a></li>
<li class="chapter" data-level="2.10" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-solutions"><i class="fa fa-check"></i><b>2.10</b> Lab 1: Solutions</a></li>
<li class="chapter" data-level="2.11" data-path="statistical-programming.html"><a href="statistical-programming.html#coding-exercises"><i class="fa fa-check"></i><b>2.11</b> Coding Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html"><i class="fa fa-check"></i><b>3</b> Probability and Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-probability"><i class="fa fa-check"></i><b>3.1</b> Topics in Probability</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#data-for-this-chapter"><i class="fa fa-check"></i><b>3.1.1</b> Data for this chapter</a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#random-variables"><i class="fa fa-check"></i><b>3.1.2</b> Random Variables</a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#pdfs-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.3</b> pdfs, pmfs, and cdfs</a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#summation-operator"><i class="fa fa-check"></i><b>3.1.4</b> Summation operator</a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.5</b> Properties of pmfs and cdfs</a></li>
<li class="chapter" data-level="3.1.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.1.6</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="3.1.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#expected-values"><i class="fa fa-check"></i><b>3.1.7</b> Expected Values</a></li>
<li class="chapter" data-level="3.1.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance"><i class="fa fa-check"></i><b>3.1.8</b> Variance</a></li>
<li class="chapter" data-level="3.1.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-and-variance-of-linear-functions"><i class="fa fa-check"></i><b>3.1.9</b> Mean and Variance of Linear Functions</a></li>
<li class="chapter" data-level="3.1.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.1.10</b> Multiple Random Variables</a></li>
<li class="chapter" data-level="3.1.11" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#conditional-expectations"><i class="fa fa-check"></i><b>3.1.11</b> Conditional Expectations</a></li>
<li class="chapter" data-level="3.1.12" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>3.1.12</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="3.1.13" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#covariance"><i class="fa fa-check"></i><b>3.1.13</b> Covariance</a></li>
<li class="chapter" data-level="3.1.14" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#correlation"><i class="fa fa-check"></i><b>3.1.14</b> Correlation</a></li>
<li class="chapter" data-level="3.1.15" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-expectationsvariances-of-sums-of-rvs"><i class="fa fa-check"></i><b>3.1.15</b> Properties of Expectations/Variances of Sums of RVs</a></li>
<li class="chapter" data-level="3.1.16" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#normal-distribution"><i class="fa fa-check"></i><b>3.1.16</b> Normal Distribution</a></li>
<li class="chapter" data-level="3.1.17" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding"><i class="fa fa-check"></i><b>3.1.17</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-statistics"><i class="fa fa-check"></i><b>3.2</b> Topics in Statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#simple-random-sample"><i class="fa fa-check"></i><b>3.2.1</b> Simple Random Sample</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#estimating-mathbbey"><i class="fa fa-check"></i><b>3.2.2</b> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-of-bary"><i class="fa fa-check"></i><b>3.2.3</b> Mean of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance-of-bary"><i class="fa fa-check"></i><b>3.2.4</b> Variance of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-estimators"><i class="fa fa-check"></i><b>3.2.5</b> Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#relative-efficiency"><i class="fa fa-check"></i><b>3.2.6</b> Relative Efficiency</a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-squared-error"><i class="fa fa-check"></i><b>3.2.7</b> Mean Squared Error</a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#large-sample-properties-of-estimators"><i class="fa fa-check"></i><b>3.2.8</b> Large Sample Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#inference-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.9</b> Inference / Hypothesis Testing</a></li>
<li class="chapter" data-level="3.2.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-1"><i class="fa fa-check"></i><b>3.2.10</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-monte-carlo-simulations"><i class="fa fa-check"></i><b>3.3</b> Lab 2: Monte Carlo Simulations</a></li>
<li class="chapter" data-level="3.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-solutions"><i class="fa fa-check"></i><b>3.4</b> Lab 2 Solutions</a></li>
<li class="chapter" data-level="3.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-questions"><i class="fa fa-check"></i><b>3.5</b> Coding Questions</a></li>
<li class="chapter" data-level="3.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#extra-questions"><i class="fa fa-check"></i><b>3.6</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#nonparametric-regression-curse-of-dimensionality"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Regression / Curse of Dimensionality</a></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>4.2</b> Linear Regression Models</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computation"><i class="fa fa-check"></i><b>4.3</b> Computation</a></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#partial-effects"><i class="fa fa-check"></i><b>4.4</b> Partial Effects</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-1"><i class="fa fa-check"></i><b>4.4.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-regression.html"><a href="linear-regression.html#binary-regressors"><i class="fa fa-check"></i><b>4.5</b> Binary Regressors</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-2"><i class="fa fa-check"></i><b>4.5.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-regression.html"><a href="linear-regression.html#nonlinear-regression-functions"><i class="fa fa-check"></i><b>4.6</b> Nonlinear Regression Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-3"><i class="fa fa-check"></i><b>4.6.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-interaction-terms"><i class="fa fa-check"></i><b>4.7</b> Interpreting Interaction Terms</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-4"><i class="fa fa-check"></i><b>4.7.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="linear-regression.html"><a href="linear-regression.html#elasticities"><i class="fa fa-check"></i><b>4.8</b> Elasticities</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-5"><i class="fa fa-check"></i><b>4.8.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="linear-regression.html"><a href="linear-regression.html#omitted-variable-bias"><i class="fa fa-check"></i><b>4.9</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="4.10" data-path="linear-regression.html"><a href="linear-regression.html#how-to-estimate-the-parameters-in-a-regression-model"><i class="fa fa-check"></i><b>4.10</b> How to estimate the parameters in a regression model</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-6"><i class="fa fa-check"></i><b>4.10.1</b> Computation</a></li>
<li class="chapter" data-level="4.10.2" data-path="linear-regression.html"><a href="linear-regression.html#more-than-one-regressor"><i class="fa fa-check"></i><b>4.10.2</b> More than one regressor</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i><b>4.11</b> Inference</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-7"><i class="fa fa-check"></i><b>4.11.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-birthweight-and-smoking"><i class="fa fa-check"></i><b>4.12</b> Lab 3: Birthweight and Smoking</a></li>
<li class="chapter" data-level="4.13" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-solutions"><i class="fa fa-check"></i><b>4.13</b> Lab 3: Solutions</a></li>
<li class="chapter" data-level="4.14" data-path="linear-regression.html"><a href="linear-regression.html#coding-questions-1"><i class="fa fa-check"></i><b>4.14</b> Coding Questions</a></li>
<li class="chapter" data-level="4.15" data-path="linear-regression.html"><a href="linear-regression.html#extra-questions-1"><i class="fa fa-check"></i><b>4.15</b> Extra Questions</a></li>
<li class="chapter" data-level="4.16" data-path="linear-regression.html"><a href="linear-regression.html#answers-to-some-extra-questions"><i class="fa fa-check"></i><b>4.16</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>5</b> Prediction</a>
<ul>
<li class="chapter" data-level="5.1" data-path="prediction.html"><a href="prediction.html#measures-of-regression-fit"><i class="fa fa-check"></i><b>5.1</b> Measures of Regression Fit</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="prediction.html"><a href="prediction.html#tss-ess-ssr"><i class="fa fa-check"></i><b>5.1.1</b> TSS, ESS, SSR</a></li>
<li class="chapter" data-level="5.1.2" data-path="prediction.html"><a href="prediction.html#r2"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="prediction.html"><a href="prediction.html#model-selection"><i class="fa fa-check"></i><b>5.2</b> Model Selection</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="prediction.html"><a href="prediction.html#limitations-of-r2"><i class="fa fa-check"></i><b>5.2.1</b> Limitations of <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.2" data-path="prediction.html"><a href="prediction.html#adjusted-r2"><i class="fa fa-check"></i><b>5.2.2</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.3" data-path="prediction.html"><a href="prediction.html#aic-bic"><i class="fa fa-check"></i><b>5.2.3</b> AIC, BIC</a></li>
<li class="chapter" data-level="5.2.4" data-path="prediction.html"><a href="prediction.html#cross-validation"><i class="fa fa-check"></i><b>5.2.4</b> Cross-Validation</a></li>
<li class="chapter" data-level="5.2.5" data-path="prediction.html"><a href="prediction.html#model-averaging"><i class="fa fa-check"></i><b>5.2.5</b> Model Averaging</a></li>
<li class="chapter" data-level="5.2.6" data-path="prediction.html"><a href="prediction.html#computation-8"><i class="fa fa-check"></i><b>5.2.6</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prediction.html"><a href="prediction.html#machine-learning"><i class="fa fa-check"></i><b>5.3</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="prediction.html"><a href="prediction.html#lasso"><i class="fa fa-check"></i><b>5.3.1</b> Lasso</a></li>
<li class="chapter" data-level="5.3.2" data-path="prediction.html"><a href="prediction.html#ridge-regression"><i class="fa fa-check"></i><b>5.3.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.3.3" data-path="prediction.html"><a href="prediction.html#computation-9"><i class="fa fa-check"></i><b>5.3.3</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="prediction.html"><a href="prediction.html#binary-outcome-models"><i class="fa fa-check"></i><b>5.4</b> Binary Outcome Models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="prediction.html"><a href="prediction.html#linear-probability-model"><i class="fa fa-check"></i><b>5.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="5.4.2" data-path="prediction.html"><a href="prediction.html#probit-and-logit"><i class="fa fa-check"></i><b>5.4.2</b> Probit and Logit</a></li>
<li class="chapter" data-level="5.4.3" data-path="prediction.html"><a href="prediction.html#average-partial-effects"><i class="fa fa-check"></i><b>5.4.3</b> Average Partial Effects</a></li>
<li class="chapter" data-level="5.4.4" data-path="prediction.html"><a href="prediction.html#computation-10"><i class="fa fa-check"></i><b>5.4.4</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prediction.html"><a href="prediction.html#lab-4-predicting-diamond-prices"><i class="fa fa-check"></i><b>5.5</b> Lab 4: Predicting Diamond Prices</a></li>
<li class="chapter" data-level="5.6" data-path="prediction.html"><a href="prediction.html#lab-4-solutions"><i class="fa fa-check"></i><b>5.6</b> Lab 4: Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="prediction.html"><a href="prediction.html#coding-questions-2"><i class="fa fa-check"></i><b>5.7</b> Coding Questions</a></li>
<li class="chapter" data-level="5.8" data-path="prediction.html"><a href="prediction.html#extra-questions-2"><i class="fa fa-check"></i><b>5.8</b> Extra Questions</a></li>
<li class="chapter" data-level="5.9" data-path="prediction.html"><a href="prediction.html#answers-to-some-extra-questions-1"><i class="fa fa-check"></i><b>5.9</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>6</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="causal-inference.html"><a href="causal-inference.html#potential-outcomes"><i class="fa fa-check"></i><b>6.1</b> Potential Outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="causal-inference.html"><a href="causal-inference.html#parameters-of-interest"><i class="fa fa-check"></i><b>6.2</b> Parameters of Interest</a></li>
<li class="chapter" data-level="6.3" data-path="causal-inference.html"><a href="causal-inference.html#experiments"><i class="fa fa-check"></i><b>6.3</b> Experiments</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="causal-inference.html"><a href="causal-inference.html#estimating-att-with-a-regression"><i class="fa fa-check"></i><b>6.3.1</b> Estimating ATT with a Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="causal-inference.html"><a href="causal-inference.html#internal-and-external-validity"><i class="fa fa-check"></i><b>6.3.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="6.3.3" data-path="causal-inference.html"><a href="causal-inference.html#example-project-star"><i class="fa fa-check"></i><b>6.3.3</b> Example: Project STAR</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="causal-inference.html"><a href="causal-inference.html#unconfoundedness"><i class="fa fa-check"></i><b>6.4</b> Unconfoundedness</a></li>
<li class="chapter" data-level="6.5" data-path="causal-inference.html"><a href="causal-inference.html#panel-data-approaches"><i class="fa fa-check"></i><b>6.5</b> Panel Data Approaches</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="causal-inference.html"><a href="causal-inference.html#difference-in-differences"><i class="fa fa-check"></i><b>6.5.1</b> Difference in differences</a></li>
<li class="chapter" data-level="6.5.2" data-path="causal-inference.html"><a href="causal-inference.html#computation-11"><i class="fa fa-check"></i><b>6.5.2</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="causal-inference.html"><a href="causal-inference.html#instrumental-variables"><i class="fa fa-check"></i><b>6.6</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="causal-inference.html"><a href="causal-inference.html#example-return-to-education"><i class="fa fa-check"></i><b>6.6.1</b> Example: Return to Education</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="causal-inference.html"><a href="causal-inference.html#regression-discontinuity"><i class="fa fa-check"></i><b>6.7</b> Regression Discontinuity</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="causal-inference.html"><a href="causal-inference.html#example-causal-effect-of-alcohol-on-driving-deaths"><i class="fa fa-check"></i><b>6.7.1</b> Example: Causal effect of Alcohol on Driving Deaths</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-drunk-driving-laws"><i class="fa fa-check"></i><b>6.8</b> Lab 5: Drunk Driving Laws</a></li>
<li class="chapter" data-level="6.9" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-solutions"><i class="fa fa-check"></i><b>6.9</b> Lab 5: Solutions</a></li>
<li class="chapter" data-level="6.10" data-path="causal-inference.html"><a href="causal-inference.html#coding-questions-3"><i class="fa fa-check"></i><b>6.10</b> Coding Questions</a></li>
<li class="chapter" data-level="6.11" data-path="causal-inference.html"><a href="causal-inference.html#extra-questions-3"><i class="fa fa-check"></i><b>6.11</b> Extra Questions</a></li>
<li class="chapter" data-level="6.12" data-path="causal-inference.html"><a href="causal-inference.html#answers-to-some-extra-questions-2"><i class="fa fa-check"></i><b>6.12</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supplementary Notes and References for ECON 4750</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-and-statistics" class="section level1" number="3">
<h1><span class="header-section-number">Topic 3</span> Probability and Statistics</h1>
<p>This section contains our crash course review of topics in probability and statistics. The discussion mostly follows Chapters 2 &amp; 3 in the Stock and Watson textbook, and I have cross-listed the relevant sections in the textbook here.</p>
<div id="topics-in-probability" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Topics in Probability</h2>
<p>At a very high level, probability is the set of mathematical tools that allow us to think about <strong>random</strong> events.</p>
<p>Just to be clear, random means <em>uncertain</em>, not 50:50.</p>
<p>A simple example of a random event is the outcome from rolling a die.</p>
<p>Eventually, we will treat data as being random draws from some population. Examples of things that we will treat as random draws are things like a person’s hair color, height, income, etc. We will think of all of these as being random draws because <em>ex ante</em> we don’t know what they will be.</p>
<div id="data-for-this-chapter" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Data for this chapter</h3>
<p>For this chapter, we’ll use data from the U.S. Census Bureau from 2019. It is not quite a full census, but we’ll treat it as the population throughout this chapter.</p>
</div>
<div id="random-variables" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Random Variables</h3>
<p>SW 2.1</p>
<p>A <strong>random variable</strong> is a numerical summary of some random event.</p>
<p>Some examples:</p>
<ul>
<li><p>Outcome of roll of a die</p></li>
<li><p>A person’s height in inches</p></li>
<li><p>A firm’s profits in a particular year</p></li>
<li><p>Creating a random variable sometime involves “coding” non-numeric outcomes, e.g., setting <code>hair=1</code> if a person’s hair color is black, <code>hair=2</code> if a person’s hair is blonde, etc.</p></li>
</ul>
<p>We’ll generally classify random variables into one of two categories</p>
<ul>
<li><p><strong>Discrete</strong> — A random variable that takes on discrete values such as 0, 1, 2</p></li>
<li><p><strong>Continuous</strong> — Takes on a continuum of values</p></li>
</ul>
<p>These are broad categories because a lot of random variables in economics sit in between these two.</p>
</div>
<div id="pdfs-pmfs-and-cdfs" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> pdfs, pmfs, and cdfs</h3>
<p>SW 2.1</p>
<p>The <strong>distribution</strong> of a random variable describes how likely it is take on certain values.</p>
<p>A random variable’s distribution is fully summarized by its:</p>
<ul>
<li><p><strong>probability mass function (pmf)</strong> if the random variable is discrete</p></li>
<li><p><strong>probability density function (pdf)</strong> if the random variable is continuous</p></li>
</ul>
<p>The pmf is somewhat easier to explain, so let’s start there. For some discrete random variable <span class="math inline">\(X\)</span>, its pmf is given by</p>
<p><span class="math display">\[
  f_X(x) = \mathrm{P}(X=x)
\]</span>
That is, the probability that <span class="math inline">\(X\)</span> takes on some particular value <span class="math inline">\(x\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 3.1  </strong></span>Suppose that <span class="math inline">\(X\)</span> denotes the outcome of a roll of a die. Then, <span class="math inline">\(f_X(1)\)</span> is the probability of rolling a one. And, in particular,</p>
<p><span class="math display">\[
  f_X(1) = \mathrm{P}(X=1) = \frac{1}{6}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 3.2  </strong></span>Let’s do a bit more realistic example where we look at the pmf of education in the U.S. Suppose that <span class="math inline">\(X\)</span> denotes the years of education that a person has. Then, <span class="math inline">\(f_X(x)\)</span> is the probability that a person has exactly <span class="math inline">\(x\)</span> years of education. We can set <span class="math inline">\(x\)</span> to different values and calculate the probabilities of a person having different amounts of education. That’s what we do in the following figure:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-79"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-79-1.png" alt="pmf of U.S. education" width="672" />
<p class="caption">
Figure 3.1: pmf of U.S. education
</p>
</div>
<p>There are some things that are perhaps worth pointing out here. The most common amount of education in the U.S. appears to be exactly 12 years — corresponding to graduating from high school; about 32% of the population has that level of education. The next most common number of years of education is 16 — corresponding to graduating from college; about 24% of individuals have this level of education. Other relatively common values of education are 13 years (14% of individuals) and 18 (13% of individuals). About 1% of individuals report 0 years of education. It’s not clear to me whether or not that is actually true or reflects some individuals mis-reporting their education.</p>
</div>
<p>Before going back to the pdf, let me describe another way to fully summarize the distribution of a random variable.</p>
<ul>
<li><strong>Cumulative distribution function (cdf)</strong> - The cdf of some random variable <span class="math inline">\(X\)</span> is defined as</li>
</ul>
<p><span class="math display">\[
  F_X(x) = \mathrm{P}(X \leq x)
\]</span>
In words, this cdf is the probability that the random <span class="math inline">\(X\)</span> takes a value less than or equal to <span class="math inline">\(x\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 3.3  </strong></span>Suppose <span class="math inline">\(X\)</span> is the outcome of a roll of a die. Then, <span class="math inline">\(F_X(3) = \mathrm{P}(X \leq 3)\)</span> is the probability of rolling 3 or lower. Thus,</p>
<p><span class="math display">\[
  F_X(3) = \mathrm{P}(X \leq 3) = \frac{1}{2}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 3.4  </strong></span>Let’s go back to our example of years of education in the U.S. In this case, <span class="math inline">\(F_X(x)\)</span> is the fraction of the population that has less than <span class="math inline">\(x\)</span> years of education. We can calculate this for different values of <span class="math inline">\(x\)</span>. That’s what we do in the following figure:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-80"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-80-1.png" alt="cdf of U.S. educ" width="672" />
<p class="caption">
Figure 3.2: cdf of U.S. educ
</p>
</div>
<p>You can see that the cdf is increasing in the years of education. And there are big “jumps” in the cdf at values of years of education that are common such as 12 and 16.</p>
</div>
<p>We’ll go over some properties of pmfs and cdfs momentarily (perhaps you can already deduce some of them from the above figures), but before we do that, we need to go over some (perhaps new) tools.</p>
</div>
<div id="summation-operator" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Summation operator</h3>
<p>It will be convenient for us to have a notation that allows us to add up many numbers/variables at the same time. To do this, we’ll introduce the <span class="math inline">\(\sum\)</span> operation.</p>
<p>As a simple example, suppose that we have three variables (it doesn’t matter if they are random or not): <span class="math inline">\(x_1,x_2,x_3\)</span> and we want to add them up. Then, we can write
<span class="math display">\[
  \sum_{i=1}^3 x_i := x_1 + x_2 + x_3
\]</span>
Many timess, once we have data, there will be n “observations” and we can add them up by:
<span class="math display">\[
  \sum_{i=1}^n x_i = x_1 + x_2 + \cdots + x_n
\]</span>
<strong>Properties:</strong></p>
<ol style="list-style-type: decimal">
<li><p>For any constant <span class="math inline">\(c\)</span>,</p>
<p><span class="math display">\[
 \sum_{i=1}^n c = n \cdot c
 \]</span></p>
<p>[This is just the definition of multiplication]</p></li>
<li><p>For any constant c,</p>
<p><span class="math display">\[
   \sum_{i=1}^n c x_i = c \sum_{i=1}^n x_i
 \]</span></p>
<p>In words: constants can be moved out of the summation.</p>
<p>We will use the property often throughout the semester.</p>
<p>As an example,</p>
<p><span class="math display">\[
   \begin{aligned}
   \sum_{i=1}^3 7 x_i &amp;= 7x_1 + 7x_2 + 7x_3 \\
   &amp;= 7(x_1 + x_2 + x_3) \\
   &amp;= 7 \sum_{i=1}^3 x_i
   \end{aligned}
 \]</span></p>
<p>where the first line is just the definition of the summation, the second equality factors out the 7, and the last equality writes the part about adding up the <span class="math inline">\(x\)</span>’s using summation notation.</p></li>
</ol>
</div>
<div id="properties-of-pmfs-and-cdfs" class="section level3" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Properties of pmfs and cdfs</h3>
<p>Let’s define the <strong>support</strong> of a random variable <span class="math inline">\(X\)</span> — this is the set of all possible values that <span class="math inline">\(X\)</span> can possibly take. We’ll use the notation <span class="math inline">\(\mathcal{X}\)</span> to denote the support of <span class="math inline">\(X\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 3.5  </strong></span>Suppose <span class="math inline">\(X\)</span> is the outcome from a roll of a die. Then, the support of <span class="math inline">\(X\)</span> is given by <span class="math inline">\(\mathcal{X} = \{1,2,3,4,5,6\}\)</span>. In other words, the only possible values for <span class="math inline">\(X\)</span> are from <span class="math inline">\(1,\ldots,6\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 3.6  </strong></span>Suppose <span class="math inline">\(X\)</span> is the number of years of education that a person has. The support of <span class="math inline">\(X\)</span> is given by <span class="math inline">\(\mathcal{X} = \{0, 1, 2, \ldots, 20\}\)</span>. Perhaps I should have chosen a larger number than 20 to be the maximum possible value that <span class="math inline">\(X\)</span> could take, but you will get the idea — a person’s years of education can be 0 or 1 or 2 or up to some maximum value.</p>
</div>
<p><strong>Properties of pmfs</strong></p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(x\)</span>, <span class="math inline">\(0 \leq f_X(x) \leq 1\)</span></p>
<p>In words: the probability of <span class="math inline">\(X\)</span> taking some particular value can’t be less than 0 or greater than 1 (neither of those would make any sense)</p></li>
<li><p><span class="math inline">\(\sum_{x \in \mathcal{X}} f_X(x) = 1\)</span></p>
<p>In words: if you add up <span class="math inline">\(\mathrm{P}(X=x)\)</span> across all possible values that <span class="math inline">\(X\)</span> could take, they sum to 1.</p></li>
</ol>
<p><strong>Properties of cdfs for discrete random variables</strong></p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(x\)</span>, <span class="math inline">\(0 \leq F_X(x) \leq 1\)</span></p>
<p>In words: the probability that <span class="math inline">\(X\)</span> is less than or equal to some particular value <span class="math inline">\(x\)</span> has to be between 0 and 1.</p></li>
<li><p>If <span class="math inline">\(x_1 &lt; x_2\)</span>, then <span class="math inline">\(F_X(x_1) \leq F_X(x_2)\)</span></p>
<p>In words: the cdf is increasing in <span class="math inline">\(x\)</span> (e.g., it will always be the case that <span class="math inline">\(\mathrm{P}(X \leq 3) \leq \mathrm{P}(X \leq 4)\)</span>).</p></li>
<li><p><span class="math inline">\(F_X(-\infty)=0\)</span> and <span class="math inline">\(F_X(\infty)=1\)</span></p>
<p>In words: if you choose small enough values of <span class="math inline">\(x\)</span>, the probability that <span class="math inline">\(X\)</span> will be less than that is 0; similar (but opposite) logic applies for big values of <span class="math inline">\(x\)</span>.</p></li>
</ol>
<p><strong>Connection between pmfs and cdfs</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(F_X(x) = \displaystyle \sum_{z \in \mathcal{X} \\ z \leq x} f_X(z)\)</span></p>
<p>In words: you can “recover” the cdf from the pmf by adding up the pmf across all possible values that the random variable could take that are less than or equal to <span class="math inline">\(x\)</span>. This will be clearer with an example:</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 3.7  </strong></span>Suppose that <span class="math inline">\(X\)</span> is the outcome of a roll of a die. Earlier we showed that <span class="math inline">\(F_X(3) = 1/2\)</span>. We can calculate this by</p>
<p><span class="math display">\[
  \begin{aligned}
  F_X(3) &amp;= \sum_{z \in \mathcal{X} \\ z \leq 3} f_X(z) \\
  &amp;= \sum_{z=1}^3 f_X(z) \\
  &amp;= f_X(1) + f_X(2) + f_X(3) \\
  &amp;= \frac{1}{6} + \frac{1}{6} + \frac{1}{6} \\
  &amp;= \frac{1}{2}
  \end{aligned}
\]</span></p>
</div>
</div>
<div id="continuous-random-variables" class="section level3" number="3.1.6">
<h3><span class="header-section-number">3.1.6</span> Continuous Random Variables</h3>
<p>SW 2.1</p>
<p>For continuous random variables, you can define the cdf in exactly the same way as we did for discrete random variables. That is, if <span class="math inline">\(X\)</span> is a continuous random variable,</p>
<p><span class="math display">\[
  F_X(x) = \mathrm{P}(X \leq x)
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 3.8  </strong></span>Suppose <span class="math inline">\(X\)</span> denotes an individual’s yearly wage income. The cdf of <span class="math inline">\(X\)</span> looks like</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-81"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-81-1.png" alt="cdf of U.S. wage income" width="672" />
<p class="caption">
Figure 3.3: cdf of U.S. wage income
</p>
</div>
<p>From the figure, we can see that about 24% of working individuals in the U.S. each $20,000 or less per year, 61% of working individuals earn $50,000 or less, and 88% earn $100,000 or less.</p>
</div>
<p>It’s trickier to define an analogue to the pmf for a continuous random variable (in fact, this is the main reason for our separate treatment of discrete and continuous random variables). For example, suppose <span class="math inline">\(X\)</span> denotes the length of a phone conversation. As long as we can measure time finely enough, the probability that a phone conversation lasts exactly 1189.23975381 seconds (this is about 20 minutes) is 0. Instead, for a continuous random variable, we’ll define its probability density function (pdf) as the derivative of its cdf, that is,</p>
<p><span class="math display">\[
  f_X(x) := \frac{d \, F_X(x)}{d \, x}
\]</span>
Recall that the slope of the cdf will be larger in places where <span class="math inline">\(F_X(x)\)</span> is “steeper.”</p>
<p>Regions where the pdf is larger correspond to more likely values of <span class="math inline">\(X\)</span> — in this sense the pdf is very similar to the pmf.</p>
<p>We can also write the cdf as an integral over the pdf. That is,</p>
<p><span class="math display">\[
  F_X(x) = \int_{-\infty}^x f_X(z) \, dz
\]</span>
Integration is roughly the continuous version of a summation — thus, this expression is very similar to the expression above for the cdf in terms of the pmf when <span class="math inline">\(X\)</span> is discrete.</p>
<p><strong>More properties of cdfs</strong></p>
<ol start="4" style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathrm{P}(X &gt; x) = 1 - \mathrm{P}(X \leq x) = 1-F_X(x)\)</span></p>
<p>In words, if you want to calculate the probability that <span class="math inline">\(X\)</span> <em>is greater than</em> some particular value <span class="math inline">\(x\)</span>, you can do that by calculating <span class="math inline">\(1-F_X(x)\)</span>.</p></li>
<li><p><span class="math inline">\(\mathrm{P}(a \leq X \leq b) = F_X(b) - F_X(a)\)</span></p>
<p>In words: you can also calculate the probability that <span class="math inline">\(X\)</span> falls in some range using the cdf.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 3.9  </strong></span>Suppose <span class="math inline">\(X\)</span> denotes an individual’s yearly wage income. The pdf of <span class="math inline">\(X\)</span> looks like</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-82"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-82-1.png" alt="pdf of U.S. wage income" width="672" />
<p class="caption">
Figure 3.4: pdf of U.S. wage income
</p>
</div>
<p>From the figure, we can see that the most common values of yearly income are around $25-30,000 per year. Notice that this corresponds to the steepest part of the cdf from the previous figure. The right tail of the distribution is also long. This means that, while incomes of $150,000+ are not common, there are some individuals who have incomes that high.</p>
<p>Moreover, we can use the properties of pdfs/cdfs above to calculate some specific probabilities. In particular, we can calculating probabilities by calculating integrals (i.e., regions under the curve) / relating the pdf to the cdf. First, the red region above corresponds to the probability of a person’s income being between $50,000 and $100,000. This is given by <span class="math inline">\(F(100,000) - F(50000)\)</span>. We can compute this in <code>R</code> using the <code>ecdf</code> function. In particular,</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="probability-and-statistics.html#cb76-1" aria-hidden="true" tabindex="-1"></a>incwage_cdf <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(us_data<span class="sc">$</span>incwage)</span>
<span id="cb76-2"><a href="probability-and-statistics.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">incwage_cdf</span>(<span class="dv">100000</span>) <span class="sc">-</span> <span class="fu">incwage_cdf</span>(<span class="dv">50000</span>),<span class="dv">3</span>)</span>
<span id="cb76-3"><a href="probability-and-statistics.html#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.27</span></span></code></pre></div>
<p>The green region in the figure is the probability of a person’s income being above $150,000. Using the above properties of cdfs, we can calculate it as <span class="math inline">\(1-F(150000)\)</span> which is</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="probability-and-statistics.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">incwage_cdf</span>(<span class="dv">150000</span>), <span class="dv">3</span>)</span>
<span id="cb77-2"><a href="probability-and-statistics.html#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.052</span></span></code></pre></div>
</div>
</div>
<div id="expected-values" class="section level3" number="3.1.7">
<h3><span class="header-section-number">3.1.7</span> Expected Values</h3>
<p>SW 2.2</p>
<p>The <strong>expected value</strong> of some random variable <span class="math inline">\(X\)</span> is its (population) mean and is written as <span class="math inline">\(\mathbb{E}[X]\)</span>. [I tend to write <span class="math inline">\(\mathbb{E}[X]\)</span> for the expected value, but you might also see notation like <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\mu_X\)</span> for the expected value.]</p>
<p>The expected value of a random variable is a <em>feature</em> of its distribution. In other words, if you know the distribution of a random variable, then you also know its mean.</p>
<p>The expected value is a measure of <strong>central tendency</strong> (alternative measures of central tendency are the <strong>median</strong> and <strong>mode</strong>).</p>
<p>Expected values are a main concept in the course (and in statistics/econometrics more generally). I think there are two main reasons for this:</p>
<ul>
<li><p>Unlike a cdf, pdf, or pmf, the expected value is a single number. This means that it is easy to report. And, if you only knew one feature (at least a feature that that only involves a single number) of the distribution of some random variable, probably the feature that would be most useful to know would be the mean of the random variable.</p></li>
<li><p>Besides that, there are some computational reasons (we will see these later) that the mean can be easier to estimate than, say, the median of a random variable</p></li>
</ul>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable, then the expected value is defined as</p>
<p><span class="math display">\[
  \mathbb{E}[X] = \sum_{x \in \mathcal{X}} x f_X(x)
\]</span></p>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable, then the expected value is defined as</p>
<p><span class="math display">\[
  \mathbb{E}[X] = \int_{\mathcal{X}} x f_X(x) \, dx
\]</span>
Either way, you can think of these as a weighted average of all possible realizations of the random variable <span class="math inline">\(X\)</span> where the weights are given by the probability of <span class="math inline">\(X\)</span> taking that particular value. This may be more clear with an example…</p>
<div class="example">
<p><span id="exm:unlabeled-div-13" class="example"><strong>Example 3.10  </strong></span>Suppose that <span class="math inline">\(X\)</span> is the outcome from a roll of a die. Then, its expected value is given by</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathbb{E}[X] &amp;= \sum_{x=1}^6 x f_X(x) \\
  &amp;= 1\left(\frac{1}{6}\right) + 2\left(\frac{1}{6}\right) + \cdots + 6\left(\frac{1}{6}\right) \\
  &amp;= 3.5
  \end{aligned}
\]</span></p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> When we start to consider more realistic/interesting applications, we typically won’t know (or be able to easily figure out) <span class="math inline">\(\mathbb{E}[X]\)</span>. Instead, we’ll try to estimate it using available data. We’ll carefully distinguish between <strong>population quantities</strong> like <span class="math inline">\(\mathbb{E}[X]\)</span> and <strong>sample quantities</strong> like an estimate of <span class="math inline">\(\mathbb{E}[X]\)</span> soon.</p>
</div>
</div>
<div id="variance" class="section level3" number="3.1.8">
<h3><span class="header-section-number">3.1.8</span> Variance</h3>
<p>SW 2.2</p>
<p>The next most important feature of the distribution of a random variable is its <strong>variance</strong>. The variance of a random variable <span class="math inline">\(X\)</span> is a measure of its “spread,” and we will denote it <span class="math inline">\(\mathrm{var}(X)\)</span> [You might also sometimes see the notation <span class="math inline">\(\sigma^2\)</span> or <span class="math inline">\(\sigma_X^2\)</span> for the variance.] The variance is defined as</p>
<p><span class="math display">\[
  \mathrm{var}(X) := \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
\]</span>
Before we move forward, let’s think about why this is a measure of the spread of a random variable.</p>
<ul>
<li><p><span class="math inline">\((X-\mathbb{E}[X])^2\)</span> is a common way to measure the “distance” between <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathbb{E}[X]\)</span>. It is always positive (whether <span class="math inline">\((X - \mathbb{E}[X])\)</span> is positive or negative) which is a good feature for a measure of distance to have. It is also increasing in <span class="math inline">\(|X-\mathbb{E}[X]|\)</span> which also seems a requirement for a reasonable measure of distance.</p></li>
<li><p>Then, the outer expectation averages the above distance across the distribution of <span class="math inline">\(X\)</span>.</p></li>
</ul>
<p>An alternative expression for <span class="math inline">\(\mathrm{var}(X)\)</span> that is often useful in calculations is</p>
<p><span class="math display">\[
  \mathrm{var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\]</span></p>
<p>Sometimes, we will also consider the <strong>standard deviation</strong> of a random variable. The standard deviation is defined as</p>
<p><span class="math display">\[
  \textrm{sd}(X) := \sqrt{\mathrm{var}(X)}
\]</span>
You might also see the notation <span class="math inline">\(\sigma\)</span> or <span class="math inline">\(\sigma_X\)</span> for the standard deviation.</p>
<p>The standard deviation is often easier to interpret than the variance because it has the same “units” as <span class="math inline">\(X\)</span>. Variance “units” are squared units of <span class="math inline">\(X\)</span>.</p>
<p>That said, variances more often show up in formulas/derivations this semester.</p>
</div>
<div id="mean-and-variance-of-linear-functions" class="section level3" number="3.1.9">
<h3><span class="header-section-number">3.1.9</span> Mean and Variance of Linear Functions</h3>
<p>SW 2.2</p>
<p>For this part, suppose that <span class="math inline">\(Y=a + bX\)</span> where <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are random variables while <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are fixed constants.</p>
<p><strong>Properties of Expectations</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}[a] = a\)</span> [In words: the expected value of a constant is just the constant. This holds because there is nothing random about <span class="math inline">\(a\)</span> — we just know what it is.]</p></li>
<li><p><span class="math inline">\(\mathbb{E}[bX] = b\mathbb{E}[X]\)</span> [In words: the expected value of a constant times a random variable is equal to the constant times the expected value of the random variable. We will use this property often this semester.]</p></li>
<li><p><span class="math inline">\(\mathbb{E}[a + bX] = a + b\mathbb{E}[X]\)</span> [In words: expected values “pass through” sums. We will use this property often this semester.]</p></li>
</ol>
<p>You’ll also notice the similarity between the properties of summations and expectations. This is not a coincidence — it holds because expectations are defined as summations (or very closely related, as integrals).</p>
<p><strong>Properties of Variance</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathrm{var}(a) = 0\)</span> [In words: the variance of a constant is equal to 0.]</p></li>
<li><p><span class="math inline">\(\mathrm{var}(bX) = b^2 \mathrm{var}(X)\)</span> [In words: A constant can come out of the variance, but it needs to be squared first.]</p></li>
<li><p><span class="math inline">\(\mathrm{var}(a + bX) = \mathrm{var}(bX) = b^2 \mathrm{var}(X)\)</span></p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 3.11  </strong></span>Later on in the semester, it will sometimes be convenient for us to “standardize” some random variables. We’ll talk more about the reason to do this later, but for now, I’ll just give the typical formula for standardizing a random variable and we’ll see if we can figure out what the mean and variance of the standardized random variable are.</p>
<p><span class="math display">\[ 
  Y = \frac{ X - \mathbb{E}[X]}{\sqrt{\mathrm{var}(X)}}
\]</span>
Just to be clear here, we are standardizing the random variable <span class="math inline">\(X\)</span> and calling its standardized version <span class="math inline">\(Y\)</span>. Let’s calculate its mean</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[Y] &amp;= \mathbb{E}\left[ \frac{X - \mathbb{E}[X]}{\sqrt{\mathrm{var}(X)}} \right] \\
    &amp;= \frac{1}{\sqrt{\mathrm{var}(X)}} \mathbb{E}\big[ X - \mathbb{E}[X] \big] \\
    &amp;= \frac{1}{\sqrt{\mathrm{var}(X)}} \left( \mathbb{E}[X] - \mathbb{E}\big[\mathbb{E}[X]\big] \right) \\
    &amp;= \frac{1}{\sqrt{\mathrm{var}(X)}} \left( \mathbb{E}[X] - \mathbb{E}[X] \right) \\
    &amp;= 0
  \end{aligned}
\]</span>
where the first equality just comes from the definition of <span class="math inline">\(Y\)</span>, the second equality holds because <span class="math inline">\(1/\sqrt{\mathrm{var}(X)}\)</span> is a constant and can therefore come out of the expectation, the third equality holds because the expectation can pass through the difference, the fourth equality holds because <span class="math inline">\(\mathbb{E}[X]\)</span> is a constant and therefore <span class="math inline">\(\mathbb{E}\big[\mathbb{E}[X]\big] = \mathbb{E}[X]\)</span>, and the last equality holds because the term in parentheses is equal to 0. Thus, the mean of <span class="math inline">\(Y\)</span> is equal to 0. Now let’s calculate the variance.</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{var}(Y) &amp;= \mathrm{var}\left( \frac{X}{\sqrt{\mathrm{var}(X)}} - \frac{\mathbb{E}[X]}{\sqrt{\mathrm{var}(X)}} \right) \\
  &amp;= \mathrm{var}\left( \frac{X}{\sqrt{\mathrm{var}(X)}}\right) \\
  &amp;= \left( \frac{1}{\sqrt{\mathrm{var}(X)}} \right)^2 \mathrm{var}(X) \\
  &amp;= \frac{\mathrm{var}(X)}{\mathrm{var}(X)} \\
  &amp;= 1
  \end{aligned}
\]</span>
where the first equality holds by the definition of <span class="math inline">\(Y\)</span>, the second equality holds because the second term is a constant and by Variance Property 3 above, the third equality holds because <span class="math inline">\((1/\sqrt{\mathrm{var}(X)})\)</span> is a constant and can come out of the variance but needs to be squared, the fourth equality holds by squaring the term on the left, and the last equality holds by cancelling the numerator and denominator.</p>
<p>Therefore, we have showed that the mean of the standardized random variable is 0 and its variance is 1. This is, in fact, the goal of standardizing a random variable — to transform it so that it has mean 0 and variance 1 and the particular transformation given in this example is one that delivers a new random variable with these properties.</p>
</div>
</div>
<div id="multiple-random-variables" class="section level3" number="3.1.10">
<h3><span class="header-section-number">3.1.10</span> Multiple Random Variables</h3>
<p>SW 2.3</p>
<p>Most often in economics, we want to consider two (or more) random variables jointly rather than just a single random variable. For example, mean income is interesting, but mean income as a function of education is more interesting.</p>
<p>When there is more than one random variable, you can define <strong>joint pmfs</strong>, <strong>joint pdfs</strong>, and <strong>joint cdfs</strong>.</p>
<p>Let’s quickly go over these for the case where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two discrete random variables.</p>
<p><strong>Joint pmf:</strong> <span class="math inline">\(f_{X,Y}(x,y) := \mathrm{P}(X=x, Y=y)\)</span></p>
<p><strong>Joint cdf:</strong> <span class="math inline">\(F_{X,Y}(x,y) := \mathrm{P}(X \leq x, Y \leq y)\)</span></p>
<p><strong>Conditional pmf:</strong> <span class="math inline">\(f_{Y|X}(y|x) := \mathrm{P}(Y=y | X=x)\)</span></p>
<p><strong>Properties</strong></p>
<p>We use the notation that <span class="math inline">\(\mathcal{X}\)</span> denotes the support of <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> denotes the support of <span class="math inline">\(Y\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} f_{X,Y}(x,y) = 1\)</span></p></li>
<li><p><span class="math inline">\(f_{X,Y}(x,y) \geq 0\)</span> for all <span class="math inline">\(x,y\)</span></p></li>
<li><p>If you know the joint pmf, then you can recover the “marginal” pmf, that is,</p>
<p><span class="math display">\[
 f_Y(y) = \sum_{x \in \mathcal{X}} f_{X,Y}(x,y)
 \]</span></p>
<p>This amounts to just adding up the joint pmf across all values of <span class="math inline">\(x\)</span> while holding <span class="math inline">\(y\)</span> fixed.</p></li>
</ol>
<p><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>independent</strong> if <span class="math inline">\(f_{Y|X}(y|x) = f_Y(y)\)</span>. In other words, if knowing the value of <span class="math inline">\(X\)</span> doesn’t provide any information about the distribution <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="conditional-expectations" class="section level3" number="3.1.11">
<h3><span class="header-section-number">3.1.11</span> Conditional Expectations</h3>
<p>SW 2.3</p>
<p>It is useful to know about joint pmfs/pdfs/cdfs, but they are often hard to work with in practice. For example, if you have two random variables, visualizing their joint distribution would involve interpreting a 3D plot which is often challenging in practice. If you had more than two random variables, then fully visualizing their joint distribution would not be possible. Therefore, we will typically look at summaries of the joint distribution. Probably the most useful one is the conditional expectation that we study in this section; in fact, we will spend much of the semester trying to estimate conditional expectations.</p>
<p>For two random variables, <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, the <strong>conditional expectation</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is the mean value of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span> taking the particular value <span class="math inline">\(x\)</span>. In math, this is written</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X=x]
\]</span></p>
<p>One useful way to think of a conditional expectation is as a <em>function</em> of <span class="math inline">\(x\)</span>. For example, suppose that <span class="math inline">\(Y\)</span> is a person’s yearly income and <span class="math inline">\(X\)</span> is a person’s years of education. Clearly, mean income can change for different values of education.</p>
<p>Conditional expectations will be a main focus of ours throughout the semester</p>
<p>An extremely useful property of conditional expectations is that they generalize from the case with two variables to the case with multiple variables. For example, suppose that we have four random variables <span class="math inline">\(Y\)</span>, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>.</p>
</div>
<div id="law-of-iterated-expectations" class="section level3" number="3.1.12">
<h3><span class="header-section-number">3.1.12</span> Law of Iterated Expectations</h3>
<p>SW 2.3</p>
<p>Another important property of conditional expectations is called the <strong>law of iterated expectations</strong>. It says that</p>
<p><span class="math display">\[
  \mathbb{E}[Y] = \mathbb{E}\big[ \mathbb{E}[Y|X] \big]
\]</span>
In words: The expected value of <span class="math inline">\(Y\)</span> is equal to the expected value (this expectation is with respect to <span class="math inline">\(X\)</span>) of the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>This may seem like a technical property, but I think the right way to think about the law of iterated expectations is that there is an inherent relationship between unconditional expectations and conditional expectations. In other words, although conditional expectations can vary arbitrarily for different values of <span class="math inline">\(X\)</span>, if you know what the conditional expectations are, the overall expected value of <span class="math inline">\(Y\)</span> is fully determined.</p>
<p>A simple example is one where <span class="math inline">\(X\)</span> takes only two values. Suppose we are interested in mean birthweight (<span class="math inline">\(Y\)</span>) for children of mother’s who either drank alcohol during their pregnancy (<span class="math inline">\(X=1\)</span>) or who didn’t drink alcohol during their pregnancy (<span class="math inline">\(X=0\)</span>). Suppose the following (just to be clear, these are completely made up numbers), <span class="math inline">\(\mathbb{E}[Y|X=1] = 7\)</span>, <span class="math inline">\(\mathbb{E}[Y|X=0]=8\)</span> <span class="math inline">\(\mathrm{P}(X=1) = 0.1\)</span> and <span class="math inline">\(\mathrm{P}(X=0)=0.9\)</span>. The law of iterated expectation says that
<span class="math display">\[
  \begin{aligned}
  \mathbb{E}[Y] &amp;= \mathbb{E}\big[ \mathbb{E}[Y|X] \big] \\
  &amp;= \sum_{x \in \mathcal{X}} \mathbb{E}[Y|X=x] \mathrm{P}(X=x) \\
  &amp;= \mathbb{E}[Y|X=0]\mathrm{P}(X=0) + \mathbb{E}[Y|X=1]\mathrm{P}(X=1) \\
  &amp;= (8)(0.9) + (7)(0.1) \\
  &amp;= 7.9
  \end{aligned}
\]</span></p>
<p>The law of iterated expectations still applies in more complicated cases (e.g., <span class="math inline">\(X\)</span> takes more than two values, <span class="math inline">\(X\)</span> is continuous, or <span class="math inline">\(X_1\)</span>,<span class="math inline">\(X_2\)</span>,<span class="math inline">\(X_3\)</span>) but the intuition is still the same.</p>
</div>
<div id="covariance" class="section level3" number="3.1.13">
<h3><span class="header-section-number">3.1.13</span> Covariance</h3>
<p>SW 2.3</p>
<p>The <strong>covariance</strong> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a masure of the extent to which they “move together.” It is defined as</p>
<p><span class="math display">\[
  \mathrm{cov}(X,Y) := \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]
\]</span>
A natural first question to ask is: why does this measure how <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> move together. Notice that covariance can be positive or negative. It will tend to be negative if big values of <span class="math inline">\(X\)</span> (so that <span class="math inline">\(X\)</span> is above its mean) tend to happen at the same time as big values of <span class="math inline">\(Y\)</span> (so that <span class="math inline">\(Y\)</span> is above its mean) while small values of <span class="math inline">\(X\)</span> (so that <span class="math inline">\(X\)</span> is below its mean) tend to happen at the same time as small values of <span class="math inline">\(Y\)</span> (so that <span class="math inline">\(Y\)</span> is below its mean).</p>
<p>An alternative and useful expression for covariance is
<span class="math display">\[
  \mathrm{cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\]</span>
Relative to the first expression, this one is probably less of a natural definition but often more useful in mathematical problems.</p>
<p>One more thing to notice, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\mathrm{cov}(X,Y) = 0\)</span>.</p>
</div>
<div id="correlation" class="section level3" number="3.1.14">
<h3><span class="header-section-number">3.1.14</span> Correlation</h3>
<p>SW 2.3</p>
<p>It’s often hard to interpret covariances directly (the “units” are whatever the units of <span class="math inline">\(X\)</span> are times the units of <span class="math inline">\(Y\)</span>), so it is common to scale the covariance to get the <strong>correlation</strong> between two random variables:</p>
<p><span class="math display">\[
  \mathrm{corr}(X,Y) := \frac{\mathrm{cov}(X,Y)}{\sqrt{\mathrm{var}(X)} \sqrt{\mathrm{var}(Y)}}
\]</span>
The correlation has the property that it is always between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>If <span class="math inline">\(\mathrm{corr}(X,Y) = 0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>uncorrelated</strong>.</p>
</div>
<div id="properties-of-expectationsvariances-of-sums-of-rvs" class="section level3" number="3.1.15">
<h3><span class="header-section-number">3.1.15</span> Properties of Expectations/Variances of Sums of RVs</h3>
<p>SW 2.3</p>
<p>Here are some more properties of expectations and variances when there are multiple random variables. For two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)</span></p></li>
<li><p><span class="math inline">\(\mathrm{var}(X+Y) = \mathrm{var}(X) + \mathrm{var}(Y) + 2\mathrm{cov}(X,Y)\)</span></p></li>
</ol>
<p>The first property is probably not surprising — expectations continue to pass through sums. The second property, particularly the covariance term, needs more explanation. To start with, you can just plug <span class="math inline">\(X+Y\)</span> into the definition of variance and (with a few lines of algebra) show that the second property is true. But, for the intuition, let me explain with an example. Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are rolls of two dice, but <em>somehow</em> these dice are positively correlated with each other — i.e., both rolls coming up with high numbers (and low numbers) are more likely than with regular dice. Now, think about what the sum of two dice rolls can be: the smallest possible sum is 2 and other values are possible up to 12. Moreover, the smallest and largest possible sum of the rolls (2 and 12), which are farthest away from the mean value of 7, are relatively uncommon. You have to roll either <span class="math inline">\((1,1)\)</span> or <span class="math inline">\((6,6)\)</span> to get either of these and the probability of each of those rolls is just <span class="math inline">\(1/36\)</span>. However, when the dice are positively correlated, the probability of both rolls being very high or very low becomes more likely — thus, since outcomes far away from the mean become more likely, the variance increases.</p>
<p>One last comment here is that, when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent (or even just uncorrelated), the formula for the variance does not involve the extra covariance term because it is equal to 0.</p>
<p>These properties for sums of random variables generalize to the case with more than two random variables. For example, suppose that <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are random variables, then</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}\left[ \displaystyle \sum_{i=1}^n Y_i \right] = \displaystyle \sum_{i=1}^n \mathbb{E}[Y_i]\)</span></p></li>
<li><p>If <span class="math inline">\(Y_i\)</span> are mutually independent, then <span class="math inline">\(\mathrm{var}\left( \displaystyle \sum_{i=1}^n Y_i \right) = \displaystyle \sum_{i=1}^n \mathrm{var}(Y_i)\)</span></p></li>
</ol>
<p>Notice that the last line does not involve any covariance terms, but this is only because of the caveat that the <span class="math inline">\(Y_i\)</span> are mutually independent. Otherwise, there would actually be <em>tons</em> of covariance terms that would need to be accounted for.</p>
</div>
<div id="normal-distribution" class="section level3" number="3.1.16">
<h3><span class="header-section-number">3.1.16</span> Normal Distribution</h3>
<p>SW 2.4</p>
<p>You probably learned about a lot of particular distributions of random variables in your Stats class. There are a number of important distributions:</p>
<ul>
<li><p>Normal</p></li>
<li><p>Binomial</p></li>
<li><p>t-distribution</p></li>
<li><p>F-distribution</p></li>
<li><p>Chi-squared distribution</p></li>
<li><p>others</p></li>
</ul>
<p>SW discusses a number of these distributions, and I recommend that you read/review those distributions. For us, the most important distribution is the Normal distribution [we’ll see why a few classes from now].</p>
<p>If a random variable <span class="math inline">\(X\)</span> follows a <strong>normal distribution</strong> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, we write</p>
<p><span class="math display">\[
  X \sim N(\mu, \sigma^2)
\]</span>
where <span class="math inline">\(\mu = \mathbb{E}[X]\)</span> and <span class="math inline">\(\sigma^2 = \mathrm{var}(X)\)</span>.</p>
<p>Importantly, if we know that <span class="math inline">\(X\)</span> follows a normal distribution, its entire distribution is fully characterized by its mean and variance. In other words, if <span class="math inline">\(X\)</span> is normally distributed, and we also know its mean and variance, then we know everything about its distribution. [Notice that this is not generally true — if we did not know the distribution of <span class="math inline">\(X\)</span> but knew its mean and variance, we would know two important features of the distribution of <span class="math inline">\(X\)</span>, but we would not know everything about its distribution.]</p>
<p>You are probably familiar with the pdf of a normal distribution — it is “bell-shaped.”</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>From the figure, you can see that a normal distribution is unimodal (there is just one “peak”) and symmetric (the pdf is the same if you move the same distance above <span class="math inline">\(\mu\)</span> as when you move the same distance below <span class="math inline">\(\mu\)</span>). This means that, for a random variable that follows a normal distribution, its median and mode are also equal to <span class="math inline">\(\mu\)</span>.</p>
<p>From the plot of the pdf, we can also tell that, if you make a draw from <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, the most likely values are near the mean. As you move further away from <span class="math inline">\(\mu\)</span>, it becomes less likely (though not impossible) for a draw of <span class="math inline">\(X\)</span> to take that value.</p>
<p>Recall that we can calculate the probability that <span class="math inline">\(X\)</span> takes on a value in a range by calculating the area under the curve of the pdf. For each shaded region in the figure, there is a 2.5% chance that <span class="math inline">\(X\)</span> falls into that region (so the probability of <span class="math inline">\(X\)</span> falling into either region is 5%). Another way to think about this is that there is a 95% probability that a draw of <span class="math inline">\(X\)</span> will be in the region <span class="math inline">\([\mu-1.96\sigma, \mu+1.96\sigma]\)</span>. Later, we we talk about hypothesis testing, this will be an important property.</p>
<p>Earlier, we talked about standardizing random variables. If you know that a random variable follows a normal distribution, it is very common to standardize it. In particular notice that, if you create the standardized random variable</p>
<p><span class="math display">\[
  Z := \frac{X - \mu}{\sigma} \quad \textrm{then} \quad Z \sim N(0,1)
\]</span>
If you think back to your statistics class, you may have done things like calculating a p-value by looking at a “Z-table” in the back of a textbook (I’m actually not sure if this is still commonly done because it is often easier to just do this on a computer, but, back in “my day” this was a very common exercise in statistics classes). Standardizing allows you to look at just one table for any normally distributed random variable that you could encounter rather than requiring you to have different Z table for each value of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="coding" class="section level3" number="3.1.17">
<h3><span class="header-section-number">3.1.17</span> Coding</h3>
<p>To conclude this section, we’ll use R to compute the features of the joint distribution of income and education that we have discussed above.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="probability-and-statistics.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create vectors of income and educ</span></span>
<span id="cb78-2"><a href="probability-and-statistics.html#cb78-2" aria-hidden="true" tabindex="-1"></a>income <span class="ot">&lt;-</span> us_data<span class="sc">$</span>incwage</span>
<span id="cb78-3"><a href="probability-and-statistics.html#cb78-3" aria-hidden="true" tabindex="-1"></a>educ <span class="ot">&lt;-</span> us_data<span class="sc">$</span>educ</span>
<span id="cb78-4"><a href="probability-and-statistics.html#cb78-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-5"><a href="probability-and-statistics.html#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="co"># mean of income</span></span>
<span id="cb78-6"><a href="probability-and-statistics.html#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(income)</span>
<span id="cb78-7"><a href="probability-and-statistics.html#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 58605.75</span></span>
<span id="cb78-8"><a href="probability-and-statistics.html#cb78-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-9"><a href="probability-and-statistics.html#cb78-9" aria-hidden="true" tabindex="-1"></a><span class="co"># mean of education</span></span>
<span id="cb78-10"><a href="probability-and-statistics.html#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(educ)</span>
<span id="cb78-11"><a href="probability-and-statistics.html#cb78-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 13.96299</span></span>
<span id="cb78-12"><a href="probability-and-statistics.html#cb78-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-13"><a href="probability-and-statistics.html#cb78-13" aria-hidden="true" tabindex="-1"></a><span class="co"># variance</span></span>
<span id="cb78-14"><a href="probability-and-statistics.html#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(income)</span>
<span id="cb78-15"><a href="probability-and-statistics.html#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 4776264026</span></span>
<span id="cb78-16"><a href="probability-and-statistics.html#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(educ)</span>
<span id="cb78-17"><a href="probability-and-statistics.html#cb78-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 8.345015</span></span>
<span id="cb78-18"><a href="probability-and-statistics.html#cb78-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-19"><a href="probability-and-statistics.html#cb78-19" aria-hidden="true" tabindex="-1"></a><span class="co"># standard deviation</span></span>
<span id="cb78-20"><a href="probability-and-statistics.html#cb78-20" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(income)</span>
<span id="cb78-21"><a href="probability-and-statistics.html#cb78-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 69110.52</span></span>
<span id="cb78-22"><a href="probability-and-statistics.html#cb78-22" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(educ)</span>
<span id="cb78-23"><a href="probability-and-statistics.html#cb78-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 2.888774</span></span>
<span id="cb78-24"><a href="probability-and-statistics.html#cb78-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-25"><a href="probability-and-statistics.html#cb78-25" aria-hidden="true" tabindex="-1"></a><span class="co"># covariance</span></span>
<span id="cb78-26"><a href="probability-and-statistics.html#cb78-26" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(income,educ)</span>
<span id="cb78-27"><a href="probability-and-statistics.html#cb78-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 63766.72</span></span>
<span id="cb78-28"><a href="probability-and-statistics.html#cb78-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-29"><a href="probability-and-statistics.html#cb78-29" aria-hidden="true" tabindex="-1"></a><span class="co"># correlation</span></span>
<span id="cb78-30"><a href="probability-and-statistics.html#cb78-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(income, educ)</span>
<span id="cb78-31"><a href="probability-and-statistics.html#cb78-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.3194011</span></span></code></pre></div>
<div id="basic-plots" class="section level4" number="3.1.17.1">
<h4><span class="header-section-number">3.1.17.1</span> Basic Plots</h4>
<p>Related Reading: IDS 9.4 (if you are interested, you can read IDS Chapters 6-10 for much more information about plotting in <code>R</code>)</p>
<p>Finally, in this section, I’ll introduce you to some basic plotting. Probably the most common type of plot that I use is a line plot. We’ll go for trying to make a line plot of average income as a function of education.</p>
<p>In this section, I’ll introduce you to R’s <code>ggplot2</code> package. This is one of the most famous plot-producing packages (not just in R, but for any programming language). The syntax may be somewhat challenging to learn, but I think it is worth it to exert some effort here.</p>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> Base <code>R</code> has several plotting functions (e.g., <code>plot</code>). Check IDS 2.15 for an introduction to these functions. These are generally easier to learn but less beautiful than plots coming from <code>ggplot2</code>.</p>
</div>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="probability-and-statistics.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load ggplot2 package </span></span>
<span id="cb79-2"><a href="probability-and-statistics.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (if you haven&#39;t installed it, you would need to do that first)</span></span>
<span id="cb79-3"><a href="probability-and-statistics.html#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb79-4"><a href="probability-and-statistics.html#cb79-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-5"><a href="probability-and-statistics.html#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="co"># load dplyr package for &quot;wrangling&quot; data</span></span>
<span id="cb79-6"><a href="probability-and-statistics.html#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="probability-and-statistics.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># arrange data</span></span>
<span id="cb80-2"><a href="probability-and-statistics.html#cb80-2" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> us_data <span class="sc">%&gt;%</span></span>
<span id="cb80-3"><a href="probability-and-statistics.html#cb80-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(educ) <span class="sc">%&gt;%</span></span>
<span id="cb80-4"><a href="probability-and-statistics.html#cb80-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">income=</span><span class="fu">mean</span>(incwage))</span>
<span id="cb80-5"><a href="probability-and-statistics.html#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="probability-and-statistics.html#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb80-7"><a href="probability-and-statistics.html#cb80-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>plot_data,</span>
<span id="cb80-8"><a href="probability-and-statistics.html#cb80-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>educ,<span class="at">y=</span>income)) <span class="sc">+</span></span>
<span id="cb80-9"><a href="probability-and-statistics.html#cb80-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb80-10"><a href="probability-and-statistics.html#cb80-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb80-11"><a href="probability-and-statistics.html#cb80-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>Let me explain what’s going on here piece-by-piece. Let’s start with this code</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="probability-and-statistics.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># arrange data</span></span>
<span id="cb81-2"><a href="probability-and-statistics.html#cb81-2" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> us_data <span class="sc">%&gt;%</span></span>
<span id="cb81-3"><a href="probability-and-statistics.html#cb81-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(educ) <span class="sc">%&gt;%</span></span>
<span id="cb81-4"><a href="probability-and-statistics.html#cb81-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">income=</span><span class="fu">mean</span>(incwage))</span></code></pre></div>
<p>At a high-level, making plots often involves two steps — first arranging the data in the “appropriate” way (that’s this step) and then actually making the plot.</p>
<p>This is “tidyverse-style” code too — in my view, it is a little awkward, but it is also common so I think it is worth explaining here a bit.</p>
<p>First, the <strong>pipe operator</strong>, <code>%&gt;%</code> takes the thing on the left of it and applies the function to the right of it. So, the line <code>us_data %&gt;% group_by(educ)</code> takes <code>us_data</code> and applies the function <code>group_by</code> to it, and what we group by is <code>educ</code>. That creates a new data frame (you could just run that code and see what you get). The next line takes that new data frame and applies the function <code>summarize</code> to it. In this case, <code>summarize</code> creates a new variable called <code>income</code> that is the mean of the column <code>incwage</code> and it is the mean by <code>educ</code> (since we grouped by education in the previous step).</p>
<p>Take a second and look through what has actually been created here. <code>plot_data</code> is a new data frame, but it only has 18 observations — corresponding to each distinct value of education in the data. It also has two columns, the first one is <code>educ</code> which is the years of education, and the second one is <code>income</code> which is the average income among individuals that have that amount of education.</p>
<p>An alternative way of writing the exact same code (that seems more natural to me) is</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="probability-and-statistics.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># arrange data</span></span>
<span id="cb82-2"><a href="probability-and-statistics.html#cb82-2" aria-hidden="true" tabindex="-1"></a>grouped_data <span class="ot">&lt;-</span> <span class="fu">group_by</span>(us_data, educ)</span>
<span id="cb82-3"><a href="probability-and-statistics.html#cb82-3" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">summarize</span>(grouped_data, <span class="at">income=</span><span class="fu">mean</span>(incwage))</span></code></pre></div>
<p>If you’re familiar with other programming languages, the second version of the code probably seems more familiar. Either way is fine with me — tidyverse-style seems to be trendy in R programming these days, but (for me) I think the second version is a little easier to understand. You can find long “debates” about these two styles of writing code if you happen to be interested…</p>
<p>Before moving on, let me mention a few other <code>dplyr</code> functions that you might find useful</p>
<ul>
<li><p><code>filter</code> — this is tidy version of <code>subset</code></p></li>
<li><p><code>select</code> — selects particular columns of interest from your data</p></li>
<li><p><code>mutate</code> — creates a new variable from existing columns in your data</p></li>
<li><p><code>arrange</code> — useful for sorting your data</p></li>
</ul>
<p>Next, let’s consider the second part of the code.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="probability-and-statistics.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb83-2"><a href="probability-and-statistics.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>plot_data,</span>
<span id="cb83-3"><a href="probability-and-statistics.html#cb83-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>educ,<span class="at">y=</span>income)) <span class="sc">+</span></span>
<span id="cb83-4"><a href="probability-and-statistics.html#cb83-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb83-5"><a href="probability-and-statistics.html#cb83-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb83-6"><a href="probability-and-statistics.html#cb83-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<p>The main function here is <code>ggplot</code>. It takes in two main arguments: <code>data</code> and <code>mapping</code>. Notice that we set <code>data</code> to be equal to <code>plot_data</code> which is the data frame that we just created. The <code>mapping</code> is set equal to <code>aes(x=educ,y=income)</code>. <code>aes</code> stands for “aesthetic,” and here you are just telling <code>ggplot</code> the names of the columns in the data frame that should be on the x-axis (here: <code>educ</code>) and on the y-axis (here: <code>income</code>) in the plot. Also, notice the <code>+</code> at the end of the line; you can interpret this as saying “keep going” to the next line before executing.</p>
<p>If we just stopped there, we actually wouldn’t plot anything. We still need to tell <code>ggplot</code> what kind of plot we want to make. That’s where the line <code>geom_line</code> comes in. It tells <code>ggplot</code> that we want to plot a line. Try running the code with just those two lines — you will see that you will get a similar (but not exactly the same) plot.</p>
<p><code>geom_point</code> adds the dots in the figure. <code>size=3</code> controls the size of the points. I didn’t add this argument originally, but the dots were hard to see so I made them bigger.</p>
<p><code>theme_bw</code> changes the color scheme of the plot. It stands for “theme black white.”</p>
<p>There is a ton of flexibility with <code>ggplot</code> — way more than I could list here. But let me give you some extras that I tend to use quite frequently.</p>
<ul>
<li><p>In <code>geom_line</code> and <code>geom_point</code>, you can add the extra argument <code>color</code>; for example, you could try <code>geom_line(color="blue")</code> and it would change the color of the line to blue.</p></li>
<li><p>In <code>geom_line</code>, you can change the “type” of the line by using the argument <code>linetype</code>; for example, <code>geom_line(linetype="dashed")</code> would change the line from being solid to being dashed.</p></li>
<li><p>In <code>geom_line</code>, the argument <code>size</code> controls the thickness of the line.</p></li>
<li><p>The functions <code>ylab</code> and <code>xlab</code> control the labels on the y-axis and x-axis</p></li>
<li><p>The functions <code>ylim</code> and <code>xlim</code> control the “limits” of the y-axis and x-axis. Here’s how you can use these:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="probability-and-statistics.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb84-2"><a href="probability-and-statistics.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>plot_data,</span>
<span id="cb84-3"><a href="probability-and-statistics.html#cb84-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>educ,<span class="at">y=</span>income)) <span class="sc">+</span></span>
<span id="cb84-4"><a href="probability-and-statistics.html#cb84-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb84-5"><a href="probability-and-statistics.html#cb84-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb84-6"><a href="probability-and-statistics.html#cb84-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb84-7"><a href="probability-and-statistics.html#cb84-7" aria-hidden="true" tabindex="-1"></a>  ylim<span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">150000</span>) <span class="sc">+</span> </span>
<span id="cb84-8"><a href="probability-and-statistics.html#cb84-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Income&quot;</span>) <span class="sc">+</span> </span>
<span id="cb84-9"><a href="probability-and-statistics.html#cb84-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Education&quot;</span>)</span></code></pre></div>
<p>which will adjust the y-axis and change the labels on each axis.</p></li>
</ul>
<p>Besides the line plot (using <code>geom_line</code>) and the scatter plot (using <code>geom_point</code>), probably two other types of plots that I make the most are</p>
<ul>
<li><p>Histogram (using <code>geom_histogram</code>) — this is how I made the plot of the pmf of education earlier in this chapter</p></li>
<li><p>Adding a straight line to a plot (using <code>geom_abline</code> which takes in <code>slope</code> and <code>intercept</code> arguments) — we haven’t used this yet, but we will win once we start talking about regressions</p></li>
<li><p>If you’re interested, here is a to a large number of different types of plots that are available using <code>ggplot</code>: <a href="http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html">http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html</a></p></li>
</ul>
</div>
</div>
</div>
<div id="topics-in-statistics" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Topics in Statistics</h2>
<p>So far, we have been talking about <strong>population quantities</strong> such as <span class="math inline">\(f_{Y|X}\)</span> (conditional pdf/pmf), <span class="math inline">\(\mathbb{E}[Y]\)</span> (expected value of <span class="math inline">\(Y\)</span>), or <span class="math inline">\(\mathbb{E}[Y|X]\)</span> (expected value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>).</p>
<p>In practice, most often we do not know what these population quantities are equal to (with the exception of some trivial cases like flipping a coin or rolling a die).</p>
<p>A fundamental challenge is that it is uncommon that we observe the entire population.</p>
<p>Instead, we will take the approach that we have access to a <strong>sample</strong> of data from the original population. We’ll use the sample to try to <strong>estimate</strong> whatever population quantities we are interested in as well as develop the tools to <strong>conduct inference</strong>, paying particular interest to questions like: how precisely can we estimate particular population quantities of interest.</p>
<p>More broadly, <strong>statistics</strong> is the set of tools to learn about population quantities using data.</p>
<div id="simple-random-sample" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Simple Random Sample</h3>
<p>SW 2.5</p>
<p>Let’s start by talking about how the data that we have access to is collected. There are several possibilities here, but let us start with the most straightforward case (which is also a very common case) called a <strong>simple random sample</strong>.</p>
<p>In math: <span class="math inline">\(\{Y_i\}_{i=1}^n\)</span> is called a simple random sample if <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> are independent random variables with a common probability distribution <span class="math inline">\(f_Y\)</span>. The two key conditions here are (i) independence and (ii) from a common distribution. For this reason, you may sometimes see a random sample called an iid sample which stands for independent and identically distributed.</p>
<p>In words: We have access to <span class="math inline">\(n\)</span> observations that are drawn at random from some underlying population and each observation is equally likely to be drawn.</p>
</div>
<div id="estimating-mathbbey" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span></h3>
<p>SW 2.5, 3.1</p>
<p>Let’s start with trying to estimate <span class="math inline">\(\mathbb{E}[Y]\)</span> as this is probably the simplest, non-trivial thing that we can estimate.</p>
<p>A natural way to estimate population quantities is with their sample analogue. This is called the <strong>analogy principle</strong>. This is perhaps technical jargon, but it is the way you would immediately think to estimate <span class="math inline">\(\mathbb{E}[Y]\)</span>:</p>
<p><span class="math display">\[
  \hat{\mathbb{E}}[Y] = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
\]</span>
In this course, we will typically put a “hat” on estimated quantities. The expression <span class="math inline">\(\displaystyle \frac{1}{n}\sum_{i=1}^n Y_i\)</span> is just the average value of <span class="math inline">\(Y\)</span> in our sample. Since we will calculate a ton of averages like this one over the course of the rest of the semester, it’s also convenient to give it a shorthand notation, which is what <span class="math inline">\(\bar{Y}\)</span> means — it is just the sample average of <span class="math inline">\(Y\)</span>.</p>
<p>One thing that is important to be clear about at this point is that, in general, <span class="math inline">\(\mathbb{E}[Y] \neq \bar{Y}\)</span>. <span class="math inline">\(\mathbb{E}[Y]\)</span> is a population quantity while <span class="math inline">\(\bar{Y}\)</span> is a sample quantity. We will hope (and provide some related conditions/discussions below) that <span class="math inline">\(\bar{Y}\)</span> would be close to <span class="math inline">\(\mathbb{E}[Y]\)</span>, but, in general, they will not be exactly the same.</p>
</div>
<div id="mean-of-bary" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Mean of <span class="math inline">\(\bar{Y}\)</span></h3>
<p>SW 2.5, 3.1</p>
<p>Another important thing to notice about <span class="math inline">\(\bar{Y}\)</span> is that it is a random variable (as it is the average of random variables). This is in sharp contrast to <span class="math inline">\(\mathbb{E}[Y]\)</span> which is non-random.</p>
<p>One related thought experiment is the following: if we could repeatedly collect new samples of size <span class="math inline">\(n\)</span> from the same population and each time were able to estimate <span class="math inline">\(\bar{Y}\)</span>, these estimates would be different from each other.</p>
<p>In fact, this means that <span class="math inline">\(\bar{Y}\)</span> has a distribution. The distribution of a statistic, like <span class="math inline">\(\bar{Y}\)</span>, is called its <strong>sampling distribution</strong>. We’d like to know about the features of the sampling distribution. Let’s start with its mean. That is, let’s calculate</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[\bar{Y}] &amp;= \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n Y_i \right] \\
    &amp;= \frac{1}{n} \mathbb{E}\left[ \sum_{i=1}^n Y_i \right] \\
    &amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[Y_i] \\
    &amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[Y] \\
    &amp;= \frac{1}{n} n \mathbb{E}[Y] \\
    &amp;= \mathbb{E}[Y]
  \end{aligned}
\]</span>
Let’s think carefully about each step here — the arguments rely heavily on the properties of expectations and summations that we have learned earlier. The first equality holds from the definition of <span class="math inline">\(\bar{Y}\)</span>. The second equality holds because <span class="math inline">\(1/n\)</span> is a constant and can therefore come out of the expectation. The third equality holds because the expectation can pass through the sum. The fourth equality holds because <span class="math inline">\(Y_i\)</span> are all from the same distribution which implies that they all of the same mean and that it is equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>. The fifth equality holds because <span class="math inline">\(\mathbb{E}[Y]\)</span> is a constant and we add it up <span class="math inline">\(n\)</span> times. And the last equality just cancels the <span class="math inline">\(n\)</span> in the numerator with the <span class="math inline">\(n\)</span> in the denominator.</p>
<p>Before moving on, let me make an additional comment:</p>
<ul>
<li>The fourth equality might be a little confusing. Certainly it is not saying that all the <span class="math inline">\(Y_i\)</span>’s are equal to each other. Rather, they come from the same distribution. For example, if you roll a die <span class="math inline">\(n\)</span> times, you get different outcomes on different rolls, but they are all from the same distribution so that the population expectation of each roll is always 3.5, but you get different realizations on different particular rolls. Another example is if <span class="math inline">\(Y\)</span> is a person’s income. Again, we are not saying that everyone has the same income, but just that we are thinking of income as being a draw from some distribution — sometimes you get a draw of a person with a very high income; other times you get a draw of a person with a low income, but <span class="math inline">\(\mathbb{E}[Y]\)</span> is a feature of the underlying distribution itself where these draws come from.</li>
</ul>
<p>How should interpret the above result? It says that, <span class="math inline">\(\mathbb{E}[\bar{Y}] = \mathbb{E}[Y]\)</span>. This doesn’t mean that <span class="math inline">\(\bar{Y}\)</span> itself is equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>. Rather, it means that, if we could repeatedly obtain (a huge number of times) new samples of size <span class="math inline">\(n\)</span> and compute <span class="math inline">\(\bar{Y}\)</span> each time, the average of <span class="math inline">\(\bar{Y}\)</span> across repeated samples would be equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>.</p>
</div>
<div id="variance-of-bary" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Variance of <span class="math inline">\(\bar{Y}\)</span></h3>
<p>SW 2.5, 3.1</p>
<p>Next, let’s calculate the variance of <span class="math inline">\(\bar{Y}\)</span>. As before, we are continuing with the thought experiment of being able to repeatedly draw new samples of size <span class="math inline">\(n\)</span>, and, therefore, we call this variance the <strong>sampling variance</strong>.</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathrm{var}(\bar{Y}) &amp;= \mathrm{var}\left(\frac{1}{n} \sum_{i=1}^n Y_i\right) \\
    &amp;= \frac{1}{n^2} \mathrm{var}\left(\sum_{i=1}^n Y_i\right) \\
    &amp;= \frac{1}{n^2} \left( \sum_{i=1}^n \mathrm{var}(Y_i) + \textrm{lots of covariance terms} \right) \\
    &amp;= \frac{1}{n^2} \left( \sum_{i=1}^n \mathrm{var}(Y_i) \right) \\
    &amp;= \frac{1}{n^2} \sum_{i=1}^n \mathrm{var}(Y) \\
    &amp;= \frac{1}{n^2} n \mathrm{var}(Y) \\
    &amp;= \frac{\mathrm{var}(Y)}{n}
  \end{aligned}
\]</span>
Let’s go carefully through each step — these arguments rely heavily on the properties of variance that we talked about earlier. The first equality holds by the definition of <span class="math inline">\(\bar{Y}\)</span>. The second equality holds because <span class="math inline">\(1/n\)</span> is a constant and can come out of the variance after squaring it. The third equality holds because the variance of the sum of random variables is equal to the sum of the variances plus all the covariances between the random variables. In the fourth equality, all of the covariance terms go away — this holds because of random sampling which implies that the <span class="math inline">\(Y_i\)</span> are all independent which implies that their covariances are equal to 0. The fifth equality holds because all <span class="math inline">\(Y_i\)</span> are identically distributed so their variances are all the same and equal to <span class="math inline">\(\mathrm{var}(Y)\)</span>. The sixth equality holds by adding up <span class="math inline">\(\mathrm{var}(Y)\)</span> <span class="math inline">\(n\)</span> times. The last equality holds by canceling the <span class="math inline">\(n\)</span> in the numerator with one of the <span class="math inline">\(n\)</span>’s in the denominator.</p>
<p>Interestingly, the variance of <span class="math inline">\(\bar{Y}\)</span> depends not just on <span class="math inline">\(\mathrm{var}(Y)\)</span> but also on <span class="math inline">\(n\)</span> — the number of observations in the sample. Notice that <span class="math inline">\(n\)</span> is in the denominator, so the variance of <span class="math inline">\(\bar{Y}\)</span> will be lower for large values of <span class="math inline">\(n\)</span>. Here is an example that may be helpful for understanding this. Suppose that you are rolling a die. If <span class="math inline">\(n=1\)</span>, then clearly, the variance of <span class="math inline">\(\bar{Y}\)</span> is just equal to the variance of <span class="math inline">\(Y\)</span> — sometimes you roll extreme values like <span class="math inline">\(1\)</span> or <span class="math inline">\(6\)</span>. Now, when you increase <span class="math inline">\(n\)</span>, say, to 10, then these extreme values of <span class="math inline">\(\bar{Y}\)</span> are substantially less common. For <span class="math inline">\(\bar{Y}\)</span> to be equal to <span class="math inline">\(6\)</span> in this case, you’d need to roll 10 <span class="math inline">\(6\)</span>’s in a row. This illustrates that the sampling variance of <span class="math inline">\(\bar{Y}\)</span> is decreasing in <span class="math inline">\(n\)</span>. If this is not perfectly clear, we will look at some data soon, and I think that should confirm to you that the variance of <span class="math inline">\(\bar{Y}\)</span> is decreasing in the sample size.</p>
</div>
<div id="properties-of-estimators" class="section level3" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Properties of Estimators</h3>
<p>SW 2.5, 3.1</p>
<p>Suppose we are interested in some population parameter <span class="math inline">\(\theta\)</span> — we’ll write this pretty generically now, but it could be <span class="math inline">\(\mathbb{E}[Y]\)</span> or <span class="math inline">\(\mathbb{E}[Y|X]\)</span> or really any other population quantity that you’d like to estimate.</p>
<p>Also, suppose that we have access to a random sample of size <span class="math inline">\(n\)</span> and we have some estimate of <span class="math inline">\(\theta\)</span> that we’ll call <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>As before, we are going to consider the repeated sampling thought experiment where we imagine that we could repeatedly obtain new samples of size <span class="math inline">\(n\)</span> and with each new sample calculate a new <span class="math inline">\(\hat{\theta}\)</span>. Under this thought experiment, <span class="math inline">\(\hat{\theta}\)</span> would have a sampling distribution. One possibility for what it could look like is the following</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>In this case, values of <span class="math inline">\(\hat{\theta}\)</span> are more common around 3 and 4, but it is not highly unusual to get a value of <span class="math inline">\(\hat{\theta}\)</span> that is around 1 or 2 or 5 or 6 either.</p>
<p>The first property of an estimator that we will take about is called <strong>unbiasedness</strong>. An estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased if <span class="math inline">\(\mathbb{E}[\hat{\theta}] = \theta\)</span>. Alternatively, we can define the <strong>bias</strong> of an estimator as</p>
<p><span class="math display">\[
  \textrm{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta
\]</span>
For example, if <span class="math inline">\(\textrm{Bias}(\hat{\theta}) &gt; 0\)</span>, it means that, on average (in the repeated sampling thought experiment), our estimates of <span class="math inline">\(\theta\)</span> would be greater than the actual value of <span class="math inline">\(\theta\)</span>.</p>
<p>In general, unbiasedness is a good property for an estimator to have. That being said, we can come up with examples of not-very-good unbiased estimators and good biased estimators, but all-else-equal, it is better for an estimator to be unbiased.</p>
<p>The next property of estimators that we will talk about is their <strong>sampling variance</strong>. This is just <span class="math inline">\(\mathrm{var}(\hat{\theta})\)</span>. In general, we would like estimators with low (or 0) bias and low sampling variance. Let me give an example</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-94-1.png" width="672" />
This is a helpful figure for thinking about the properties of estimators. In this case, <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> are both unbiased (because their means are <span class="math inline">\(\theta\)</span>) while <span class="math inline">\(\hat{\theta}_3\)</span> is biased — it’s mean is greater than <span class="math inline">\(\theta\)</span>. On the other hand the sampling variance of <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_3\)</span> are about the same and both substantially smaller than for <span class="math inline">\(\hat{\theta}_1\)</span>. Clearly, <span class="math inline">\(\hat{\theta}_2\)</span> is the best estimator of <span class="math inline">\(\theta\)</span> out of the three. But which is the second best? It is not clear. <span class="math inline">\(\hat{\theta}_3\)</span> systematically over-estimates <span class="math inline">\(\theta\)</span>, but since the variance is relatively small, the misses are systematic but tend to be relatively small. On the other hand, <span class="math inline">\(\hat{\theta}_1\)</span> is, on average, equal to <span class="math inline">\(\theta\)</span>, but sometimes the estimate of <span class="math inline">\(\theta\)</span> could be quite poor due to the large sampling variance.</p>
</div>
<div id="relative-efficiency" class="section level3" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> Relative Efficiency</h3>
<p>SW 3.1</p>
<p>If <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> are two unbiased estimators of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\hat{\theta}_1\)</span> is <strong>more efficient</strong> than <span class="math inline">\(\hat{\theta}_2\)</span> if <span class="math inline">\(\mathrm{var}(\hat{\theta}_1) &lt; \mathrm{var}(\hat{\theta}_2)\)</span>.</p>
<p>Relative efficiency gives us a way to rank unbiased estimators.</p>
</div>
<div id="mean-squared-error" class="section level3" number="3.2.7">
<h3><span class="header-section-number">3.2.7</span> Mean Squared Error</h3>
<p>More generally, two estimators can be compared by their <strong>mean squared error</strong> which is defined as</p>
<p><span class="math display">\[
  \textrm{MSE}(\hat{\theta}) := \mathbb{E}\left[ (\hat{\theta} - \theta)^2\right]
\]</span></p>
<p>The mean squared error of <span class="math inline">\(\hat{\theta}\)</span> is the average “distance” between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta\)</span> in the thought experiment of having repeated samples of size <span class="math inline">\(n\)</span>.</p>
<p>Another equivalent expression for the mean squared error is</p>
<p><span class="math display">\[
  \textrm{MSE}(\hat{\theta}) = \textrm{Bias}(\hat{\theta})^2 + \mathrm{var}(\hat{\theta})
\]</span>
In other words, if we can figure out the bias and variance of <span class="math inline">\(\hat{\theta}\)</span>, then we can recover mean squared error.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> I think it is worth quickly explaining where the second expression for <span class="math inline">\(\textrm{MSE}(\hat{\theta})\)</span> comes from. Starting from the definition of <span class="math inline">\(\textrm{MSE}(\hat{\theta})\)</span>,</p>
<p><span class="math display">\[
  \begin{aligned}
    \textrm{MSE}(\hat{\theta}) &amp;= \mathbb{E}\left[ (\hat{\theta} - \theta)^2\right] \\
    &amp;= \mathbb{E}\left[ \left( (\hat{\theta} - \mathbb{E}[\hat{\theta}]) + (\mathbb{E}[\hat{\theta}] - \theta)\right)^2 \right] \\
    &amp;= \mathbb{E}\left[ (\hat{\theta} - \mathbb{E}[\hat{\theta}])^2 \right] + \mathbb{E}\left[ (\mathbb{E}[\hat{\theta}] - \theta)^2\right] + 2 \mathbb{E}\left[ (\hat{\theta} - \mathbb{E}[\hat{\theta}])(\mathbb{E}[\hat{\theta}] - \theta) \right] \\
    &amp;= \mathrm{var}(\hat{\theta}) + \textrm{Bias}(\hat{\theta})^2 
  \end{aligned}
\]</span>
where the first equality is just the definition of <span class="math inline">\(\textrm{MSE}(\hat{\theta})\)</span>, the second equality adds and subtracts <span class="math inline">\(\mathbb{E}[\hat{\theta}]\)</span>, the third equality squares everything in parentheses from the previous line and pushes the expectation through the sum. For the last equality, the first term in the previous line corresponds to the definition of <span class="math inline">\(\mathrm{var}(\hat{\theta})\)</span>; for the second term, recall that <span class="math inline">\(\textrm{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}-\theta]\)</span> (and this is non-random so the outside expectation just goes away); the last term is equal to 0 which just holds by the properties of expectations after noticing that <span class="math inline">\((\mathbb{E}[\hat{\theta}] - \theta)\)</span> is non-random and can therefore come out of the expectation.</p>
</div>
<p>Generally, we would like to choose estimators that have low mean squared error (this essentially means that they have low bias and variance). Moreover, mean squared error gives us a way to compare estimators that are potentially biased. [Also, notice that for unbiased estimators, comparing mean squared errors of different estimators just compares their variance (because the bias term is equal to 0), so this is a <em>generalization</em> of relative efficiency from the previous section.]</p>
<div class="example">
<p><span id="exm:unlabeled-div-15" class="example"><strong>Example 3.12  </strong></span>Let’s compare three estimators of <span class="math inline">\(\mathbb{E}[Y]\)</span> based on their mean squared error. Let’s consider the three following estimators</p>
<p><span class="math display">\[
  \begin{aligned}
    \hat{\mu} &amp;:= \frac{1}{n} \sum_{i=1}^n Y_i \\
    \hat{\mu}_1 &amp;:= Y_1 \\
    \hat{\mu}_\lambda &amp;:= \lambda \bar{Y} \quad \textrm{for some } \lambda &gt; 0 
  \end{aligned}
\]</span>
<span class="math inline">\(\hat{\mu}\)</span> is just the sample average of <span class="math inline">\(Y\)</span>’s that we have already discussed. <span class="math inline">\(\hat{\mu}_1\)</span> is the (somewhat strange) estimator of <span class="math inline">\(\mathbb{E}[Y]\)</span> that just uses the first observation in the data (regardless of the sample size). <span class="math inline">\(\hat{\mu}_\lambda\)</span> is an estimator of <span class="math inline">\(\mathbb{E}[Y]\)</span> that multiplies <span class="math inline">\(\bar{Y}\)</span> by some positive constant <span class="math inline">\(\lambda\)</span>.</p>
<p>To calculate the mean squared error of each of these estimators, let’s calculate their means and their variances.</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[\hat{\mu}] &amp;= \mathbb{E}[Y] \\
    \mathbb{E}[\hat{\mu}_1] &amp;= \mathbb{E}[Y_1] = \mathbb{E}[Y] \\
    \mathbb{E}[\hat{\mu}_\lambda] &amp;= \lambda \mathbb{E}[\bar{Y}] = \lambda \mathbb{E}[Y]
  \end{aligned}
\]</span>
This means that <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\mu}_1\)</span> are both unbiased. <span class="math inline">\(\hat{\mu}_\lambda\)</span> is biased (unless <span class="math inline">\(\lambda=1\)</span> though this is a relatively uninteresting case as it would mean that <span class="math inline">\(\hat{\mu}_\lambda\)</span> is exactly the same as <span class="math inline">\(\hat{\mu}\)</span>) with <span class="math inline">\(\textrm{Bias}(\hat{\mu}_\lambda) = (\lambda - 1) \mathbb{E}[Y]\)</span>.</p>
<p>Next, let’s calculate the variance for each estimator</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{var}(\hat{\mu}) &amp;= \frac{\mathrm{var}(Y)}{n} \\
  \mathrm{var}(\hat{\mu}_1) &amp;= \mathrm{var}(Y_1) = \mathrm{var}(Y) \\
  \mathrm{var}(\hat{\mu}_\lambda) &amp;= \lambda^2 \mathrm{var}(\bar{Y}) = \lambda^2 \frac{\mathrm{var}(Y)}{n}
  \end{aligned}
\]</span>
This means that we can now calculate mean squared error for each estimator.</p>
<p><span class="math display">\[
  \begin{aligned}
    \textrm{MSE}(\hat{\mu}) &amp;= \frac{\mathrm{var}{Y}}{n} \\
    \textrm{MSE}(\hat{\mu}_1) &amp;= \mathrm{var}(Y) \\
    \textrm{MSE}(\hat{\mu}_\lambda) &amp;= (\lambda-1)^2\mathbb{E}[Y]^2 + \lambda^2 \frac{\mathrm{var}(Y)}{n}
  \end{aligned}
\]</span>
The first thing to notice is that <span class="math inline">\(\hat{\mu}\)</span> <em>dominates</em> <span class="math inline">\(\hat{\mu}_1\)</span> (where dominates means that there isn’t any scenario where you could make a reasonable case that <span class="math inline">\(\hat{\mu}_1\)</span> is a better estimator) because its MSE is strictly lower (they tie only if <span class="math inline">\(n=1\)</span> when they become the same estimator). This is probably not surprising — <span class="math inline">\(\hat{\mu}_1\)</span> just throws away a lot of potentially useful information.</p>
<p>The more interesting case is <span class="math inline">\(\hat{\mu}_\lambda\)</span>. The first term is the bias term — it is greater than the bias from <span class="math inline">\(\hat{\mu}\)</span> or <span class="math inline">\(\hat{\mu}_1\)</span> because the bias of both of these is equal to 0. However, relative to <span class="math inline">\(\hat{\mu}\)</span>, the variance of <span class="math inline">\(\hat{\mu}_\lambda\)</span> can be smaller when <span class="math inline">\(\lambda\)</span> is less than 1. In fact, you can show that there are estimators that have smaller mean squared error than <span class="math inline">\(\hat{\mu}\)</span> by choosing a <span class="math inline">\(\lambda\)</span> that is smaller than (usually just slightly smaller than) 1. This sort of estimator would be biased, but are able to compensate introducing some bias by having smaller variance. For now, we won’t talk much about this sort of estimator (and stick to <span class="math inline">\(\bar{Y}\)</span>), but this sort of estimator has the “flavor” of modern machine learning estimators that typically introduce some bias while reducing variance. One last comment: if you were to make a “bad” choice of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\hat{\mu}_\lambda\)</span> could have higher mean squared error than even <span class="math inline">\(\hat{\mu}_1\)</span>, so if you wanted to proceed this way, you’d have to choose <span class="math inline">\(\lambda\)</span> with some care.</p>
</div>
</div>
<div id="large-sample-properties-of-estimators" class="section level3" number="3.2.8">
<h3><span class="header-section-number">3.2.8</span> Large Sample Properties of Estimators</h3>
<p>SW 2.6</p>
<p>Statistics/Econometrics often relies on “large sample” (meaning: the number of observations, <span class="math inline">\(n\)</span>, is large) properties of estimators.</p>
<p>Intuition: We generally expect that estimators that use a large number of observations will perform better than in the case with only a few observations.</p>
<p>The second goal of this section will be to introduce an approach to conduct hypothesis testing. In particular, we may have some theory and want a way to test whether or not the data that we have “is consistent with” the theory or not. These arguments typically involve either making strong assumptions or having a large sample — we’ll mainly study the large sample case as I think this is more useful.</p>
<div id="consistency" class="section level4" number="3.2.8.1">
<h4><span class="header-section-number">3.2.8.1</span> Consistency</h4>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is said to be <strong>consistent</strong> if <span class="math inline">\(\hat{\theta}\)</span> gets close to <span class="math inline">\(\theta\)</span> for large values of <span class="math inline">\(n\)</span>.</p>
<p>The main tool for studying consistency is the <strong>law of large numbers</strong>. The law of large numbers says that sample averages converge to population averages as the sample size gets large. In math, this is</p>
<p><span class="math display">\[
  \frac{1}{n} \sum_{i=1}^n Y_i \rightarrow \mathbb{E}[Y] \quad \textrm{as } n \rightarrow \infty
\]</span>
In my view, the law of large numbers is very intuitive. If you have a large sample and calculate a sample average, it should be close to the population average.</p>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 3.13  </strong></span>Let’s consider the same three estimators as before and whether or not they are consistent. First, the LLN implies that</p>
<p><span class="math display">\[
  \hat{\mu} = \frac{1}{n} \sum_{i=1}^n Y_i \rightarrow \mathbb{E}[Y]
\]</span>
This implies that <span class="math inline">\(\hat{\mu}\)</span> is consistent. Next,</p>
<p><span class="math display">\[
  \hat{\mu}_1 = Y_1
\]</span>
doesn’t change depending on the size of the sample (you just use the first observation), so this is not consistent. This is an example of an unbiased estimator that is not consistent. Next,</p>
<p><span class="math display">\[
  \hat{\mu}_\lambda = \lambda \bar{Y} \rightarrow \lambda \mathbb{E}[Y] \neq \mathbb{E}[Y]
\]</span>
which implies that (as long as <span class="math inline">\(\lambda\)</span> doesn’t change with <span class="math inline">\(n\)</span>), <span class="math inline">\(\hat{\mu}_{\lambda}\)</span> is not consistent. Let’s give one more example. Consider the estimator</p>
<p><span class="math display">\[
  \hat{\mu}_c := \bar{Y} + \frac{c}{n}
\]</span>
where <span class="math inline">\(c\)</span> is some constant (this is a strange estimate of <span class="math inline">\(\mathbb{E}[Y]\)</span> where we take <span class="math inline">\(\bar{Y}\)</span> and add a constant divided by the sample size). In this case,</p>
<p><span class="math display">\[
  \hat{\mu}_c \rightarrow \mathbb{E}[Y] + 0 = \mathbb{E}[Y]
\]</span></p>
<p>which implies that it is consistent. It is interesting to note that</p>
<p><span class="math display">\[
  \mathbb{E}[\hat{\mu}_c] = \mathbb{E}[Y] + \frac{c}{n}
\]</span>
which implies that it is biased. This is an example of a biased estimator that is consistent.</p>
</div>
</div>
<div id="asymptotic-normality" class="section level4" number="3.2.8.2">
<h4><span class="header-section-number">3.2.8.2</span> Asymptotic Normality</h4>
<p>The next large sample property that we’ll talk about is <strong>asymptotic normality</strong>. This is a hard one to wrap your mind around, but I’ll try to explain as clearly as possible. We’ll start by talking about what it is, and then we’ll move to why it’s useful.</p>
<p>Most of the estimators that we will talk about this semester have the following property</p>
<p><span class="math display">\[
  \sqrt{n}\left( \hat{\theta} - \theta \right) \rightarrow N(0,V) \quad \textrm{as } n \rightarrow \infty
\]</span>
In words, what this says is that we can learn something about the sampling distribution of <span class="math inline">\(\hat{\theta}\)</span> as long as we have a large enough sample. More specifically, if <span class="math inline">\(\hat{\theta}\)</span> is asymptotically normal, it means that if we take <span class="math inline">\(\hat{\theta}\)</span> subtract the true value of the parameter <span class="math inline">\(\theta\)</span> (this is often referred to as “centering”) and multiply by <span class="math inline">\(\sqrt{n}\)</span>, then that object (as long as the sample size is large enough) will <em>seem like</em> a draw from a normal distribution with mean 0 and variance <span class="math inline">\(V\)</span>. Since we know lots about normal distributions, we’ll be able to exploit this in very useful ways in the next section.</p>
<p>An equivalent, alternative expression that is sometimes useful is</p>
<p><span class="math display">\[
  \frac{\sqrt{n}\left( \hat{\theta} - \theta\right)}{\sqrt{V}} \rightarrow N(0,1) \quad \textrm{as } n \rightarrow \infty
\]</span></p>
<p>To establish asymptotic normality of a particular estimator, the main tool is the <strong>central limit theorem</strong>. The central limit theorem (sometimes abbreviated CLT) says that</p>
<p><span class="math display">\[
  \sqrt{n}\left( \frac{1}{n} \sum_{i=1}^n Y_i - \mathbb{E}[Y]\right) \rightarrow N(0,V) \quad \textrm{as } n \rightarrow \infty
\]</span>
where <span class="math inline">\(V = \mathrm{var}(Y)\)</span>.</p>
<p>In words, the CLT says that if you take the difference between <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\mathbb{E}[Y]\)</span> (which, by the LLN converges to 0 as <span class="math inline">\(n \rightarrow \infty\)</span>) and “scale it up” by <span class="math inline">\(\sqrt{n}\)</span> (which goes to <span class="math inline">\(\infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>), then <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y])\)</span> will act like a draw from a normal distribution with variance <span class="math inline">\(\mathrm{var}(Y)\)</span>.</p>
<p>There are a few things to point out:</p>
<ul>
<li><p>Just to start with, this is not nearly as “natural” a result as the LLN. The LLN basically makes perfect sense. For me, I know how to prove the CLT (though we are not going to do it in class), but I don’t think that I would have ever been able to come up with this on my own.</p></li>
<li><p>Notice that the CLT does not rely on any distributional assumptions. We do not need to assume that <span class="math inline">\(Y\)</span> follows a normal distribution and it will apply when <span class="math inline">\(Y\)</span> follows any distribution (up to some relatively minor technical conditions that we will not worry about).</p></li>
<li><p>It is also quite remarkable. We usually have the sense that as the sample size gets large that things will converge to something (e.g., LLN saying that sample averages converge to population averages) or that they will diverge (i.e., go off to positive or negative infinity themselves). The CLT provides an intermediate case — <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y])\)</span> is neither converging to a particular value or diverging to infinity. Instead, it is <em>converging in distribution</em> — meaning: it is settling down to something that looks like a draw from some distribution rather than converging to a particular number. In some sense, you can think of this as a “tie” between the part <span class="math inline">\((\bar{Y}-\mathbb{E}[Y])\)</span> which, by itself, is converging to 0, and <span class="math inline">\(\sqrt{n}\)</span> which, by itself, is diverging to infinity. In fact, if you multiplied instead by something somewhat smaller, say, <span class="math inline">\(n^{1/3}\)</span>, then the term <span class="math inline">\((\bar{Y}-\mathbb{E}[Y])\)</span> would “win” and the whole expression would converge to 0. On the other hand, if you multiplied by something somewhat larger, say, <span class="math inline">\(n\)</span>, then the <span class="math inline">\(n\)</span> part would “win” and the whole thing would diverge. <span class="math inline">\(\sqrt{n}\)</span> turns out to be “just right” so that there is essentially a “tie” and this term neither converges to a particular number nor diverges.</p></li>
<li><p>A very common question for students is: “how large does <span class="math inline">\(n\)</span> need to be for the central limit theorem to apply?” Unfortunately, there is a not a great answer to this (though some textbooks have sometimes given explicit numbers here). Here is a basic explanation for why it is hard to give a definite number. Suppose <span class="math inline">\(Y\)</span> follows a normal distribution, then it will not take many observations for the normal approximation to hold. On the other hand, if <span class="math inline">\(Y\)</span> were to come from a discrete distribution or just a generally complicated distribution, then it might take many more observations for the normal approximation to hold.</p></li>
</ul>
<p>All that to say, I know that the CLT is hard to understand, but the flip-side of that is that it really is a fascinating result. We’ll see how its useful next.</p>
</div>
</div>
<div id="inference-hypothesis-testing" class="section level3" number="3.2.9">
<h3><span class="header-section-number">3.2.9</span> Inference / Hypothesis Testing</h3>
<p>SW 3.2, 3.3</p>
<p>Often in statistics/econometrics, we have some theory that we would like to test. Pretty soon, we will be interested in testing a theory like: some economic policy had no effect on some outcome of interest.</p>
<p>In this section, we’ll focus on the relatively simple case of conducting inference on <span class="math inline">\(\mathbb{E}[Y]\)</span>, but very similar arguments will apply when we try to start estimating more complicated things soon. Because we’re just focusing on <span class="math inline">\(\mathbb{E}[Y]\)</span>, the examples in this section may be a somewhat trivial/uninteresting, but I want us to learn some mechanics, and then we’ll be able to apply these in more complicated situations.</p>
<p>Let’s start with defining some terms.</p>
<p><strong>Null Hypothesis</strong> This is the hypothesis (or theory) that we want to test. We’ll often write it in the following way</p>
<p><span class="math display">\[
  H_0 : \mathbb{E}[Y] = \mu_0
\]</span>
where <span class="math inline">\(\mu_0\)</span> is some actual number (e.g., 0 or 10 or just whatever coincides with the theory you want to test).</p>
<p><strong>Alternative Hypothesis</strong> This is what is true if <span class="math inline">\(H_0\)</span> is not. There are other possibilities, but I think the only alternative hypothesis that we will consider this semester is</p>
<p><span class="math display">\[
  H_1 : \mathbb{E}[Y] \neq \mu_0
\]</span>
i.e., that <span class="math inline">\(\mathbb{E}[Y]\)</span> is not equal to the particular value <span class="math inline">\(\mu_0\)</span>.</p>
<p>The key conceptual issue is that, even if the null hypothesis is true, because we estimate <span class="math inline">\(\mathbb{E}[Y]\)</span> with a sample, it will generally be the case that <span class="math inline">\(\bar{Y} \neq \mu_0\)</span>. This is just the nature of trying to estimate things with a sample.</p>
<p>What we are going to go for is essentially trying to tell the difference (or at least be able to weigh the evidence) regarding whether the difference between <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\mu_0\)</span> can be fully explained by sampling variation or that the difference is “too big” to be explained by sampling variation. Things will start to get “mathy” in this section, but I think it is helpful to just hold this high-level idea in your head as we go along.</p>
<p>Next, let’s define the <strong>standard error</strong> of an estimator. Suppose that we know that our estimator is asymptotically normal so that</p>
<p><span class="math display">\[
  \sqrt{n}(\hat{\theta} - \theta) \rightarrow N(0,V) \quad \textrm{as } n \rightarrow \infty
\]</span>
Then, we define the standard error of <span class="math inline">\(\hat{\theta}\)</span> as</p>
<p><span class="math display">\[
  \textrm{s.e.}(\hat{\theta}) := \frac{\sqrt{\hat{V}}}{\sqrt{n}}
\]</span>
which is just the square root of the estimate of the asymptotic variance <span class="math inline">\(V\)</span> divided by the square root of the sample size. For example, in the case where we are trying to estimate <span class="math inline">\(\mathbb{E}[Y]\)</span>, recall that, by the CLT, <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y]) \rightarrow N(0,V)\)</span> where <span class="math inline">\(V=\mathrm{var}(Y)\)</span>, so that</p>
<p><span class="math display">\[
  \textrm{s.e.}(\bar{Y}) = \frac{\sqrt{\widehat{\mathrm{var}}(Y)}}{\sqrt{n}}
\]</span>
where <span class="math inline">\(\widehat{\mathrm{var}}(Y)\)</span> is just an estimate of the variance of <span class="math inline">\(Y\)</span>, i.e., just run <code>var(Y)</code> in <code>R</code>.</p>
<p>Over the next few sections, we are going to consider several different way to conduct inference (i.e., weigh the evidence) about some theory (i.e., the null hypothesis) using the data that we have. For all of the approaches that we consider below, the key ingredients are going to an estimate of the parameter of interest (e.g., <span class="math inline">\(\bar{Y}\)</span>), the value of <span class="math inline">\(\mu_0\)</span> coming from the null hypothesis, and the standard error of the estimator.</p>
<div id="t-statistics" class="section level4" number="3.2.9.1">
<h4><span class="header-section-number">3.2.9.1</span> t-statistics</h4>
<p>A <strong>t-statistic</strong> is given by</p>
<p><span class="math display">\[
  t = \frac{\sqrt{n} (\bar{Y} - \mu_0)}{\sqrt{\hat{V}}}
\]</span>
Alternatively (from the definition of standard error), we can write</p>
<p><span class="math display">\[
  t = \frac{(\bar{Y} - \mu_0)}{\textrm{s.e.}(\bar{Y})}
\]</span>
though I’ll tend to use the first expression, just because I think it makes the arguments below slightly more clear.</p>
<p>Notice that <span class="math inline">\(t\)</span> is something that we can calculate with our available data. <span class="math inline">\(\sqrt{n}\)</span> is the square root of the sample size, <span class="math inline">\(\bar{Y}\)</span> is the sample average of <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mu_0\)</span> is a number (that we have picked) coming from the null hypothesis, and <span class="math inline">\(\hat{V}\)</span> is the sample variance of <span class="math inline">\(Y\)</span> (e.g., computed with <code>var(Y)</code> in <code>R</code>).</p>
<p>Now, here is the interesting thing about t-statistics. If the null hypothesis is true, then</p>
<p><span class="math display">\[
  t = \frac{\sqrt{n} (\bar{Y} - \mathbb{E}[Y])}{\sqrt{\hat{V}}} \approx \frac{\sqrt{n} (\bar{Y} - \mathbb{E}[Y])}{\sqrt{V}}
\]</span></p>
<p>where we have substituted in <span class="math inline">\(\mathbb{E}[Y]\)</span> for <span class="math inline">\(\mu_0\)</span> (due to <span class="math inline">\(H_0\)</span> being true) and then replaced <span class="math inline">\(\hat{V}\)</span> with <span class="math inline">\(V\)</span> (which holds under the law of large numbers). This is something that we can apply the CLT to, and, in particular, if <span class="math inline">\(H_0\)</span> holds, then
<span class="math display">\[
  t \rightarrow N(0,1)
\]</span>
That is, if <span class="math inline">\(H_0\)</span> is true, then <span class="math inline">\(t\)</span> should look like a draw from a normal distribution.</p>
<p>Now, let’s think about what happens when the null hypothesis isn’t true. Then, we can write</p>
<p><span class="math display">\[
  t = \frac{\sqrt{n} (\bar{Y} - \mu_0)}{\sqrt{\hat{V}}}
\]</span>
which is just the definition of <span class="math inline">\(t\)</span>, but something different will happen here. In order for <span class="math inline">\(t\)</span> to follow a normal distribution, we need <span class="math inline">\((\bar{Y} - \mu_0)\)</span> to converge to 0. But <span class="math inline">\(\bar{Y}\)</span> converges to <span class="math inline">\(\mathbb{E}[Y]\)</span>, and if the null hypothesis does not hold, then <span class="math inline">\(\mathbb{E}[Y] \neq \mu_0\)</span> which implies that <span class="math inline">\((\bar{Y} - \mu_0) \rightarrow (\mathbb{E}[Y] - \mu_0) \neq 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. It’s still the case that <span class="math inline">\(\sqrt{n} \rightarrow \infty\)</span>. Thus, if <span class="math inline">\(H_0\)</span> is not true, then <span class="math inline">\(t\)</span> will diverge (recall: this means that it will either go to positive infinity or negative infinity depending on the sign of <span class="math inline">\((\mathbb{E}[Y] - \mu_0)\)</span>).</p>
<p>This gives us a very good way to start to think about whether or not the data is compatible with our theory. For example, suppose that you calculate <span class="math inline">\(t\)</span> (using your data and under your null hypothesis) and that it is equal to 1. 1 is not an “unusual” looking draw from a standard normal distribution — this suggests that you at least do not have strong evidence from data against your theory. Alternatively, suppose that you calculate that <span class="math inline">\(t=-24\)</span>. While its technically possible that you could draw <span class="math inline">\(-24\)</span> from a standard normal distribution — it is exceedingly unlikely. We would interpret this as strong evidence against the null hypothesis, and it should probably lead you to “reject” the null hypothesis.</p>
<p>We have talked about some clear cases, but what about the “close calls?” Suppose you calculate that <span class="math inline">\(t=2\)</span>. Under the null hypothesis, there is about a 4.6% chance of getting a t-statistic at least this large (in absolute value). So…if <span class="math inline">\(H_0\)</span> is true, this is a fairly unusual t-statistic, but it is not extremely unusual. What should you do?</p>
<p>Before we decide what to do, let’s introduce a little more terminology regarding what could go wrong with hypothesis testing. There are two ways that we could go wrong:</p>
<p><strong>Type I Error</strong> — This would be to reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true</p>
<p><strong>Type II Error</strong> — This would be to fail to reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is false</p>
<p>Clearly, there is a tradeoff here. If you are really concerned with type I errors, you can be very cautious about rejecting <span class="math inline">\(H_0\)</span>. If you are very concerned about type II errors, you could aggressively reject <span class="math inline">\(H_0\)</span>. The traditional approach to trading these off in statistics is to pre-specify a <strong>significance level</strong> indicating what percentage of the time you are willing to commit a type I error. Usually the significance level is denoted by <span class="math inline">\(\alpha\)</span> and the most common choice of <span class="math inline">\(\alpha\)</span> is 0.05 and other common choices are <span class="math inline">\(\alpha=0.1\)</span> or <span class="math inline">\(\alpha=0.01\)</span>. Then, good statistical tests try to make as few type II errors as possible subject to the constraint on the rate of type I errors.</p>
<p>Often, once you have specified a significance level, it comes with a <strong>critical value</strong>. The critical value is the value of a test statistic for which the test just rejects <span class="math inline">\(H_0\)</span>.</p>
<p>In practice, this leads to the following decision rule:</p>
<ul>
<li><p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &gt; c_{1-\alpha}\)</span> where <span class="math inline">\(c_{1-\alpha}\)</span> is the critical value corresponding to the significance level <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Fail to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &lt; c_{1-\alpha}\)</span></p></li>
</ul>
<p>In our case, since <span class="math inline">\(t\)</span> follows a normal distribution under <span class="math inline">\(H_0\)</span>, the corresponding critical value (when <span class="math inline">\(\alpha=0.05\)</span>) is 1.96. In particular, recall what the pdf of a standard normal random variable looks like</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<p>The sum of the two blue, shaded areas is 0.05. In other words, under <span class="math inline">\(H_0\)</span>, there is a 5% chance that, by chance, <span class="math inline">\(t\)</span> would fall in the shaded areas. If you want to change the significance level, it would result in a corresponding change in the critical value so that the area in the new shaded region would adjust too. For example, if you set the significance level to be <span class="math inline">\(\alpha=0.1\)</span>, then you would need to adjust the critical value to be 1.64, and if you set <span class="math inline">\(\alpha=0.01\)</span>, then you would need to adjust the critical value to be 2.58.</p>
</div>
<div id="p-values" class="section level4" number="3.2.9.2">
<h4><span class="header-section-number">3.2.9.2</span> P-values</h4>
<p>Choosing a significance level is somewhat arbitrary. What did we choose 5%?</p>
<p>Perhaps more importantly, we are essentially throwing away a lot of information if we are to reduce the information from standard errors/t-statistics to a binary “reject” or “fail to reject.”</p>
<p>One alternative is to report a <strong>p-value</strong>. A p-value is the probability of observing a t-statistic as “extreme” as we did if <span class="math inline">\(H_0\)</span> were true.</p>
<p>Here is an example of how to calculate a p-value. Suppose we calculate <span class="math inline">\(t=1.85\)</span>. Then,</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<p>Then, under <span class="math inline">\(H_0\)</span>, the probability of getting a t-statistic “as extreme” as 1.85 corresponds to the area of the two shaded regions above. In other words, we need to to compute</p>
<p><span class="math display">\[
  \textrm{p-value} = \mathrm{P}(Z \leq -1.85) + \mathrm{P}(Z \geq 1.85)
\]</span>
where <span class="math inline">\(Z \sim N(0,1)\)</span>. One thing that is helpful to notice here is that, a standard normal random variable is symmetric. This means that <span class="math inline">\(\mathrm{P}(Z \leq -1.85) = \mathrm{P}(Z \geq 1.85)\)</span>. We also typically denote the cdf of a standard normal random variable with the symbol <span class="math inline">\(\Phi\)</span>. Thus,</p>
<p><span class="math display">\[
  \textrm{p-value} = 2 \Phi(-1.85)
\]</span>
I don’t know what this is off the top of my head, but it is easy to compute from a table or using <code>R</code>. In <code>R</code>, you can use the function <code>pnorm</code> — here, the p-value is given by <code>2*pnorm(-1.85)</code> which is equal to 0.064.</p>
<p>More generally, if you calculate a t-statistic, <span class="math inline">\(t\)</span>, using your data and under <span class="math inline">\(H_0\)</span>, then,</p>
<p><span class="math display">\[
  \textrm{p-value} = 2 \Phi(-|t|)
\]</span></p>
</div>
<div id="confidence-interval" class="section level4" number="3.2.9.3">
<h4><span class="header-section-number">3.2.9.3</span> Confidence Interval</h4>
<p>Another idea is to report a <span class="math inline">\((1-\alpha)\%\)</span> (e.g., 95%) confidence interval.</p>
<p>The interpretation of a confidence interval is a bit subtle. It is this: if we collected a large number of samples, and computed a confidence interval each time, 95% of these would contain the true value. This is subtly different than: there is a 95% probability that <span class="math inline">\(\theta\)</span> (the population parameter of interest) falls within the confidence interval — this second interpretation doesn’t make sense because <span class="math inline">\(\theta\)</span> is non-random.</p>
<p>A 95% confidence interval is given by</p>
<p><span class="math display">\[
  CI_{95\%} = \left[\hat{\theta} - 1.96 \ \textrm{s.e.}(\hat{\theta}), \hat{\theta} + 1.96 \  \textrm{s.e.}(\hat{\theta})\right]
\]</span></p>
<p>For the particular case where we are interested in <span class="math inline">\(\mathbb{E}[Y]\)</span>, this becomes</p>
<p><span class="math display">\[
  CI_{95\%} = \left[ \bar{Y} - 1.96 \ \textrm{s.e.}(\bar{Y}), \bar{Y} + 1.96 \ \textrm{s.e.}(\bar{Y}) \right]
\]</span></p>
</div>
<div id="inference-in-practice" class="section level4" number="3.2.9.4">
<h4><span class="header-section-number">3.2.9.4</span> Inference in Practice</h4>
<p>I have covered the main approaches to inference in this section. I’d like to make a couple of concluding comments. First, all of the approaches discussed here (standard errors, t-statistics, p-values, and confidence intervals) are very closely related (in some sense, they are just alternative ways to report the same information). They all rely heavily on establishing asymptotic normality of the estimate of the parameter of interest — in fact, this is why we were interested in asymptotic normality in the first place. My sense is that the most common thing to report (at least in economics) is an estimate of the parameter of interest (e.g., <span class="math inline">\(\hat{\theta}\)</span> or <span class="math inline">\(\bar{Y}\)</span>) along with its standard error. If you know this information, you (or your reader) can easily compute any of the other expressions that we’ve considered in this section.</p>
<p>Another important thing to mention is that there is often a distinction between <strong>statistical significance</strong> and <strong>economic significance</strong>.</p>
<p>In the next chapter, we’ll start to think about the <em>effect</em> of one variable on another (e.g., the effect of some economic policy on some outcome of interest). By far the most common null hypothesis in this case is that “the effect” is equal to 0. However, in economics/social sciences/business applications, there probably aren’t too many cases where (i) it would be interesting enough to consider the effect of one variable on another (ii) while simultaneously the effect is literally equal to 0. Since, all else equal, standard errors get smaller with more observations, as datasets in economics tend to get larger over time, we tend to find more statistically significant effects. This doesn’t mean that effects are getting bigger or more important — just that we are able to detect smaller and smaller effects if we have enough data. And most questions in economics involve more than just answering the binary question: does variable <span class="math inline">\(X\)</span> have any effect at all on variable <span class="math inline">\(Y\)</span>? For example, if you are trying to evaluate the effect of some economic policy, it is usually more helpful to think in terms of a cost-benefit analysis — what are the benefits or the policy relative to the costs and these sorts of comparisons inherently involve thinking about magnitudes of effects.</p>
<p>A more succinct way to say all this is: the effect of one variable on another can be both “statistically significant” and “economically” small at the same time. Alternatively, if you do not have much data or the data is very “noisy,” it may be possible that there are relatively large effects, but that the estimates are not statistically significant (i.e., you are not able to detect them very well with the data that you have). Therefore, it is important to not become too fixated on statistical significance and to additionally think carefully about the magnitudes of estimates.</p>
</div>
</div>
<div id="coding-1" class="section level3" number="3.2.10">
<h3><span class="header-section-number">3.2.10</span> Coding</h3>
<p>In this section, we’ll use the <code>acs</code> data to calculate an estimate of average wage/salary income among employed individuals in the United States. We’ll test the null hypothesis that the mean income in the United States is $50,000 as well as report the standard error of our estimate of mean income, as well as corresponding p-values, t-statistics, and 95% confidence interval. Finally, we’ll report a table of summary statistics using the <code>modelsummary</code> package separately by college graduates relative to non-college graduates.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="probability-and-statistics.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/acs.RData&quot;</span>)</span>
<span id="cb85-2"><a href="probability-and-statistics.html#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="probability-and-statistics.html#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate of mean income</span></span>
<span id="cb85-4"><a href="probability-and-statistics.html#cb85-4" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(acs<span class="sc">$</span>incwage)</span>
<span id="cb85-5"><a href="probability-and-statistics.html#cb85-5" aria-hidden="true" tabindex="-1"></a>ybar</span>
<span id="cb85-6"><a href="probability-and-statistics.html#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 59263.46</span></span>
<span id="cb85-7"><a href="probability-and-statistics.html#cb85-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-8"><a href="probability-and-statistics.html#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate standard error</span></span>
<span id="cb85-9"><a href="probability-and-statistics.html#cb85-9" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="fu">var</span>(acs<span class="sc">$</span>incwage)</span>
<span id="cb85-10"><a href="probability-and-statistics.html#cb85-10" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(acs)</span>
<span id="cb85-11"><a href="probability-and-statistics.html#cb85-11" aria-hidden="true" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(V) <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb85-12"><a href="probability-and-statistics.html#cb85-12" aria-hidden="true" tabindex="-1"></a>se</span>
<span id="cb85-13"><a href="probability-and-statistics.html#cb85-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 713.8138</span></span>
<span id="cb85-14"><a href="probability-and-statistics.html#cb85-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-15"><a href="probability-and-statistics.html#cb85-15" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate t-statistic</span></span>
<span id="cb85-16"><a href="probability-and-statistics.html#cb85-16" aria-hidden="true" tabindex="-1"></a>t_stat <span class="ot">&lt;-</span> (ybar <span class="sc">-</span> <span class="dv">50000</span>) <span class="sc">/</span> se</span>
<span id="cb85-17"><a href="probability-and-statistics.html#cb85-17" aria-hidden="true" tabindex="-1"></a>t_stat</span>
<span id="cb85-18"><a href="probability-and-statistics.html#cb85-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 12.97742</span></span></code></pre></div>
<p>This clearly exceeds 1.96 (or any common critical value) which implies that we would reject the null hypothesis that mean income is equal to $50,000.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="probability-and-statistics.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate p-value</span></span>
<span id="cb86-2"><a href="probability-and-statistics.html#cb86-2" aria-hidden="true" tabindex="-1"></a>p_val <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">abs</span>(t_stat))</span></code></pre></div>
<p>The p-value is essentially equal to 0. This is expected given the value of the t-statistic that we calculated earlier.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="probability-and-statistics.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% confidence interval</span></span>
<span id="cb87-2"><a href="probability-and-statistics.html#cb87-2" aria-hidden="true" tabindex="-1"></a>ci_L <span class="ot">&lt;-</span> ybar <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb87-3"><a href="probability-and-statistics.html#cb87-3" aria-hidden="true" tabindex="-1"></a>ci_U <span class="ot">&lt;-</span> ybar <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb87-4"><a href="probability-and-statistics.html#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="fu">paste0</span>(<span class="st">&quot;[&quot;</span>,<span class="fu">round</span>(ci_L,<span class="dv">1</span>),<span class="st">&quot;,&quot;</span>,<span class="fu">round</span>(ci_U,<span class="dv">1</span>),<span class="st">&quot;]&quot;</span>)</span>
<span id="cb87-5"><a href="probability-and-statistics.html#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;[57864.4,60662.5]&quot;</span></span></code></pre></div>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="probability-and-statistics.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb88-2"><a href="probability-and-statistics.html#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb88-3"><a href="probability-and-statistics.html#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create a factor variable for going to college</span></span>
<span id="cb88-4"><a href="probability-and-statistics.html#cb88-4" aria-hidden="true" tabindex="-1"></a>acs<span class="sc">$</span>col <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(acs<span class="sc">$</span>educ <span class="sc">&gt;=</span> <span class="dv">16</span>, <span class="st">&quot;college&quot;</span>, <span class="st">&quot;non-college&quot;</span>)</span>
<span id="cb88-5"><a href="probability-and-statistics.html#cb88-5" aria-hidden="true" tabindex="-1"></a>acs<span class="sc">$</span>col <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(acs<span class="sc">$</span>col)</span>
<span id="cb88-6"><a href="probability-and-statistics.html#cb88-6" aria-hidden="true" tabindex="-1"></a>acs<span class="sc">$</span>female <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(acs<span class="sc">$</span>sex<span class="sc">==</span><span class="dv">2</span>)</span>
<span id="cb88-7"><a href="probability-and-statistics.html#cb88-7" aria-hidden="true" tabindex="-1"></a>acs<span class="sc">$</span>incwage <span class="ot">&lt;-</span> acs<span class="sc">$</span>incwage<span class="sc">/</span><span class="dv">1000</span></span>
<span id="cb88-8"><a href="probability-and-statistics.html#cb88-8" aria-hidden="true" tabindex="-1"></a><span class="fu">datasummary_balance</span>(<span class="sc">~</span> col, <span class="at">data=</span>dplyr<span class="sc">::</span><span class="fu">select</span>(acs, incwage, female, age, col),</span>
<span id="cb88-9"><a href="probability-and-statistics.html#cb88-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">fmt=</span><span class="dv">2</span>)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
college (N=3871)
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
non-college (N=6129)
</div>
</th>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="2">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Diff. in Means
</th>
<th style="text-align:right;">
Std. Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
incwage
</td>
<td style="text-align:right;">
89.69
</td>
<td style="text-align:right;">
96.15
</td>
<td style="text-align:right;">
40.05
</td>
<td style="text-align:right;">
39.01
</td>
<td style="text-align:right;">
-49.65
</td>
<td style="text-align:right;">
1.62
</td>
</tr>
<tr>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
0.51
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
-0.04
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
44.38
</td>
<td style="text-align:right;">
13.43
</td>
<td style="text-align:right;">
42.80
</td>
<td style="text-align:right;">
15.71
</td>
<td style="text-align:right;">
-1.58
</td>
<td style="text-align:right;">
0.29
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="lab-2-monte-carlo-simulations" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Lab 2: Monte Carlo Simulations</h2>
<p>In this lab, we will study the theoretical properties of the estimators that we have been discussing in this chapter.</p>
<p><strong>Monte Carlo simulations</strong> are a useful way to study/understand the properties of an estimation procedure. The basic idea is that, instead of using real data, we are going to use simulated data where we control the data generating process. This will be useful for two reasons. First, we will <em>know</em> what <strong>the truth</strong> is and compare results coming from our estimation procedure to the truth. Second, because we are simulating data, we can actually carry out our thought experiment of repeatedly drawing a sample of some particular size.</p>
<p>For this lab, we are going to make simulated coin flips.</p>
<ol style="list-style-type: decimal">
<li><p>Write a function called <code>flip</code> that takes in an argument <code>p</code> where <code>p</code> stands for the probability of flipping a heads (you can code this as a <code>1</code> and <code>0</code> for tails) and outputs either <code>1</code> or <code>0</code>. Run the code</p>
<pre><code>flip(0.5)</code></pre>
<p><strong>Hint:</strong> It may be helpful to use the <code>R</code> function <code>sample</code>.</p></li>
<li><p>Write a function called <code>generate_sample</code> that takes in the arguments <code>n</code> and <code>p</code> and generates a sample of <code>n</code> coin flips where the probability of flipping heads is <code>p</code>. Run the code</p>
<pre><code>generate_sample(10,0.5)</code></pre></li>
<li><p>Next, over 1000 Monte Carlo simulations (i.e., do the following 1000 times),</p>
<ol style="list-style-type: lower-roman">
<li><p>generate a new sample with 10 observations</p></li>
<li><p>calculate an estimate of <span class="math inline">\(p\)</span></p></li>
</ol>
<p>(<strong>Hint:</strong> you can estimate <span class="math inline">\(p\)</span> by just calculating the average number of heads flipped in a particular simulation)</p>
<ol start="3" style="list-style-type: lower-roman">
<li><p>a t-statistic for the null hypothesis that <span class="math inline">\(p=0.5\)</span></p></li>
<li><p>and record whether or not you reject the null hypothesis that <span class="math inline">\(p=0.5\)</span> in that simulation</p></li>
</ol></li>
</ol>
<p>Then, using all 1000 Monte Carlo simulations, report (i) an estimate of the bias of your estimator, (ii) an estimate of the variance of your estimator, (iii) an estimate of the mean squared error of your estimator, (iv) plot a histogram of the t-statistics across iterations, and (v) report the fraction of times that you reject <span class="math inline">\(H_0\)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Same as #3, but with 50 observations in each simulation. What differences do you notice?</p></li>
<li><p>Same as #3, but with 50 observations and test <span class="math inline">\(H_0:p=0.6\)</span>. What differences do you notice?</p></li>
<li><p>Same as #3, but with 50 observations and test <span class="math inline">\(H_0:p=0.9\)</span>. What differences do you notice?</p></li>
<li><p>Same as #3, but with 1000 observations and test <span class="math inline">\(H_0:p=0.6\)</span>. What differences do you notice?</p></li>
<li><p>Same as #3, but now set <span class="math inline">\(p=0.95\)</span> (so that this is an unfair coin that flips heads 95% of the time) and with 10 observations and test <span class="math inline">\(H_0:p=0.95\)</span>. What differences do you notice?</p></li>
<li><p>Same as #8, but with 50 observations. What differences do you notice?</p></li>
<li><p>Same as #8, but with 1000 observations. What differences do you notice?</p></li>
</ol>
<p><strong>Hint:</strong> Since problems 3-10 ask you to do roughly the same thing over and over, it is probably useful to try to write a function to do all of these but with arguments that allow you to change the number of observations per simulation, the true value of <span class="math inline">\(p\)</span>, and the null hypothesis that you are testing.</p>
</div>
<div id="lab-2-solutions" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Lab 2 Solutions</h2>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="probability-and-statistics.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># function to flip a coin with probability p</span></span>
<span id="cb91-2"><a href="probability-and-statistics.html#cb91-2" aria-hidden="true" tabindex="-1"></a>flip <span class="ot">&lt;-</span> <span class="cf">function</span>(p) {</span>
<span id="cb91-3"><a href="probability-and-statistics.html#cb91-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">size=</span><span class="dv">1</span>, <span class="at">prob=</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>p,p)))</span>
<span id="cb91-4"><a href="probability-and-statistics.html#cb91-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb91-5"><a href="probability-and-statistics.html#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="probability-and-statistics.html#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="co"># test out flip function</span></span>
<span id="cb91-7"><a href="probability-and-statistics.html#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="fu">flip</span>(<span class="fl">0.5</span>)</span>
<span id="cb91-8"><a href="probability-and-statistics.html#cb91-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="probability-and-statistics.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># function to generate a sample of size n</span></span>
<span id="cb92-2"><a href="probability-and-statistics.html#cb92-2" aria-hidden="true" tabindex="-1"></a>generate_sample <span class="ot">&lt;-</span> <span class="cf">function</span>(n,p) {</span>
<span id="cb92-3"><a href="probability-and-statistics.html#cb92-3" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb92-4"><a href="probability-and-statistics.html#cb92-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb92-5"><a href="probability-and-statistics.html#cb92-5" aria-hidden="true" tabindex="-1"></a>    Y[i] <span class="ot">&lt;-</span> <span class="fu">flip</span>(p)</span>
<span id="cb92-6"><a href="probability-and-statistics.html#cb92-6" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb92-7"><a href="probability-and-statistics.html#cb92-7" aria-hidden="true" tabindex="-1"></a>  Y</span>
<span id="cb92-8"><a href="probability-and-statistics.html#cb92-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb92-9"><a href="probability-and-statistics.html#cb92-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-10"><a href="probability-and-statistics.html#cb92-10" aria-hidden="true" tabindex="-1"></a><span class="co"># test out generate_sample function</span></span>
<span id="cb92-11"><a href="probability-and-statistics.html#cb92-11" aria-hidden="true" tabindex="-1"></a><span class="fu">generate_sample</span>(<span class="dv">10</span>,<span class="fl">0.5</span>)</span>
<span id="cb92-12"><a href="probability-and-statistics.html#cb92-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 1 0 0 1 1 1 1 1 0 0</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="probability-and-statistics.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># carry out monte carlo simulations</span></span>
<span id="cb93-2"><a href="probability-and-statistics.html#cb93-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb93-3"><a href="probability-and-statistics.html#cb93-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb93-4"><a href="probability-and-statistics.html#cb93-4" aria-hidden="true" tabindex="-1"></a>nsims <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co"># need to pick large number of monte carlo simulations</span></span>
<span id="cb93-5"><a href="probability-and-statistics.html#cb93-5" aria-hidden="true" tabindex="-1"></a>mc_est <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimation results</span></span>
<span id="cb93-6"><a href="probability-and-statistics.html#cb93-6" aria-hidden="true" tabindex="-1"></a>mc_var <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimated variance</span></span>
<span id="cb93-7"><a href="probability-and-statistics.html#cb93-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-8"><a href="probability-and-statistics.html#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsims) {</span>
<span id="cb93-9"><a href="probability-and-statistics.html#cb93-9" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">generate_sample</span>(n,p)</span>
<span id="cb93-10"><a href="probability-and-statistics.html#cb93-10" aria-hidden="true" tabindex="-1"></a>  mc_est[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(Y)</span>
<span id="cb93-11"><a href="probability-and-statistics.html#cb93-11" aria-hidden="true" tabindex="-1"></a>  mc_var[i] <span class="ot">&lt;-</span> <span class="fu">var</span>(Y)</span>
<span id="cb93-12"><a href="probability-and-statistics.html#cb93-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb93-13"><a href="probability-and-statistics.html#cb93-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-14"><a href="probability-and-statistics.html#cb93-14" aria-hidden="true" tabindex="-1"></a><span class="co"># compute bias</span></span>
<span id="cb93-15"><a href="probability-and-statistics.html#cb93-15" aria-hidden="true" tabindex="-1"></a>bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(mc_est) <span class="sc">-</span> p</span>
<span id="cb93-16"><a href="probability-and-statistics.html#cb93-16" aria-hidden="true" tabindex="-1"></a>bias</span>
<span id="cb93-17"><a href="probability-and-statistics.html#cb93-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.0046</span></span>
<span id="cb93-18"><a href="probability-and-statistics.html#cb93-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-19"><a href="probability-and-statistics.html#cb93-19" aria-hidden="true" tabindex="-1"></a><span class="co"># compute sampling variance</span></span>
<span id="cb93-20"><a href="probability-and-statistics.html#cb93-20" aria-hidden="true" tabindex="-1"></a>var <span class="ot">&lt;-</span> <span class="fu">var</span>(mc_est)</span>
<span id="cb93-21"><a href="probability-and-statistics.html#cb93-21" aria-hidden="true" tabindex="-1"></a>var</span>
<span id="cb93-22"><a href="probability-and-statistics.html#cb93-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.0240629</span></span>
<span id="cb93-23"><a href="probability-and-statistics.html#cb93-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-24"><a href="probability-and-statistics.html#cb93-24" aria-hidden="true" tabindex="-1"></a><span class="co"># compute mean squared error</span></span>
<span id="cb93-25"><a href="probability-and-statistics.html#cb93-25" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> bias<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var</span>
<span id="cb93-26"><a href="probability-and-statistics.html#cb93-26" aria-hidden="true" tabindex="-1"></a>mse</span>
<span id="cb93-27"><a href="probability-and-statistics.html#cb93-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.02408406</span></span>
<span id="cb93-28"><a href="probability-and-statistics.html#cb93-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-29"><a href="probability-and-statistics.html#cb93-29" aria-hidden="true" tabindex="-1"></a>H0 <span class="ot">&lt;-</span> p</span>
<span id="cb93-30"><a href="probability-and-statistics.html#cb93-30" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(mc_est <span class="sc">-</span> H0) <span class="sc">/</span> <span class="fu">sqrt</span>(mc_var)</span>
<span id="cb93-31"><a href="probability-and-statistics.html#cb93-31" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">t=</span>t), <span class="fu">aes</span>(<span class="at">x=</span>t)) <span class="sc">+</span></span>
<span id="cb93-32"><a href="probability-and-statistics.html#cb93-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins=</span><span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb93-33"><a href="probability-and-statistics.html#cb93-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb93-34"><a href="probability-and-statistics.html#cb93-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Warning: Removed 2 rows containing non-finite values (stat_bin).</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="probability-and-statistics.html#cb94-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-2"><a href="probability-and-statistics.html#cb94-2" aria-hidden="true" tabindex="-1"></a>rej <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="dv">1</span><span class="sc">*</span>(<span class="fu">abs</span>(t) <span class="sc">&gt;=</span> <span class="fl">1.96</span>))</span>
<span id="cb94-3"><a href="probability-and-statistics.html#cb94-3" aria-hidden="true" tabindex="-1"></a>rej</span>
<span id="cb94-4"><a href="probability-and-statistics.html#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.107</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="probability-and-statistics.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># since we are going to do this over and over, let&#39;s write a function to do it</span></span>
<span id="cb95-2"><a href="probability-and-statistics.html#cb95-2" aria-hidden="true" tabindex="-1"></a>mc_sim <span class="ot">&lt;-</span> <span class="cf">function</span>(n, p, H0) {</span>
<span id="cb95-3"><a href="probability-and-statistics.html#cb95-3" aria-hidden="true" tabindex="-1"></a>  mc_est <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimation results</span></span>
<span id="cb95-4"><a href="probability-and-statistics.html#cb95-4" aria-hidden="true" tabindex="-1"></a>  mc_var <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimated variance</span></span>
<span id="cb95-5"><a href="probability-and-statistics.html#cb95-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-6"><a href="probability-and-statistics.html#cb95-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsims) {</span>
<span id="cb95-7"><a href="probability-and-statistics.html#cb95-7" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">&lt;-</span> <span class="fu">generate_sample</span>(n,p)</span>
<span id="cb95-8"><a href="probability-and-statistics.html#cb95-8" aria-hidden="true" tabindex="-1"></a>    mc_est[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(Y)</span>
<span id="cb95-9"><a href="probability-and-statistics.html#cb95-9" aria-hidden="true" tabindex="-1"></a>    mc_var[i] <span class="ot">&lt;-</span> <span class="fu">var</span>(Y)</span>
<span id="cb95-10"><a href="probability-and-statistics.html#cb95-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb95-11"><a href="probability-and-statistics.html#cb95-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-12"><a href="probability-and-statistics.html#cb95-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute bias</span></span>
<span id="cb95-13"><a href="probability-and-statistics.html#cb95-13" aria-hidden="true" tabindex="-1"></a>  bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(mc_est) <span class="sc">-</span> p</span>
<span id="cb95-14"><a href="probability-and-statistics.html#cb95-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-15"><a href="probability-and-statistics.html#cb95-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute sampling variance</span></span>
<span id="cb95-16"><a href="probability-and-statistics.html#cb95-16" aria-hidden="true" tabindex="-1"></a>  var <span class="ot">&lt;-</span> <span class="fu">var</span>(mc_est)</span>
<span id="cb95-17"><a href="probability-and-statistics.html#cb95-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-18"><a href="probability-and-statistics.html#cb95-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute mean squared error</span></span>
<span id="cb95-19"><a href="probability-and-statistics.html#cb95-19" aria-hidden="true" tabindex="-1"></a>  mse <span class="ot">&lt;-</span> bias<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var</span>
<span id="cb95-20"><a href="probability-and-statistics.html#cb95-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb95-21"><a href="probability-and-statistics.html#cb95-21" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(mc_est <span class="sc">-</span> H0) <span class="sc">/</span> <span class="fu">sqrt</span>(mc_var)</span>
<span id="cb95-22"><a href="probability-and-statistics.html#cb95-22" aria-hidden="true" tabindex="-1"></a>  hist_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">t=</span>t), <span class="fu">aes</span>(<span class="at">x=</span>t)) <span class="sc">+</span></span>
<span id="cb95-23"><a href="probability-and-statistics.html#cb95-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="at">bins=</span><span class="dv">30</span>) <span class="sc">+</span> </span>
<span id="cb95-24"><a href="probability-and-statistics.html#cb95-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb95-25"><a href="probability-and-statistics.html#cb95-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-26"><a href="probability-and-statistics.html#cb95-26" aria-hidden="true" tabindex="-1"></a>  rej <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="dv">1</span><span class="sc">*</span>(<span class="fu">abs</span>(t) <span class="sc">&gt;=</span> <span class="fl">1.96</span>))</span>
<span id="cb95-27"><a href="probability-and-statistics.html#cb95-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb95-28"><a href="probability-and-statistics.html#cb95-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print results</span></span>
<span id="cb95-29"><a href="probability-and-statistics.html#cb95-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;bias: &quot;</span>, <span class="fu">round</span>(bias,<span class="dv">4</span>)))</span>
<span id="cb95-30"><a href="probability-and-statistics.html#cb95-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;var : &quot;</span>, <span class="fu">round</span>(var,<span class="dv">4</span>)))</span>
<span id="cb95-31"><a href="probability-and-statistics.html#cb95-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;mse : &quot;</span>, <span class="fu">round</span>(mse,<span class="dv">4</span>)))</span>
<span id="cb95-32"><a href="probability-and-statistics.html#cb95-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(hist_plot)</span>
<span id="cb95-33"><a href="probability-and-statistics.html#cb95-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;rej : &quot;</span>, <span class="fu">round</span>(rej,<span class="dv">4</span>)))</span>
<span id="cb95-34"><a href="probability-and-statistics.html#cb95-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb95-35"><a href="probability-and-statistics.html#cb95-35" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb95-36"><a href="probability-and-statistics.html#cb95-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 0.0014&quot;</span></span>
<span id="cb95-37"><a href="probability-and-statistics.html#cb95-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.0056&quot;</span></span>
<span id="cb95-38"><a href="probability-and-statistics.html#cb95-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.0056&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.085&quot;</code></pre>
<ol start="5" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="probability-and-statistics.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>)</span>
<span id="cb97-2"><a href="probability-and-statistics.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: -0.0015&quot;</span></span>
<span id="cb97-3"><a href="probability-and-statistics.html#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.0048&quot;</span></span>
<span id="cb97-4"><a href="probability-and-statistics.html#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.0048&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-105-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.339&quot;</code></pre>
<ol start="6" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="probability-and-statistics.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>)</span>
<span id="cb99-2"><a href="probability-and-statistics.html#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 0.0011&quot;</span></span>
<span id="cb99-3"><a href="probability-and-statistics.html#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.005&quot;</span></span>
<span id="cb99-4"><a href="probability-and-statistics.html#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.005&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 1&quot;</code></pre>
<ol start="7" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="probability-and-statistics.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">1000</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>)</span>
<span id="cb101-2"><a href="probability-and-statistics.html#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 4e-04&quot;</span></span>
<span id="cb101-3"><a href="probability-and-statistics.html#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 2e-04&quot;</span></span>
<span id="cb101-4"><a href="probability-and-statistics.html#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 2e-04&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 1&quot;</code></pre>
<ol start="8" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="probability-and-statistics.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">10</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>)</span>
<span id="cb103-2"><a href="probability-and-statistics.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: -0.0058&quot;</span></span>
<span id="cb103-3"><a href="probability-and-statistics.html#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.005&quot;</span></span>
<span id="cb103-4"><a href="probability-and-statistics.html#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.005&quot;</span></span>
<span id="cb103-5"><a href="probability-and-statistics.html#cb103-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Warning: Removed 555 rows containing non-finite values (stat_bin).</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.556&quot;</code></pre>
<ol start="9" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="probability-and-statistics.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>)</span>
<span id="cb105-2"><a href="probability-and-statistics.html#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: -0.001&quot;</span></span>
<span id="cb105-3"><a href="probability-and-statistics.html#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 9e-04&quot;</span></span>
<span id="cb105-4"><a href="probability-and-statistics.html#cb105-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 9e-04&quot;</span></span>
<span id="cb105-5"><a href="probability-and-statistics.html#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Warning: Removed 59 rows containing non-finite values (stat_bin).</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.064&quot;</code></pre>
<ol start="10" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="probability-and-statistics.html#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">1000</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>)</span>
<span id="cb107-2"><a href="probability-and-statistics.html#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 1e-04&quot;</span></span>
<span id="cb107-3"><a href="probability-and-statistics.html#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0&quot;</span></span>
<span id="cb107-4"><a href="probability-and-statistics.html#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.057&quot;</code></pre>
</div>
<div id="coding-questions" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Coding Questions</h2>
<ol style="list-style-type: decimal">
<li><p>Run the following code to create the data that we will use in the problem</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="probability-and-statistics.html#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>) <span class="co"># setting the seed means that we will get the same results</span></span>
<span id="cb109-2"><a href="probability-and-statistics.html#cb109-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rexp</span>(<span class="dv">100</span>) <span class="co"># make 100 draws from an exponential distribution</span></span></code></pre></div>
<p>Use the <code>ggplot2</code> package to plot a histogram of <code>x</code>.</p></li>
<li><p>For this question, we’ll use the data <code>fertilizer_2000</code>. A scatter plot is a useful way to visualize 2-dimensional data. Use the <code>ggplot2</code> package to make a scatter plot with crop yield (<code>avyield</code>) on the y-axis and fertilizer (<code>avfert</code>) on the x-axis. Label the y-axis “Crop Yield” and the x-axis “Fertilizer.” Do you notice any pattern from the scatter plot?</p></li>
<li><p>For this question, we’ll use the data <code>Airq</code>. The variable <code>rain</code> contains the amount of rainfall in the county in a year (in inches). For this question, we’ll be interested in testing whether or not the mean rainfall across counties in California is 25 inches.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimate the mean rainfall across counties.</p></li>
<li><p>Calculate the standard error of your estimate of rainfall.</p></li>
<li><p>Calculate a t-statistic for <span class="math inline">\(H_0 : \mathbb{E}[Y] = 25\)</span> where <span class="math inline">\(Y\)</span> denotes rainfall. Do you reject <span class="math inline">\(H_0\)</span> at a 5% significance level? Explain.</p></li>
<li><p>Calculate a p-value for <span class="math inline">\(H_0: \mathbb{E}[Y] = 25\)</span>. How should you interpret this?</p></li>
<li><p>Calculate a 95% confidence interval for average rainfall.</p></li>
<li><p>Use the <code>datasummary_balance</code> function from the <code>modelsummary</code> package to report average air quality, value added, rain, population density, and average income, separately by whether or not the county is located in a coastal area.</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Extra Questions</h2>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(\mathbb{E}[X] = 10\)</span> and <span class="math inline">\(\mathrm{var}(X) = 2\)</span>. Also, suppose that <span class="math inline">\(Y=5 + 9 X\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What is <span class="math inline">\(\mathbb{E}[Y]\)</span>?</p></li>
<li><p>What is <span class="math inline">\(\mathrm{var}(Y)\)</span>?</p></li>
</ol></li>
<li><p>Use the definition of variance to show that <span class="math inline">\(\mathrm{var}(bX) = b^2 \mathrm{var}(X)\)</span> (where <span class="math inline">\(b\)</span> is a constant and <span class="math inline">\(X\)</span> is some random variable).</p></li>
<li><p>Suppose you are interested in average height of students at UGA. Let <span class="math inline">\(Y\)</span> denote a student’s height; also let <span class="math inline">\(X\)</span> denote a binary variable that is equal to 1 if a student is female. Suppose that you know that <span class="math inline">\(\mathbb{E}[Y|X=1] = 5\&#39; \ 4\&quot;\)</span> and that <span class="math inline">\(\mathbb{E}[Y|X=0] = 5\&#39; \ 9\&quot;\)</span> (and that <span class="math inline">\(\mathrm{P}(X=1) = 0.5\)</span>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>What is <span class="math inline">\(\mathbb{E}[Y]\)</span>?</p></li>
<li><p>Explain how the answer to part (a) is related to the Law of Iterated Expectations.</p></li>
</ol></li>
<li><p>Consider a random variable <span class="math inline">\(X\)</span> with support <span class="math inline">\(\mathcal{X} = \{2,7,13,21\}\)</span>. Suppose that it has the following pmf:</p>
<p><span class="math display">\[
    \begin{aligned}
     f_X(2) &amp;= 0.5 \\
     f_X(7) &amp;= 0.25 \\
     f_X(13) &amp;= 0.15 \\
     f_X(21) &amp;= ??
   \end{aligned}
 \]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>What is <span class="math inline">\(f_X(21)\)</span>? How do you know?</p></li>
<li><p>What is the expected value of <span class="math inline">\(X\)</span>? [Show your calculation.]</p></li>
<li><p>What is the variance of <span class="math inline">\(X\)</span>? [Show your calculation.]</p></li>
<li><p>Calculate <span class="math inline">\(F_X(x)\)</span> for <span class="math inline">\(x=1\)</span>, <span class="math inline">\(x=7\)</span>, <span class="math inline">\(x=8\)</span>, and <span class="math inline">\(x=25\)</span>.</p></li>
</ol></li>
<li><p>What is the difference between consistency and unbiasedness?</p></li>
<li><p>Suppose you have an estimator that is unbiased. Will it necessarily be consistent? If not, provide an example of an unbiased estimator that is not consistent.</p></li>
<li><p>Suppose you have an estimator that is consistent. Will it necessarily be unbiased? If not, provide an example of a consistent estimator that is not unbiased.</p></li>
<li><p>The Central Limit Theorem says that, <span class="math inline">\(\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbb{E}[Y])\right) \rightarrow N(0,V)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> where <span class="math inline">\(V = \mathrm{var}(Y)\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What happens to <span class="math inline">\(n \left(\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbb{E}[Y])\right)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>? Explain.</p></li>
<li><p>What happens to <span class="math inline">\(n^{1/3} \left(\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbb{E}[Y])\right)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>? Explain.</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-programming.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Detailed Course Notes.pdf", "Detailed Course Notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
