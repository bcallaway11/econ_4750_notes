<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Supplementary Notes and References for ECON 4750</title>
  <meta name="description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Supplementary Notes and References for ECON 4750" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Supplementary Notes and References for ECON 4750" />
  
  <meta name="twitter:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

<meta name="author" content="Brantly Callaway" />


<meta name="date" content="2023-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Outline/Notes for ECON 4750</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#what-is-this"><i class="fa fa-check"></i><b>1.1</b> What is this?</a></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#what-is-this-not"><i class="fa fa-check"></i><b>1.2</b> What is this not?</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#why-did-i-write-this"><i class="fa fa-check"></i><b>1.3</b> Why did I write this?</a></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#additional-references"><i class="fa fa-check"></i><b>1.4</b> Additional References</a></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#goals-for-the-course"><i class="fa fa-check"></i><b>1.5</b> Goals for the Course</a></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#studying-for-the-class"><i class="fa fa-check"></i><b>1.6</b> Studying for the Class</a></li>
<li class="chapter" data-level="1.7" data-path=""><a href="#data-used-in-the-course"><i class="fa fa-check"></i><b>1.7</b> Data Used in the Course</a></li>
<li class="chapter" data-level="1.8" data-path=""><a href="#first-week-of-class"><i class="fa fa-check"></i><b>1.8</b> First Week of Class</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#statistical-programming"><i class="fa fa-check"></i><b>2</b> Statistical Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path=""><a href="#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up R</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path=""><a href="#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path=""><a href="#downloading-r"><i class="fa fa-check"></i><b>2.1.2</b> Downloading R</a></li>
<li class="chapter" data-level="2.1.3" data-path=""><a href="#rstudio"><i class="fa fa-check"></i><b>2.1.3</b> RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path=""><a href="#rstudio-development-environment"><i class="fa fa-check"></i><b>2.1.4</b> RStudio Development Environment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#installing-r-packages"><i class="fa fa-check"></i><b>2.2</b> Installing R Packages</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path=""><a href="#a-list-of-useful-r-packages"><i class="fa fa-check"></i><b>2.2.1</b> A list of useful R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#r-basics"><i class="fa fa-check"></i><b>2.3</b> R Basics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path=""><a href="#objects"><i class="fa fa-check"></i><b>2.3.1</b> Objects</a></li>
<li class="chapter" data-level="2.3.2" data-path=""><a href="#workspace"><i class="fa fa-check"></i><b>2.3.2</b> Workspace</a></li>
<li class="chapter" data-level="2.3.3" data-path=""><a href="#importing-data"><i class="fa fa-check"></i><b>2.3.3</b> Importing Data</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path=""><a href="#functions-in-r"><i class="fa fa-check"></i><b>2.4</b> Functions in R</a></li>
<li class="chapter" data-level="2.5" data-path=""><a href="#data-types"><i class="fa fa-check"></i><b>2.5</b> Data types</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path=""><a href="#numeric-vectors"><i class="fa fa-check"></i><b>2.5.1</b> Numeric Vectors</a></li>
<li class="chapter" data-level="2.5.2" data-path=""><a href="#vector-arithmetic"><i class="fa fa-check"></i><b>2.5.2</b> Vector arithmetic</a></li>
<li class="chapter" data-level="2.5.3" data-path=""><a href="#more-helpful-functions-in-r"><i class="fa fa-check"></i><b>2.5.3</b> More helpful functions in R</a></li>
<li class="chapter" data-level="2.5.4" data-path=""><a href="#other-types-of-vectors"><i class="fa fa-check"></i><b>2.5.4</b> Other types of vectors</a></li>
<li class="chapter" data-level="2.5.5" data-path=""><a href="#data-frames"><i class="fa fa-check"></i><b>2.5.5</b> Data Frames</a></li>
<li class="chapter" data-level="2.5.6" data-path=""><a href="#lists"><i class="fa fa-check"></i><b>2.5.6</b> Lists</a></li>
<li class="chapter" data-level="2.5.7" data-path=""><a href="#matrices"><i class="fa fa-check"></i><b>2.5.7</b> Matrices</a></li>
<li class="chapter" data-level="2.5.8" data-path=""><a href="#factors"><i class="fa fa-check"></i><b>2.5.8</b> Factors</a></li>
<li class="chapter" data-level="2.5.9" data-path=""><a href="#understanding-an-object-in-r"><i class="fa fa-check"></i><b>2.5.9</b> Understanding an object in R</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path=""><a href="#logicals"><i class="fa fa-check"></i><b>2.6</b> Logicals</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path=""><a href="#additional-logical-operators"><i class="fa fa-check"></i><b>2.6.1</b> Additional Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path=""><a href="#programming-basics"><i class="fa fa-check"></i><b>2.7</b> Programming basics</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path=""><a href="#writing-functions"><i class="fa fa-check"></i><b>2.7.1</b> Writing functions</a></li>
<li class="chapter" data-level="2.7.2" data-path=""><a href="#ifelse"><i class="fa fa-check"></i><b>2.7.2</b> if/else</a></li>
<li class="chapter" data-level="2.7.3" data-path=""><a href="#for-loops"><i class="fa fa-check"></i><b>2.7.3</b> for loops</a></li>
<li class="chapter" data-level="2.7.4" data-path=""><a href="#vectorization"><i class="fa fa-check"></i><b>2.7.4</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path=""><a href="#advanced-topics"><i class="fa fa-check"></i><b>2.8</b> Advanced Topics</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path=""><a href="#tidyverse"><i class="fa fa-check"></i><b>2.8.1</b> Tidyverse</a></li>
<li class="chapter" data-level="2.8.2" data-path=""><a href="#data-visualization"><i class="fa fa-check"></i><b>2.8.2</b> Data Visualization</a></li>
<li class="chapter" data-level="2.8.3" data-path=""><a href="#reproducible-research"><i class="fa fa-check"></i><b>2.8.3</b> Reproducible Research</a></li>
<li class="chapter" data-level="2.8.4" data-path=""><a href="#technical-writing-tools"><i class="fa fa-check"></i><b>2.8.4</b> Technical Writing Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path=""><a href="#lab-1-introduction-to-r-programming"><i class="fa fa-check"></i><b>2.9</b> Lab 1: Introduction to R Programming</a></li>
<li class="chapter" data-level="2.10" data-path=""><a href="#lab-1-solutions"><i class="fa fa-check"></i><b>2.10</b> Lab 1: Solutions</a></li>
<li class="chapter" data-level="2.11" data-path=""><a href="#coding-exercises"><i class="fa fa-check"></i><b>2.11</b> Coding Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#probability"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#data-for-this-chapter"><i class="fa fa-check"></i><b>3.1</b> Data for this chapter</a></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#random-variables"><i class="fa fa-check"></i><b>3.2</b> Random Variables</a></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#pdfs-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.3</b> pdfs, pmfs, and cdfs</a></li>
<li class="chapter" data-level="3.4" data-path=""><a href="#summation-operator"><i class="fa fa-check"></i><b>3.4</b> Summation operator</a></li>
<li class="chapter" data-level="3.5" data-path=""><a href="#properties-of-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.5</b> Properties of pmfs and cdfs</a></li>
<li class="chapter" data-level="3.6" data-path=""><a href="#continuous-random-variables"><i class="fa fa-check"></i><b>3.6</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="3.7" data-path=""><a href="#expected-values"><i class="fa fa-check"></i><b>3.7</b> Expected Values</a></li>
<li class="chapter" data-level="3.8" data-path=""><a href="#variance"><i class="fa fa-check"></i><b>3.8</b> Variance</a></li>
<li class="chapter" data-level="3.9" data-path=""><a href="#mean-and-variance-of-linear-functions"><i class="fa fa-check"></i><b>3.9</b> Mean and Variance of Linear Functions</a></li>
<li class="chapter" data-level="3.10" data-path=""><a href="#multiple-random-variables"><i class="fa fa-check"></i><b>3.10</b> Multiple Random Variables</a></li>
<li class="chapter" data-level="3.11" data-path=""><a href="#conditional-expectations"><i class="fa fa-check"></i><b>3.11</b> Conditional Expectations</a></li>
<li class="chapter" data-level="3.12" data-path=""><a href="#law-of-iterated-expectations"><i class="fa fa-check"></i><b>3.12</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="3.13" data-path=""><a href="#covariance"><i class="fa fa-check"></i><b>3.13</b> Covariance</a></li>
<li class="chapter" data-level="3.14" data-path=""><a href="#correlation"><i class="fa fa-check"></i><b>3.14</b> Correlation</a></li>
<li class="chapter" data-level="3.15" data-path=""><a href="#properties-of-expectationsvariances-of-sums-of-rvs"><i class="fa fa-check"></i><b>3.15</b> Properties of Expectations/Variances of Sums of RVs</a></li>
<li class="chapter" data-level="3.16" data-path=""><a href="#normal-distribution"><i class="fa fa-check"></i><b>3.16</b> Normal Distribution</a></li>
<li class="chapter" data-level="3.17" data-path=""><a href="#coding"><i class="fa fa-check"></i><b>3.17</b> Coding</a></li>
<li class="chapter" data-level="3.18" data-path=""><a href="#lab-2-basic-plots"><i class="fa fa-check"></i><b>3.18</b> Lab 2: Basic Plots</a></li>
<li class="chapter" data-level="3.19" data-path=""><a href="#coding-questions"><i class="fa fa-check"></i><b>3.19</b> Coding Questions</a></li>
<li class="chapter" data-level="3.20" data-path=""><a href="#extra-questions"><i class="fa fa-check"></i><b>3.20</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#properties-of-estimators"><i class="fa fa-check"></i><b>4</b> Properties of Estimators</a>
<ul>
<li class="chapter" data-level="4.1" data-path=""><a href="#simple-random-sample"><i class="fa fa-check"></i><b>4.1</b> Simple Random Sample</a></li>
<li class="chapter" data-level="4.2" data-path=""><a href="#estimating-mathbbey"><i class="fa fa-check"></i><b>4.2</b> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span></a></li>
<li class="chapter" data-level="4.3" data-path=""><a href="#mean-of-bary"><i class="fa fa-check"></i><b>4.3</b> Mean of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="4.4" data-path=""><a href="#variance-of-bary"><i class="fa fa-check"></i><b>4.4</b> Variance of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="4.5" data-path=""><a href="#properties-of-estimators-1"><i class="fa fa-check"></i><b>4.5</b> Properties of Estimators</a></li>
<li class="chapter" data-level="4.6" data-path=""><a href="#relative-efficiency"><i class="fa fa-check"></i><b>4.6</b> Relative Efficiency</a></li>
<li class="chapter" data-level="4.7" data-path=""><a href="#mean-squared-error"><i class="fa fa-check"></i><b>4.7</b> Mean Squared Error</a></li>
<li class="chapter" data-level="4.8" data-path=""><a href="#large-sample-properties-of-estimators"><i class="fa fa-check"></i><b>4.8</b> Large Sample Properties of Estimators</a></li>
<li class="chapter" data-level="4.9" data-path=""><a href="#consistency"><i class="fa fa-check"></i><b>4.9</b> Consistency</a></li>
<li class="chapter" data-level="4.10" data-path=""><a href="#asymptotic-normality"><i class="fa fa-check"></i><b>4.10</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.11" data-path=""><a href="#inference-hypothesis-testing"><i class="fa fa-check"></i><b>4.11</b> Inference / Hypothesis Testing</a></li>
<li class="chapter" data-level="4.12" data-path=""><a href="#t-statistics"><i class="fa fa-check"></i><b>4.12</b> t-statistics</a></li>
<li class="chapter" data-level="4.13" data-path=""><a href="#p-values"><i class="fa fa-check"></i><b>4.13</b> P-values</a></li>
<li class="chapter" data-level="4.14" data-path=""><a href="#confidence-interval"><i class="fa fa-check"></i><b>4.14</b> Confidence Interval</a></li>
<li class="chapter" data-level="4.15" data-path=""><a href="#inference-in-practice"><i class="fa fa-check"></i><b>4.15</b> Inference in Practice</a></li>
<li class="chapter" data-level="4.16" data-path=""><a href="#coding-1"><i class="fa fa-check"></i><b>4.16</b> Coding</a></li>
<li class="chapter" data-level="4.17" data-path=""><a href="#lab-3-monte-carlo-simulations"><i class="fa fa-check"></i><b>4.17</b> Lab 3: Monte Carlo Simulations</a></li>
<li class="chapter" data-level="4.18" data-path=""><a href="#lab-3-solutions"><i class="fa fa-check"></i><b>4.18</b> Lab 3 Solutions</a></li>
<li class="chapter" data-level="4.19" data-path=""><a href="#coding-questions-1"><i class="fa fa-check"></i><b>4.19</b> Coding Questions</a></li>
<li class="chapter" data-level="4.20" data-path=""><a href="#extra-questions-1"><i class="fa fa-check"></i><b>4.20</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path=""><a href="#linear-regression"><i class="fa fa-check"></i><b>5</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path=""><a href="#nonparametric-regression-curse-of-dimensionality"><i class="fa fa-check"></i><b>5.1</b> Nonparametric Regression / Curse of Dimensionality</a></li>
<li class="chapter" data-level="5.2" data-path=""><a href="#linear-regression-models"><i class="fa fa-check"></i><b>5.2</b> Linear Regression Models</a></li>
<li class="chapter" data-level="5.3" data-path=""><a href="#computation"><i class="fa fa-check"></i><b>5.3</b> Computation</a></li>
<li class="chapter" data-level="5.4" data-path=""><a href="#partial-effects"><i class="fa fa-check"></i><b>5.4</b> Partial Effects</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path=""><a href="#computation-1"><i class="fa fa-check"></i><b>5.4.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path=""><a href="#binary-regressors"><i class="fa fa-check"></i><b>5.5</b> Binary Regressors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path=""><a href="#computation-2"><i class="fa fa-check"></i><b>5.5.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path=""><a href="#nonlinear-regression-functions"><i class="fa fa-check"></i><b>5.6</b> Nonlinear Regression Functions</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path=""><a href="#computation-3"><i class="fa fa-check"></i><b>5.6.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path=""><a href="#interpreting-interaction-terms"><i class="fa fa-check"></i><b>5.7</b> Interpreting Interaction Terms</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path=""><a href="#computation-4"><i class="fa fa-check"></i><b>5.7.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path=""><a href="#elasticities"><i class="fa fa-check"></i><b>5.8</b> Elasticities</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path=""><a href="#computation-5"><i class="fa fa-check"></i><b>5.8.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path=""><a href="#omitted-variable-bias"><i class="fa fa-check"></i><b>5.9</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="5.10" data-path=""><a href="#how-to-estimate-the-parameters-in-a-regression-model"><i class="fa fa-check"></i><b>5.10</b> How to estimate the parameters in a regression model</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path=""><a href="#computation-6"><i class="fa fa-check"></i><b>5.10.1</b> Computation</a></li>
<li class="chapter" data-level="5.10.2" data-path=""><a href="#more-than-one-regressor"><i class="fa fa-check"></i><b>5.10.2</b> More than one regressor</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path=""><a href="#inference"><i class="fa fa-check"></i><b>5.11</b> Inference</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path=""><a href="#computation-7"><i class="fa fa-check"></i><b>5.11.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path=""><a href="#lab-4-birthweight-and-smoking"><i class="fa fa-check"></i><b>5.12</b> Lab 4: Birthweight and Smoking</a></li>
<li class="chapter" data-level="5.13" data-path=""><a href="#lab-4-solutions"><i class="fa fa-check"></i><b>5.13</b> Lab 4: Solutions</a></li>
<li class="chapter" data-level="5.14" data-path=""><a href="#coding-questions-2"><i class="fa fa-check"></i><b>5.14</b> Coding Questions</a></li>
<li class="chapter" data-level="5.15" data-path=""><a href="#extra-questions-2"><i class="fa fa-check"></i><b>5.15</b> Extra Questions</a></li>
<li class="chapter" data-level="5.16" data-path=""><a href="#answers-to-some-extra-questions"><i class="fa fa-check"></i><b>5.16</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path=""><a href="#prediction"><i class="fa fa-check"></i><b>6</b> Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path=""><a href="#measures-of-regression-fit"><i class="fa fa-check"></i><b>6.1</b> Measures of Regression Fit</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path=""><a href="#tss-ess-ssr"><i class="fa fa-check"></i><b>6.1.1</b> TSS, ESS, SSR</a></li>
<li class="chapter" data-level="6.1.2" data-path=""><a href="#r2"><i class="fa fa-check"></i><b>6.1.2</b> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path=""><a href="#model-selection"><i class="fa fa-check"></i><b>6.2</b> Model Selection</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path=""><a href="#limitations-of-r2"><i class="fa fa-check"></i><b>6.2.1</b> Limitations of <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path=""><a href="#adjusted-r2"><i class="fa fa-check"></i><b>6.2.2</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.2.3" data-path=""><a href="#aic-bic"><i class="fa fa-check"></i><b>6.2.3</b> AIC, BIC</a></li>
<li class="chapter" data-level="6.2.4" data-path=""><a href="#cross-validation"><i class="fa fa-check"></i><b>6.2.4</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.5" data-path=""><a href="#model-averaging"><i class="fa fa-check"></i><b>6.2.5</b> Model Averaging</a></li>
<li class="chapter" data-level="6.2.6" data-path=""><a href="#computation-8"><i class="fa fa-check"></i><b>6.2.6</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path=""><a href="#machine-learning"><i class="fa fa-check"></i><b>6.3</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path=""><a href="#lasso"><i class="fa fa-check"></i><b>6.3.1</b> Lasso</a></li>
<li class="chapter" data-level="6.3.2" data-path=""><a href="#ridge-regression"><i class="fa fa-check"></i><b>6.3.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.3.3" data-path=""><a href="#computation-9"><i class="fa fa-check"></i><b>6.3.3</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path=""><a href="#lab-5-predicting-diamond-prices"><i class="fa fa-check"></i><b>6.4</b> Lab 5: Predicting Diamond Prices</a></li>
<li class="chapter" data-level="6.5" data-path=""><a href="#lab-5-solutions"><i class="fa fa-check"></i><b>6.5</b> Lab 5: Solutions</a></li>
<li class="chapter" data-level="6.6" data-path=""><a href="#extra-questions-3"><i class="fa fa-check"></i><b>6.6</b> Extra Questions</a></li>
<li class="chapter" data-level="6.7" data-path=""><a href="#answers-to-some-extra-questions-1"><i class="fa fa-check"></i><b>6.7</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path=""><a href="#binary-outcome-models"><i class="fa fa-check"></i><b>7</b> Binary Outcome Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path=""><a href="#linear-probability-model"><i class="fa fa-check"></i><b>7.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.2" data-path=""><a href="#probit-and-logit"><i class="fa fa-check"></i><b>7.2</b> Probit and Logit</a></li>
<li class="chapter" data-level="7.3" data-path=""><a href="#average-partial-effects"><i class="fa fa-check"></i><b>7.3</b> Average Partial Effects</a></li>
<li class="chapter" data-level="7.4" data-path=""><a href="#lab-6-estimating-binary-outcome-models"><i class="fa fa-check"></i><b>7.4</b> Lab 6: Estimating Binary Outcome Models</a></li>
<li class="chapter" data-level="7.5" data-path=""><a href="#coding-questions-3"><i class="fa fa-check"></i><b>7.5</b> Coding Questions</a></li>
<li class="chapter" data-level="7.6" data-path=""><a href="#extra-questions-4"><i class="fa fa-check"></i><b>7.6</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path=""><a href="#causal-inference"><i class="fa fa-check"></i><b>8</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path=""><a href="#potential-outcomes"><i class="fa fa-check"></i><b>8.1</b> Potential Outcomes</a></li>
<li class="chapter" data-level="8.2" data-path=""><a href="#parameters-of-interest"><i class="fa fa-check"></i><b>8.2</b> Parameters of Interest</a></li>
<li class="chapter" data-level="8.3" data-path=""><a href="#experiments"><i class="fa fa-check"></i><b>8.3</b> Experiments</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path=""><a href="#estimating-att-with-a-regression"><i class="fa fa-check"></i><b>8.3.1</b> Estimating ATT with a Regression</a></li>
<li class="chapter" data-level="8.3.2" data-path=""><a href="#internal-and-external-validity"><i class="fa fa-check"></i><b>8.3.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="8.3.3" data-path=""><a href="#example-project-star"><i class="fa fa-check"></i><b>8.3.3</b> Example: Project STAR</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path=""><a href="#unconfoundedness"><i class="fa fa-check"></i><b>8.4</b> Unconfoundedness</a></li>
<li class="chapter" data-level="8.5" data-path=""><a href="#panel-data-approaches"><i class="fa fa-check"></i><b>8.5</b> Panel Data Approaches</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path=""><a href="#difference-in-differences"><i class="fa fa-check"></i><b>8.5.1</b> Difference in differences</a></li>
<li class="chapter" data-level="8.5.2" data-path=""><a href="#computation-10"><i class="fa fa-check"></i><b>8.5.2</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path=""><a href="#instrumental-variables"><i class="fa fa-check"></i><b>8.6</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path=""><a href="#example-return-to-education"><i class="fa fa-check"></i><b>8.6.1</b> Example: Return to Education</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path=""><a href="#regression-discontinuity"><i class="fa fa-check"></i><b>8.7</b> Regression Discontinuity</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path=""><a href="#example-causal-effect-of-alcohol-on-driving-deaths"><i class="fa fa-check"></i><b>8.7.1</b> Example: Causal effect of Alcohol on Driving Deaths</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path=""><a href="#lab-7-drunk-driving-laws"><i class="fa fa-check"></i><b>8.8</b> Lab 7: Drunk Driving Laws</a></li>
<li class="chapter" data-level="8.9" data-path=""><a href="#lab-7-solutions"><i class="fa fa-check"></i><b>8.9</b> Lab 7: Solutions</a></li>
<li class="chapter" data-level="8.10" data-path=""><a href="#coding-questions-4"><i class="fa fa-check"></i><b>8.10</b> Coding Questions</a></li>
<li class="chapter" data-level="8.11" data-path=""><a href="#extra-questions-5"><i class="fa fa-check"></i><b>8.11</b> Extra Questions</a></li>
<li class="chapter" data-level="8.12" data-path=""><a href="#answers-to-some-extra-questions-2"><i class="fa fa-check"></i><b>8.12</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supplementary Notes and References for ECON 4750</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Supplementary Notes and References for ECON 4750</h1>
<p class="author"><em>Brantly Callaway</em></p>
<p class="date"><em>2023-09-06</em></p>
</div>
<div id="introduction" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Topic 1</span> Introduction<a href="#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>These are a set of detailed course notes for ECON 4750 at UGA. They largely come from my personal notes that I have used to teach this course in previous years, which, in turn, are largely based on the <a href="https://www.amazon.com/Introduction-Econometrics-4th-Pearson-Economics/dp/0134461991/ref=sr_1_1?dchild=1&amp;keywords=stock+and+watson&amp;qid=1618424276&amp;s=books&amp;sr=1-1">Stock and Watson</a> textbook serves as the other main reference for the course.</p>
<p>These notes have improved drastically over the course of several years. However, they are a work in progress and have not been professionally edited or anything like that…so it is possible that there are some mistakes (if you find any, please let me know). That said, I hope you will find this to be a useful resource this semester.</p>
<div id="what-is-this" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> What is this?<a href="#what-is-this" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous years, I used to provide these notes as supplementary material for the textbook. Now, I think that there is enough detail here that it can be used as a main reference for the course. And, for example, there are several topics that we will cover in class in substantially more detail than in the textbook.</p>
<p>The material provided here can also be useful as a quasi-study guide. In particular, the notes cover pretty much exactly what we will be able to cover in a one semester course. In some ways, covering less material relative to the textbook can (in my view) make things easier for students. The notes also provide cross-references to the corresponding section in the textbook. For some topics here, I provide substantially less detail than the book; this may help you when it comes to studying as it may give you a sense of the material I see as being most important. In addition, there are additional practice questions (some with answers) provided at the end of each section.</p>
</div>
<div id="what-is-this-not" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> What is this not?<a href="#what-is-this-not" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are a couple of things that I want to explicitly say that this material is not. These include:</p>
<ul>
<li><p>A substitute for coming to class — I will take attendance anyway, but not attending class and relying on this material is not a good plan. Please do not do this.</p></li>
<li><p>A full substitute for the textbook — In a number of places, the textbook contains substantially more details than I am able to provide here. In my experience, it is also useful to have different “voices” that you can refer to (e.g., you may find my explanation unclear but may find the textbook much clearer). The textbook also contains substantially more material than what I provide in these notes. If there are additional topics in the textbook that we do not cover that you would like to learn about, these should all be understandable for you by the end of this semester.</p></li>
<li><p>Sufficient for making good grades on exams — I don’t suspect that it will be a good strategy to rely exclusively on the material provided here in order to do well on the exams. I think this material should be helpful and perhaps even a good starting point when it comes to studying, but it is not sufficient for making a good grade in the class.</p></li>
</ul>
</div>
<div id="why-did-i-write-this" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Why did I write this?<a href="#why-did-i-write-this" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I have a strong opinion about the best order to teach the material in ECON 4750. And, although, there are a number of advanced undergraduate textbooks in Econometrics that I like and reference as I teach the course, none of them go in the same order that I would like to teach. Therefore, I think one way that I can both go in the order that I want to during the semester without confusing you (the student) too much is to provide a detailed set of references to material that we are covering throughout the semester.</p>
<p>The book that I strongly suggest for the course is <a href="https://www.amazon.com/Introduction-Econometrics-4th-Pearson-Economics/dp/0134461991/ref=sr_1_1?dchild=1&amp;keywords=stock+and+watson&amp;qid=1618424276&amp;s=books&amp;sr=1-1">Stock and Watson</a>. <!--I think this is a really great book, but, unfortunately, it is quite expensive --- at the moment, it costs \$172 on Amazon.  My longer term goal for these notes is to make it possible to, say, buy an older edition of this sort of textbook while still being able to follow the course, do the homework problems, etc.  At the moment, I am not there yet; I don't think it is possible to complete the course without buying the textbook, but hopefully, this will be available over the coming years.  For this reason too, I also sincerely welcome and appreciate comments or suggestions about these notes.-->
By the end of the semester, we will have covered in much detail the first 14 chapters of the textbook (and some topics we will have covered in substantially more detail than in the textbook). If I had more time, the next topic that I would cover in this course would be Time Series Econometrics. I used to cover this material in the current course, but I have found that I am happier with the tradeoff of understanding slightly fewer topics better relative to covering more topics but at a faster pace. Time series is especially important for students who are interested in macroeconomics or finance. Fortunately, we offer a time series econometrics course, and, if you take it, you should be well prepared for it coming out of this course.</p>
</div>
<div id="additional-references" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Additional References<a href="#additional-references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>These are all free to download; they are not main textbooks but I sometimes consult them for the class and could potentially be useful for you to consult in the future:</p>
<ol style="list-style-type: decimal">
<li><p>For R programming: <a href="https://www.econometrics-with-r.org/">Introduction to Econometrics</a> with R, by Cristoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer</p></li>
<li><p>For prediction/machine learning: <a href="https://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning</a>, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani</p></li>
<li><p>For causal inference: <a href="http://scunning.com/mixtape.html">Causal Inference: The Mixtape</a>, by Scott Cunningham</p></li>
</ol>
<p><strong>Additional R References:</strong></p>
<p>There are tons of free R resources available online. Here are some that seem particularly useful to me.</p>
<ol style="list-style-type: decimal">
<li><p>Manageable Introduction: <a href="https://sjspielman.github.io/datascience_for_biologists/tutorials/introduction_to_R.html">Introduction to R and RStudio</a>, by Stephanie Spielman</p></li>
<li><p>Full length book: <a href="(https://rafalab.github.io/dsbook/)">Introduction to Data Science: Data Analysis and Prediction Algorithms with R</a>, by Rafael Irizarry (this is way more than you will need for this course, but I suggest checking out Chapters 1, 2, 3, and 5, and there’s plenty more that you might find interesting).</p></li>
<li><p>Full length book: <a href="https://stat545.com/">STAT 545: Data Wrangling, exploration, and analysis with R</a>, by Jenny Bryan</p></li>
</ol>
</div>
<div id="goals-for-the-course" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Goals for the Course<a href="#goals-for-the-course" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I have three high level goals — that by the end of the semester, students should be able to</p>
<ol style="list-style-type: decimal">
<li><p>Run regressions and be able to interpret them (this includes even complex regressions) and also be able to think through which regression you <em>ought</em> to run</p></li>
<li><p>Use data in order to be able to predict outcomes of interest</p></li>
<li><p>Be able to think clearly about when statistical results can be interpreted as causal effects.</p></li>
</ol>
<p>In order to make progress towards these three goals, we also will need to learn about two additional topics:</p>
<ul>
<li><p>Statistical Programming</p></li>
<li><p>Probability and Statistics</p></li>
</ul>
<p>We’ll start the course off talking about these two topics. Perhaps this will be review material for some of you, but I have found that it is worth it to spend several weeks getting everyone on the same page with respect to these topics.</p>
</div>
<div id="studying-for-the-class" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Studying for the Class<a href="#studying-for-the-class" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Students ask me all the time “How should I study for your class?” My advice (and I think this applies to most classes, not just my class) is for you to start by studying the notes from class. The things that I have discussed in class are the things that I think are most important for you learn in this sort of class and are the material that will be covered on the exam. That said, it may sometimes be the case that you do not fully understand a lecture or the notes that you took from a lecture when you are studying (this certainly applied to me when I was a student). If there are places that you do not understand what the notes mean, then I think that is the time when you should find the relevant portion of the textbook (or supplementary notes provided here) in order to “supplement” what the notes say.</p>
</div>
<div id="data-used-in-the-course" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Data Used in the Course<a href="#data-used-in-the-course" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following provides the full list of data that we will use this semester. Some of the links below are to data posted on my website; others point to data hosted externally (if you notice any broken links, please let me know).</p>
<ul>
<li><p><code>acs</code></p>
<ul>
<li><p>Data from the 2019 American Community Survey about Education and Earnings</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>Airq</code></p>
<ul>
<li><p>Data about air quality in California counties in 1972.</p></li>
<li><p>Access: `data(Airq, package=“Ecdat”)</p></li>
<li><p>Description: <code>?Ecdat::Airq</code></p></li>
</ul></li>
<li><p><code>AJR</code></p>
<ul>
<li><p>Data about GDP, institutions, and settler mortality</p></li>
<li><p>Access: `data(AJR, package=“hdm”)</p></li>
<li><p>Description: <code>?hdm::AJR</code></p></li>
</ul></li>
<li><p><code>banks</code></p>
<ul>
<li><p>Data about bank closures in Mississippi during the Great Depression</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>Birthweight_Smoking</code></p>
<ul>
<li><p>Data about infant birthweights and mother’s smoking from PA 1989</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="https://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/EE_Datasets/Birthweight_Smoking_Description.pdf">Mark Watson’s website</a></p></li>
</ul></li>
<li><p><code>Caschool</code></p>
<ul>
<li><p>School-level test score data from California in 1998-1999</p></li>
<li><p>Access: <code>data(Caschool, package="Ecdat")</code></p></li>
<li><p>Description: <code>?Ecdat::Caschool</code></p></li>
</ul></li>
<li><p><code>diamond_train</code></p>
<ul>
<li><p>Data about diamond prices. The full version of this data I got from Kaggle, and then I split it into training and testing data.</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: A description of each column in the data is available under the <code>Description</code> tab on <a href="https://www.kaggle.com/shivam2503/diamonds/">Kaggle</a></p></li>
</ul></li>
<li><p><code>diamond_test</code></p>
<ul>
<li><p>Out of sample version of <code>diamond_train</code> data</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: A description of each column in the data is available under the <code>Description</code> tab on <a href="https://www.kaggle.com/shivam2503/diamonds/">Kaggle</a></p></li>
</ul></li>
<li><p><code>Fair</code></p>
<ul>
<li><p>Individual-level data about affairs in the United States</p></li>
<li><p>Access: <code>data(Fair, package="Ecdat")</code></p></li>
<li><p>Description: <code>?Ecdat::Fair</code></p></li>
</ul></li>
<li><p><code>Fatalities</code></p>
<ul>
<li><p>State-level panel data about drunk driving laws and traffic fatalities</p></li>
<li><p>Access: `data(Fatality, package=“AER”)</p></li>
<li><p>Description: <code>?AER::Fatality</code></p></li>
</ul></li>
<li><p><code>fertilizer_2000</code></p>
<ul>
<li><p>Country-level data about fertilizer and crop yields from the year 2000. See description of <code>fertilizer_panel</code> below for more details</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>fertilizer_panel</code></p>
<ul>
<li><p>Country-level panel data from 1965-2000 (every 5 years) about fertilizer and crop yields for 68 developing countries. This data is a smaller version of the data used in McArthur, John W., and Gordon C. McCord. “Fertilizing growth: Agricultural inputs and their effects in economic development.” Journal of Development Economics 127 (2017): 133-152. url: <a href="https://doi.org/10.1016/j.jdeveco.2017.02.007">https://doi.org/10.1016/j.jdeveco.2017.02.007</a>.</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>house</code></p>
<ul>
<li><p>U.S. House of Representatives elections data from 1946-1998</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>intergenerational_mobility</code></p>
<ul>
<li><p>Intergenerational mobility data from PSID</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>Lead_Mortality</code></p>
<ul>
<li><p>Infant mortality and lead pipes in 1900</p></li>
<li><p>Access: Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/EE_Datasets/Lead_Mortality_Description.pdf">Mark Watson’s website</a></p></li>
</ul></li>
<li><p><code>mlda</code></p>
<ul>
<li><p>Car accident deaths by age group</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>mroz</code></p>
<ul>
<li><p>Labor force particpation of married women</p></li>
<li><p>Access: <code>data(mroz, package="wooldridge")</code></p></li>
<li><p>Description: <code>?wooldridge::mroz</code></p></li>
</ul></li>
<li><p><code>mutual_funds</code></p>
<ul>
<li><p>Mutual fund performance data</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>rand_hie</code></p>
<ul>
<li><p>RAND health insurance experiment</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
<li><p><code>Star</code></p>
<ul>
<li><p>Data from Project STAR that randomly assigned some students to smaller class sizes</p></li>
<li><p>Access: `data(Star, package=“Ecdat”)</p></li>
<li><p>Description: <code>?Ecdat::Star</code></p></li>
</ul></li>
<li><p><code>titanic_training</code></p>
<ul>
<li><p>Passenger level data on surviving Titanic. This data is a slightly adapted version of the <code>titanic</code> data on <a href="https://www.kaggle.com/c/titanic">Kaggle</a></p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="https://www.kaggle.com/c/titanic/data">Kaggle</a></p></li>
</ul></li>
<li><p><code>titanic_testing</code></p>
<ul>
<li><p>Out of sample version of <code>titanic_training</code> data</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="https://www.kaggle.com/c/titanic/data">Kaggle</a></p></li>
</ul></li>
<li><p><code>us_data</code></p>
<ul>
<li><p>Data from the 2019 American Community Survey via IPUMS</p></li>
<li><p>Access: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
<li><p>Description: <a href="http://bcallaway11.github.io/Courses/ECON_4750_Fall_2023/">Course Website</a></p></li>
</ul></li>
</ul>
</div>
<div id="first-week-of-class" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> First Week of Class<a href="#first-week-of-class" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: SW All of Chapter 1</p>
<p>In the first few classes, we will talk at very high level about the objectives of Econometrics.</p>

</div>
</div>
<div id="statistical-programming" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Topic 2</span> Statistical Programming<a href="#statistical-programming" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We will learn a lot more about statistical programming this semester, but we’ll start with a crash course on <code>R</code> with the idea of getting you up-and-running.</p>
<p>I listed a few references in the Introduction, but this section will mostly follow the discussion in <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science: Data Analysis and Prediction Algorithms with R</a>, by Rafael Irizarry. I’ll abbreviate this reference as IDS throughout this section.</p>
<p>IDS is not specifically geared towards Econometrics, but I think it is a really fantastic book and resource. In this section, I cover what I think are the most important basics of R programming and additionally point you to the references for the material that I cover in class. But I would strongly recommend reading all of the first 5 chapters of IDS over the next couple of weeks. We will basically only cover the first 5 chapters in our class, but the course should set you up so that the remaining 35 chapters of the book can serve as helpful reference material throughout the rest of the semester.</p>
<div id="setting-up-r" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Setting up R<a href="#setting-up-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section covers how to set up R and RStudio and then what RStudio will look like when you open it up.</p>
<div id="what-is-r" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> What is R?<a href="#what-is-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 1.1</p>
<p>R is a statistical programming language. This is important for two reasons</p>
<ul>
<li><p>It looks like a “real” programming language. In my view, this is a big advantage. And many of the programming skills that we will learn in this class will be transferable. What I mean is that, if you one day want to switch to writing code in Stata or Python, I think the switch should be not-too-painful because learning new “syntax” (things like where to put the semi-colons) is usually relatively easy compared to the “way of thinking” about how to write code. Some other statistical programming languages are more “canned” than R. In some sense, this makes them easier to learn, but this also comes with the drawback that whatever skills that you learn are quite specific to that one language.</p></li>
<li><p>Even though R is a real programming language, it is geared towards statistics. Compared to say, Matlab, a lot of common statistical procedures (e.g., running a regression) will be quite easy for you.</p></li>
</ul>
<p>R is very popular among statisticians, computer scientists, economists.</p>
<p>It is easy to share code across platforms: Linux, Windows, Mac. Besides that, it is easy to write and contribute extensions. I have 10+ R packages that you can easily download and immediately use.</p>
<p>There is a large community, and lots of available, helpful resources.</p>
<ul>
<li><p>First place to look if you don’t know how to do something: DuckDuckGo (or…err…Google)!</p></li>
<li><p>StackOverflow</p></li>
</ul>
</div>
<div id="downloading-r" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Downloading R<a href="#downloading-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will use R (<a href="https://www.r-project.org/">https://www.r-project.org/</a>) to analyze data. R is freely available and available across platforms. You should go ahead and download R for your personal computer as soon as possible — this should be relatively straightforward. It is also available at most computer labs on campus.</p>
</div>
<div id="rstudio" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> RStudio<a href="#rstudio" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Base <code>R</code> comes with a lightweight development environment (i.e., a place to write and execute code), but most folks prefer RStudio as it has more features. You can download it here: <a href="https://www.rstudio.com/products/rstudio/download/#download">https://www.rstudio.com/products/rstudio/download/#download</a>; choose the free version based on your operating system (Linux, Windows, Mac, etc.).</p>
</div>
<div id="rstudio-development-environment" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> RStudio Development Environment<a href="#rstudio-development-environment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 1.4</p>
<p>When you first open Rstudio, it will look something like this</p>
<p><img src="open_rstudio.png"></p>
<p>Typically, we will write <strong>scripts</strong>, basically just as a way to save the code that we have written. Go to <code>File -&gt; New File -&gt; R Script</code>. This will open up a new pane, and your screen should look something like this</p>
<p><img src="rstudio_script.png"></p>
<p>Let’s look around here. The top left pane is called the “Source Pane”. It is where you can write an R script. Try typing</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="dv">1</span><span class="sc">+</span><span class="dv">1</span></span></code></pre></div>
<p>in that pane. This is a very simple R program. Now, type <code>Ctrl+s</code> to save the script. This will likely prompt you to provide a name for the script. You can call it <code>first_script.R</code> or something like that. The only thing that really matters is that the file name ends in “.R” (although you should at least give the file a reasonably descriptive name).</p>
<p>Now let’s move to the bottom left pane. This is called the “Console Pane”. It is where the actual computations happen in R (Notice that, although we have already saved our first script, we haven’t actually run any code). Beside the blue arrow in that pane, try typing</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="dv">2</span><span class="sc">+</span><span class="dv">2</span></span></code></pre></div>
<p>and then press <code>ENTER</code>. This time you should actually see the answer.</p>
<p>Now, let’s go back to the Source pane. Often, it is convenient to run R programs line by line (mainly in order for it to be easy for you to digest the results). You can do this by pressing <code>Ctrl+ENTER</code> on any line in your script for it to run next. Try this on the first line of your script file where we previously typed <code>1+1</code>. This code should now run, and you should be able to see the result down in the bottom left Console pane.</p>
<p>We will ignore the two panes on the right for now and come back to them once we get a little more experience programming in R.</p>
</div>
</div>
<div id="installing-r-packages" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Installing R Packages<a href="#installing-r-packages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: IDS 1.5</p>
<p>When you download <code>R</code>, you get “base” <code>R</code>. Base R contains “basic” functions that are commonly used by most R users. To give some examples, base R gives you the ability add, subtract, divide, or multiply numbers. Base R gives you the ability to calculate the mean (the function is called <code>mean</code>) or standard deviation (the function is called <code>sd</code>) of a vector of numbers.</p>
<p>Base R is quite powerful and probably the majority of code you will write in R will only involve Base R.</p>
<p>That being said, there are many cases where it is useful to expand the base functionality of <code>R</code>. This is done through <strong>packages</strong>. Packages expand the functionality of R. R is open source so these packages are contributed by users.</p>
<p>It also typically wouldn’t make sense for someone to install <em>all</em> available R packages. For example, a geographer might want to install a much different set of packages relative to an economist. Therefore, we will typically install only the additional functionality that we specifically want.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 2.1  </strong></span>In this example, we’ll install the <code>dslabs</code> package (which is from the IDS book) and the <code>lubridate</code> package (which is a package for working with dates in R).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># install dslabs package</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;dslabs&quot;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># install lubridate package</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;lubridate&quot;</span>)</span></code></pre></div>
</div>
<p>Installing a package is only the first step to using a package. You can think of installing a package like <em>downloading</em> a package. To actually use a package, you need to load it into memory (i.e., “attach” it) or at least be clear about the package where a function that you are trying to call comes from.</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 2.2  </strong></span>Dates can be tricky to work with in R (and in programming languages generally). For example, they are not exactly numbers, but they also have more structure than just a character string. The <code>lubridate</code> package contains functions for converting numbers/strings into dates.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>bday <span class="ot">&lt;-</span> <span class="st">&quot;07-15-1985&quot;</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="fu">class</span>(bday) <span class="co"># R doesn&#39;t know this is actually a date yet</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;character&quot;</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co"># load the package</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="fu">library</span>(lubridate)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co"># mdy stands for &quot;month, day, year&quot;</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co"># if date were in different format, could use ymd, etc.</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>date_bday <span class="ot">&lt;-</span> <span class="fu">mdy</span>(bday)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>date_bday</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co">#&gt; [1] &quot;1985-07-15&quot;</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co"># now R knows this is a date</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="fu">class</span>(date_bday)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="co">#&gt; [1] &quot;Date&quot;</span></span></code></pre></div>
<p>Another (and perhaps better) way to call a function from a package is to use the <code>::</code> syntax. In this case, you do not need the call to <code>library</code> from above. Instead, you can try</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>lubridate<span class="sc">::</span><span class="fu">mdy</span>(bday)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;1985-07-15&quot;</span></span></code></pre></div>
<p>This does exactly the same thing as the code before. What is somewhat better about this code is that it is easier to tell that the <code>mdy</code> function came from the <code>lubridate</code> package.</p>
</div>
<div id="a-list-of-useful-r-packages" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> A list of useful R packages<a href="#a-list-of-useful-r-packages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><code>AER</code> — package containing data from <em>Applied Econometrics with R</em></p></li>
<li><p><code>wooldridge</code> — package containing data from Wooldridge’s text book</p></li>
<li><p><code>ggplot2</code> — package to produce sophisticated looking plots</p></li>
<li><p><code>dplyr</code> — package containing tools to manipulate data</p></li>
<li><p><code>haven</code> — package for loading different types of data files</p></li>
<li><p><code>plm</code> — package for working with panel data</p></li>
<li><p><code>fixest</code> — another package for working with panel data</p></li>
<li><p><code>ivreg</code> — package for IV regressions, diagnostics, etc.</p></li>
<li><p><code>estimatr</code> — package that runs regressions but with standard errors that economists often like more than the default options in <code>R</code></p></li>
<li><p><code>modelsummary</code> — package for producing nice output of more than one regression and summary statistics</p></li>
</ul>
<p>As of this writing, there are currently 18,004 R packages available on CRAN (R’s main repository for contributed packages).</p>
</div>
</div>
<div id="r-basics" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> R Basics<a href="#r-basics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: IDS 2.1</p>
<p>In this section, we’ll start to work towards writing useful R code.</p>
<div id="objects" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Objects<a href="#objects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 2.2</p>
<p>The very first step to writing code that can actually do something is to able to store things. In R, we store things in <strong>objects</strong> (perhaps sometimes I will also use the word variables).</p>
<p>Earlier, we used R to calculate <span class="math inline">\(1+1\)</span>. Let’s go back to the Source pane (top left pane in RStudio) and type</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>answer <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span></span></code></pre></div>
<p>Press <code>Ctrl+ENTER</code> on this line to run it. You should see the same line down in the Console now.</p>
<p>Let’s think carefully about what is happening here</p>
<ul>
<li><p><code>answer</code> is the name of the variable (or object) that we are creating here.</p></li>
<li><p>the <code>&lt;-</code> is the <strong>assignment</strong> operator. It means that we should <em>assign</em> whatever is on the right hand side of it to the variable that is on the left hand side of it</p></li>
<li><p><code>1+1</code> just computes <span class="math inline">\(1+1\)</span> as we did earlier. Soon we will put more complicated expressions here.</p></li>
</ul>
<p>You can think about the above code as computing <span class="math inline">\(1+1\)</span> and then <em>saving</em> it in the variable <code>answer</code>.</p>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> The assignment operator, <code>&lt;-</code>, is a “less than sign” followed by a “hyphen”. It’s often convenient though to use the keyboard shortcut <code>Alt+-</code> (i.e., hold down <code>Alt</code> and press the hypen key) to insert it. You can also use an <code>=</code> for assignment, but this is less commonly done in R.</p>
</div>
<div class="practice">
<p><span class="practice">Practice:</span> Try creating variable called <code>five_squared</code> that is equal to <span class="math inline">\(5 \times 5\)</span> (multiplication in R is done using the <code>*</code> symbol).</p>
</div>
<p>There are a number of reasons why you might like to create an object in R. Perhaps the main one is so that you can reuse it. Let’s try multiplying <code>answer</code> by <span class="math inline">\(3\)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>answer<span class="sc">*</span><span class="dv">3</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co">#&gt; [1] 6</span></span></code></pre></div>
<p>If you wanted, you could also save this as its own variable too.</p>
</div>
<div id="workspace" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Workspace<a href="#workspace" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 2.2</p>
<p>Before we move on, I just want to show you what my workspace looks like now.</p>
<p><img src="rstudio_environment.png"></p>
<p>As we talked about above, you can see the code in my script in the Source pane in the top left. You can also see the code that I actually ran in the Console pane on the bottom left.</p>
<p>Now, take a look at the top right pane. You will see under the Environment tab that <code>answer</code> shows up there with a value of <code>2</code>. The Environment tab keeps track of all the variables that you have created in your current session. A couple of other things that might be useful to point out there.</p>
<ul>
<li><p>Later on in the class, we will often import data to work with. The “Import Dataset” button that is located in this top right pane is often useful for this.</p></li>
<li><p>Occasionally, you might get into the case where you have saved a bunch of variables and it would be helpful to “start over”. The broom in this pane will “clean” your workspace (this just means delete everything).</p></li>
</ul>
</div>
<div id="importing-data" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Importing Data<a href="#importing-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To work with actual data in R, we will need to import it. I mentioned the “Import Data” button above, but let me mention a few other possibilities here, including how to import data by writing code.</p>
<p>On the course website, I posted three files <code>firm.data.csv</code>, <code>firm_data.RData</code>, and <code>firm_data.dta</code>. All three of these contain exactly the same small, fictitious dataset, but are saved in different formats.</p>
<p>Probably the easiest way to import data in R is through the Files pane on the bottom right. In particular, suppose that you saved <code>firm_data.csv</code> in your “Downloads” folder. Try clicking the “…” (which, in the screenshot of my workspace above, is right beside the folder that I am in which is called “Detailed Course Notes”), then select your Downloads folder. This will switch the content of the Files pane to show the files in your Downloads folder. Now click <code>firm_data.csv</code>. This will open a menu to import the data. <code>R</code> is quite good at recognizing different types of data files and importing them, so this same procedure will work for <code>firm_data.RData</code> and <code>firm_data.dta</code> even though they are different types of files.</p>
<p>Next, let’s discuss how to import data by writing computer code (by the way, this is actually what is happening behind the scenes when you import data through the user interface as described above). “csv” stands for “Comma Separated Values”. This is basically a plain text file (e.g., try opening it in Notepad or Text Editor) where the columns are separated by commas and the rows are separated by being on different lines. Most any computer program can read this type of file; that is, you could easily import this file into, say, R, Excel, or Stata. You can import a <code>.csv</code> file using <code>R</code> code by</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>firm_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;firm_data.csv&quot;</span>)</span></code></pre></div>
<p>An <code>RData</code> file is the native format for saving data in <code>R</code>. You can import an <code>RData</code> file using the following command:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>firm_data <span class="ot">&lt;-</span> <span class="fu">load</span>(<span class="st">&quot;firm_data.RData&quot;</span>)</span></code></pre></div>
<p>Similarly, a <code>dta</code> file the native format for saving data in Stata. You can import a <code>dta</code> file using the following command:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">library</span>(haven) <span class="co"># external package for reading dta file</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>firm_data <span class="ot">&lt;-</span> <span class="fu">read_dta</span>(<span class="st">&quot;firm_data.dta&quot;</span>)</span></code></pre></div>
<p>In all three cases above, what we have done is to create a new <code>data.frame</code> (a <code>data.frame</code> is a type of object that we’ll talk about in detail later on in this chapter) called <code>firm_data</code> that contains the data that we were trying to load.</p>
</div>
</div>
<div id="functions-in-r" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Functions in R<a href="#functions-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: IDS 2.2</p>
<p>R has a large number of helpful, built-in functions. Let’s start with a pretty representative example: computing logarithms. This can be done using the R function <code>log</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">log</span>(<span class="dv">5</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="co">#&gt; [1] 1.609438</span></span></code></pre></div>
<p>You can tell this is a function because of the parentheses. The <code>5</code> inside of the parentheses is called the <strong>argument</strong> of the function. As practice, try computing the <span class="math inline">\(\log\)</span> of 7.</p>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> As a reminder, the logarithm of some number, let’s call it <span class="math inline">\(b\)</span>, is is the value of <span class="math inline">\(a\)</span> that solves <span class="math inline">\(\textrm{base}^a = b\)</span>.</p>
</div>
<p>The default base in R is <span class="math inline">\(e \approx 2.718\)</span>, so that <code>log(5)</code> actually computes what you might be more used to calling the “natural logarithm”. You can change the default value of the base by adding an extra argument to the function.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">log</span>(<span class="dv">5</span>, <span class="at">base=</span><span class="dv">10</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.69897</span></span></code></pre></div>
<p>In order to learn about what arguments are available (and what they mean), you can access the help files for a particular function by running either</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">help</span>(log)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>?log</span></code></pre></div>
<p>and, of course, substituting the name of whatever function you want to learn about in place of <code>log</code>.</p>
<p>In RStudio, it can also be helpful to press <code>Tab</code> and RStudio will provide possible completions to the function you are typing as well as what arguments can be provided to that function.</p>
<div class="practice">
<p><span class="practice">Practice:</span> R has a function for computing absolute value (you’ll have to find the name of it on your own). Try computing the absolute value of <span class="math inline">\(5\)</span> and <span class="math inline">\(-5\)</span>. Try creating a variable called <code>negative_three</code> that is equal to <span class="math inline">\(-3\)</span>; then, try to compute the absolute value of <code>negative_three</code>.</p>
</div>
</div>
<div id="data-types" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Data types<a href="#data-types" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: IDS 2.4</p>
<div id="numeric-vectors" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Numeric Vectors<a href="#numeric-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most basic data type in <code>R</code> is the vector. In fact, above when we created variables that were just a single number, they are actually stored as a numeric vector.</p>
<p>To more explicitly create a vector, you can use the <code>c</code> function in <code>R</code>. For example, let’s create a vector called <code>five</code> that contains the numbers 1 through 5.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>  five <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)</span></code></pre></div>
<p>We can print the contents of the vector <code>five</code> just by typing its name</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>five</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co">#&gt; [1] 1 2 3 4 5</span></span></code></pre></div>
<p>Another common operation on vectors is to get a particular element of a vector. Let me give an example</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>five[<span class="dv">3</span>]</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="co">#&gt; [1] 3</span></span></code></pre></div>
<p>This code takes the vector <code>five</code> and returns the third element in the vector. Notice that the above line contains braces, <code>[</code> and <code>]</code> rather than parentheses.</p>
<p>If you want several different elements from a vector, you can do the following</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>five[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>)]</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="co">#&gt; [1] 1 4</span></span></code></pre></div>
<p>This code takes the vector <code>five</code> and returns the first and fourth element in the vector.</p>
<p>One more useful function for vectors is the function <code>length</code>. This tells you the number of elements in vector. For example,</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="fu">length</span>(five)</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="co">#&gt; [1] 5</span></span></code></pre></div>
<p>which means that there are five total elements in the vector <code>five</code>.</p>
</div>
<div id="vector-arithmetic" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Vector arithmetic<a href="#vector-arithmetic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 2.11</p>
<p>The main operations on numeric vectors are <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code> which correspond to addition, subtraction, multiplication, and division. Often, we would like to carry out these operations on vectors.</p>
<p>There are two main cases. The first case is when you try to add a single number (i.e., a scalar) to all the elements in a vector. In this setup, the operation will happen element-wise which means the same number will be added to all numbers in the vector. This will be clear with some examples.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>five <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="co"># adds one to each element in vector</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>five <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="co">#&gt; [1] 2 3 4 5 6</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="co"># also adds one to each element in vector</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="dv">1</span> <span class="sc">+</span> five</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="co">#&gt; [1] 2 3 4 5 6</span></span></code></pre></div>
<p>Similar things will happen with the other mathematical operations above. Here are some more examples:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>five <span class="sc">*</span> <span class="dv">3</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="co">#&gt; [1]  3  6  9 12 15</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>five <span class="sc">-</span> <span class="dv">3</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="co">#&gt; [1] -2 -1  0  1  2</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>five <span class="sc">/</span> <span class="dv">3</span></span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a><span class="co">#&gt; [1] 0.3333333 0.6666667 1.0000000 1.3333333 1.6666667</span></span></code></pre></div>
<p>The other interesting case is what happens when you try to apply any of the same mathematical operators to two different vectors.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="co"># just some random numbers</span></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>vec2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">8</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">7</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>five <span class="sc">+</span> vec2</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a><span class="co">#&gt; [1]  9 -1  7  5 12</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>five <span class="sc">-</span> vec2</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a><span class="co">#&gt; [1] -7  5 -1  3 -2</span></span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>five <span class="sc">*</span> vec2</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a><span class="co">#&gt; [1]  8 -6 12  4 35</span></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>five <span class="sc">/</span> vec2</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a><span class="co">#&gt; [1]  0.1250000 -0.6666667  0.7500000  4.0000000  0.7142857</span></span></code></pre></div>
<p>You can immediately see what happens here. For example, for <code>five + vec2</code>, the first element of <code>five</code> is added to the first element of <code>vec2</code>, the second element of <code>five</code> is added to the second element of <code>vec2</code> and so on. Similar things happen for each of the other mathematical operations too.</p>
<p>There’s one other case that might be interesting to consider too. What happens if you try to apply these mathematical operations to two vectors of different lengths? Let’s find out</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>vec3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">6</span>)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>five <span class="sc">+</span> vec3</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="co">#&gt; Warning in five + vec3: longer object length is not a multiple of shorter object length</span></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="co">#&gt; [1]  3  8  5 10  7</span></span></code></pre></div>
<p>You’ll notice that this computes <em>something</em> but it also issues a warning. What happens here is that the result is equal to the first element of <code>five</code> plus the first element of <code>vec3</code>, the second of <code>five</code> plus the second element of <code>vec3</code>, the third element of <code>five</code> plus <em>the first element of <code>vec3</code></em>, the fourth element of <code>five</code> plus <em>the second element of <code>vec3</code></em>, and the fifth element of <code>five</code> plus <em>the first element of <code>vec3</code></em>. What’s happening here is that, since <code>vec3</code> contains fewere elements that <code>five</code>, the elements of <code>vec3</code> are getting <em>recycled</em>. In my experience, this warning often indicates a coding mistake. There are many cases where I want to add the same number to all elements in a vector, and many other cases where I want to add two vectors that have the same length, but I cannot think of any cases where I would want to add two vectors the way that is being carried out here.</p>
<p>The same sort of things will happen with subtraction, multiplication, and division (feel free to try it out).</p>
</div>
<div id="more-helpful-functions-in-r" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> More helpful functions in R<a href="#more-helpful-functions-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is definitely an incomplete list, but I’ll point you here to some more functions in R that are often helpful along with quick examples of them.</p>
<ul>
<li><p><code>seq</code> function — creates a “sequence” of numbers</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="fu">seq</span>(<span class="dv">2</span>,<span class="dv">7</span>)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a><span class="co">#&gt; [1] 2 3 4 5 6 7</span></span></code></pre></div></li>
<li><p><code>sum</code> function — computes the sum of a vector of numbers</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">8</span>))</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="co">#&gt; [1] 14</span></span></code></pre></div></li>
<li><p><code>sort</code>, <code>order</code>, and <code>rev</code> functions — functions for understanding the order or changing the order of a vector</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="co">#&gt; [1] 1 3 5</span></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="fu">order</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="co">#&gt; [1] 2 1 3</span></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a><span class="fu">rev</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a><span class="co">#&gt; [1] 5 1 3</span></span></code></pre></div></li>
<li><p><code>%%</code> — modulo function (i.e., returns the remainder from dividing one number by another)</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="dv">8</span> <span class="sc">%%</span> <span class="dv">3</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="co">#&gt; [1] 2</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a><span class="dv">1</span> <span class="sc">%%</span> <span class="dv">3</span></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div></li>
</ul>
<div class="practice">
<p><span class="practice">Practice:</span> The function <code>seq</code> contains an optional argument <code>length.out</code>. Try running the following code and seeing if you can figure out what <code>length.out</code> does.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="at">length.out=</span><span class="dv">5</span>)</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="at">length.out=</span><span class="dv">10</span>)</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="fu">seq</span>(<span class="fl">1.10</span>,<span class="at">length.out=</span><span class="dv">20</span>)</span></code></pre></div>
</div>
</div>
<div id="other-types-of-vectors" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Other types of vectors<a href="#other-types-of-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are other types of vectors in R too. Probably the main two other types of vectors are <strong>character vectors</strong> and <strong>logical vectors</strong>. We’ll talk about character vectors here and defer logical vectors until later. Character vectors are often referred to as <strong>strings</strong>.</p>
<p>We can create a character vector as follows</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>string1 <span class="ot">&lt;-</span> <span class="st">&quot;econometrics&quot;</span></span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>string2 <span class="ot">&lt;-</span> <span class="st">&quot;class&quot;</span></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>string1</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;econometrics&quot;</span></span></code></pre></div>
<p>The above code creates two character vectors and then prints the first one.</p>
<div class="side-comment">
<p><span class="side-comment">Side Comment</span> <code>c</code> stands for “concatenate”. Concatenate is a computer science word that means to combine two vectors. Probably the most well known version of this is “string concatenation” that combines two vectors of characters. Here is an example of string concatenation.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">c</span>(string1, string2)</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;econometrics&quot; &quot;class&quot;</span></span></code></pre></div>
<p>Sometimes string concatenation means to put two (or more strings) into the same string. This can be done using the <code>paste</code> command in R.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="fu">paste</span>(string1, string2)</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;econometrics class&quot;</span></span></code></pre></div>
<p>Notice that <code>paste</code> puts in a space between <code>string1</code> and <code>string2</code>. For practice, see if you can find an argument to the <code>paste</code> function that allows you to remove the space between the two strings.</p>
</div>
</div>
<div id="data-frames" class="section level3 hasAnchor" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Data Frames<a href="#data-frames" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another very important type of object in R is the <strong>data frame</strong>. I think it is helpful to think of a data frame as being very similar to an Excel spreadsheet — sort of like a matrix or a two-dimensional array. Each row typically corresponds to a particular observation, and each column typically provides the value of a particular variable for that observation.</p>
<p>Just to give a simple example, suppose that we had firm-level data about the name of the firm, what industry a firm was in, what county they were located in, and their number of employees. I created a data frame like this (it is totally made up, BTW) and show it to you next</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>firm_data</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:left;">
name
</th>
<th style="text-align:left;">
industry
</th>
<th style="text-align:left;">
county
</th>
<th style="text-align:right;">
employees
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ABC Manufacturing
</td>
<td style="text-align:left;">
Manufacturing
</td>
<td style="text-align:left;">
Clarke
</td>
<td style="text-align:right;">
531
</td>
</tr>
<tr>
<td style="text-align:left;">
Martin’s Muffins
</td>
<td style="text-align:left;">
Food Services
</td>
<td style="text-align:left;">
Oconee
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
Down Home Appliances
</td>
<td style="text-align:left;">
Manufacturing
</td>
<td style="text-align:left;">
Clarke
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:left;">
Classic City Widgets
</td>
<td style="text-align:left;">
Manufacturing
</td>
<td style="text-align:left;">
Clarke
</td>
<td style="text-align:right;">
211
</td>
</tr>
<tr>
<td style="text-align:left;">
Watkinsville Diner
</td>
<td style="text-align:left;">
Food Services
</td>
<td style="text-align:left;">
Oconee
</td>
<td style="text-align:right;">
25
</td>
</tr>
</tbody>
</table>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> If you are following along on R, I created this data frame using the following code</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>firm_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">name=</span><span class="fu">c</span>(<span class="st">&quot;ABC Manufacturing&quot;</span>, <span class="st">&quot;Martin</span><span class="sc">\&#39;</span><span class="st">s Muffins&quot;</span>, <span class="st">&quot;Down Home Appliances&quot;</span>, <span class="st">&quot;Classic City Widgets&quot;</span>, <span class="st">&quot;Watkinsville Diner&quot;</span>),</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>                        <span class="at">industry=</span><span class="fu">c</span>(<span class="st">&quot;Manufacturing&quot;</span>, <span class="st">&quot;Food Services&quot;</span>, <span class="st">&quot;Manufacturing&quot;</span>, <span class="st">&quot;Manufacturing&quot;</span>, <span class="st">&quot;Food Services&quot;</span>),</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>                        <span class="at">county=</span><span class="fu">c</span>(<span class="st">&quot;Clarke&quot;</span>, <span class="st">&quot;Oconee&quot;</span>, <span class="st">&quot;Clarke&quot;</span>, <span class="st">&quot;Clarke&quot;</span>, <span class="st">&quot;Oconee&quot;</span>),</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>                        <span class="at">employees=</span><span class="fu">c</span>(<span class="dv">531</span>, <span class="dv">6</span>, <span class="dv">15</span>, <span class="dv">211</span>, <span class="dv">25</span>))</span></code></pre></div>
<p>This is also the same data that we loaded earlier in Section 2.3.</p>
</div>
<p>Often, we’ll like to access a particular column in a data frame. For example, you might want to calculate the average number of employees across all the firms in our data.</p>
<p>Typically, the easiest way to do this, is to use the <strong>accessor</strong> symbol, which is <code>$</code> in R. This will make more sense with an example:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a>firm_data<span class="sc">$</span>employees</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="co">#&gt; [1] 531   6  15 211  25</span></span></code></pre></div>
<p><code>firm_data$employees</code> just provides the column called “employees” in the data frame called “firm_data”. You can also notice that <code>firm_data$employees</code> is just a numeric vector. This means that you can apply any of the functions that we have been covering on it</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="fu">mean</span>(firm_data<span class="sc">$</span>employees)</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a><span class="co">#&gt; [1] 157.6</span></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a><span class="fu">log</span>(firm_data<span class="sc">$</span>employees)</span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a><span class="co">#&gt; [1] 6.274762 1.791759 2.708050 5.351858 3.218876</span></span></code></pre></div>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> Notice that the function <code>mean</code> and <code>log</code> behave differently. <code>mean</code> calculates the average over all the elements in the vector <code>firm_data$employees</code> and therefore returns a single number. <code>log</code> calculates the logarithm of each element in the vector <code>firm_data$employees</code> and therefore returns a numeric vector with five elements.</p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span></p>
<p>The <code>$</code> is not the only way to access the elements in a data frame. You can also access them by their position. For example, if you want whatever is in the third row and second column of the data frame, you can get it by</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>firm_data[<span class="dv">3</span>,<span class="dv">2</span>]</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;Manufacturing&quot;</span></span></code></pre></div>
<p>Sometimes it is also convenient to recover a particular row or column by its position in the data frame. Here is an example of recovering the entire fourth row</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>firm_data[<span class="dv">4</span>,]</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="co">#&gt;                   name      industry county employees</span></span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a><span class="co">#&gt; 4 Classic City Widgets Manufacturing Clarke       211</span></span></code></pre></div>
<p>Notice that you just leave the “column index” (which is the second one) blank</p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> One other thing that sometimes takes some getting used to is that, for programming in general, you have to be very precise. Suppose you were to make a very small typo. R is not going to understand what you mean. See if you can spot the typo in the next line of code.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a>firm_data<span class="sc">$</span>employes</span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a><span class="co">#&gt; NULL</span></span></code></pre></div>
</div>
<p>A few more useful functions for working with data frames are:</p>
<ul>
<li><p><code>nrow</code> and <code>ncol</code> — returns the number of rows or columns in the data frame</p></li>
<li><p><code>colnames</code> and <code>rownames</code> — returns the names of the columns or rows</p></li>
</ul>
</div>
<div id="lists" class="section level3 hasAnchor" number="2.5.6">
<h3><span class="header-section-number">2.5.6</span> Lists<a href="#lists" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Vectors and data frames are the main two types of objects that we’ll use this semester, but let me give you a quick overview of a few other types of objects. Let’s start with <strong>lists</strong>. Lists are very generic in the sense that they can carry around complicated data. If you are familiar with any object oriented programming language like Java or C++, they have the flavor of an “object”, in the object-oriented sense.</p>
<p>I’m not sure if we will see any examples this semester where you <em>have</em> to use a list. But here is an example. Suppose that we wanted to put the vector that we created earlier <code>five</code> and the data frame that we created earlier <code>firm_data</code> into the same object. We could do it as follows</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a>unusual_list <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">numbers=</span>five, <span class="at">df=</span>firm_data)</span></code></pre></div>
<p>You can access the elements of a list in a few different ways. Sometimes it is convenient to access them via the <code>$</code></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a>unusual_list<span class="sc">$</span>numbers</span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a><span class="co">#&gt; [1] 1 2 3 4 5</span></span></code></pre></div>
<p>Other times, it is convenient to access them via their position in the list</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a>unusual_list[[<span class="dv">2</span>]] <span class="co"># notice the double brackets</span></span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a><span class="co">#&gt;                   name      industry county employees</span></span>
<span id="cb40-3"><a href="#cb40-3" tabindex="-1"></a><span class="co">#&gt; 1    ABC Manufacturing Manufacturing Clarke       531</span></span>
<span id="cb40-4"><a href="#cb40-4" tabindex="-1"></a><span class="co">#&gt; 2     Martin&#39;s Muffins Food Services Oconee         6</span></span>
<span id="cb40-5"><a href="#cb40-5" tabindex="-1"></a><span class="co">#&gt; 3 Down Home Appliances Manufacturing Clarke        15</span></span>
<span id="cb40-6"><a href="#cb40-6" tabindex="-1"></a><span class="co">#&gt; 4 Classic City Widgets Manufacturing Clarke       211</span></span>
<span id="cb40-7"><a href="#cb40-7" tabindex="-1"></a><span class="co">#&gt; 5   Watkinsville Diner Food Services Oconee        25</span></span></code></pre></div>
</div>
<div id="matrices" class="section level3 hasAnchor" number="2.5.7">
<h3><span class="header-section-number">2.5.7</span> Matrices<a href="#matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Matrices are very similar to data frames, but the data should all be of the same type. Matrices are very useful in some numerical calculations that are beyond the scope of this class. Here is an example of a matrix.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), <span class="at">nrow=</span><span class="dv">2</span>, <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a>mat</span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2]</span></span>
<span id="cb41-4"><a href="#cb41-4" tabindex="-1"></a><span class="co">#&gt; [1,]    1    2</span></span>
<span id="cb41-5"><a href="#cb41-5" tabindex="-1"></a><span class="co">#&gt; [2,]    3    4</span></span></code></pre></div>
<p>You can access elements of a matrix by their position in the matrix, just like for the data frame above.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="co"># first row, second column</span></span>
<span id="cb42-2"><a href="#cb42-2" tabindex="-1"></a>mat[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb42-3"><a href="#cb42-3" tabindex="-1"></a><span class="co">#&gt; [1] 2</span></span>
<span id="cb42-4"><a href="#cb42-4" tabindex="-1"></a><span class="co"># all rows in second column</span></span>
<span id="cb42-5"><a href="#cb42-5" tabindex="-1"></a>mat[,<span class="dv">2</span>] </span>
<span id="cb42-6"><a href="#cb42-6" tabindex="-1"></a><span class="co">#&gt; [1] 2 4</span></span></code></pre></div>
</div>
<div id="factors" class="section level3 hasAnchor" number="2.5.8">
<h3><span class="header-section-number">2.5.8</span> Factors<a href="#factors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes variables in economics are <strong>categorical</strong>. This sort of variable is somewhat between a numeric variable and a string. In <code>R</code>, categorical variables are called <strong>factors</strong>.</p>
<p>A good example of a categorical variable is <code>firm_data$industry</code>. It tells you the “category” of the industry that a firm is in.</p>
<p>Oftentimes, we may have to tell R that a variable is a “factor” rather than just a string. Let’s create a variable called <code>industry</code> that contains the industry from <code>firm_data</code> but as a factor.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a>industry <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(firm_data<span class="sc">$</span>industry)</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>industry</span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a><span class="co">#&gt; [1] Manufacturing Food Services Manufacturing Manufacturing Food Services</span></span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a><span class="co">#&gt; Levels: Food Services Manufacturing</span></span></code></pre></div>
<p>A useful package for working with factor variables is the <code>forcats</code> package.</p>
</div>
<div id="understanding-an-object-in-r" class="section level3 hasAnchor" number="2.5.9">
<h3><span class="header-section-number">2.5.9</span> Understanding an object in R<a href="#understanding-an-object-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes you may be in the case where there is a variable where you don’t know what exactly it contains. Some functions that are helpful in this case are</p>
<ul>
<li><p><code>class</code> — tells you, err, the class of an object (i.e., its “type”)</p></li>
<li><p><code>head</code> — shows you the “beginning” of an object; this is especially helpful for large objects (like some data frames)</p></li>
<li><p><code>str</code> — stands for “structure” of an object</p></li>
</ul>
<p>Let’s try these out</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="fu">class</span>(firm_data)</span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;data.frame&quot;</span></span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a><span class="co"># typically would show the first five rows of a data frame,</span></span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a><span class="co"># but that is the whole data frame here</span></span>
<span id="cb44-5"><a href="#cb44-5" tabindex="-1"></a><span class="fu">head</span>(firm_data) </span>
<span id="cb44-6"><a href="#cb44-6" tabindex="-1"></a><span class="co">#&gt;                   name      industry county employees</span></span>
<span id="cb44-7"><a href="#cb44-7" tabindex="-1"></a><span class="co">#&gt; 1    ABC Manufacturing Manufacturing Clarke       531</span></span>
<span id="cb44-8"><a href="#cb44-8" tabindex="-1"></a><span class="co">#&gt; 2     Martin&#39;s Muffins Food Services Oconee         6</span></span>
<span id="cb44-9"><a href="#cb44-9" tabindex="-1"></a><span class="co">#&gt; 3 Down Home Appliances Manufacturing Clarke        15</span></span>
<span id="cb44-10"><a href="#cb44-10" tabindex="-1"></a><span class="co">#&gt; 4 Classic City Widgets Manufacturing Clarke       211</span></span>
<span id="cb44-11"><a href="#cb44-11" tabindex="-1"></a><span class="co">#&gt; 5   Watkinsville Diner Food Services Oconee        25</span></span>
<span id="cb44-12"><a href="#cb44-12" tabindex="-1"></a><span class="fu">str</span>(firm_data)</span>
<span id="cb44-13"><a href="#cb44-13" tabindex="-1"></a><span class="co">#&gt; &#39;data.frame&#39;:    5 obs. of  4 variables:</span></span>
<span id="cb44-14"><a href="#cb44-14" tabindex="-1"></a><span class="co">#&gt;  $ name     : chr  &quot;ABC Manufacturing&quot; &quot;Martin&#39;s Muffins&quot; &quot;Down Home Appliances&quot; &quot;Classic City Widgets&quot; ...</span></span>
<span id="cb44-15"><a href="#cb44-15" tabindex="-1"></a><span class="co">#&gt;  $ industry : chr  &quot;Manufacturing&quot; &quot;Food Services&quot; &quot;Manufacturing&quot; &quot;Manufacturing&quot; ...</span></span>
<span id="cb44-16"><a href="#cb44-16" tabindex="-1"></a><span class="co">#&gt;  $ county   : chr  &quot;Clarke&quot; &quot;Oconee&quot; &quot;Clarke&quot; &quot;Clarke&quot; ...</span></span>
<span id="cb44-17"><a href="#cb44-17" tabindex="-1"></a><span class="co">#&gt;  $ employees: num  531 6 15 211 25</span></span></code></pre></div>
<div class="practice">
<p><span class="practice">Practice:</span> Try running <code>class</code>, <code>head</code>, and <code>str</code> on the vector <code>five</code> that we created earlier.</p>
</div>
</div>
</div>
<div id="logicals" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Logicals<a href="#logicals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: IDS 2.13</p>
<p>All programming languages have ways of tracking whether variables meet certain criteria. These are often called Booleans or Logicals. For us, this will particularly come up in the context of subsetting data (i.e., selecting data based on some condition) and in running particular portions of code based on some condition.</p>
<p>Some main logical operators are <code>==</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&gt;</code> corresponding to whether or not two things are equal, less than or equal to, greater than or equal, strictly less than, and strictly greater than. These can be applied to vectors. And the comparisons result in either <code>TRUE</code> or <code>FALSE</code>. Here are some examples</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a>five <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)</span>
<span id="cb45-2"><a href="#cb45-2" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" tabindex="-1"></a><span class="co"># only 3 is equal to 3</span></span>
<span id="cb45-4"><a href="#cb45-4" tabindex="-1"></a>five <span class="sc">==</span> <span class="dv">3</span></span>
<span id="cb45-5"><a href="#cb45-5" tabindex="-1"></a><span class="co">#&gt; [1] FALSE FALSE  TRUE FALSE FALSE</span></span>
<span id="cb45-6"><a href="#cb45-6" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" tabindex="-1"></a><span class="co"># 1,2,3 are all less than or equal to 3</span></span>
<span id="cb45-8"><a href="#cb45-8" tabindex="-1"></a>five <span class="sc">&lt;=</span> <span class="dv">3</span></span>
<span id="cb45-9"><a href="#cb45-9" tabindex="-1"></a><span class="co">#&gt; [1]  TRUE  TRUE  TRUE FALSE FALSE</span></span>
<span id="cb45-10"><a href="#cb45-10" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" tabindex="-1"></a><span class="co"># 3,4,5, are all greater than or equal to 3</span></span>
<span id="cb45-12"><a href="#cb45-12" tabindex="-1"></a>five <span class="sc">&gt;=</span> <span class="dv">3</span></span>
<span id="cb45-13"><a href="#cb45-13" tabindex="-1"></a><span class="co">#&gt; [1] FALSE FALSE  TRUE  TRUE  TRUE</span></span>
<span id="cb45-14"><a href="#cb45-14" tabindex="-1"></a></span>
<span id="cb45-15"><a href="#cb45-15" tabindex="-1"></a><span class="co"># 1,2 are strictly less than 3</span></span>
<span id="cb45-16"><a href="#cb45-16" tabindex="-1"></a>five <span class="sc">&lt;</span> <span class="dv">3</span></span>
<span id="cb45-17"><a href="#cb45-17" tabindex="-1"></a><span class="co">#&gt; [1]  TRUE  TRUE FALSE FALSE FALSE</span></span>
<span id="cb45-18"><a href="#cb45-18" tabindex="-1"></a></span>
<span id="cb45-19"><a href="#cb45-19" tabindex="-1"></a><span class="co"># 4,5 are strictly greater than 3</span></span>
<span id="cb45-20"><a href="#cb45-20" tabindex="-1"></a>five <span class="sc">&gt;</span> <span class="dv">3</span></span>
<span id="cb45-21"><a href="#cb45-21" tabindex="-1"></a><span class="co">#&gt; [1] FALSE FALSE FALSE  TRUE  TRUE</span></span></code></pre></div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 2.3  </strong></span>Often, we might be interested in learning about a subset of our data. As a simple example, using our <code>firm_data</code> from earlier, you could imagine being interested in average employment for manufacturing firms.</p>
<p>We can do this using the <code>subset</code> function along with the logical operations we’ve learned in this section.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a>manufacturing_firms <span class="ot">&lt;-</span> <span class="fu">subset</span>(firm_data, industry<span class="sc">==</span><span class="st">&quot;Manufacturing&quot;</span>)</span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a><span class="fu">mean</span>(manufacturing_firms<span class="sc">$</span>employees)</span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a><span class="co">#&gt; [1] 252.3333</span></span></code></pre></div>
<p>As practice, try creating a subset of <code>firm_data</code> based on firms having more than 100 employees.</p>
</div>
<div id="additional-logical-operators" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Additional Logical Operators<a href="#additional-logical-operators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 2.13</p>
<p>There are a number of additional logical operators that can be useful in practice. Here, we quickly cover several more.</p>
<ul>
<li><p><code>!=</code> — not equal</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>) <span class="sc">!=</span> <span class="dv">3</span></span>
<span id="cb47-2"><a href="#cb47-2" tabindex="-1"></a><span class="co">#&gt; [1]  TRUE  TRUE FALSE</span></span></code></pre></div></li>
<li><p>We can link together multiple logical comparisons. If we want to check whether multiple conditions hold, we can use “logical AND” <code>&amp;</code>; if we want to check whether any of multiple conditions hold, we can use “logical OR” <code>|</code>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a><span class="co"># AND</span></span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a>( <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>) <span class="sc">&gt;=</span> <span class="dv">3</span> ) <span class="sc">&amp;</span> ( <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>) <span class="sc">&lt;</span> <span class="dv">5</span> )</span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a><span class="co">#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE</span></span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" tabindex="-1"></a><span class="co"># OR</span></span>
<span id="cb48-6"><a href="#cb48-6" tabindex="-1"></a>( <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>) <span class="sc">&gt;=</span> <span class="dv">4</span> ) <span class="sc">|</span> ( <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>) <span class="sc">&lt;</span> <span class="dv">2</span> )</span>
<span id="cb48-7"><a href="#cb48-7" tabindex="-1"></a><span class="co">#&gt; [1]  TRUE FALSE FALSE  TRUE  TRUE</span></span></code></pre></div></li>
<li><p><code>%in%</code> — checks whether the elements of one vector show up <em>in</em> another vector</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" tabindex="-1"></a><span class="co"># 1 is in the 2nd vector, but 7 is not</span></span>
<span id="cb49-2"><a href="#cb49-2" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">7</span>) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)</span>
<span id="cb49-3"><a href="#cb49-3" tabindex="-1"></a><span class="co">#&gt; [1]  TRUE FALSE</span></span></code></pre></div></li>
<li><p>Often it is useful to check whether any logical conditions are true or all logical conditions are true. This can be done as follows</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a><span class="co"># this one is TRUE because 1 is in the 2nd vector  </span></span>
<span id="cb50-2"><a href="#cb50-2" tabindex="-1"></a><span class="fu">any</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">7</span>) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>))</span>
<span id="cb50-3"><a href="#cb50-3" tabindex="-1"></a><span class="co">#&gt; [1] TRUE</span></span>
<span id="cb50-4"><a href="#cb50-4" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" tabindex="-1"></a><span class="co"># this one is FALSE because 7 is not in the 2nd vector</span></span>
<span id="cb50-6"><a href="#cb50-6" tabindex="-1"></a><span class="fu">all</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">7</span>) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>))</span>
<span id="cb50-7"><a href="#cb50-7" tabindex="-1"></a><span class="co">#&gt; [1] FALSE</span></span></code></pre></div></li>
</ul>
</div>
</div>
<div id="programming-basics" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Programming basics<a href="#programming-basics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="writing-functions" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Writing functions<a href="#writing-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 3.2</p>
<p>It is often helpful to write your own functions in R. If you ever find yourself repeating the same code over and over, this suggests that you should write this code as a function and repeatedly call the function.</p>
<p>Suppose we are interesting in solving the quadratic equation
<span class="math display">\[
  ax^2 + bx + c = 0
\]</span>
If you remember the quadratic formula, the solution to this equation is
<span class="math display">\[
  x = \frac{-b \pm \sqrt{b^2-4ac}}{2a}
\]</span>
It would be tedious to calculate this by hand (especially if we wanted to calculate it for many different values of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span>), so let’s write a function to do it.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" tabindex="-1"></a>quadratic_solver <span class="ot">&lt;-</span> <span class="cf">function</span>(a, b, c) {</span>
<span id="cb51-2"><a href="#cb51-2" tabindex="-1"></a>  root1 <span class="ot">&lt;-</span> ( <span class="sc">-</span>b <span class="sc">+</span> <span class="fu">sqrt</span>(b<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">4</span><span class="sc">*</span>a<span class="sc">*</span>c) ) <span class="sc">/</span> <span class="dv">2</span><span class="sc">*</span>a</span>
<span id="cb51-3"><a href="#cb51-3" tabindex="-1"></a>  root1</span>
<span id="cb51-4"><a href="#cb51-4" tabindex="-1"></a>}</span></code></pre></div>
<p>Before we try this out, let’s notice a few things. First, while this particular function is for solving the quadratic equation, this is quite representative of what a function looks like in R.</p>
<ul>
<li><p><code>quadratic_solver</code> — This is the name of the function. It’s good to give your function a descriptive name related to what it does. But you could call it anything you want. If you wanted to call this function <code>uga</code>, it would still work.</p></li>
<li><p>the part <code>&lt;- function</code> finishes off assigning the function the name <code>quadratic_solver</code> and implies that we are writing down a function rather than a <code>vector</code> or <code>data.frame</code> or something else. This part will show up in all function definitions.</p></li>
<li><p>the part <code>(a, b, c)</code>, <code>a</code>, <code>b</code>, and <code>c</code> are the names of the <em>arguments</em> to the function. In a minute when we call the function, we need to tell the function the particular values of <code>a</code>, <code>b</code>, and <code>c</code> for which to solve the quadratic equation. We could name these whatever we want, but, again, it is good to have descriptive names. When you write a different function, it can have as many arguments as you want it to have.</p></li>
<li><p>the part <code>{ ... }</code> everything that the function does should go between the curly brackets</p></li>
<li><p>the line <code>root1 &lt;- ( -b + sqrt(b^2 - 4*a*c) ) / 2*a</code> contains the main thing that is calculated by our function. Notice that we only calculate one of the “roots” (i.e., solutions to the quadratic equation) because of the <span class="math inline">\(+\)</span> in this expression.</p></li>
<li><p>the line <code>root1</code> R returns whatever variable is on the last line of the function. It might be somewhat more clear to write <code>return(root1)</code>. The behavior of the code would be exactly the same, but it is just the more common “style” in R programming to not include the explicit <code>return</code>.</p></li>
</ul>
<p>Now let’s try out our function</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a><span class="co"># solves quadratic equation for a=1, b=4, c=3</span></span>
<span id="cb52-2"><a href="#cb52-2" tabindex="-1"></a><span class="fu">quadratic_solver</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">3</span>)</span>
<span id="cb52-3"><a href="#cb52-3" tabindex="-1"></a><span class="co">#&gt; [1] -1</span></span>
<span id="cb52-4"><a href="#cb52-4" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" tabindex="-1"></a><span class="co"># solves quadratic equation for a=-1, b=5, c=10</span></span>
<span id="cb52-6"><a href="#cb52-6" tabindex="-1"></a><span class="fu">quadratic_solver</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>)</span>
<span id="cb52-7"><a href="#cb52-7" tabindex="-1"></a><span class="co">#&gt; [1] -1.531129</span></span></code></pre></div>
<p>Two last things that are worth pointing out about functions:</p>
<ul>
<li><p>Functions in R can be set up to take default values for some of their arguments</p></li>
<li><p>Because the arguments have names, if you are explicit about the name of the argument, then the order of the argument does not matter.</p></li>
</ul>
<p>To give examples, let’s write a slightly modified version of our function to solve quadratic equations.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" tabindex="-1"></a>quadratic_solver2 <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">a=</span><span class="dv">1</span>, b, c) {</span>
<span id="cb53-2"><a href="#cb53-2" tabindex="-1"></a>  root1 <span class="ot">&lt;-</span> ( <span class="sc">-</span>b <span class="sc">+</span> <span class="fu">sqrt</span>(b<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">4</span><span class="sc">*</span>a<span class="sc">*</span>c) ) <span class="sc">/</span> <span class="dv">2</span><span class="sc">*</span>a</span>
<span id="cb53-3"><a href="#cb53-3" tabindex="-1"></a>  root1</span>
<span id="cb53-4"><a href="#cb53-4" tabindex="-1"></a>}</span></code></pre></div>
<p>The only thing different here is that <code>a</code> takes the default value of 1. Now let’s try some different calls to <code>quadratic_solver</code> and <code>quadratic_solver2</code></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" tabindex="-1"></a><span class="co"># solve again for a=1,b=4,c=3</span></span>
<span id="cb54-2"><a href="#cb54-2" tabindex="-1"></a><span class="fu">quadratic_solver2</span>(<span class="at">b=</span><span class="dv">4</span>,<span class="at">c=</span><span class="dv">3</span>)</span>
<span id="cb54-3"><a href="#cb54-3" tabindex="-1"></a><span class="co">#&gt; [1] -1</span></span>
<span id="cb54-4"><a href="#cb54-4" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" tabindex="-1"></a><span class="co"># replace default and change order</span></span>
<span id="cb54-6"><a href="#cb54-6" tabindex="-1"></a><span class="fu">quadratic_solver2</span>(<span class="at">c=</span><span class="dv">10</span>,<span class="at">b=</span><span class="dv">5</span>,<span class="at">a=</span><span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb54-7"><a href="#cb54-7" tabindex="-1"></a><span class="co">#&gt; [1] -1.531129</span></span>
<span id="cb54-8"><a href="#cb54-8" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" tabindex="-1"></a><span class="co"># no default set for quadratic_solver so it will crash if a not provided</span></span>
<span id="cb54-10"><a href="#cb54-10" tabindex="-1"></a><span class="fu">quadratic_solver</span>(<span class="at">b=</span><span class="dv">4</span>,<span class="at">c=</span><span class="dv">3</span>)</span>
<span id="cb54-11"><a href="#cb54-11" tabindex="-1"></a><span class="co">#&gt; Error in quadratic_solver(b = 4, c = 3): argument &quot;a&quot; is missing, with no default</span></span></code></pre></div>
</div>
<div id="ifelse" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> if/else<a href="#ifelse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 3.1</p>
<p>Often when writing code, you will want to do different things depending on some condition. Let’s write a function that takes in the number of employees that are in a firm and prints “large” if the firm has more than 100 employees and “small” otherwise.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" tabindex="-1"></a>large_or_small <span class="ot">&lt;-</span> <span class="cf">function</span>(employees) {</span>
<span id="cb55-2"><a href="#cb55-2" tabindex="-1"></a>  <span class="cf">if</span> (employees <span class="sc">&gt;</span> <span class="dv">100</span>) {</span>
<span id="cb55-3"><a href="#cb55-3" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">&quot;large&quot;</span>)</span>
<span id="cb55-4"><a href="#cb55-4" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb55-5"><a href="#cb55-5" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">&quot;small&quot;</span>)</span>
<span id="cb55-6"><a href="#cb55-6" tabindex="-1"></a>  }</span>
<span id="cb55-7"><a href="#cb55-7" tabindex="-1"></a>}</span></code></pre></div>
<p>I think, at this point, this code should make sense to you. The only new thing is the if/else. The following is not code that will actually run but is just to help understand the logic of if/else.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" tabindex="-1"></a><span class="cf">if</span> (condition) {</span>
<span id="cb56-2"><a href="#cb56-2" tabindex="-1"></a>  <span class="co"># do something</span></span>
<span id="cb56-3"><a href="#cb56-3" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb56-4"><a href="#cb56-4" tabindex="-1"></a>  <span class="co"># do something else</span></span>
<span id="cb56-5"><a href="#cb56-5" tabindex="-1"></a>}</span></code></pre></div>
<p>All that happens with if/else is that we check whether <code>condition</code> evaluate to <code>TRUE</code> or <code>FALSE</code>. If it is <code>TRUE</code>, the code will do whatever is inside the first set of brackets; if it is <code>FALSE</code>, the code will do whatever is in the set of brackets following <code>else</code>.</p>
</div>
<div id="for-loops" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> for loops<a href="#for-loops" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 3.4</p>
<p>Often, we need to run the same code over and over again. A <code>for</code> loop is a main programming tool for this case (<code>for</code> loops show up in pretty much all programming languages).</p>
<p>We’ll have more realistic examples later on in the semester, but we’ll do something trivial for now.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb57-2"><a href="#cb57-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb57-3"><a href="#cb57-3" tabindex="-1"></a>  out[i] <span class="ot">&lt;-</span> i<span class="sc">*</span><span class="dv">3</span></span>
<span id="cb57-4"><a href="#cb57-4" tabindex="-1"></a>}</span>
<span id="cb57-5"><a href="#cb57-5" tabindex="-1"></a>out</span>
<span id="cb57-6"><a href="#cb57-6" tabindex="-1"></a><span class="co">#&gt;  [1]  3  6  9 12 15 18 21 24 27 30</span></span></code></pre></div>
<p>The above code, starts with <span class="math inline">\(i=1\)</span>, calculates <span class="math inline">\(i*3\)</span> (which is 3), and then stores that result in the first element of the vector <code>out</code>, then <span class="math inline">\(i\)</span> increases to 2, the code calculates <span class="math inline">\(i*3\)</span> (which is now 6), and stores this result in the second element of <code>out</code>, and so on through <span class="math inline">\(i=10\)</span>.</p>
</div>
<div id="vectorization" class="section level3 hasAnchor" number="2.7.4">
<h3><span class="header-section-number">2.7.4</span> Vectorization<a href="#vectorization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS 3.5</p>
<p>Vectorizing functions is a relatively advanced topic in R programming, but it is an important one, so I am including it here.</p>
<p>Because we will often be working with data, we will often be performing the same operation on all of the observations in the data. For example, suppose that you wanted to take the logarithm of the number of employees for all the firms in <code>firm_data</code>. One way to do this is to use a <code>for</code> loop, but this code would be a bit of a mess. Instead, the function <code>log</code> is <strong>vectorized</strong> — this means that if we apply it to a vector, it will calculate the logarithm of each element in the vector. Besides this, vectorized functions are often faster than <code>for</code> loops.</p>
<p>Not all functions are vectorized though. Let’s go back to our function earlier called <code>large_or_small</code>. This took in the number of employees at a firm and then printed “large” if the firm had more than 100 employees and “small” otherwise. Let’s see what happens if we call this function on a vector of employees (Ideally, we’d like the function to be applied to each element in the vector).</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" tabindex="-1"></a>employees <span class="ot">&lt;-</span> firm_data<span class="sc">$</span>employees</span>
<span id="cb58-2"><a href="#cb58-2" tabindex="-1"></a>employees</span>
<span id="cb58-3"><a href="#cb58-3" tabindex="-1"></a><span class="co">#&gt; [1] 531   6  15 211  25</span></span>
<span id="cb58-4"><a href="#cb58-4" tabindex="-1"></a><span class="fu">large_or_small</span>(employees)</span>
<span id="cb58-5"><a href="#cb58-5" tabindex="-1"></a><span class="co">#&gt; Error in if (employees &gt; 100) {: the condition has length &gt; 1</span></span></code></pre></div>
<p>This is not what we wanted to have happen. Instead of determining whether each firm was large or small, we get an error basically said that something may be going wrong here. What’s going on here is that the function <code>large_or_small</code> is not vectorized.</p>
<p>In order to vectorize a function, we can use one of a number of “apply” functions in R. I’ll list them here</p>
<ul>
<li><p><code>sapply</code> — this stands for “simplify” apply; it “applies” the function to all the elements in the vector or list that you pass in and then tries to “simplify” the result</p></li>
<li><p><code>lapply</code> — stands for “list” apply; applies a function to all elements in a vector or list and then returns a list</p></li>
<li><p><code>vapply</code> — stands for “vector” apply; applies a function to all elements in a vector or list and then returns a vector</p></li>
<li><p><code>apply</code> — applies a function to either the rows or columns of a matrix-like object (i.e., a matrix or a data frame) depending on the value of the argument <code>MARGIN</code></p></li>
</ul>
<p>Let’s use <code>sapply</code> to vectorize <code>large_or_small</code>.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" tabindex="-1"></a>large_or_small_vectorized <span class="ot">&lt;-</span> <span class="cf">function</span>(employees_vec) {</span>
<span id="cb59-2"><a href="#cb59-2" tabindex="-1"></a>  <span class="fu">sapply</span>(employees_vec, <span class="at">FUN =</span> large_or_small)</span>
<span id="cb59-3"><a href="#cb59-3" tabindex="-1"></a>}</span></code></pre></div>
<p>All that this will do is call the function <code>large_or_small</code> for each element in the vector <code>employees</code>. Let’s see it in action</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" tabindex="-1"></a><span class="fu">large_or_small_vectorized</span>(employees)</span>
<span id="cb60-2"><a href="#cb60-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;large&quot;</span></span>
<span id="cb60-3"><a href="#cb60-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;small&quot;</span></span>
<span id="cb60-4"><a href="#cb60-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;small&quot;</span></span>
<span id="cb60-5"><a href="#cb60-5" tabindex="-1"></a><span class="co">#&gt; [1] &quot;large&quot;</span></span>
<span id="cb60-6"><a href="#cb60-6" tabindex="-1"></a><span class="co">#&gt; [1] &quot;small&quot;</span></span>
<span id="cb60-7"><a href="#cb60-7" tabindex="-1"></a><span class="co">#&gt; [1] &quot;large&quot; &quot;small&quot; &quot;small&quot; &quot;large&quot; &quot;small&quot;</span></span></code></pre></div>
<p>This is what we were hoping for.</p>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> I also typically replace most all <code>for</code> loops with an <code>apply</code> function. In most cases, I don’t think there is much of a performance gain, but the code seems easier to read (or at least more concise).</p>
<p>Earlier we wrote a function to take a vector of numbers from 1 to 10 and multiply all of them by 3. Here’s how you could do this using <code>sapply</code></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" tabindex="-1"></a><span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="cf">function</span>(i) i<span class="sc">*</span><span class="dv">3</span>)</span>
<span id="cb61-2"><a href="#cb61-2" tabindex="-1"></a><span class="co">#&gt;  [1]  3  6  9 12 15 18 21 24 27 30</span></span></code></pre></div>
<p>which is considerably shorter.</p>
<p>One last thing worth pointing out though is that multiplication is already vectorized, so you don’t actually need to do <code>sapply</code> or the <code>for</code> loop; a better way is just</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" tabindex="-1"></a>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">3</span></span>
<span id="cb62-2"><a href="#cb62-2" tabindex="-1"></a><span class="co">#&gt;  [1]  3  6  9 12 15 18 21 24 27 30</span></span></code></pre></div>
</div>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> A relatively popular alternative to <code>apply</code> functions are <code>map</code> functions provided in the <code>purrr</code> package.</p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> It’s often helpful to have a vectorized version of if/else. In <code>R</code>, this is available in the function <code>ifelse</code>. Here is an alternative way to vectorize the function <code>large_or_small</code>:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" tabindex="-1"></a>large_or_small_vectorized2 <span class="ot">&lt;-</span> <span class="cf">function</span>(employees_vec) {</span>
<span id="cb63-2"><a href="#cb63-2" tabindex="-1"></a>  <span class="fu">ifelse</span>(employees_vec <span class="sc">&gt;</span> <span class="dv">100</span>, <span class="st">&quot;large&quot;</span>, <span class="st">&quot;small&quot;</span>)</span>
<span id="cb63-3"><a href="#cb63-3" tabindex="-1"></a>}</span>
<span id="cb63-4"><a href="#cb63-4" tabindex="-1"></a><span class="fu">large_or_small_vectorized2</span>(firm_data<span class="sc">$</span>employees)</span>
<span id="cb63-5"><a href="#cb63-5" tabindex="-1"></a><span class="co">#&gt; [1] &quot;large&quot; &quot;small&quot; &quot;small&quot; &quot;large&quot; &quot;small&quot;</span></span></code></pre></div>
<p>Here you can see that <code>ifelse</code> makes every comparison in its first argument, and then returns the second element for every <code>TRUE</code> coming from the first argument, and returns the third element for every <code>FALSE</code> coming from the first argument.</p>
<p><code>ifelse</code> also works with vectors in the second and third element. For example:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" tabindex="-1"></a>  <span class="fu">ifelse</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>) <span class="sc">&lt;</span> <span class="dv">4</span>, <span class="at">yes=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="at">no=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>))</span>
<span id="cb64-2"><a href="#cb64-2" tabindex="-1"></a><span class="co">#&gt; [1] 1 2 6</span></span></code></pre></div>
<p>which picks up 1 and 2 from the second (<code>yes</code>) argument and 6 from the third (<code>no</code>) argument.</p>
</div>
</div>
</div>
<div id="advanced-topics" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Advanced Topics<a href="#advanced-topics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To conclude this section, I want to briefly point you towards some advanced material. We will probably brush up against some of this material this semester. That being said, R has some very advanced capabilities related to data science, data manipulation, and data visualization. If you have time/interest you might push further in all of these directions. By the end of the semester, we may not have mastered these topics, but they should at least be accessible to you.</p>
<div id="tidyverse" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Tidyverse<a href="#tidyverse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS Chapter 4 — strongly recommend that you read this</p>
<ul>
<li><p>R has very good data cleaning / manipulating tools</p>
<ul>
<li><p>Many of them are in the <a href="https://www.tidyverse.org/">“tidyverse”</a></p></li>
<li><p>Mostly this semester, I’ll just give you a data set that is ready to be worked with. But as you move to your own research projects or do work for a company one day, you will realize that a major step in analyzing data is organizing (“cleaning”) the data in a way that you can analyze it</p></li>
</ul></li>
<li><p>Main packages</p>
<ul>
<li><p><code>ggplot2</code> – see below</p></li>
<li><p><code>dplyr</code> — package to manipulate data</p></li>
<li><p><code>tidyr</code> — more ways to manipulate data</p></li>
<li><p><code>readr</code> — read in data</p></li>
<li><p><code>purrr</code> — alternative versions of <code>apply</code> functions and <code>for</code> loops</p></li>
<li><p><code>tibble</code> — alternative versions of <code>data.frame</code></p></li>
<li><p><code>stringr</code> — tools for working with strings</p></li>
<li><p><code>forcats</code> — tools for working with factors</p></li>
</ul></li>
<li><p>I won’t emphasize these too much as they are somewhat advanced topics, but if you are interested, these are good (and marketable) skills to have</p></li>
</ul>
</div>
<div id="data-visualization" class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> Data Visualization<a href="#data-visualization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS Ch. 6-11 — <code>R</code> has very good data visualization tools; strongly recommend that you read this</p>
<ul>
<li><p>Another very strong point of <code>R</code></p></li>
<li><p>Base <code>R</code> comes with the <code>plot</code> command, but the <code>ggplot2</code> package provides cutting edge plotting tools. These tools will be somewhat harder to learn, but we’ll use <code>ggplot2</code> this semester as I think it is worth it.</p></li>
<li><p>538’s graphs produced with ggplot</p></li>
</ul>
</div>
<div id="reproducible-research" class="section level3 hasAnchor" number="2.8.3">
<h3><span class="header-section-number">2.8.3</span> Reproducible Research<a href="#reproducible-research" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Related Reading: IDS Ch. 40</p>
<ul>
<li><p>Rmarkdown is a very useful way to mix code and content</p></li>
<li><p>These notes are written in Rmarkdown, and I usually write homework solutions in Rmarkdown</p></li>
<li><p>If you are interested, you can view the source for this book at <a href="http://github.com/bcallaway11/econ_4750_notes">http://github.com/bcallaway11/econ_4750_notes</a>. The source code for this chapter is in the file <code>01-statistical-programming.Rmd</code>.</p></li>
<li><p>If you are interested, <a href="http://github.com">Github</a> is a very useful version control tool (i.e., keeps track of the version of your project, useful for merging projects, and sharing or co-authoring code) and <a href="http://dropbox.com">Dropbox</a> (also useful for sharing code). I use both of these extensively — in general, I use Github relatively more for bigger projects and more public projects and Dropbox more for smaller projects and early versions of projects.</p></li>
</ul>
</div>
<div id="technical-writing-tools" class="section level3 hasAnchor" number="2.8.4">
<h3><span class="header-section-number">2.8.4</span> Technical Writing Tools<a href="#technical-writing-tools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is largely beyond the scope of the course, but, especially for students in ECON 6750, I recommend that you look up Latex. This is a markup language mainly for writing technical, academic writing. The big payoff is on writing mathematical equations. The equations in the Course notes are written in Latex.</p>
<p>An easy way to get started here is to use the website <a href="http://www.overleaf.com">Overleaf</a>. This is also closely related to markdown/R-markdown discussed above (Latex tends to be somewhat more complicate which comes with some associated advantages and disadvantages).</p>
</div>
</div>
<div id="lab-1-introduction-to-r-programming" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Lab 1: Introduction to R Programming<a href="#lab-1-introduction-to-r-programming" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this lab, we will do several practice problems related to programming in R.</p>
<ol style="list-style-type: decimal">
<li><p>Create two vectors as follows</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">2</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="dv">2</span>)</span>
<span id="cb65-2"><a href="#cb65-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">11</span>,<span class="dv">13</span>)</span></code></pre></div>
<p>Add <code>x</code> and <code>y</code>, subtract <code>y</code> from <code>x</code>, multiply <code>x</code> and <code>y</code>, and divide <code>x</code> by <code>y</code> and report your results.</p></li>
<li><p>The geometric mean of a set of numbers is an alternative measure of central tendency to the more common “arithmetic mean” (this is the mean that we are used to). For a set of <span class="math inline">\(J\)</span> numbers, <span class="math inline">\(x_1,x_2,\ldots,x_J\)</span>, the geometric mean is defined as</p>
<p><span class="math display">\[
   (x_1 \cdot x_2 \cdot \cdots \cdot x_J)^{1/J}
\]</span></p>
<p>Write a function called <code>geometric_mean</code> that takes in a vector of numbers and computes their geometric mean. Compute the geometric mean of <code>c(10,8,13)</code></p></li>
<li><p>Use the <code>lubridate</code> package to figure out how many days it has been since Jan. 1, 1981.</p></li>
<li><p><code>mtcars</code> is one of the data frames that comes packaged with base R.</p>
<ol style="list-style-type: lower-alpha">
<li><p>How many observations does <code>mtcars</code> have?</p></li>
<li><p>How many columns does <code>mtcars</code> have?</p></li>
<li><p>What are the names of the columns of <code>mtcars</code>?</p></li>
<li><p>Print only the rows of <code>mtcars</code> for cars that get at least 20 mpg</p></li>
<li><p>Print only the rows of <code>mtcars</code> that get at least 20 mpg and have at least 100 horsepower (it is in the column called <code>hp</code>)</p></li>
<li><p>Print only the rows of <code>mtcars</code> that have 6 or more cylinders (it is in the column labeld <code>cyl</code>) or at least 100 horsepower</p></li>
<li><p>Recover the 10th row of <code>mtcars</code></p></li>
<li><p>Sort the rows of <code>mtcars</code> by mpg (from highest to lowest)</p></li>
</ol></li>
</ol>
</div>
<div id="lab-1-solutions" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Lab 1: Solutions<a href="#lab-1-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">2</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="dv">2</span>)</span>
<span id="cb66-2"><a href="#cb66-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">11</span>,<span class="dv">13</span>)</span>
<span id="cb66-3"><a href="#cb66-3" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" tabindex="-1"></a>x<span class="sc">+</span>y</span>
<span id="cb66-5"><a href="#cb66-5" tabindex="-1"></a><span class="co">#&gt; [1]  5  9 13 19 23</span></span>
<span id="cb66-6"><a href="#cb66-6" tabindex="-1"></a>x<span class="sc">-</span>y</span>
<span id="cb66-7"><a href="#cb66-7" tabindex="-1"></a><span class="co">#&gt; [1] -1 -1 -1 -3 -3</span></span>
<span id="cb66-8"><a href="#cb66-8" tabindex="-1"></a>x<span class="sc">*</span>y</span>
<span id="cb66-9"><a href="#cb66-9" tabindex="-1"></a><span class="co">#&gt; [1]   6  20  42  88 130</span></span>
<span id="cb66-10"><a href="#cb66-10" tabindex="-1"></a>x<span class="sc">/</span>y</span>
<span id="cb66-11"><a href="#cb66-11" tabindex="-1"></a><span class="co">#&gt; [1] 0.6666667 0.8000000 0.8571429 0.7272727 0.7692308</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" tabindex="-1"></a>geometric_mean <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb67-2"><a href="#cb67-2" tabindex="-1"></a>  J <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb67-3"><a href="#cb67-3" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">prod</span>(x)<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>J)</span>
<span id="cb67-4"><a href="#cb67-4" tabindex="-1"></a>  res</span>
<span id="cb67-5"><a href="#cb67-5" tabindex="-1"></a>}</span>
<span id="cb67-6"><a href="#cb67-6" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" tabindex="-1"></a><span class="fu">geometric_mean</span>(<span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">8</span>,<span class="dv">13</span>))</span>
<span id="cb67-8"><a href="#cb67-8" tabindex="-1"></a><span class="co">#&gt; [1] 10.13159</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" tabindex="-1"></a>the_date <span class="ot">&lt;-</span> lubridate<span class="sc">::</span><span class="fu">mdy</span>(<span class="st">&quot;01-01-1981&quot;</span>)</span>
<span id="cb68-2"><a href="#cb68-2" tabindex="-1"></a>today <span class="ot">&lt;-</span> lubridate<span class="sc">::</span><span class="fu">mdy</span>(<span class="st">&quot;08-23-2021&quot;</span>)</span>
<span id="cb68-3"><a href="#cb68-3" tabindex="-1"></a>today <span class="sc">-</span> the_date</span>
<span id="cb68-4"><a href="#cb68-4" tabindex="-1"></a><span class="co">#&gt; Time difference of 14844 days</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li><ol style="list-style-type: lower-alpha">
<li><div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" tabindex="-1"></a><span class="fu">nrow</span>(mtcars)</span>
<span id="cb69-2"><a href="#cb69-2" tabindex="-1"></a><span class="co">#&gt; [1] 32</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" tabindex="-1"></a><span class="fu">ncol</span>(mtcars)</span>
<span id="cb70-2"><a href="#cb70-2" tabindex="-1"></a><span class="co">#&gt; [1] 11</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" tabindex="-1"></a><span class="fu">colnames</span>(mtcars)</span>
<span id="cb71-2"><a href="#cb71-2" tabindex="-1"></a><span class="co">#&gt;  [1] &quot;mpg&quot;  &quot;cyl&quot;  &quot;disp&quot; &quot;hp&quot;   &quot;drat&quot; &quot;wt&quot;   &quot;qsec&quot; &quot;vs&quot;   &quot;am&quot;   &quot;gear&quot; &quot;carb&quot;</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" tabindex="-1"></a><span class="fu">subset</span>(mtcars, mpg <span class="sc">&gt;=</span> <span class="dv">20</span>)</span>
<span id="cb72-2"><a href="#cb72-2" tabindex="-1"></a><span class="co">#&gt;                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb</span></span>
<span id="cb72-3"><a href="#cb72-3" tabindex="-1"></a><span class="co">#&gt; Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4</span></span>
<span id="cb72-4"><a href="#cb72-4" tabindex="-1"></a><span class="co">#&gt; Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4</span></span>
<span id="cb72-5"><a href="#cb72-5" tabindex="-1"></a><span class="co">#&gt; Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1</span></span>
<span id="cb72-6"><a href="#cb72-6" tabindex="-1"></a><span class="co">#&gt; Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1</span></span>
<span id="cb72-7"><a href="#cb72-7" tabindex="-1"></a><span class="co">#&gt; Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2</span></span>
<span id="cb72-8"><a href="#cb72-8" tabindex="-1"></a><span class="co">#&gt; Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2</span></span>
<span id="cb72-9"><a href="#cb72-9" tabindex="-1"></a><span class="co">#&gt; Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1</span></span>
<span id="cb72-10"><a href="#cb72-10" tabindex="-1"></a><span class="co">#&gt; Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2</span></span>
<span id="cb72-11"><a href="#cb72-11" tabindex="-1"></a><span class="co">#&gt; Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1</span></span>
<span id="cb72-12"><a href="#cb72-12" tabindex="-1"></a><span class="co">#&gt; Toyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1</span></span>
<span id="cb72-13"><a href="#cb72-13" tabindex="-1"></a><span class="co">#&gt; Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1</span></span>
<span id="cb72-14"><a href="#cb72-14" tabindex="-1"></a><span class="co">#&gt; Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2</span></span>
<span id="cb72-15"><a href="#cb72-15" tabindex="-1"></a><span class="co">#&gt; Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2</span></span>
<span id="cb72-16"><a href="#cb72-16" tabindex="-1"></a><span class="co">#&gt; Volvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" tabindex="-1"></a><span class="fu">subset</span>(mtcars, (mpg <span class="sc">&gt;=</span> <span class="dv">20</span>) <span class="sc">&amp;</span> (hp <span class="sc">&gt;=</span> <span class="dv">100</span>))</span>
<span id="cb73-2"><a href="#cb73-2" tabindex="-1"></a><span class="co">#&gt;                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb</span></span>
<span id="cb73-3"><a href="#cb73-3" tabindex="-1"></a><span class="co">#&gt; Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4</span></span>
<span id="cb73-4"><a href="#cb73-4" tabindex="-1"></a><span class="co">#&gt; Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4</span></span>
<span id="cb73-5"><a href="#cb73-5" tabindex="-1"></a><span class="co">#&gt; Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1</span></span>
<span id="cb73-6"><a href="#cb73-6" tabindex="-1"></a><span class="co">#&gt; Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2</span></span>
<span id="cb73-7"><a href="#cb73-7" tabindex="-1"></a><span class="co">#&gt; Volvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" tabindex="-1"></a><span class="fu">subset</span>(mtcars, (cyl <span class="sc">&gt;=</span> <span class="dv">6</span>) <span class="sc">|</span> (hp <span class="sc">&gt;=</span> <span class="dv">100</span>))</span>
<span id="cb74-2"><a href="#cb74-2" tabindex="-1"></a><span class="co">#&gt;                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb</span></span>
<span id="cb74-3"><a href="#cb74-3" tabindex="-1"></a><span class="co">#&gt; Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4</span></span>
<span id="cb74-4"><a href="#cb74-4" tabindex="-1"></a><span class="co">#&gt; Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4</span></span>
<span id="cb74-5"><a href="#cb74-5" tabindex="-1"></a><span class="co">#&gt; Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1</span></span>
<span id="cb74-6"><a href="#cb74-6" tabindex="-1"></a><span class="co">#&gt; Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2</span></span>
<span id="cb74-7"><a href="#cb74-7" tabindex="-1"></a><span class="co">#&gt; Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1</span></span>
<span id="cb74-8"><a href="#cb74-8" tabindex="-1"></a><span class="co">#&gt; Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4</span></span>
<span id="cb74-9"><a href="#cb74-9" tabindex="-1"></a><span class="co">#&gt; Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4</span></span>
<span id="cb74-10"><a href="#cb74-10" tabindex="-1"></a><span class="co">#&gt; Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4</span></span>
<span id="cb74-11"><a href="#cb74-11" tabindex="-1"></a><span class="co">#&gt; Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3</span></span>
<span id="cb74-12"><a href="#cb74-12" tabindex="-1"></a><span class="co">#&gt; Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3</span></span>
<span id="cb74-13"><a href="#cb74-13" tabindex="-1"></a><span class="co">#&gt; Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3</span></span>
<span id="cb74-14"><a href="#cb74-14" tabindex="-1"></a><span class="co">#&gt; Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4</span></span>
<span id="cb74-15"><a href="#cb74-15" tabindex="-1"></a><span class="co">#&gt; Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4</span></span>
<span id="cb74-16"><a href="#cb74-16" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4</span></span>
<span id="cb74-17"><a href="#cb74-17" tabindex="-1"></a><span class="co">#&gt; Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2</span></span>
<span id="cb74-18"><a href="#cb74-18" tabindex="-1"></a><span class="co">#&gt; AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2</span></span>
<span id="cb74-19"><a href="#cb74-19" tabindex="-1"></a><span class="co">#&gt; Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4</span></span>
<span id="cb74-20"><a href="#cb74-20" tabindex="-1"></a><span class="co">#&gt; Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2</span></span>
<span id="cb74-21"><a href="#cb74-21" tabindex="-1"></a><span class="co">#&gt; Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2</span></span>
<span id="cb74-22"><a href="#cb74-22" tabindex="-1"></a><span class="co">#&gt; Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4</span></span>
<span id="cb74-23"><a href="#cb74-23" tabindex="-1"></a><span class="co">#&gt; Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6</span></span>
<span id="cb74-24"><a href="#cb74-24" tabindex="-1"></a><span class="co">#&gt; Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8</span></span>
<span id="cb74-25"><a href="#cb74-25" tabindex="-1"></a><span class="co">#&gt; Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" tabindex="-1"></a>mtcars[<span class="dv">10</span>,]</span>
<span id="cb75-2"><a href="#cb75-2" tabindex="-1"></a><span class="co">#&gt;           mpg cyl  disp  hp drat   wt qsec vs am gear carb</span></span>
<span id="cb75-3"><a href="#cb75-3" tabindex="-1"></a><span class="co">#&gt; Merc 280 19.2   6 167.6 123 3.92 3.44 18.3  1  0    4    4</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" tabindex="-1"></a><span class="co"># without reversing the order, we would order from lowest to smallest</span></span>
<span id="cb76-2"><a href="#cb76-2" tabindex="-1"></a>mtcars[<span class="fu">rev</span>(<span class="fu">order</span>(mtcars<span class="sc">$</span>mpg)),]</span>
<span id="cb76-3"><a href="#cb76-3" tabindex="-1"></a><span class="co">#&gt;                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb</span></span>
<span id="cb76-4"><a href="#cb76-4" tabindex="-1"></a><span class="co">#&gt; Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1</span></span>
<span id="cb76-5"><a href="#cb76-5" tabindex="-1"></a><span class="co">#&gt; Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1</span></span>
<span id="cb76-6"><a href="#cb76-6" tabindex="-1"></a><span class="co">#&gt; Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2</span></span>
<span id="cb76-7"><a href="#cb76-7" tabindex="-1"></a><span class="co">#&gt; Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2</span></span>
<span id="cb76-8"><a href="#cb76-8" tabindex="-1"></a><span class="co">#&gt; Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1</span></span>
<span id="cb76-9"><a href="#cb76-9" tabindex="-1"></a><span class="co">#&gt; Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2</span></span>
<span id="cb76-10"><a href="#cb76-10" tabindex="-1"></a><span class="co">#&gt; Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2</span></span>
<span id="cb76-11"><a href="#cb76-11" tabindex="-1"></a><span class="co">#&gt; Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2</span></span>
<span id="cb76-12"><a href="#cb76-12" tabindex="-1"></a><span class="co">#&gt; Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1</span></span>
<span id="cb76-13"><a href="#cb76-13" tabindex="-1"></a><span class="co">#&gt; Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1</span></span>
<span id="cb76-14"><a href="#cb76-14" tabindex="-1"></a><span class="co">#&gt; Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</span></span>
<span id="cb76-15"><a href="#cb76-15" tabindex="-1"></a><span class="co">#&gt; Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1</span></span>
<span id="cb76-16"><a href="#cb76-16" tabindex="-1"></a><span class="co">#&gt; Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4</span></span>
<span id="cb76-17"><a href="#cb76-17" tabindex="-1"></a><span class="co">#&gt; Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4</span></span>
<span id="cb76-18"><a href="#cb76-18" tabindex="-1"></a><span class="co">#&gt; Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6</span></span>
<span id="cb76-19"><a href="#cb76-19" tabindex="-1"></a><span class="co">#&gt; Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2</span></span>
<span id="cb76-20"><a href="#cb76-20" tabindex="-1"></a><span class="co">#&gt; Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4</span></span>
<span id="cb76-21"><a href="#cb76-21" tabindex="-1"></a><span class="co">#&gt; Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2</span></span>
<span id="cb76-22"><a href="#cb76-22" tabindex="-1"></a><span class="co">#&gt; Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1</span></span>
<span id="cb76-23"><a href="#cb76-23" tabindex="-1"></a><span class="co">#&gt; Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4</span></span>
<span id="cb76-24"><a href="#cb76-24" tabindex="-1"></a><span class="co">#&gt; Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3</span></span>
<span id="cb76-25"><a href="#cb76-25" tabindex="-1"></a><span class="co">#&gt; Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3</span></span>
<span id="cb76-26"><a href="#cb76-26" tabindex="-1"></a><span class="co">#&gt; Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4</span></span>
<span id="cb76-27"><a href="#cb76-27" tabindex="-1"></a><span class="co">#&gt; Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2</span></span>
<span id="cb76-28"><a href="#cb76-28" tabindex="-1"></a><span class="co">#&gt; AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2</span></span>
<span id="cb76-29"><a href="#cb76-29" tabindex="-1"></a><span class="co">#&gt; Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3</span></span>
<span id="cb76-30"><a href="#cb76-30" tabindex="-1"></a><span class="co">#&gt; Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8</span></span>
<span id="cb76-31"><a href="#cb76-31" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4</span></span>
<span id="cb76-32"><a href="#cb76-32" tabindex="-1"></a><span class="co">#&gt; Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4</span></span>
<span id="cb76-33"><a href="#cb76-33" tabindex="-1"></a><span class="co">#&gt; Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4</span></span>
<span id="cb76-34"><a href="#cb76-34" tabindex="-1"></a><span class="co">#&gt; Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4</span></span>
<span id="cb76-35"><a href="#cb76-35" tabindex="-1"></a><span class="co">#&gt; Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4</span></span></code></pre></div></li>
</ol></li>
</ol>
</div>
<div id="coding-exercises" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Coding Exercises<a href="#coding-exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>The <code>stringr</code> package contains a number of functions for working with strings. For this problem create the following character vector in R</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;economics&quot;</span>, <span class="st">&quot;econometrics&quot;</span>, <span class="st">&quot;ECON 4750&quot;</span>)</span></code></pre></div>
<p>Install the <code>stringr</code> package and use the <code>str_length</code> function in the package in order to calculate the length (number of characters) in each element of <code>x</code>.</p></li>
<li><p>For this problem, we are going to write a function to calculate the sum of the numbers from 1 to <span class="math inline">\(n\)</span> where <span class="math inline">\(n\)</span> is some positive integer. There are actually a lot of different ways to do this.</p>
<ul>
<li><p>Approach 1: write a function called <code>sum_one_to_n_1</code> that uses the R functions <code>seq</code> to create a list of numbers from 1 to <span class="math inline">\(n\)</span> and then the function <code>sum</code> to sum over that list.</p></li>
<li><p>Approach 2: The sum of numbers from 1 to <span class="math inline">\(n\)</span> is equal to <span class="math inline">\(n(n+1)/2\)</span>. Use this expression to write a function called <code>sum_one_to_n_2</code> to calculate the sum from 1 to <span class="math inline">\(n\)</span>.<br />
</p></li>
<li><p>Approach 3: A more brute force approach is to create a list of numbers from 1 to <span class="math inline">\(n\)</span> (you can use <code>seq</code> here) and add them up using a <code>for</code> loop — basically, just keep track of what the current total is and add the next number to the total in each iteration of the for loop. Write a function called <code>sum_one_to_n_3</code> that does this.</p></li>
</ul>
<p><strong>Hint:</strong> All of the functions should look like</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" tabindex="-1"></a>sum_one_to_n <span class="ot">&lt;-</span> <span class="cf">function</span>(n) {</span>
<span id="cb78-2"><a href="#cb78-2" tabindex="-1"></a>  <span class="co"># do something</span></span>
<span id="cb78-3"><a href="#cb78-3" tabindex="-1"></a>}</span></code></pre></div>
<p>Try out all three approaches that you came up with above for <span class="math inline">\(n=100\)</span>. What is the answer? Do you get the same answer using all three approaches?</p></li>
<li><p>The Fibonacci sequence is the sequence of numbers <span class="math inline">\(0,1,1,2,3,5,8,13,21,34,55,\ldots\)</span> that comes from starting with <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and where each subsequent number is the sum of the previous two. For example, the 5 in the sequence comes from adding 2 and 3; the 55 in the sequence comes from adding 21 and 34.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Write a function called <code>fibonacci</code> that takes in a number <code>n</code> and computes the nth element in the Fibonacci sequence. For example <code>fibonacci(5)</code> should return <code>3</code> and <code>fibonacci(8)</code> should return <code>13</code>.</p></li>
<li><p>Consider an alternative sequence where, starting with the third element, each element is computed as the sum of the previous two elements (the same as with the Fibonacci sequence) but where the first two elements can be arbitrary. Write a function <code>alt_seq(a,b,n)</code> where <code>a</code> is the first element in the sequence, <code>b</code> is the second element in the sequence, and <code>n</code> is which element in the sequence to return. For example, if <span class="math inline">\(a=3\)</span> and <span class="math inline">\(b=7\)</span>, then the sequence would be <span class="math inline">\(3,7,10,17,27,44,71,\ldots\)</span> and <code>alt_seq(a=3,b=7,n=4) = 17</code>.</p></li>
</ol></li>
<li><p>Write a function <code>prime</code> that takes <code>n</code> as an argument and returns a vector of all the prime numbers from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span>.</p></li>
<li><p>Base <code>R</code> includes a data frame called <code>iris</code>. This is data about iris flowers (you can read the details by running <code>?iris</code>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>How many observations are there in the entire data frame?</p></li>
<li><p>Calculate the average <code>Sepal.Length</code> across all observations in <code>iris</code>.</p></li>
<li><p>Calculate the average <code>Sepal.Width</code> among the <code>setosa</code> iris species.</p></li>
<li><p>Sort <code>iris</code> by <code>Petal.Length</code> and print the first 10 rows.</p></li>
</ol></li>
</ol>

</div>
</div>
<div id="probability" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Topic 3</span> Probability<a href="#probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section contains our crash course review of topics in probability. The discussion mostly follows Chapter 2 in the Stock and Watson textbook, and I have cross-listed the relevant sections in the textbook here.</p>
<p>At a very high level, probability is the set of mathematical tools that allow us to think about <strong>random</strong> events.</p>
<p>Just to be clear, random means <em>uncertain</em>, not 50:50.</p>
<p>A simple example of a random event is the outcome from rolling a die.</p>
<p>Eventually, we will treat data as being random draws from some population. Examples of things that we will treat as random draws are things like a person’s hair color, height, income, etc. We will think of all of these as being random draws because <em>ex ante</em> we don’t know what they will be.</p>
<div id="data-for-this-chapter" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Data for this chapter<a href="#data-for-this-chapter" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this chapter, we’ll use data from the U.S. Census Bureau from 2019. It is not quite a full census, but we’ll treat it as the population throughout this chapter.</p>
</div>
<div id="random-variables" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Random Variables<a href="#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.1</p>
<p>A <strong>random variable</strong> is a numerical summary of some random event.</p>
<p>Some examples:</p>
<ul>
<li><p>Outcome of roll of a die</p></li>
<li><p>A person’s height in inches</p></li>
<li><p>A firm’s profits in a particular year</p></li>
<li><p>Creating a random variable sometime involves “coding” non-numeric outcomes, e.g., setting <code>hair=1</code> if a person’s hair color is black, <code>hair=2</code> if a person’s hair is blonde, etc.</p></li>
</ul>
<p>We’ll generally classify random variables into one of two categories</p>
<ul>
<li><p><strong>Discrete</strong> — A random variable that takes on discrete values such as 0, 1, 2</p></li>
<li><p><strong>Continuous</strong> — Takes on a continuum of values</p></li>
</ul>
<p>These are broad categories because a lot of random variables in economics sit in between these two.</p>
</div>
<div id="pdfs-pmfs-and-cdfs" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> pdfs, pmfs, and cdfs<a href="#pdfs-pmfs-and-cdfs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.1</p>
<p>The <strong>distribution</strong> of a random variable describes how likely it is take on certain values.</p>
<p>A random variable’s distribution is fully summarized by its:</p>
<ul>
<li><p><strong>probability mass function (pmf)</strong> if the random variable is discrete</p></li>
<li><p><strong>probability density function (pdf)</strong> if the random variable is continuous</p></li>
</ul>
<p>The pmf is somewhat easier to explain, so let’s start there. For some discrete random variable <span class="math inline">\(X\)</span>, its pmf is given by</p>
<p><span class="math display">\[
  f_X(x) = \mathrm{P}(X=x)
\]</span>
That is, the probability that <span class="math inline">\(X\)</span> takes on some particular value <span class="math inline">\(x\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 3.1  </strong></span>Suppose that <span class="math inline">\(X\)</span> denotes the outcome of a roll of a die. Then, <span class="math inline">\(f_X(1)\)</span> is the probability of rolling a one. And, in particular,</p>
<p><span class="math display">\[
  f_X(1) = \mathrm{P}(X=1) = \frac{1}{6}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 3.2  </strong></span>Let’s do a bit more realistic example where we look at the pmf of education in the U.S. Suppose that <span class="math inline">\(X\)</span> denotes the years of education that a person has. Then, <span class="math inline">\(f_X(x)\)</span> is the probability that a person has exactly <span class="math inline">\(x\)</span> years of education. We can set <span class="math inline">\(x\)</span> to different values and calculate the probabilities of a person having different amounts of education. That’s what we do in the following figure:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-82"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-82-1.png" alt="pmf of U.S. education" width="672" />
<p class="caption">
Figure 3.1: pmf of U.S. education
</p>
</div>
<p>There are some things that are perhaps worth pointing out here. The most common amount of education in the U.S. appears to be exactly 12 years — corresponding to graduating from high school; about 32% of the population has that level of education. The next most common number of years of education is 16 — corresponding to graduating from college; about 24% of individuals have this level of education. Other relatively common values of education are 13 years (14% of individuals) and 18 (13% of individuals). About 1% of individuals report 0 years of education. It’s not clear to me whether or not that is actually true or reflects some individuals mis-reporting their education.</p>
</div>
<p>Before going back to the pdf, let me describe another way to fully summarize the distribution of a random variable.</p>
<ul>
<li><strong>Cumulative distribution function (cdf)</strong> - The cdf of some random variable <span class="math inline">\(X\)</span> is defined as</li>
</ul>
<p><span class="math display">\[
  F_X(x) = \mathrm{P}(X \leq x)
\]</span>
In words, this cdf is the probability that the random <span class="math inline">\(X\)</span> takes a value less than or equal to <span class="math inline">\(x\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 3.3  </strong></span>Suppose <span class="math inline">\(X\)</span> is the outcome of a roll of a die. Then, <span class="math inline">\(F_X(3) = \mathrm{P}(X \leq 3)\)</span> is the probability of rolling 3 or lower. Thus,</p>
<p><span class="math display">\[
  F_X(3) = \mathrm{P}(X \leq 3) = \frac{1}{2}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 3.4  </strong></span>Let’s go back to our example of years of education in the U.S. In this case, <span class="math inline">\(F_X(x)\)</span> is the fraction of the population that has less than <span class="math inline">\(x\)</span> years of education. We can calculate this for different values of <span class="math inline">\(x\)</span>. That’s what we do in the following figure:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-83"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-83-1.png" alt="cdf of U.S. educ" width="672" />
<p class="caption">
Figure 3.2: cdf of U.S. educ
</p>
</div>
<p>You can see that the cdf is increasing in the years of education. And there are big “jumps” in the cdf at values of years of education that are common such as 12 and 16.</p>
</div>
<p>We’ll go over some properties of pmfs and cdfs momentarily (perhaps you can already deduce some of them from the above figures), but before we do that, we need to go over some (perhaps new) tools.</p>
</div>
<div id="summation-operator" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Summation operator<a href="#summation-operator" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It will be convenient for us to have a notation that allows us to add up many numbers/variables at the same time. To do this, we’ll introduce the <span class="math inline">\(\sum\)</span> operation.</p>
<p>As a simple example, suppose that we have three variables (it doesn’t matter if they are random or not): <span class="math inline">\(x_1,x_2,x_3\)</span> and we want to add them up. Then, we can write
<span class="math display">\[
  \sum_{i=1}^3 x_i := x_1 + x_2 + x_3
\]</span>
Many times, once we have data, there will be n “observations” and we can add them up by:
<span class="math display">\[
  \sum_{i=1}^n x_i = x_1 + x_2 + \cdots + x_n
\]</span>
<strong>Properties:</strong></p>
<ol style="list-style-type: decimal">
<li><p>For any constant <span class="math inline">\(c\)</span>,</p>
<p><span class="math display">\[
\sum_{i=1}^n c = n \cdot c
\]</span></p>
<p>[This is just the definition of multiplication]</p></li>
<li><p>For any constant c,</p>
<p><span class="math display">\[
   \sum_{i=1}^n c x_i = c \sum_{i=1}^n x_i
\]</span></p>
<p>In words: constants can be moved out of the summation.</p>
<p>We will use the property often throughout the semester.</p>
<p>As an example,</p>
<p><span class="math display">\[
   \begin{aligned}
   \sum_{i=1}^3 7 x_i &amp;= 7x_1 + 7x_2 + 7x_3 \\
   &amp;= 7(x_1 + x_2 + x_3) \\
   &amp;= 7 \sum_{i=1}^3 x_i
   \end{aligned}
\]</span></p>
<p>where the first line is just the definition of the summation, the second equality factors out the 7, and the last equality writes the part about adding up the <span class="math inline">\(x\)</span>’s using summation notation.</p></li>
</ol>
</div>
<div id="properties-of-pmfs-and-cdfs" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Properties of pmfs and cdfs<a href="#properties-of-pmfs-and-cdfs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s define the <strong>support</strong> of a random variable <span class="math inline">\(X\)</span> — this is the set of all possible values that <span class="math inline">\(X\)</span> can possibly take. We’ll use the notation <span class="math inline">\(\mathcal{X}\)</span> to denote the support of <span class="math inline">\(X\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 3.5  </strong></span>Suppose <span class="math inline">\(X\)</span> is the outcome from a roll of a die. Then, the support of <span class="math inline">\(X\)</span> is given by <span class="math inline">\(\mathcal{X} = \{1,2,3,4,5,6\}\)</span>. In other words, the only possible values for <span class="math inline">\(X\)</span> are from <span class="math inline">\(1,\ldots,6\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 3.6  </strong></span>Suppose <span class="math inline">\(X\)</span> is the number of years of education that a person has. The support of <span class="math inline">\(X\)</span> is given by <span class="math inline">\(\mathcal{X} = \{0, 1, 2, \ldots, 20\}\)</span>. Perhaps I should have chosen a larger number than 20 to be the maximum possible value that <span class="math inline">\(X\)</span> could take, but you will get the idea — a person’s years of education can be 0 or 1 or 2 or up to some maximum value.</p>
</div>
<p><strong>Properties of pmfs</strong></p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(x\)</span>, <span class="math inline">\(0 \leq f_X(x) \leq 1\)</span></p>
<p>In words: the probability of <span class="math inline">\(X\)</span> taking some particular value can’t be less than 0 or greater than 1 (neither of those would make any sense)</p></li>
<li><p><span class="math inline">\(\sum_{x \in \mathcal{X}} f_X(x) = 1\)</span></p>
<p>In words: if you add up <span class="math inline">\(\mathrm{P}(X=x)\)</span> across all possible values that <span class="math inline">\(X\)</span> could take, they sum to 1.</p></li>
</ol>
<p><strong>Properties of cdfs for discrete random variables</strong></p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(x\)</span>, <span class="math inline">\(0 \leq F_X(x) \leq 1\)</span></p>
<p>In words: the probability that <span class="math inline">\(X\)</span> is less than or equal to some particular value <span class="math inline">\(x\)</span> has to be between 0 and 1.</p></li>
<li><p>If <span class="math inline">\(x_1 &lt; x_2\)</span>, then <span class="math inline">\(F_X(x_1) \leq F_X(x_2)\)</span></p>
<p>In words: the cdf is increasing in <span class="math inline">\(x\)</span> (e.g., it will always be the case that <span class="math inline">\(\mathrm{P}(X \leq 3) \leq \mathrm{P}(X \leq 4)\)</span>).</p></li>
<li><p><span class="math inline">\(F_X(-\infty)=0\)</span> and <span class="math inline">\(F_X(\infty)=1\)</span></p>
<p>In words: if you choose small enough values of <span class="math inline">\(x\)</span>, the probability that <span class="math inline">\(X\)</span> will be less than that is 0; similar (but opposite) logic applies for big values of <span class="math inline">\(x\)</span>.</p></li>
</ol>
<p><strong>Connection between pmfs and cdfs</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(F_X(x) = \displaystyle \sum_{z \in \mathcal{X} \\ z \leq x} f_X(z)\)</span></p>
<p>In words: you can “recover” the cdf from the pmf by adding up the pmf across all possible values that the random variable could take that are less than or equal to <span class="math inline">\(x\)</span>. This will be clearer with an example:</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 3.7  </strong></span>Suppose that <span class="math inline">\(X\)</span> is the outcome of a roll of a die. Earlier we showed that <span class="math inline">\(F_X(3) = 1/2\)</span>. We can calculate this by</p>
<p><span class="math display">\[
  \begin{aligned}
  F_X(3) &amp;= \sum_{\substack{z \in \mathcal{X} \\ z \leq 3}} f_X(z) \\
  &amp;= \sum_{z=1}^3 f_X(z) \\
  &amp;= f_X(1) + f_X(2) + f_X(3) \\
  &amp;= \frac{1}{6} + \frac{1}{6} + \frac{1}{6} \\
  &amp;= \frac{1}{2}
  \end{aligned}
\]</span></p>
</div>
</div>
<div id="continuous-random-variables" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Continuous Random Variables<a href="#continuous-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.1</p>
<p>For continuous random variables, you can define the cdf in exactly the same way as we did for discrete random variables. That is, if <span class="math inline">\(X\)</span> is a continuous random variable,</p>
<p><span class="math display">\[
  F_X(x) = \mathrm{P}(X \leq x)
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 3.8  </strong></span>Suppose <span class="math inline">\(X\)</span> denotes an individual’s yearly wage income. The cdf of <span class="math inline">\(X\)</span> looks like</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-84"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-84-1.png" alt="cdf of U.S. wage income" width="672" />
<p class="caption">
Figure 3.3: cdf of U.S. wage income
</p>
</div>
<p>From the figure, we can see that about 24% of working individuals in the U.S. each $20,000 or less per year, 61% of working individuals earn $50,000 or less, and 88% earn $100,000 or less.</p>
</div>
<p>It’s trickier to define an analogue to the pmf for a continuous random variable (in fact, this is the main reason for our separate treatment of discrete and continuous random variables). For example, suppose <span class="math inline">\(X\)</span> denotes the length of a phone conversation. As long as we can measure time finely enough, the probability that a phone conversation lasts exactly 1189.23975381 seconds (this is about 20 minutes) is 0. Instead, for a continuous random variable, we’ll define its probability density function (pdf) as the derivative of its cdf, that is,</p>
<p><span class="math display">\[
  f_X(x) := \frac{d \, F_X(x)}{d \, x}
\]</span>
Recall that the slope of the cdf will be larger in places where <span class="math inline">\(F_X(x)\)</span> is “steeper”.</p>
<p>Regions where the pdf is larger correspond to more likely values of <span class="math inline">\(X\)</span> — in this sense the pdf is very similar to the pmf.</p>
<p>We can also write the cdf as an integral over the pdf. That is,</p>
<p><span class="math display">\[
  F_X(x) = \int_{-\infty}^x f_X(z) \, dz
\]</span>
Integration is roughly the continuous version of a summation — thus, this expression is very similar to the expression above for the cdf in terms of the pmf when <span class="math inline">\(X\)</span> is discrete.</p>
<p><strong>More properties of cdfs</strong></p>
<ol start="4" style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathrm{P}(X &gt; x) = 1 - \mathrm{P}(X \leq x) = 1-F_X(x)\)</span></p>
<p>In words, if you want to calculate the probability that <span class="math inline">\(X\)</span> <em>is greater than</em> some particular value <span class="math inline">\(x\)</span>, you can do that by calculating <span class="math inline">\(1-F_X(x)\)</span>.</p></li>
<li><p><span class="math inline">\(\mathrm{P}(a \leq X \leq b) = F_X(b) - F_X(a)\)</span></p>
<p>In words: you can also calculate the probability that <span class="math inline">\(X\)</span> falls in some range using the cdf.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 3.9  </strong></span>Suppose <span class="math inline">\(X\)</span> denotes an individual’s yearly wage income. The pdf of <span class="math inline">\(X\)</span> looks like</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-85"></span>
<img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-85-1.png" alt="pdf of U.S. wage income" width="672" />
<p class="caption">
Figure 3.4: pdf of U.S. wage income
</p>
</div>
<p>From the figure, we can see that the most common values of yearly income are around $25-30,000 per year. Notice that this corresponds to the steepest part of the cdf from the previous figure. The right tail of the distribution is also long. This means that, while incomes of $150,000+ are not common, there are some individuals who have incomes that high.</p>
<p>Moreover, we can use the properties of pdfs/cdfs above to calculate some specific probabilities. In particular, we can calculating probabilities by calculating integrals (i.e., regions under the curve) / relating the pdf to the cdf. First, the red region above corresponds to the probability of a person’s income being between $50,000 and $100,000. This is given by <span class="math inline">\(F(100,000) - F(50000)\)</span>. We can compute this in <code>R</code> using the <code>ecdf</code> function. In particular,</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" tabindex="-1"></a>incwage_cdf <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(us_data<span class="sc">$</span>incwage)</span>
<span id="cb79-2"><a href="#cb79-2" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">incwage_cdf</span>(<span class="dv">100000</span>) <span class="sc">-</span> <span class="fu">incwage_cdf</span>(<span class="dv">50000</span>),<span class="dv">3</span>)</span>
<span id="cb79-3"><a href="#cb79-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.27</span></span></code></pre></div>
<p>The green region in the figure is the probability of a person’s income being above $150,000. Using the above properties of cdfs, we can calculate it as <span class="math inline">\(1-F(150000)\)</span> which is</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">incwage_cdf</span>(<span class="dv">150000</span>), <span class="dv">3</span>)</span>
<span id="cb80-2"><a href="#cb80-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.052</span></span></code></pre></div>
</div>
</div>
<div id="expected-values" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Expected Values<a href="#expected-values" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.2</p>
<p>The <strong>expected value</strong> of some random variable <span class="math inline">\(X\)</span> is its (population) mean and is written as <span class="math inline">\(\mathbb{E}[X]\)</span>. [I tend to write <span class="math inline">\(\mathbb{E}[X]\)</span> for the expected value, but you might also see notation like <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\mu_X\)</span> for the expected value.]</p>
<p>The expected value of a random variable is a <em>feature</em> of its distribution. In other words, if you know the distribution of a random variable, then you also know its mean.</p>
<p>The expected value is a measure of <strong>central tendency</strong> (alternative measures of central tendency are the <strong>median</strong> and <strong>mode</strong>).</p>
<p>Expected values are a main concept in the course (and in statistics/econometrics more generally). I think there are two main reasons for this:</p>
<ul>
<li><p>Unlike a cdf, pdf, or pmf, the expected value is a single number. This means that it is easy to report. And, if you only knew one feature (at least a feature that that only involves a single number) of the distribution of some random variable, probably the feature that would be most useful to know would be the mean of the random variable.</p></li>
<li><p>Besides that, there are some computational reasons (we will see these later) that the mean can be easier to estimate than, say, the median of a random variable</p></li>
</ul>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable, then the expected value is defined as</p>
<p><span class="math display">\[
  \mathbb{E}[X] = \sum_{x \in \mathcal{X}} x f_X(x)
\]</span></p>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable, then the expected value is defined as</p>
<p><span class="math display">\[
  \mathbb{E}[X] = \int_{\mathcal{X}} x f_X(x) \, dx
\]</span>
Either way, you can think of these as a weighted average of all possible realizations of the random variable <span class="math inline">\(X\)</span> where the weights are given by the probability of <span class="math inline">\(X\)</span> taking that particular value. This may be more clear with an example…</p>
<div class="example">
<p><span id="exm:unlabeled-div-13" class="example"><strong>Example 3.10  </strong></span>Suppose that <span class="math inline">\(X\)</span> is the outcome from a roll of a die. Then, its expected value is given by</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathbb{E}[X] &amp;= \sum_{x=1}^6 x f_X(x) \\
  &amp;= 1\left(\frac{1}{6}\right) + 2\left(\frac{1}{6}\right) + \cdots + 6\left(\frac{1}{6}\right) \\
  &amp;= 3.5
  \end{aligned}
\]</span></p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> When we start to consider more realistic/interesting applications, we typically won’t know (or be able to easily figure out) <span class="math inline">\(\mathbb{E}[X]\)</span>. Instead, we’ll try to estimate it using available data. We’ll carefully distinguish between <strong>population quantities</strong> like <span class="math inline">\(\mathbb{E}[X]\)</span> and <strong>sample quantities</strong> like an estimate of <span class="math inline">\(\mathbb{E}[X]\)</span> soon.</p>
</div>
</div>
<div id="variance" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Variance<a href="#variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.2</p>
<p>The next most important feature of the distribution of a random variable is its <strong>variance</strong>. The variance of a random variable <span class="math inline">\(X\)</span> is a measure of its “spread”, and we will denote it <span class="math inline">\(\mathrm{var}(X)\)</span> [You might also sometimes see the notation <span class="math inline">\(\sigma^2\)</span> or <span class="math inline">\(\sigma_X^2\)</span> for the variance.] The variance is defined as</p>
<p><span class="math display">\[
  \mathrm{var}(X) := \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
\]</span>
Before we move forward, let’s think about why this is a measure of the spread of a random variable.</p>
<ul>
<li><p><span class="math inline">\((X-\mathbb{E}[X])^2\)</span> is a common way to measure the “distance” between <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathbb{E}[X]\)</span>. It is always positive (whether <span class="math inline">\((X - \mathbb{E}[X])\)</span> is positive or negative) which is a good feature for a measure of distance to have. It is also increasing in <span class="math inline">\(|X-\mathbb{E}[X]|\)</span> which also seems a requirement for a reasonable measure of distance.</p></li>
<li><p>Then, the outer expectation averages the above distance across the distribution of <span class="math inline">\(X\)</span>.</p></li>
</ul>
<p>An alternative expression for <span class="math inline">\(\mathrm{var}(X)\)</span> that is often useful in calculations is</p>
<p><span class="math display">\[
  \mathrm{var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\]</span></p>
<p>Sometimes, we will also consider the <strong>standard deviation</strong> of a random variable. The standard deviation is defined as</p>
<p><span class="math display">\[
  \textrm{sd}(X) := \sqrt{\mathrm{var}(X)}
\]</span>
You might also see the notation <span class="math inline">\(\sigma\)</span> or <span class="math inline">\(\sigma_X\)</span> for the standard deviation.</p>
<p>The standard deviation is often easier to interpret than the variance because it has the same “units” as <span class="math inline">\(X\)</span>. Variance “units” are squared units of <span class="math inline">\(X\)</span>.</p>
<p>That said, variances more often show up in formulas/derivations this semester.</p>
</div>
<div id="mean-and-variance-of-linear-functions" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Mean and Variance of Linear Functions<a href="#mean-and-variance-of-linear-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.2</p>
<p>For this part, suppose that <span class="math inline">\(Y=a + bX\)</span> where <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are random variables while <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are fixed constants.</p>
<p><strong>Properties of Expectations</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}[a] = a\)</span> [In words: the expected value of a constant is just the constant. This holds because there is nothing random about <span class="math inline">\(a\)</span> — we just know what it is.]</p></li>
<li><p><span class="math inline">\(\mathbb{E}[bX] = b\mathbb{E}[X]\)</span> [In words: the expected value of a constant times a random variable is equal to the constant times the expected value of the random variable. We will use this property often this semester.]</p></li>
<li><p><span class="math inline">\(\mathbb{E}[a + bX] = a + b\mathbb{E}[X]\)</span> [In words: expected values “pass through” sums. We will use this property often this semester.]</p></li>
</ol>
<p>You’ll also notice the similarity between the properties of summations and expectations. This is not a coincidence — it holds because expectations are defined as summations (or very closely related, as integrals).</p>
<p><strong>Properties of Variance</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathrm{var}(a) = 0\)</span> [In words: the variance of a constant is equal to 0.]</p></li>
<li><p><span class="math inline">\(\mathrm{var}(bX) = b^2 \mathrm{var}(X)\)</span> [In words: A constant can come out of the variance, but it needs to be squared first.]</p></li>
<li><p><span class="math inline">\(\mathrm{var}(a + bX) = \mathrm{var}(bX) = b^2 \mathrm{var}(X)\)</span></p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 3.11  </strong></span>Later on in the semester, it will sometimes be convenient for us to “standardize” some random variables. We’ll talk more about the reason to do this later, but for now, I’ll just give the typical formula for standardizing a random variable and we’ll see if we can figure out what the mean and variance of the standardized random variable are.</p>
<p><span class="math display">\[
  Y = \frac{ X - \mathbb{E}[X]}{\sqrt{\mathrm{var}(X)}}
\]</span>
Just to be clear here, we are standardizing the random variable <span class="math inline">\(X\)</span> and calling its standardized version <span class="math inline">\(Y\)</span>. Let’s calculate its mean</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[Y] &amp;= \mathbb{E}\left[ \frac{X - \mathbb{E}[X]}{\sqrt{\mathrm{var}(X)}} \right] \\
    &amp;= \frac{1}{\sqrt{\mathrm{var}(X)}} \mathbb{E}\big[ X - \mathbb{E}[X] \big] \\
    &amp;= \frac{1}{\sqrt{\mathrm{var}(X)}} \left( \mathbb{E}[X] - \mathbb{E}\big[\mathbb{E}[X]\big] \right) \\
    &amp;= \frac{1}{\sqrt{\mathrm{var}(X)}} \left( \mathbb{E}[X] - \mathbb{E}[X] \right) \\
    &amp;= 0
  \end{aligned}
\]</span>
where the first equality just comes from the definition of <span class="math inline">\(Y\)</span>, the second equality holds because <span class="math inline">\(1/\sqrt{\mathrm{var}(X)}\)</span> is a constant and can therefore come out of the expectation, the third equality holds because the expectation can pass through the difference, the fourth equality holds because <span class="math inline">\(\mathbb{E}[X]\)</span> is a constant and therefore <span class="math inline">\(\mathbb{E}\big[\mathbb{E}[X]\big] = \mathbb{E}[X]\)</span>, and the last equality holds because the term in parentheses is equal to 0. Thus, the mean of <span class="math inline">\(Y\)</span> is equal to 0. Now let’s calculate the variance.</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{var}(Y) &amp;= \mathrm{var}\left( \frac{X}{\sqrt{\mathrm{var}(X)}} - \frac{\mathbb{E}[X]}{\sqrt{\mathrm{var}(X)}} \right) \\
  &amp;= \mathrm{var}\left( \frac{X}{\sqrt{\mathrm{var}(X)}}\right) \\
  &amp;= \left( \frac{1}{\sqrt{\mathrm{var}(X)}} \right)^2 \mathrm{var}(X) \\
  &amp;= \frac{\mathrm{var}(X)}{\mathrm{var}(X)} \\
  &amp;= 1
  \end{aligned}
\]</span>
where the first equality holds by the definition of <span class="math inline">\(Y\)</span>, the second equality holds because the second term is a constant and by Variance Property 3 above, the third equality holds because <span class="math inline">\((1/\sqrt{\mathrm{var}(X)})\)</span> is a constant and can come out of the variance but needs to be squared, the fourth equality holds by squaring the term on the left, and the last equality holds by cancelling the numerator and denominator.</p>
<p>Therefore, we have showed that the mean of the standardized random variable is 0 and its variance is 1. This is, in fact, the goal of standardizing a random variable — to transform it so that it has mean 0 and variance 1 and the particular transformation given in this example is one that delivers a new random variable with these properties.</p>
</div>
</div>
<div id="multiple-random-variables" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Multiple Random Variables<a href="#multiple-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.3</p>
<p>Most often in economics, we want to consider two (or more) random variables jointly rather than just a single random variable. For example, mean income is interesting, but mean income as a function of education is more interesting.</p>
<p>When there is more than one random variable, you can define <strong>joint pmfs</strong>, <strong>joint pdfs</strong>, and <strong>joint cdfs</strong>.</p>
<p>Let’s quickly go over these for the case where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two discrete random variables.</p>
<p><strong>Joint pmf:</strong> <span class="math inline">\(f_{X,Y}(x,y) := \mathrm{P}(X=x, Y=y)\)</span></p>
<p><strong>Joint cdf:</strong> <span class="math inline">\(F_{X,Y}(x,y) := \mathrm{P}(X \leq x, Y \leq y)\)</span></p>
<p><strong>Conditional pmf:</strong> <span class="math inline">\(f_{Y|X}(y|x) := \mathrm{P}(Y=y | X=x)\)</span></p>
<p><strong>Properties</strong></p>
<p>We use the notation that <span class="math inline">\(\mathcal{X}\)</span> denotes the support of <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> denotes the support of <span class="math inline">\(Y\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(0 \leq f_{X,Y}(x,y) \geq 1\)</span> for all <span class="math inline">\(x,y\)</span></p>
<p>In words: the probability of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> taking any particular values can’t be less than 0 or greater than 1 (because these are probabilities)</p></li>
<li><p><span class="math inline">\(\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} f_{X,Y}(x,y) = 1\)</span></p>
<p>In words: If you add up <span class="math inline">\(\mathrm{P}(X=x, Y=y)\)</span> across all possible values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, they sum up to 1 (again, this is just a property of probabilities)</p></li>
<li><p>If you know the joint pmf, then you can recover the “marginal” pmf, that is,</p>
<p><span class="math display">\[
f_Y(y) = \sum_{x \in \mathcal{X}} f_{X,Y}(x,y)
\]</span></p>
<p>This amounts to just adding up the joint pmf across all values of <span class="math inline">\(x\)</span> while holding <span class="math inline">\(y\)</span> fixed.</p></li>
</ol>
<p><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>independent</strong> if <span class="math inline">\(f_{Y|X}(y|x) = f_Y(y)\)</span>. In other words, if knowing the value of <span class="math inline">\(X\)</span> doesn’t provide any information about the distribution <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="conditional-expectations" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> Conditional Expectations<a href="#conditional-expectations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.3</p>
<p>It is useful to know about joint pmfs/pdfs/cdfs, but they are often hard to work with in practice. For example, if you have two random variables, visualizing their joint distribution would involve interpreting a 3D plot which is often challenging in practice. If you had more than two random variables, then fully visualizing their joint distribution would not be possible. Therefore, we will typically look at summaries of the joint distribution. Probably the most useful one is the conditional expectation that we study in this section; in fact, we will spend much of the semester trying to estimate conditional expectations.</p>
<p>For two random variables, <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, the <strong>conditional expectation</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is the mean value of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span> taking the particular value <span class="math inline">\(x\)</span>. In math, this is written</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X=x]
\]</span></p>
<p>One useful way to think of a conditional expectation is as a <em>function</em> of <span class="math inline">\(x\)</span>. For example, suppose that <span class="math inline">\(Y\)</span> is a person’s yearly income and <span class="math inline">\(X\)</span> is a person’s years of education. Clearly, mean income can change for different values of education.</p>
<p>Conditional expectations will be a main focus of ours throughout the semester</p>
<p>An extremely useful property of conditional expectations is that they generalize from the case with two variables to the case with multiple variables. For example, suppose that we have four random variables <span class="math inline">\(Y\)</span>, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>.</p>
</div>
<div id="law-of-iterated-expectations" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> Law of Iterated Expectations<a href="#law-of-iterated-expectations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.3</p>
<p>Another important property of conditional expectations is called the <strong>law of iterated expectations</strong>. It says that</p>
<p><span class="math display">\[
  \mathbb{E}[Y] = \mathbb{E}\big[ \mathbb{E}[Y|X] \big]
\]</span>
In words: The expected value of <span class="math inline">\(Y\)</span> is equal to the expected value (this expectation is with respect to <span class="math inline">\(X\)</span>) of the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>This may seem like a technical property, but I think the right way to think about the law of iterated expectations is that there is an inherent relationship between unconditional expectations and conditional expectations. In other words, although conditional expectations can vary arbitrarily for different values of <span class="math inline">\(X\)</span>, if you know what the conditional expectations are, the overall expected value of <span class="math inline">\(Y\)</span> is fully determined.</p>
<p>A simple example is one where <span class="math inline">\(X\)</span> takes only two values. Suppose we are interested in mean birthweight (<span class="math inline">\(Y\)</span>) for children of mother’s who either drank alcohol during their pregnancy (<span class="math inline">\(X=1\)</span>) or who didn’t drink alcohol during their pregnancy (<span class="math inline">\(X=0\)</span>). Suppose the following (just to be clear, these are completely made up numbers), <span class="math inline">\(\mathbb{E}[Y|X=1] = 7\)</span>, <span class="math inline">\(\mathbb{E}[Y|X=0]=8\)</span> <span class="math inline">\(\mathrm{P}(X=1) = 0.1\)</span> and <span class="math inline">\(\mathrm{P}(X=0)=0.9\)</span>. The law of iterated expectation says that
<span class="math display">\[
  \begin{aligned}
  \mathbb{E}[Y] &amp;= \mathbb{E}\big[ \mathbb{E}[Y|X] \big] \\
  &amp;= \sum_{x \in \mathcal{X}} \mathbb{E}[Y|X=x] \mathrm{P}(X=x) \\
  &amp;= \mathbb{E}[Y|X=0]\mathrm{P}(X=0) + \mathbb{E}[Y|X=1]\mathrm{P}(X=1) \\
  &amp;= (8)(0.9) + (7)(0.1) \\
  &amp;= 7.9
  \end{aligned}
\]</span></p>
<p>The law of iterated expectations still applies in more complicated cases (e.g., <span class="math inline">\(X\)</span> takes more than two values, <span class="math inline">\(X\)</span> is continuous, or <span class="math inline">\(X_1\)</span>,<span class="math inline">\(X_2\)</span>,<span class="math inline">\(X_3\)</span>) but the intuition is still the same.</p>
</div>
<div id="covariance" class="section level2 hasAnchor" number="3.13">
<h2><span class="header-section-number">3.13</span> Covariance<a href="#covariance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.3</p>
<p>The <strong>covariance</strong> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a masure of the extent to which they “move together”. It is defined as</p>
<p><span class="math display">\[
  \mathrm{cov}(X,Y) := \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]
\]</span>
A natural first question to ask is: why does this measure how <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> move together. Notice that covariance can be positive or negative. It will tend to be negative if big values of <span class="math inline">\(X\)</span> (so that <span class="math inline">\(X\)</span> is above its mean) tend to happen at the same time as big values of <span class="math inline">\(Y\)</span> (so that <span class="math inline">\(Y\)</span> is above its mean) while small values of <span class="math inline">\(X\)</span> (so that <span class="math inline">\(X\)</span> is below its mean) tend to happen at the same time as small values of <span class="math inline">\(Y\)</span> (so that <span class="math inline">\(Y\)</span> is below its mean).</p>
<p>An alternative and useful expression for covariance is
<span class="math display">\[
  \mathrm{cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\]</span>
Relative to the first expression, this one is probably less of a natural definition but often more useful in mathematical problems.</p>
<p>One more thing to notice, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\mathrm{cov}(X,Y) = 0\)</span>.</p>
</div>
<div id="correlation" class="section level2 hasAnchor" number="3.14">
<h2><span class="header-section-number">3.14</span> Correlation<a href="#correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.3</p>
<p>It’s often hard to interpret covariances directly (the “units” are whatever the units of <span class="math inline">\(X\)</span> are times the units of <span class="math inline">\(Y\)</span>), so it is common to scale the covariance to get the <strong>correlation</strong> between two random variables:</p>
<p><span class="math display">\[
  \mathrm{corr}(X,Y) := \frac{\mathrm{cov}(X,Y)}{\sqrt{\mathrm{var}(X)} \sqrt{\mathrm{var}(Y)}}
\]</span>
The correlation has the property that it is always between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>If <span class="math inline">\(\mathrm{corr}(X,Y) = 0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>uncorrelated</strong>.</p>
</div>
<div id="properties-of-expectationsvariances-of-sums-of-rvs" class="section level2 hasAnchor" number="3.15">
<h2><span class="header-section-number">3.15</span> Properties of Expectations/Variances of Sums of RVs<a href="#properties-of-expectationsvariances-of-sums-of-rvs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.3</p>
<p>Here are some more properties of expectations and variances when there are multiple random variables. For two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)</span></p></li>
<li><p><span class="math inline">\(\mathrm{var}(X+Y) = \mathrm{var}(X) + \mathrm{var}(Y) + 2\mathrm{cov}(X,Y)\)</span></p></li>
</ol>
<p>The first property is probably not surprising — expectations continue to pass through sums. The second property, particularly the covariance term, needs more explanation. To start with, you can just plug <span class="math inline">\(X+Y\)</span> into the definition of variance and (with a few lines of algebra) show that the second property is true. But, for the intuition, let me explain with an example. Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are rolls of two dice, but <em>somehow</em> these dice are positively correlated with each other — i.e., both rolls coming up with high numbers (and low numbers) are more likely than with regular dice. Now, think about what the sum of two dice rolls can be: the smallest possible sum is 2 and other values are possible up to 12. Moreover, the smallest and largest possible sum of the rolls (2 and 12), which are farthest away from the mean value of 7, are relatively uncommon. You have to roll either <span class="math inline">\((1,1)\)</span> or <span class="math inline">\((6,6)\)</span> to get either of these and the probability of each of those rolls is just <span class="math inline">\(1/36\)</span>. However, when the dice are positively correlated, the probability of both rolls being very high or very low becomes more likely — thus, since outcomes far away from the mean become more likely, the variance increases.</p>
<p>One last comment here is that, when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent (or even just uncorrelated), the formula for the variance does not involve the extra covariance term because it is equal to 0.</p>
<p>These properties for sums of random variables generalize to the case with more than two random variables. For example, suppose that <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are random variables, then</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}\left[ \displaystyle \sum_{i=1}^n Y_i \right] = \displaystyle \sum_{i=1}^n \mathbb{E}[Y_i]\)</span></p></li>
<li><p>If <span class="math inline">\(Y_i\)</span> are mutually independent, then <span class="math inline">\(\mathrm{var}\left( \displaystyle \sum_{i=1}^n Y_i \right) = \displaystyle \sum_{i=1}^n \mathrm{var}(Y_i)\)</span></p></li>
</ol>
<p>Notice that the last line does not involve any covariance terms, but this is only because of the caveat that the <span class="math inline">\(Y_i\)</span> are mutually independent. Otherwise, there would actually be <em>tons</em> of covariance terms that would need to be accounted for.</p>
</div>
<div id="normal-distribution" class="section level2 hasAnchor" number="3.16">
<h2><span class="header-section-number">3.16</span> Normal Distribution<a href="#normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.4</p>
<p>You probably learned about a lot of particular distributions of random variables in your Stats class. There are a number of important distributions:</p>
<ul>
<li><p>Normal</p></li>
<li><p>Binomial</p></li>
<li><p>t-distribution</p></li>
<li><p>F-distribution</p></li>
<li><p>Chi-squared distribution</p></li>
<li><p>others</p></li>
</ul>
<p>SW discusses a number of these distributions, and I recommend that you read/review those distributions. For us, the most important distribution is the Normal distribution [we’ll see why a few classes from now].</p>
<p>If a random variable <span class="math inline">\(X\)</span> follows a <strong>normal distribution</strong> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, we write</p>
<p><span class="math display">\[
  X \sim N(\mu, \sigma^2)
\]</span>
where <span class="math inline">\(\mu = \mathbb{E}[X]\)</span> and <span class="math inline">\(\sigma^2 = \mathrm{var}(X)\)</span>.</p>
<p>Importantly, if we know that <span class="math inline">\(X\)</span> follows a normal distribution, its entire distribution is fully characterized by its mean and variance. In other words, if <span class="math inline">\(X\)</span> is normally distributed, and we also know its mean and variance, then we know everything about its distribution. [Notice that this is not generally true — if we did not know the distribution of <span class="math inline">\(X\)</span> but knew its mean and variance, we would know two important features of the distribution of <span class="math inline">\(X\)</span>, but we would not know everything about its distribution.]</p>
<p>You are probably familiar with the pdf of a normal distribution — it is “bell-shaped”.</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>From the figure, you can see that a normal distribution is unimodal (there is just one “peak”) and symmetric (the pdf is the same if you move the same distance above <span class="math inline">\(\mu\)</span> as when you move the same distance below <span class="math inline">\(\mu\)</span>). This means that, for a random variable that follows a normal distribution, its median and mode are also equal to <span class="math inline">\(\mu\)</span>.</p>
<p>From the plot of the pdf, we can also tell that, if you make a draw from <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, the most likely values are near the mean. As you move further away from <span class="math inline">\(\mu\)</span>, it becomes less likely (though not impossible) for a draw of <span class="math inline">\(X\)</span> to take that value.</p>
<p>Recall that we can calculate the probability that <span class="math inline">\(X\)</span> takes on a value in a range by calculating the area under the curve of the pdf. For each shaded region in the figure, there is a 2.5% chance that <span class="math inline">\(X\)</span> falls into that region (so the probability of <span class="math inline">\(X\)</span> falling into either region is 5%). Another way to think about this is that there is a 95% probability that a draw of <span class="math inline">\(X\)</span> will be in the region <span class="math inline">\([\mu-1.96\sigma, \mu+1.96\sigma]\)</span>. Later, we we talk about hypothesis testing, this will be an important property.</p>
<p>Earlier, we talked about standardizing random variables. If you know that a random variable follows a normal distribution, it is very common to standardize it. In particular notice that, if you create the standardized random variable</p>
<p><span class="math display">\[
  Z := \frac{X - \mu}{\sigma} \quad \textrm{then} \quad Z \sim N(0,1)
\]</span>
If you think back to your probability and statistics class, you may have done things like calculating a p-value by looking at a “Z-table” in the back of a textbook (I’m actually not sure if this is still commonly done because it is often easier to just do this on a computer, but, back in “my day” this was a very common exercise in statistics classes). Standardizing allows you to look at just one table for any normally distributed random variable that you could encounter rather than requiring you to have different Z table for each value of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="coding" class="section level2 hasAnchor" number="3.17">
<h2><span class="header-section-number">3.17</span> Coding<a href="#coding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To conclude this section, we’ll use R to compute the features of the joint distribution of income and education that we have discussed above.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" tabindex="-1"></a><span class="co"># create vectors of income and educ</span></span>
<span id="cb81-2"><a href="#cb81-2" tabindex="-1"></a>income <span class="ot">&lt;-</span> us_data<span class="sc">$</span>incwage</span>
<span id="cb81-3"><a href="#cb81-3" tabindex="-1"></a>educ <span class="ot">&lt;-</span> us_data<span class="sc">$</span>educ</span>
<span id="cb81-4"><a href="#cb81-4" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" tabindex="-1"></a><span class="co"># mean of income</span></span>
<span id="cb81-6"><a href="#cb81-6" tabindex="-1"></a><span class="fu">mean</span>(income)</span>
<span id="cb81-7"><a href="#cb81-7" tabindex="-1"></a><span class="co">#&gt; [1] 58605.75</span></span>
<span id="cb81-8"><a href="#cb81-8" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" tabindex="-1"></a><span class="co"># mean of education</span></span>
<span id="cb81-10"><a href="#cb81-10" tabindex="-1"></a><span class="fu">mean</span>(educ)</span>
<span id="cb81-11"><a href="#cb81-11" tabindex="-1"></a><span class="co">#&gt; [1] 13.96299</span></span>
<span id="cb81-12"><a href="#cb81-12" tabindex="-1"></a></span>
<span id="cb81-13"><a href="#cb81-13" tabindex="-1"></a><span class="co"># variance</span></span>
<span id="cb81-14"><a href="#cb81-14" tabindex="-1"></a><span class="fu">var</span>(income)</span>
<span id="cb81-15"><a href="#cb81-15" tabindex="-1"></a><span class="co">#&gt; [1] 4776264026</span></span>
<span id="cb81-16"><a href="#cb81-16" tabindex="-1"></a><span class="fu">var</span>(educ)</span>
<span id="cb81-17"><a href="#cb81-17" tabindex="-1"></a><span class="co">#&gt; [1] 8.345015</span></span>
<span id="cb81-18"><a href="#cb81-18" tabindex="-1"></a></span>
<span id="cb81-19"><a href="#cb81-19" tabindex="-1"></a><span class="co"># standard deviation</span></span>
<span id="cb81-20"><a href="#cb81-20" tabindex="-1"></a><span class="fu">sd</span>(income)</span>
<span id="cb81-21"><a href="#cb81-21" tabindex="-1"></a><span class="co">#&gt; [1] 69110.52</span></span>
<span id="cb81-22"><a href="#cb81-22" tabindex="-1"></a><span class="fu">sd</span>(educ)</span>
<span id="cb81-23"><a href="#cb81-23" tabindex="-1"></a><span class="co">#&gt; [1] 2.888774</span></span>
<span id="cb81-24"><a href="#cb81-24" tabindex="-1"></a></span>
<span id="cb81-25"><a href="#cb81-25" tabindex="-1"></a><span class="co"># covariance</span></span>
<span id="cb81-26"><a href="#cb81-26" tabindex="-1"></a><span class="fu">cov</span>(income,educ)</span>
<span id="cb81-27"><a href="#cb81-27" tabindex="-1"></a><span class="co">#&gt; [1] 63766.72</span></span>
<span id="cb81-28"><a href="#cb81-28" tabindex="-1"></a></span>
<span id="cb81-29"><a href="#cb81-29" tabindex="-1"></a><span class="co"># correlation</span></span>
<span id="cb81-30"><a href="#cb81-30" tabindex="-1"></a><span class="fu">cor</span>(income, educ)</span>
<span id="cb81-31"><a href="#cb81-31" tabindex="-1"></a><span class="co">#&gt; [1] 0.3194011</span></span></code></pre></div>
</div>
<div id="lab-2-basic-plots" class="section level2 hasAnchor" number="3.18">
<h2><span class="header-section-number">3.18</span> Lab 2: Basic Plots<a href="#lab-2-basic-plots" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Related Reading: IDS 9.4 (if you are interested, you can read IDS Chapters 6-10 for much more information about plotting in <code>R</code>)</p>
<p>In this lab, I’ll introduce you to some basic plotting. Probably the most common type of plot that I use is a line plot. We’ll go for trying to make a line plot of average income as a function of education.</p>
<p>To start with, I’ll introduce you to R’s <code>ggplot2</code> package. This is one of the most famous plot-producing packages (not just in R, but for any programming language). The syntax may be somewhat challenging to learn, but I think it is worth it to exert some effort here.</p>
<div class="side-comment">
<p><span class="side-comment">Side Comment:</span> Base <code>R</code> has several plotting functions (e.g., <code>plot</code>). Check IDS 2.15 for an introduction to these functions. These are generally easier to learn but less beautiful than plots coming from <code>ggplot2</code>.</p>
</div>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" tabindex="-1"></a><span class="co"># load ggplot2 package </span></span>
<span id="cb82-2"><a href="#cb82-2" tabindex="-1"></a><span class="co"># (if you haven&#39;t installed it, you would need to do that first)</span></span>
<span id="cb82-3"><a href="#cb82-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb82-4"><a href="#cb82-4" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" tabindex="-1"></a><span class="co"># load dplyr package for &quot;wrangling&quot; data</span></span>
<span id="cb82-6"><a href="#cb82-6" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" tabindex="-1"></a><span class="co"># arrange data</span></span>
<span id="cb83-2"><a href="#cb83-2" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> us_data <span class="sc">%&gt;%</span></span>
<span id="cb83-3"><a href="#cb83-3" tabindex="-1"></a>    <span class="fu">group_by</span>(educ) <span class="sc">%&gt;%</span></span>
<span id="cb83-4"><a href="#cb83-4" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">income=</span><span class="fu">mean</span>(incwage))</span>
<span id="cb83-5"><a href="#cb83-5" tabindex="-1"></a></span>
<span id="cb83-6"><a href="#cb83-6" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb83-7"><a href="#cb83-7" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>plot_data,</span>
<span id="cb83-8"><a href="#cb83-8" tabindex="-1"></a>       <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>educ,<span class="at">y=</span>income)) <span class="sc">+</span></span>
<span id="cb83-9"><a href="#cb83-9" tabindex="-1"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb83-10"><a href="#cb83-10" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb83-11"><a href="#cb83-11" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<p>Let me explain what’s going on here piece-by-piece. Let’s start with this code</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" tabindex="-1"></a><span class="co"># arrange data</span></span>
<span id="cb84-2"><a href="#cb84-2" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> us_data <span class="sc">%&gt;%</span></span>
<span id="cb84-3"><a href="#cb84-3" tabindex="-1"></a>    <span class="fu">group_by</span>(educ) <span class="sc">%&gt;%</span></span>
<span id="cb84-4"><a href="#cb84-4" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">income=</span><span class="fu">mean</span>(incwage))</span></code></pre></div>
<p>At a high-level, making plots often involves two steps — first arranging the data in the “appropriate” way (that’s this step) and then actually making the plot.</p>
<p>This is “tidyverse-style” code too — in my view, it is a little awkward, but it is also common so I think it is worth explaining here a bit.</p>
<p>First, the <strong>pipe operator</strong>, <code>%&gt;%</code> takes the thing on the left of it and applies the function to the right of it. So, the line <code>us_data %&gt;% group_by(educ)</code> takes <code>us_data</code> and applies the function <code>group_by</code> to it, and what we group by is <code>educ</code>. That creates a new data frame (you could just run that code and see what you get). The next line takes that new data frame and applies the function <code>summarize</code> to it. In this case, <code>summarize</code> creates a new variable called <code>income</code> that is the mean of the column <code>incwage</code> and it is the mean by <code>educ</code> (since we grouped by education in the previous step).</p>
<p>Take a second and look through what has actually been created here. <code>plot_data</code> is a new data frame, but it only has 18 observations — corresponding to each distinct value of education in the data. It also has two columns, the first one is <code>educ</code> which is the years of education, and the second one is <code>income</code> which is the average income among individuals that have that amount of education.</p>
<p>An alternative way of writing the exact same code (that seems more natural to me) is</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" tabindex="-1"></a><span class="co"># arrange data</span></span>
<span id="cb85-2"><a href="#cb85-2" tabindex="-1"></a>grouped_data <span class="ot">&lt;-</span> <span class="fu">group_by</span>(us_data, educ)</span>
<span id="cb85-3"><a href="#cb85-3" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">summarize</span>(grouped_data, <span class="at">income=</span><span class="fu">mean</span>(incwage))</span></code></pre></div>
<p>If you’re familiar with other programming languages, the second version of the code probably seems more familiar. Either way is fine with me — tidyverse-style seems to be trendy in R programming these days, but (for me) I think the second version is a little easier to understand. You can find long “debates” about these two styles of writing code if you happen to be interested…</p>
<p>Before moving on, let me mention a few other <code>dplyr</code> functions that you might find useful</p>
<ul>
<li><p><code>filter</code> — this is tidy version of <code>subset</code></p></li>
<li><p><code>select</code> — selects particular columns of interest from your data</p></li>
<li><p><code>mutate</code> — creates a new variable from existing columns in your data</p></li>
<li><p><code>arrange</code> — useful for sorting your data</p></li>
</ul>
<p>Next, let’s consider the second part of the code.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb86-2"><a href="#cb86-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>plot_data,</span>
<span id="cb86-3"><a href="#cb86-3" tabindex="-1"></a>       <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>educ,<span class="at">y=</span>income)) <span class="sc">+</span></span>
<span id="cb86-4"><a href="#cb86-4" tabindex="-1"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb86-5"><a href="#cb86-5" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb86-6"><a href="#cb86-6" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<p>The main function here is <code>ggplot</code>. It takes in two main arguments: <code>data</code> and <code>mapping</code>. Notice that we set <code>data</code> to be equal to <code>plot_data</code> which is the data frame that we just created. The <code>mapping</code> is set equal to <code>aes(x=educ,y=income)</code>. <code>aes</code> stands for “aesthetic”, and here you are just telling <code>ggplot</code> the names of the columns in the data frame that should be on the x-axis (here: <code>educ</code>) and on the y-axis (here: <code>income</code>) in the plot. Also, notice the <code>+</code> at the end of the line; you can interpret this as saying “keep going” to the next line before executing.</p>
<p>If we just stopped there, we actually wouldn’t plot anything. We still need to tell <code>ggplot</code> what kind of plot we want to make. That’s where the line <code>geom_line</code> comes in. It tells <code>ggplot</code> that we want to plot a line. Try running the code with just those two lines — you will see that you will get a similar (but not exactly the same) plot.</p>
<p><code>geom_point</code> adds the dots in the figure. <code>size=3</code> controls the size of the points. I didn’t add this argument originally, but the dots were hard to see so I made them bigger.</p>
<p><code>theme_bw</code> changes the color scheme of the plot. It stands for “theme black white”.</p>
<p>There is a ton of flexibility with <code>ggplot</code> — way more than I could list here. But let me give you some extras that I tend to use quite frequently.</p>
<ul>
<li><p>In <code>geom_line</code> and <code>geom_point</code>, you can add the extra argument <code>color</code>; for example, you could try <code>geom_line(color="blue")</code> and it would change the color of the line to blue.</p></li>
<li><p>In <code>geom_line</code>, you can change the “type” of the line by using the argument <code>linetype</code>; for example, <code>geom_line(linetype="dashed")</code> would change the line from being solid to being dashed.</p></li>
<li><p>In <code>geom_line</code>, the argument <code>size</code> controls the thickness of the line.</p></li>
<li><p>The functions <code>ylab</code> and <code>xlab</code> control the labels on the y-axis and x-axis</p></li>
<li><p>The functions <code>ylim</code> and <code>xlim</code> control the “limits” of the y-axis and x-axis. Here’s how you can use these:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb87-2"><a href="#cb87-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>plot_data,</span>
<span id="cb87-3"><a href="#cb87-3" tabindex="-1"></a>     <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>educ,<span class="at">y=</span>income)) <span class="sc">+</span></span>
<span id="cb87-4"><a href="#cb87-4" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb87-5"><a href="#cb87-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb87-6"><a href="#cb87-6" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb87-7"><a href="#cb87-7" tabindex="-1"></a>  ylim<span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">150000</span>) <span class="sc">+</span> </span>
<span id="cb87-8"><a href="#cb87-8" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Income&quot;</span>) <span class="sc">+</span> </span>
<span id="cb87-9"><a href="#cb87-9" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Education&quot;</span>)</span></code></pre></div>
<p>which will adjust the y-axis and change the labels on each axis.</p></li>
</ul>
<p>Besides the line plot (using <code>geom_line</code>) and the scatter plot (using <code>geom_point</code>), probably two other types of plots that I make the most are</p>
<ul>
<li><p>Histogram (using <code>geom_histogram</code>) — this is how I made the plot of the pmf of education earlier in this chapter</p></li>
<li><p>Adding a straight line to a plot (using <code>geom_abline</code> which takes in <code>slope</code> and <code>intercept</code> arguments) — we haven’t used this yet, but we will win once we start talking about regressions</p></li>
<li><p>If you’re interested, here is a to a large number of different types of plots that are available using <code>ggplot</code>: <a href="http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html">http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html</a></p></li>
</ul>
</div>
<div id="coding-questions" class="section level2 hasAnchor" number="3.19">
<h2><span class="header-section-number">3.19</span> Coding Questions<a href="#coding-questions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Run the following code to create the data that we will use in the problem</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>) <span class="co"># setting the seed means that we will get the same results</span></span>
<span id="cb88-2"><a href="#cb88-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rexp</span>(<span class="dv">100</span>) <span class="co"># make 100 draws from an exponential distribution</span></span></code></pre></div>
<p>Use the <code>ggplot2</code> package to plot a histogram of <code>x</code>.</p></li>
<li><p>For this question, we’ll use the data <code>fertilizer_2000</code>. A scatter plot is a useful way to visualize 2-dimensional data. Use the <code>ggplot2</code> package to make a scatter plot with crop yield (<code>avyield</code>) on the y-axis and fertilizer (<code>avfert</code>) on the x-axis. Label the y-axis “Crop Yield” and the x-axis “Fertilizer”. Do you notice any pattern from the scatter plot?</p></li>
</ol>
</div>
<div id="extra-questions" class="section level2 hasAnchor" number="3.20">
<h2><span class="header-section-number">3.20</span> Extra Questions<a href="#extra-questions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(\mathbb{E}[X] = 10\)</span> and <span class="math inline">\(\mathrm{var}(X) = 2\)</span>. Also, suppose that <span class="math inline">\(Y=5 + 9 X\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What is <span class="math inline">\(\mathbb{E}[Y]\)</span>?</p></li>
<li><p>What is <span class="math inline">\(\mathrm{var}(Y)\)</span>?</p></li>
</ol></li>
<li><p>Use the definition of variance to show that <span class="math inline">\(\mathrm{var}(bX) = b^2 \mathrm{var}(X)\)</span> (where <span class="math inline">\(b\)</span> is a constant and <span class="math inline">\(X\)</span> is some random variable).</p></li>
<li><p>Suppose you are interested in average height of students at UGA. Let <span class="math inline">\(Y\)</span> denote a student’s height; also let <span class="math inline">\(X\)</span> denote a binary variable that is equal to 1 if a student is female. Suppose that you know that <span class="math inline">\(\mathbb{E}[Y|X=1] = 5&#39; \,4&#39;&#39;\)</span> and that <span class="math inline">\(\mathbb{E}[Y|X=0] = 5&#39; \,9&#39;&#39;\)</span> (and that <span class="math inline">\(\mathrm{P}(X=1) = 0.5\)</span>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>What is <span class="math inline">\(\mathbb{E}[Y]\)</span>?</p></li>
<li><p>Explain how the answer to part (a) is related to the Law of Iterated Expectations.</p></li>
</ol></li>
<li><p>Consider a random variable <span class="math inline">\(X\)</span> with support <span class="math inline">\(\mathcal{X} = \{2,7,13,21\}\)</span>. Suppose that it has the following pmf:</p>
<p><span class="math display">\[
    \begin{aligned}
     f_X(2) &amp;= 0.5 \\
     f_X(7) &amp;= 0.25 \\
     f_X(13) &amp;= 0.15 \\
     f_X(21) &amp;= ??
   \end{aligned}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>What is <span class="math inline">\(f_X(21)\)</span>? How do you know?</p></li>
<li><p>What is the expected value of <span class="math inline">\(X\)</span>? [Show your calculation.]</p></li>
<li><p>What is the variance of <span class="math inline">\(X\)</span>? [Show your calculation.]</p></li>
<li><p>Calculate <span class="math inline">\(F_X(x)\)</span> for <span class="math inline">\(x=1\)</span>, <span class="math inline">\(x=7\)</span>, <span class="math inline">\(x=8\)</span>, and <span class="math inline">\(x=25\)</span>.</p></li>
</ol></li>
</ol>

</div>
</div>
<div id="properties-of-estimators" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Topic 4</span> Properties of Estimators<a href="#properties-of-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>So far, we have been talking about <strong>population quantities</strong> such as <span class="math inline">\(f_{Y|X}\)</span> (conditional pdf/pmf), <span class="math inline">\(\mathbb{E}[Y]\)</span> (expected value of <span class="math inline">\(Y\)</span>), or <span class="math inline">\(\mathbb{E}[Y|X]\)</span> (expected value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>).</p>
<p>In practice, most often we do not know what these population quantities are equal to (with the exception of some trivial cases like flipping a coin or rolling a die).</p>
<p>A fundamental challenge is that it is uncommon that we observe the entire population.</p>
<p>Instead, we will take the approach that we have access to a <strong>sample</strong> of data from the original population. We’ll use the sample to try to <strong>estimate</strong> whatever population quantities we are interested in as well as develop the tools to <strong>conduct inference</strong>, paying particular interest to questions like: how precisely can we estimate particular population quantities of interest?</p>
<p>The topics considered in this section fall broadly under the topic of <strong>statistics</strong> (a reasonable definition of statistics is that it is the set of tools to learn about population quantities using data). Some of this material may be familiar from courses that you have taken before, but this section provides a fairly advanced discussion of these issues with a particular eye towards (i) inference issues that are important econometrics and (ii) prediction problems. Many of the tools that we cover in this section will be used throughout the rest of the course.</p>
<div id="simple-random-sample" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Simple Random Sample<a href="#simple-random-sample" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.5</p>
<p>Let’s start by talking about how the data that we have access to is collected. There are several possibilities here, but let us start with the most straightforward case (which is also a very common case) called a <strong>simple random sample</strong>.</p>
<p>In math: <span class="math inline">\(\{Y_i\}_{i=1}^n\)</span> is called a simple random sample if <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> are independent random variables with a common probability distribution <span class="math inline">\(f_Y\)</span>. The two key conditions here are (i) independence and (ii) from a common distribution. For this reason, you may sometimes see a random sample called an iid sample which stands for independent and identically distributed.</p>
<p>In words: We have access to <span class="math inline">\(n\)</span> observations that are drawn at random from some underlying population and each observation is equally likely to be drawn.</p>
</div>
<div id="estimating-mathbbey" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span><a href="#estimating-mathbbey" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.5, 3.1</p>
<p>Let’s start with trying to estimate <span class="math inline">\(\mathbb{E}[Y]\)</span> as this is probably the simplest, non-trivial thing that we can estimate.</p>
<p>A natural way to estimate population quantities is with their sample analogue. This is called the <strong>analogy principle</strong>. This is perhaps technical jargon, but it is the way you would immediately think to estimate <span class="math inline">\(\mathbb{E}[Y]\)</span>:</p>
<p><span class="math display">\[
  \hat{\mathbb{E}}[Y] = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
\]</span>
In this course, we will typically put a “hat” on estimated quantities. The expression <span class="math inline">\(\displaystyle \frac{1}{n}\sum_{i=1}^n Y_i\)</span> is just the average value of <span class="math inline">\(Y\)</span> in our sample. Since we will calculate a ton of averages like this one over the course of the rest of the semester, it’s also convenient to give it a shorthand notation, which is what <span class="math inline">\(\bar{Y}\)</span> means — it is just the sample average of <span class="math inline">\(Y\)</span>.</p>
<p>One thing that is important to be clear about at this point is that, in general, <span class="math inline">\(\mathbb{E}[Y] \neq \bar{Y}\)</span>. <span class="math inline">\(\mathbb{E}[Y]\)</span> is a population quantity while <span class="math inline">\(\bar{Y}\)</span> is a sample quantity. We will hope (and provide some related conditions/discussions below) that <span class="math inline">\(\bar{Y}\)</span> would be close to <span class="math inline">\(\mathbb{E}[Y]\)</span>, but, in general, they will not be exactly the same.</p>
</div>
<div id="mean-of-bary" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Mean of <span class="math inline">\(\bar{Y}\)</span><a href="#mean-of-bary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.5, 3.1</p>
<p>Another important thing to notice about <span class="math inline">\(\bar{Y}\)</span> is that it is a random variable (as it is the average of random variables). This is in sharp contrast to <span class="math inline">\(\mathbb{E}[Y]\)</span> which is non-random.</p>
<p>One related thought experiment is the following: if we could repeatedly collect new samples of size <span class="math inline">\(n\)</span> from the same population and each time were able to estimate <span class="math inline">\(\bar{Y}\)</span>, these estimates would be different from each other.</p>
<p>In fact, this means that <span class="math inline">\(\bar{Y}\)</span> has a distribution. The distribution of a statistic, like <span class="math inline">\(\bar{Y}\)</span>, is called its <strong>sampling distribution</strong>. We’d like to know about the features of the sampling distribution. Let’s start with its mean. That is, let’s calculate</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[\bar{Y}] &amp;= \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n Y_i \right] \\
    &amp;= \frac{1}{n} \mathbb{E}\left[ \sum_{i=1}^n Y_i \right] \\
    &amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[Y_i] \\
    &amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[Y] \\
    &amp;= \frac{1}{n} n \mathbb{E}[Y] \\
    &amp;= \mathbb{E}[Y]
  \end{aligned}
\]</span>
Let’s think carefully about each step here — the arguments rely heavily on the properties of expectations and summations that we have learned earlier. The first equality holds from the definition of <span class="math inline">\(\bar{Y}\)</span>. The second equality holds because <span class="math inline">\(1/n\)</span> is a constant and can therefore come out of the expectation. The third equality holds because the expectation can pass through the sum. The fourth equality holds because <span class="math inline">\(Y_i\)</span> are all from the same distribution which implies that they all of the same mean and that it is equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>. The fifth equality holds because <span class="math inline">\(\mathbb{E}[Y]\)</span> is a constant and we add it up <span class="math inline">\(n\)</span> times. And the last equality just cancels the <span class="math inline">\(n\)</span> in the numerator with the <span class="math inline">\(n\)</span> in the denominator.</p>
<p>Before moving on, let me make an additional comment:</p>
<ul>
<li>The fourth equality might be a little confusing. Certainly it is not saying that all the <span class="math inline">\(Y_i\)</span>’s are equal to each other. Rather, they come from the same distribution. For example, if you roll a die <span class="math inline">\(n\)</span> times, you get different outcomes on different rolls, but they are all from the same distribution so that the population expectation of each roll is always 3.5, but you get different realizations on different particular rolls. Another example is if <span class="math inline">\(Y\)</span> is a person’s income. Again, we are not saying that everyone has the same income, but just that we are thinking of income as being a draw from some distribution — sometimes you get a draw of a person with a very high income; other times you get a draw of a person with a low income, but <span class="math inline">\(\mathbb{E}[Y]\)</span> is a feature of the underlying distribution itself where these draws come from.</li>
</ul>
<p>How should interpret the above result? It says that, <span class="math inline">\(\mathbb{E}[\bar{Y}] = \mathbb{E}[Y]\)</span>. This doesn’t mean that <span class="math inline">\(\bar{Y}\)</span> itself is equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>. Rather, it means that, if we could repeatedly obtain (a huge number of times) new samples of size <span class="math inline">\(n\)</span> and compute <span class="math inline">\(\bar{Y}\)</span> each time, the average of <span class="math inline">\(\bar{Y}\)</span> across repeated samples would be equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>.</p>
</div>
<div id="variance-of-bary" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Variance of <span class="math inline">\(\bar{Y}\)</span><a href="#variance-of-bary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.5, 3.1</p>
<p>Next, let’s calculate the variance of <span class="math inline">\(\bar{Y}\)</span>. As before, we are continuing with the thought experiment of being able to repeatedly draw new samples of size <span class="math inline">\(n\)</span>, and, therefore, we call this variance the <strong>sampling variance</strong>.</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathrm{var}(\bar{Y}) &amp;= \mathrm{var}\left(\frac{1}{n} \sum_{i=1}^n Y_i\right) \\
    &amp;= \frac{1}{n^2} \mathrm{var}\left(\sum_{i=1}^n Y_i\right) \\
    &amp;= \frac{1}{n^2} \left( \sum_{i=1}^n \mathrm{var}(Y_i) + \textrm{lots of covariance terms} \right) \\
    &amp;= \frac{1}{n^2} \left( \sum_{i=1}^n \mathrm{var}(Y_i) \right) \\
    &amp;= \frac{1}{n^2} \sum_{i=1}^n \mathrm{var}(Y) \\
    &amp;= \frac{1}{n^2} n \mathrm{var}(Y) \\
    &amp;= \frac{\mathrm{var}(Y)}{n}
  \end{aligned}
\]</span>
Let’s go carefully through each step — these arguments rely heavily on the properties of variance that we talked about earlier. The first equality holds by the definition of <span class="math inline">\(\bar{Y}\)</span>. The second equality holds because <span class="math inline">\(1/n\)</span> is a constant and can come out of the variance after squaring it. The third equality holds because the variance of the sum of random variables is equal to the sum of the variances plus all the covariances between the random variables. In the fourth equality, all of the covariance terms go away — this holds because of random sampling which implies that the <span class="math inline">\(Y_i\)</span> are all independent which implies that their covariances are equal to 0. The fifth equality holds because all <span class="math inline">\(Y_i\)</span> are identically distributed so their variances are all the same and equal to <span class="math inline">\(\mathrm{var}(Y)\)</span>. The sixth equality holds by adding up <span class="math inline">\(\mathrm{var}(Y)\)</span> <span class="math inline">\(n\)</span> times. The last equality holds by canceling the <span class="math inline">\(n\)</span> in the numerator with one of the <span class="math inline">\(n\)</span>’s in the denominator.</p>
<p>Interestingly, the variance of <span class="math inline">\(\bar{Y}\)</span> depends not just on <span class="math inline">\(\mathrm{var}(Y)\)</span> but also on <span class="math inline">\(n\)</span> — the number of observations in the sample. Notice that <span class="math inline">\(n\)</span> is in the denominator, so the variance of <span class="math inline">\(\bar{Y}\)</span> will be lower for large values of <span class="math inline">\(n\)</span>. Here is an example that may be helpful for understanding this. Suppose that you are rolling a die. If <span class="math inline">\(n=1\)</span>, then clearly, the variance of <span class="math inline">\(\bar{Y}\)</span> is just equal to the variance of <span class="math inline">\(Y\)</span> — sometimes you roll extreme values like <span class="math inline">\(1\)</span> or <span class="math inline">\(6\)</span>. Now, when you increase <span class="math inline">\(n\)</span>, say, to 10, then these extreme values of <span class="math inline">\(\bar{Y}\)</span> are substantially less common. For <span class="math inline">\(\bar{Y}\)</span> to be equal to <span class="math inline">\(6\)</span> in this case, you’d need to roll 10 <span class="math inline">\(6\)</span>’s in a row. This illustrates that the sampling variance of <span class="math inline">\(\bar{Y}\)</span> is decreasing in <span class="math inline">\(n\)</span>. If this is not perfectly clear, we will look at some data soon, and I think that should confirm to you that the variance of <span class="math inline">\(\bar{Y}\)</span> is decreasing in the sample size.</p>
</div>
<div id="properties-of-estimators-1" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Properties of Estimators<a href="#properties-of-estimators-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.5, 3.1</p>
<p>Suppose we are interested in some population parameter <span class="math inline">\(\theta\)</span> — we’ll write this pretty generically now, but it could be <span class="math inline">\(\mathbb{E}[Y]\)</span> or <span class="math inline">\(\mathbb{E}[Y|X]\)</span> or really any other population quantity that you’d like to estimate.</p>
<p>Also, suppose that we have access to a random sample of size <span class="math inline">\(n\)</span> and we have some estimate of <span class="math inline">\(\theta\)</span> that we’ll call <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>As before, we are going to consider the repeated sampling thought experiment where we imagine that we could repeatedly obtain new samples of size <span class="math inline">\(n\)</span> and with each new sample calculate a new <span class="math inline">\(\hat{\theta}\)</span>. Under this thought experiment, <span class="math inline">\(\hat{\theta}\)</span> would have a sampling distribution. One possibility for what it could look like is the following</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
<p>In this case, values of <span class="math inline">\(\hat{\theta}\)</span> are more common around 3 and 4, but it is not highly unusual to get a value of <span class="math inline">\(\hat{\theta}\)</span> that is around 1 or 2 or 5 or 6 either.</p>
<p>The first property of an estimator that we will take about is called <strong>unbiasedness</strong>. An estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased if <span class="math inline">\(\mathbb{E}[\hat{\theta}] = \theta\)</span>. Alternatively, we can define the <strong>bias</strong> of an estimator as</p>
<p><span class="math display">\[
  \textrm{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta
\]</span>
For example, if <span class="math inline">\(\textrm{Bias}(\hat{\theta}) &gt; 0\)</span>, it means that, on average (in the repeated sampling thought experiment), our estimates of <span class="math inline">\(\theta\)</span> would be greater than the actual value of <span class="math inline">\(\theta\)</span>.</p>
<p>In general, unbiasedness is a good property for an estimator to have. That being said, we can come up with examples of not-very-good unbiased estimators and good biased estimators, but all-else-equal, it is better for an estimator to be unbiased.</p>
<p>The next property of estimators that we will talk about is their <strong>sampling variance</strong>. This is just <span class="math inline">\(\mathrm{var}(\hat{\theta})\)</span>. In general, we would like estimators with low (or 0) bias and low sampling variance. Let me give an example</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
<p>This is a helpful figure for thinking about the properties of estimators. In this case, <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> are both unbiased (because their means are <span class="math inline">\(\theta\)</span>) while <span class="math inline">\(\hat{\theta}_3\)</span> is biased — it’s mean is greater than <span class="math inline">\(\theta\)</span>. On the other hand the sampling variance of <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_3\)</span> are about the same and both substantially smaller than for <span class="math inline">\(\hat{\theta}_1\)</span>. Clearly, <span class="math inline">\(\hat{\theta}_2\)</span> is the best estimator of <span class="math inline">\(\theta\)</span> out of the three. But which is the second best? It is not clear. <span class="math inline">\(\hat{\theta}_3\)</span> systematically over-estimates <span class="math inline">\(\theta\)</span>, but since the variance is relatively small, the misses are systematic but tend to be relatively small. On the other hand, <span class="math inline">\(\hat{\theta}_1\)</span> is, on average, equal to <span class="math inline">\(\theta\)</span>, but sometimes the estimate of <span class="math inline">\(\theta\)</span> could be quite poor due to the large sampling variance.</p>
</div>
<div id="relative-efficiency" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Relative Efficiency<a href="#relative-efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 3.1</p>
<p>If <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> are two unbiased estimators of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\hat{\theta}_1\)</span> is <strong>more efficient</strong> than <span class="math inline">\(\hat{\theta}_2\)</span> if <span class="math inline">\(\mathrm{var}(\hat{\theta}_1) &lt; \mathrm{var}(\hat{\theta}_2)\)</span>.</p>
<p>Relative efficiency gives us a way to rank unbiased estimators.</p>
</div>
<div id="mean-squared-error" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Mean Squared Error<a href="#mean-squared-error" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>More generally, two estimators can be compared by their <strong>mean squared error</strong> which is defined as</p>
<p><span class="math display">\[
  \textrm{MSE}(\hat{\theta}) := \mathbb{E}\left[ (\hat{\theta} - \theta)^2\right]
\]</span></p>
<p>The mean squared error of <span class="math inline">\(\hat{\theta}\)</span> is the average “distance” between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta\)</span> in the thought experiment of having repeated samples of size <span class="math inline">\(n\)</span>.</p>
<p>Another equivalent expression for the mean squared error is</p>
<p><span class="math display">\[
  \textrm{MSE}(\hat{\theta}) = \textrm{Bias}(\hat{\theta})^2 + \mathrm{var}(\hat{\theta})
\]</span>
In other words, if we can figure out the bias and variance of <span class="math inline">\(\hat{\theta}\)</span>, then we can recover mean squared error.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> I think it is worth quickly explaining where the second expression for <span class="math inline">\(\textrm{MSE}(\hat{\theta})\)</span> comes from. Starting from the definition of <span class="math inline">\(\textrm{MSE}(\hat{\theta})\)</span>,</p>
<p><span class="math display">\[
  \begin{aligned}
    \textrm{MSE}(\hat{\theta}) &amp;= \mathbb{E}\left[ (\hat{\theta} - \theta)^2\right] \\
    &amp;= \mathbb{E}\left[ \left( (\hat{\theta} - \mathbb{E}[\hat{\theta}]) + (\mathbb{E}[\hat{\theta}] - \theta)\right)^2 \right] \\
    &amp;= \mathbb{E}\left[ (\hat{\theta} - \mathbb{E}[\hat{\theta}])^2 \right] + \mathbb{E}\left[ (\mathbb{E}[\hat{\theta}] - \theta)^2\right] + 2 \mathbb{E}\left[ (\hat{\theta} - \mathbb{E}[\hat{\theta}])(\mathbb{E}[\hat{\theta}] - \theta) \right] \\
    &amp;= \mathrm{var}(\hat{\theta}) + \textrm{Bias}(\hat{\theta})^2
  \end{aligned}
\]</span>
where the first equality is just the definition of <span class="math inline">\(\textrm{MSE}(\hat{\theta})\)</span>, the second equality adds and subtracts <span class="math inline">\(\mathbb{E}[\hat{\theta}]\)</span>, the third equality squares everything in parentheses from the previous line and pushes the expectation through the sum. For the last equality, the first term in the previous line corresponds to the definition of <span class="math inline">\(\mathrm{var}(\hat{\theta})\)</span>; for the second term, recall that <span class="math inline">\(\textrm{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}-\theta]\)</span> (and this is non-random so the outside expectation just goes away); the last term is equal to 0 which just holds by the properties of expectations after noticing that <span class="math inline">\((\mathbb{E}[\hat{\theta}] - \theta)\)</span> is non-random and can therefore come out of the expectation.</p>
</div>
<p>Generally, we would like to choose estimators that have low mean squared error (this essentially means that they have low bias and variance). Moreover, mean squared error gives us a way to compare estimators that are potentially biased. [Also, notice that for unbiased estimators, comparing mean squared errors of different estimators just compares their variance (because the bias term is equal to 0), so this is a <em>generalization</em> of relative efficiency from the previous section.]</p>
<div class="example">
<p><span id="exm:unlabeled-div-15" class="example"><strong>Example 4.1  </strong></span>Let’s compare three estimators of <span class="math inline">\(\mathbb{E}[Y]\)</span> based on their mean squared error. Let’s consider the three following estimators</p>
<p><span class="math display">\[
  \begin{aligned}
    \hat{\mu} &amp;:= \frac{1}{n} \sum_{i=1}^n Y_i \\
    \hat{\mu}_1 &amp;:= Y_1 \\
    \hat{\mu}_\lambda &amp;:= \lambda \bar{Y} \quad \textrm{for some } \lambda &gt; 0
  \end{aligned}
\]</span>
<span class="math inline">\(\hat{\mu}\)</span> is just the sample average of <span class="math inline">\(Y\)</span>’s that we have already discussed. <span class="math inline">\(\hat{\mu}_1\)</span> is the (somewhat strange) estimator of <span class="math inline">\(\mathbb{E}[Y]\)</span> that just uses the first observation in the data (regardless of the sample size). <span class="math inline">\(\hat{\mu}_\lambda\)</span> is an estimator of <span class="math inline">\(\mathbb{E}[Y]\)</span> that multiplies <span class="math inline">\(\bar{Y}\)</span> by some positive constant <span class="math inline">\(\lambda\)</span>.</p>
<p>To calculate the mean squared error of each of these estimators, let’s calculate their means and their variances.</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[\hat{\mu}] &amp;= \mathbb{E}[Y] \\
    \mathbb{E}[\hat{\mu}_1] &amp;= \mathbb{E}[Y_1] = \mathbb{E}[Y] \\
    \mathbb{E}[\hat{\mu}_\lambda] &amp;= \lambda \mathbb{E}[\bar{Y}] = \lambda \mathbb{E}[Y]
  \end{aligned}
\]</span>
This means that <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\mu}_1\)</span> are both unbiased. <span class="math inline">\(\hat{\mu}_\lambda\)</span> is biased (unless <span class="math inline">\(\lambda=1\)</span> though this is a relatively uninteresting case as it would mean that <span class="math inline">\(\hat{\mu}_\lambda\)</span> is exactly the same as <span class="math inline">\(\hat{\mu}\)</span>) with <span class="math inline">\(\textrm{Bias}(\hat{\mu}_\lambda) = (\lambda - 1) \mathbb{E}[Y]\)</span>.</p>
<p>Next, let’s calculate the variance for each estimator</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{var}(\hat{\mu}) &amp;= \frac{\mathrm{var}(Y)}{n} \\
  \mathrm{var}(\hat{\mu}_1) &amp;= \mathrm{var}(Y_1) = \mathrm{var}(Y) \\
  \mathrm{var}(\hat{\mu}_\lambda) &amp;= \lambda^2 \mathrm{var}(\bar{Y}) = \lambda^2 \frac{\mathrm{var}(Y)}{n}
  \end{aligned}
\]</span>
This means that we can now calculate mean squared error for each estimator.</p>
<p><span class="math display">\[
  \begin{aligned}
    \textrm{MSE}(\hat{\mu}) &amp;= \frac{\mathrm{var}{Y}}{n} \\
    \textrm{MSE}(\hat{\mu}_1) &amp;= \mathrm{var}(Y) \\
    \textrm{MSE}(\hat{\mu}_\lambda) &amp;= (\lambda-1)^2\mathbb{E}[Y]^2 + \lambda^2 \frac{\mathrm{var}(Y)}{n}
  \end{aligned}
\]</span>
The first thing to notice is that <span class="math inline">\(\hat{\mu}\)</span> <em>dominates</em> <span class="math inline">\(\hat{\mu}_1\)</span> (where dominates means that there isn’t any scenario where you could make a reasonable case that <span class="math inline">\(\hat{\mu}_1\)</span> is a better estimator) because its MSE is strictly lower (they tie only if <span class="math inline">\(n=1\)</span> when they become the same estimator). This is probably not surprising — <span class="math inline">\(\hat{\mu}_1\)</span> just throws away a lot of potentially useful information.</p>
<p>The more interesting case is <span class="math inline">\(\hat{\mu}_\lambda\)</span>. The first term is the bias term — it is greater than the bias from <span class="math inline">\(\hat{\mu}\)</span> or <span class="math inline">\(\hat{\mu}_1\)</span> because the bias of both of these is equal to 0. However, relative to <span class="math inline">\(\hat{\mu}\)</span>, the variance of <span class="math inline">\(\hat{\mu}_\lambda\)</span> can be smaller when <span class="math inline">\(\lambda\)</span> is less than 1. In fact, you can show that there are estimators that have smaller mean squared error than <span class="math inline">\(\hat{\mu}\)</span> by choosing a <span class="math inline">\(\lambda\)</span> that is smaller than (usually just slightly smaller than) 1. This sort of estimator would be biased, but are able to compensate introducing some bias by having smaller variance. For now, we won’t talk much about this sort of estimator (and stick to <span class="math inline">\(\bar{Y}\)</span>), but this sort of estimator has the “flavor” of modern machine learning estimators that typically introduce some bias while reducing variance. One last comment: if you were to make a “bad” choice of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\hat{\mu}_\lambda\)</span> could have higher mean squared error than even <span class="math inline">\(\hat{\mu}_1\)</span>, so if you wanted to proceed this way, you’d have to choose <span class="math inline">\(\lambda\)</span> with some care.</p>
</div>
</div>
<div id="large-sample-properties-of-estimators" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Large Sample Properties of Estimators<a href="#large-sample-properties-of-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 2.6</p>
<p>Statistics/Econometrics often relies on “large sample” (meaning: the number of observations, <span class="math inline">\(n\)</span>, is large) properties of estimators.</p>
<p>Intuition: We generally expect that estimators that use a large number of observations will perform better than in the case with only a few observations.</p>
<p>The second goal of this section will be to introduce an approach to conduct hypothesis testing. In particular, we may have some theory and want a way to test whether or not the data that we have “is consistent with” the theory or not. These arguments typically involve either making strong assumptions or having a large sample — we’ll mainly study the large sample case as I think this is more useful.</p>
</div>
<div id="consistency" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Consistency<a href="#consistency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is said to be <strong>consistent</strong> if <span class="math inline">\(\hat{\theta}\)</span> gets close to <span class="math inline">\(\theta\)</span> for large values of <span class="math inline">\(n\)</span>.</p>
<p>The main tool for studying consistency is the <strong>law of large numbers</strong>. The law of large numbers says that sample averages converge to population averages as the sample size gets large. In math, this is</p>
<p><span class="math display">\[
  \frac{1}{n} \sum_{i=1}^n Y_i \rightarrow \mathbb{E}[Y] \quad \textrm{as } n \rightarrow \infty
\]</span>
In my view, the law of large numbers is very intuitive. If you have a large sample and calculate a sample average, it should be close to the population average.</p>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 4.2  </strong></span>Let’s consider the same three estimators as before and whether or not they are consistent. First, the LLN implies that</p>
<p><span class="math display">\[
  \hat{\mu} = \frac{1}{n} \sum_{i=1}^n Y_i \rightarrow \mathbb{E}[Y]
\]</span>
This implies that <span class="math inline">\(\hat{\mu}\)</span> is consistent. Next,</p>
<p><span class="math display">\[
  \hat{\mu}_1 = Y_1
\]</span>
doesn’t change depending on the size of the sample (you just use the first observation), so this is not consistent. This is an example of an unbiased estimator that is not consistent. Next,</p>
<p><span class="math display">\[
  \hat{\mu}_\lambda = \lambda \bar{Y} \rightarrow \lambda \mathbb{E}[Y] \neq \mathbb{E}[Y]
\]</span>
which implies that (as long as <span class="math inline">\(\lambda\)</span> doesn’t change with <span class="math inline">\(n\)</span>), <span class="math inline">\(\hat{\mu}_{\lambda}\)</span> is not consistent. Let’s give one more example. Consider the estimator</p>
<p><span class="math display">\[
  \hat{\mu}_c := \bar{Y} + \frac{c}{n}
\]</span>
where <span class="math inline">\(c\)</span> is some constant (this is a strange estimate of <span class="math inline">\(\mathbb{E}[Y]\)</span> where we take <span class="math inline">\(\bar{Y}\)</span> and add a constant divided by the sample size). In this case,</p>
<p><span class="math display">\[
  \hat{\mu}_c \rightarrow \mathbb{E}[Y] + 0 = \mathbb{E}[Y]
\]</span></p>
<p>which implies that it is consistent. It is interesting to note that</p>
<p><span class="math display">\[
  \mathbb{E}[\hat{\mu}_c] = \mathbb{E}[Y] + \frac{c}{n}
\]</span>
which implies that it is biased. This is an example of a biased estimator that is consistent.</p>
</div>
</div>
<div id="asymptotic-normality" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Asymptotic Normality<a href="#asymptotic-normality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The next large sample property that we’ll talk about is <strong>asymptotic normality</strong>. This is a hard one to wrap your mind around, but I’ll try to explain as clearly as possible. We’ll start by talking about what it is, and then we’ll move to why it’s useful.</p>
<p>Most of the estimators that we will talk about this semester have the following property</p>
<p><span class="math display">\[
  \sqrt{n}\left( \hat{\theta} - \theta \right) \rightarrow N(0,V) \quad \textrm{as } n \rightarrow \infty
\]</span>
In words, what this says is that we can learn something about the sampling distribution of <span class="math inline">\(\hat{\theta}\)</span> as long as we have a large enough sample. More specifically, if <span class="math inline">\(\hat{\theta}\)</span> is asymptotically normal, it means that if we take <span class="math inline">\(\hat{\theta}\)</span> subtract the true value of the parameter <span class="math inline">\(\theta\)</span> (this is often referred to as “centering”) and multiply by <span class="math inline">\(\sqrt{n}\)</span>, then that object (as long as the sample size is large enough) will <em>seem like</em> a draw from a normal distribution with mean 0 and variance <span class="math inline">\(V\)</span>. Since we know lots about normal distributions, we’ll be able to exploit this in very useful ways in the next section.</p>
<p>An equivalent, alternative expression that is sometimes useful is</p>
<p><span class="math display">\[
  \frac{\sqrt{n}\left( \hat{\theta} - \theta\right)}{\sqrt{V}} \rightarrow N(0,1) \quad \textrm{as } n \rightarrow \infty
\]</span></p>
<p>To establish asymptotic normality of a particular estimator, the main tool is the <strong>central limit theorem</strong>. The central limit theorem (sometimes abbreviated CLT) says that</p>
<p><span class="math display">\[
  \sqrt{n}\left( \frac{1}{n} \sum_{i=1}^n Y_i - \mathbb{E}[Y]\right) \rightarrow N(0,V) \quad \textrm{as } n \rightarrow \infty
\]</span>
where <span class="math inline">\(V = \mathrm{var}(Y)\)</span>.</p>
<p>In words, the CLT says that if you take the difference between <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\mathbb{E}[Y]\)</span> (which, by the LLN converges to 0 as <span class="math inline">\(n \rightarrow \infty\)</span>) and “scale it up” by <span class="math inline">\(\sqrt{n}\)</span> (which goes to <span class="math inline">\(\infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>), then <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y])\)</span> will act like a draw from a normal distribution with variance <span class="math inline">\(\mathrm{var}(Y)\)</span>.</p>
<p>There are a few things to point out:</p>
<ul>
<li><p>Just to start with, this is not nearly as “natural” a result as the LLN. The LLN basically makes perfect sense. For me, I know how to prove the CLT (though we are not going to do it in class), but I don’t think that I would have ever been able to come up with this on my own.</p></li>
<li><p>Notice that the CLT does not rely on any distributional assumptions. We do not need to assume that <span class="math inline">\(Y\)</span> follows a normal distribution and it will apply when <span class="math inline">\(Y\)</span> follows any distribution (up to some relatively minor technical conditions that we will not worry about).</p></li>
<li><p>It is also quite remarkable. We usually have the sense that as the sample size gets large that things will converge to something (e.g., LLN saying that sample averages converge to population averages) or that they will diverge (i.e., go off to positive or negative infinity themselves). The CLT provides an intermediate case — <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y])\)</span> is neither converging to a particular value or diverging to infinity. Instead, it is <em>converging in distribution</em> — meaning: it is settling down to something that looks like a draw from some distribution rather than converging to a particular number. In some sense, you can think of this as a “tie” between the part <span class="math inline">\((\bar{Y}-\mathbb{E}[Y])\)</span> which, by itself, is converging to 0, and <span class="math inline">\(\sqrt{n}\)</span> which, by itself, is diverging to infinity. In fact, if you multiplied instead by something somewhat smaller, say, <span class="math inline">\(n^{1/3}\)</span>, then the term <span class="math inline">\((\bar{Y}-\mathbb{E}[Y])\)</span> would “win” and the whole expression would converge to 0. On the other hand, if you multiplied by something somewhat larger, say, <span class="math inline">\(n\)</span>, then the <span class="math inline">\(n\)</span> part would “win” and the whole thing would diverge. <span class="math inline">\(\sqrt{n}\)</span> turns out to be “just right” so that there is essentially a “tie” and this term neither converges to a particular number nor diverges.</p></li>
<li><p>A very common question for students is: “how large does <span class="math inline">\(n\)</span> need to be for the central limit theorem to apply?” Unfortunately, there is a not a great answer to this (though some textbooks have sometimes given explicit numbers here). Here is a basic explanation for why it is hard to give a definite number. Suppose <span class="math inline">\(Y\)</span> follows a normal distribution, then it will not take many observations for the normal approximation to hold. On the other hand, if <span class="math inline">\(Y\)</span> were to come from a discrete distribution or just a generally complicated distribution, then it might take many more observations for the normal approximation to hold.</p></li>
</ul>
<p>All that to say, I know that the CLT is hard to understand, but the flip-side of that is that it really is a fascinating result. We’ll see how its useful next.</p>
</div>
<div id="inference-hypothesis-testing" class="section level2 hasAnchor" number="4.11">
<h2><span class="header-section-number">4.11</span> Inference / Hypothesis Testing<a href="#inference-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 3.2, 3.3</p>
<p>Often in statistics/econometrics, we have some theory that we would like to test. Pretty soon, we will be interested in testing a theory like: some economic policy had no effect on some outcome of interest.</p>
<p>In this section, we’ll focus on the relatively simple case of conducting inference on <span class="math inline">\(\mathbb{E}[Y]\)</span>, but very similar arguments will apply when we try to start estimating more complicated things soon. Because we’re just focusing on <span class="math inline">\(\mathbb{E}[Y]\)</span>, the examples in this section may be a somewhat trivial/uninteresting, but I want us to learn some mechanics, and then we’ll be able to apply these in more complicated situations.</p>
<p>Let’s start with defining some terms.</p>
<p><strong>Null Hypothesis</strong> This is the hypothesis (or theory) that we want to test. We’ll often write it in the following way</p>
<p><span class="math display">\[
  H_0 : \mathbb{E}[Y] = \mu_0
\]</span>
where <span class="math inline">\(\mu_0\)</span> is some actual number (e.g., 0 or 10 or just whatever coincides with the theory you want to test).</p>
<p><strong>Alternative Hypothesis</strong> This is what is true if <span class="math inline">\(H_0\)</span> is not. There are other possibilities, but I think the only alternative hypothesis that we will consider this semester is</p>
<p><span class="math display">\[
  H_1 : \mathbb{E}[Y] \neq \mu_0
\]</span>
i.e., that <span class="math inline">\(\mathbb{E}[Y]\)</span> is not equal to the particular value <span class="math inline">\(\mu_0\)</span>.</p>
<p>The key conceptual issue is that, even if the null hypothesis is true, because we estimate <span class="math inline">\(\mathbb{E}[Y]\)</span> with a sample, it will generally be the case that <span class="math inline">\(\bar{Y} \neq \mu_0\)</span>. This is just the nature of trying to estimate things with a sample.</p>
<p>What we are going to go for is essentially trying to tell the difference (or at least be able to weigh the evidence) regarding whether the difference between <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\mu_0\)</span> can be fully explained by sampling variation or that the difference is “too big” to be explained by sampling variation. Things will start to get “mathy” in this section, but I think it is helpful to just hold this high-level idea in your head as we go along.</p>
<p>Next, let’s define the <strong>standard error</strong> of an estimator. Suppose that we know that our estimator is asymptotically normal so that</p>
<p><span class="math display">\[
  \sqrt{n}(\hat{\theta} - \theta) \rightarrow N(0,V) \quad \textrm{as } n \rightarrow \infty
\]</span>
Then, we define the standard error of <span class="math inline">\(\hat{\theta}\)</span> as</p>
<p><span class="math display">\[
  \textrm{s.e.}(\hat{\theta}) := \frac{\sqrt{\hat{V}}}{\sqrt{n}}
\]</span>
which is just the square root of the estimate of the asymptotic variance <span class="math inline">\(V\)</span> divided by the square root of the sample size. For example, in the case where we are trying to estimate <span class="math inline">\(\mathbb{E}[Y]\)</span>, recall that, by the CLT, <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y]) \rightarrow N(0,V)\)</span> where <span class="math inline">\(V=\mathrm{var}(Y)\)</span>, so that</p>
<p><span class="math display">\[
  \textrm{s.e.}(\bar{Y}) = \frac{\sqrt{\widehat{\mathrm{var}}(Y)}}{\sqrt{n}}
\]</span>
where <span class="math inline">\(\widehat{\mathrm{var}}(Y)\)</span> is just an estimate of the variance of <span class="math inline">\(Y\)</span>, i.e., just run <code>var(Y)</code> in <code>R</code>.</p>
<p>Over the next few sections, we are going to consider several different way to conduct inference (i.e., weigh the evidence) about some theory (i.e., the null hypothesis) using the data that we have. For all of the approaches that we consider below, the key ingredients are going to an estimate of the parameter of interest (e.g., <span class="math inline">\(\bar{Y}\)</span>), the value of <span class="math inline">\(\mu_0\)</span> coming from the null hypothesis, and the standard error of the estimator.</p>
</div>
<div id="t-statistics" class="section level2 hasAnchor" number="4.12">
<h2><span class="header-section-number">4.12</span> t-statistics<a href="#t-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong>t-statistic</strong> is given by</p>
<p><span class="math display">\[
  t = \frac{\sqrt{n} (\bar{Y} - \mu_0)}{\sqrt{\hat{V}}}
\]</span>
Alternatively (from the definition of standard error), we can write</p>
<p><span class="math display">\[
  t = \frac{(\bar{Y} - \mu_0)}{\textrm{s.e.}(\bar{Y})}
\]</span>
though I’ll tend to use the first expression, just because I think it makes the arguments below slightly more clear.</p>
<p>Notice that <span class="math inline">\(t\)</span> is something that we can calculate with our available data. <span class="math inline">\(\sqrt{n}\)</span> is the square root of the sample size, <span class="math inline">\(\bar{Y}\)</span> is the sample average of <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mu_0\)</span> is a number (that we have picked) coming from the null hypothesis, and <span class="math inline">\(\hat{V}\)</span> is the sample variance of <span class="math inline">\(Y\)</span> (e.g., computed with <code>var(Y)</code> in <code>R</code>).</p>
<p>Now, here is the interesting thing about t-statistics. If the null hypothesis is true, then</p>
<p><span class="math display">\[
  t = \frac{\sqrt{n} (\bar{Y} - \mathbb{E}[Y])}{\sqrt{\hat{V}}} \approx \frac{\sqrt{n} (\bar{Y} - \mathbb{E}[Y])}{\sqrt{V}}
\]</span></p>
<p>where we have substituted in <span class="math inline">\(\mathbb{E}[Y]\)</span> for <span class="math inline">\(\mu_0\)</span> (due to <span class="math inline">\(H_0\)</span> being true) and then replaced <span class="math inline">\(\hat{V}\)</span> with <span class="math inline">\(V\)</span> (which holds under the law of large numbers). This is something that we can apply the CLT to, and, in particular, if <span class="math inline">\(H_0\)</span> holds, then
<span class="math display">\[
  t \rightarrow N(0,1)
\]</span>
That is, if <span class="math inline">\(H_0\)</span> is true, then <span class="math inline">\(t\)</span> should look like a draw from a normal distribution.</p>
<p>Now, let’s think about what happens when the null hypothesis isn’t true. Then, we can write</p>
<p><span class="math display">\[
  t = \frac{\sqrt{n} (\bar{Y} - \mu_0)}{\sqrt{\hat{V}}}
\]</span>
which is just the definition of <span class="math inline">\(t\)</span>, but something different will happen here. In order for <span class="math inline">\(t\)</span> to follow a normal distribution, we need <span class="math inline">\((\bar{Y} - \mu_0)\)</span> to converge to 0. But <span class="math inline">\(\bar{Y}\)</span> converges to <span class="math inline">\(\mathbb{E}[Y]\)</span>, and if the null hypothesis does not hold, then <span class="math inline">\(\mathbb{E}[Y] \neq \mu_0\)</span> which implies that <span class="math inline">\((\bar{Y} - \mu_0) \rightarrow (\mathbb{E}[Y] - \mu_0) \neq 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. It’s still the case that <span class="math inline">\(\sqrt{n} \rightarrow \infty\)</span>. Thus, if <span class="math inline">\(H_0\)</span> is not true, then <span class="math inline">\(t\)</span> will diverge (recall: this means that it will either go to positive infinity or negative infinity depending on the sign of <span class="math inline">\((\mathbb{E}[Y] - \mu_0)\)</span>).</p>
<p>This gives us a very good way to start to think about whether or not the data is compatible with our theory. For example, suppose that you calculate <span class="math inline">\(t\)</span> (using your data and under your null hypothesis) and that it is equal to 1. 1 is not an “unusual” looking draw from a standard normal distribution — this suggests that you at least do not have strong evidence from data against your theory. Alternatively, suppose that you calculate that <span class="math inline">\(t=-24\)</span>. While its technically possible that you could draw <span class="math inline">\(-24\)</span> from a standard normal distribution — it is exceedingly unlikely. We would interpret this as strong evidence against the null hypothesis, and it should probably lead you to “reject” the null hypothesis.</p>
<p>We have talked about some clear cases, but what about the “close calls”? Suppose you calculate that <span class="math inline">\(t=2\)</span>. Under the null hypothesis, there is about a 4.6% chance of getting a t-statistic at least this large (in absolute value). So…if <span class="math inline">\(H_0\)</span> is true, this is a fairly unusual t-statistic, but it is not extremely unusual. What should you do?</p>
<p>Before we decide what to do, let’s introduce a little more terminology regarding what could go wrong with hypothesis testing. There are two ways that we could go wrong:</p>
<p><strong>Type I Error</strong> — This would be to reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true</p>
<p><strong>Type II Error</strong> — This would be to fail to reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is false</p>
<p>Clearly, there is a tradeoff here. If you are really concerned with type I errors, you can be very cautious about rejecting <span class="math inline">\(H_0\)</span>. If you are very concerned about type II errors, you could aggressively reject <span class="math inline">\(H_0\)</span>. The traditional approach to trading these off in statistics is to pre-specify a <strong>significance level</strong> indicating what percentage of the time you are willing to commit a type I error. Usually the significance level is denoted by <span class="math inline">\(\alpha\)</span> and the most common choice of <span class="math inline">\(\alpha\)</span> is 0.05 and other common choices are <span class="math inline">\(\alpha=0.1\)</span> or <span class="math inline">\(\alpha=0.01\)</span>. Then, good statistical tests try to make as few type II errors as possible subject to the constraint on the rate of type I errors.</p>
<p>Often, once you have specified a significance level, it comes with a <strong>critical value</strong>. The critical value is the value of a test statistic for which the test just rejects <span class="math inline">\(H_0\)</span>.</p>
<p>In practice, this leads to the following decision rule:</p>
<ul>
<li><p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &gt; c_{1-\alpha}\)</span> where <span class="math inline">\(c_{1-\alpha}\)</span> is the critical value corresponding to the significance level <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Fail to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &lt; c_{1-\alpha}\)</span></p></li>
</ul>
<p>In our case, since <span class="math inline">\(t\)</span> follows a normal distribution under <span class="math inline">\(H_0\)</span>, the corresponding critical value (when <span class="math inline">\(\alpha=0.05\)</span>) is 1.96. In particular, recall what the pdf of a standard normal random variable looks like</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<p>The sum of the two blue, shaded areas is 0.05. In other words, under <span class="math inline">\(H_0\)</span>, there is a 5% chance that, by chance, <span class="math inline">\(t\)</span> would fall in the shaded areas. If you want to change the significance level, it would result in a corresponding change in the critical value so that the area in the new shaded region would adjust too. For example, if you set the significance level to be <span class="math inline">\(\alpha=0.1\)</span>, then you would need to adjust the critical value to be 1.64, and if you set <span class="math inline">\(\alpha=0.01\)</span>, then you would need to adjust the critical value to be 2.58.</p>
</div>
<div id="p-values" class="section level2 hasAnchor" number="4.13">
<h2><span class="header-section-number">4.13</span> P-values<a href="#p-values" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Choosing a significance level is somewhat arbitrary. What did we choose 5%?</p>
<p>Perhaps more importantly, we are essentially throwing away a lot of information if we are to reduce the information from standard errors/t-statistics to a binary “reject” or “fail to reject”.</p>
<p>One alternative is to report a <strong>p-value</strong>. A p-value is the probability of observing a t-statistic as “extreme” as we did if <span class="math inline">\(H_0\)</span> were true.</p>
<p>Here is an example of how to calculate a p-value. Suppose we calculate <span class="math inline">\(t=1.85\)</span>. Then,</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>Then, under <span class="math inline">\(H_0\)</span>, the probability of getting a t-statistic “as extreme” as 1.85 corresponds to the area of the two shaded regions above. In other words, we need to to compute</p>
<p><span class="math display">\[
  \textrm{p-value} = \mathrm{P}(Z \leq -1.85) + \mathrm{P}(Z \geq 1.85)
\]</span>
where <span class="math inline">\(Z \sim N(0,1)\)</span>. One thing that is helpful to notice here is that, a standard normal random variable is symmetric. This means that <span class="math inline">\(\mathrm{P}(Z \leq -1.85) = \mathrm{P}(Z \geq 1.85)\)</span>. We also typically denote the cdf of a standard normal random variable with the symbol <span class="math inline">\(\Phi\)</span>. Thus,</p>
<p><span class="math display">\[
  \textrm{p-value} = 2 \Phi(-1.85)
\]</span>
I don’t know what this is off the top of my head, but it is easy to compute from a table or using <code>R</code>. In <code>R</code>, you can use the function <code>pnorm</code> — here, the p-value is given by <code>2*pnorm(-1.85)</code> which is equal to 0.064.</p>
<p>More generally, if you calculate a t-statistic, <span class="math inline">\(t\)</span>, using your data and under <span class="math inline">\(H_0\)</span>, then,</p>
<p><span class="math display">\[
  \textrm{p-value} = 2 \Phi(-|t|)
\]</span></p>
</div>
<div id="confidence-interval" class="section level2 hasAnchor" number="4.14">
<h2><span class="header-section-number">4.14</span> Confidence Interval<a href="#confidence-interval" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another idea is to report a <span class="math inline">\((1-\alpha)\%\)</span> (e.g., 95%) confidence interval.</p>
<p>The interpretation of a confidence interval is a bit subtle. It is this: if we collected a large number of samples, and computed a confidence interval each time, 95% of these would contain the true value. This is subtly different than: there is a 95% probability that <span class="math inline">\(\theta\)</span> (the population parameter of interest) falls within the confidence interval — this second interpretation doesn’t make sense because <span class="math inline">\(\theta\)</span> is non-random.</p>
<p>A 95% confidence interval is given by</p>
<p><span class="math display">\[
  CI_{95\%} = \left[\hat{\theta} - 1.96 \ \textrm{s.e.}(\hat{\theta}), \hat{\theta} + 1.96 \  \textrm{s.e.}(\hat{\theta})\right]
\]</span></p>
<p>For the particular case where we are interested in <span class="math inline">\(\mathbb{E}[Y]\)</span>, this becomes</p>
<p><span class="math display">\[
  CI_{95\%} = \left[ \bar{Y} - 1.96 \ \textrm{s.e.}(\bar{Y}), \bar{Y} + 1.96 \ \textrm{s.e.}(\bar{Y}) \right]
\]</span></p>
</div>
<div id="inference-in-practice" class="section level2 hasAnchor" number="4.15">
<h2><span class="header-section-number">4.15</span> Inference in Practice<a href="#inference-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I have covered the main approaches to inference in this section. I’d like to make a couple of concluding comments. First, all of the approaches discussed here (standard errors, t-statistics, p-values, and confidence intervals) are very closely related (in some sense, they are just alternative ways to report the same information). They all rely heavily on establishing asymptotic normality of the estimate of the parameter of interest — in fact, this is why we were interested in asymptotic normality in the first place. My sense is that the most common thing to report (at least in economics) is an estimate of the parameter of interest (e.g., <span class="math inline">\(\hat{\theta}\)</span> or <span class="math inline">\(\bar{Y}\)</span>) along with its standard error. If you know this information, you (or your reader) can easily compute any of the other expressions that we’ve considered in this section.</p>
<p>Another important thing to mention is that there is often a distinction between <strong>statistical significance</strong> and <strong>economic significance</strong>.</p>
<p>In the next chapter, we’ll start to think about the <em>effect</em> of one variable on another (e.g., the effect of some economic policy on some outcome of interest). By far the most common null hypothesis in this case is that “the effect” is equal to 0. However, in economics/social sciences/business applications, there probably aren’t too many cases where (i) it would be interesting enough to consider the effect of one variable on another (ii) while simultaneously the effect is literally equal to 0. Since, all else equal, standard errors get smaller with more observations, as datasets in economics tend to get larger over time, we tend to find more statistically significant effects. This doesn’t mean that effects are getting bigger or more important — just that we are able to detect smaller and smaller effects if we have enough data. And most questions in economics involve more than just answering the binary question: does variable <span class="math inline">\(X\)</span> have any effect at all on variable <span class="math inline">\(Y\)</span>? For example, if you are trying to evaluate the effect of some economic policy, it is usually more helpful to think in terms of a cost-benefit analysis — what are the benefits or the policy relative to the costs and these sorts of comparisons inherently involve thinking about magnitudes of effects.</p>
<p>A more succinct way to say all this is: the effect of one variable on another can be both “statistically significant” and “economically” small at the same time. Alternatively, if you do not have much data or the data is very “noisy”, it may be possible that there are relatively large effects, but that the estimates are not statistically significant (i.e., you are not able to detect them very well with the data that you have). Therefore, it is important to not become too fixated on statistical significance and to additionally think carefully about the magnitudes of estimates.</p>
</div>
<div id="coding-1" class="section level2 hasAnchor" number="4.16">
<h2><span class="header-section-number">4.16</span> Coding<a href="#coding-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we’ll use the <code>acs</code> data to calculate an estimate of average wage/salary income among employed individuals in the United States. We’ll test the null hypothesis that the mean income in the United States is $50,000 as well as report the standard error of our estimate of mean income, as well as corresponding p-values, t-statistics, and 95% confidence interval. Finally, we’ll report a table of summary statistics using the <code>modelsummary</code> package separately by college graduates relative to non-college graduates.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/acs.RData&quot;</span>)</span>
<span id="cb89-2"><a href="#cb89-2" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" tabindex="-1"></a><span class="co"># estimate of mean income</span></span>
<span id="cb89-4"><a href="#cb89-4" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(acs<span class="sc">$</span>incwage)</span>
<span id="cb89-5"><a href="#cb89-5" tabindex="-1"></a>ybar</span>
<span id="cb89-6"><a href="#cb89-6" tabindex="-1"></a><span class="co">#&gt; [1] 59263.46</span></span>
<span id="cb89-7"><a href="#cb89-7" tabindex="-1"></a></span>
<span id="cb89-8"><a href="#cb89-8" tabindex="-1"></a><span class="co"># calculate standard error</span></span>
<span id="cb89-9"><a href="#cb89-9" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="fu">var</span>(acs<span class="sc">$</span>incwage)</span>
<span id="cb89-10"><a href="#cb89-10" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(acs)</span>
<span id="cb89-11"><a href="#cb89-11" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(V) <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb89-12"><a href="#cb89-12" tabindex="-1"></a>se</span>
<span id="cb89-13"><a href="#cb89-13" tabindex="-1"></a><span class="co">#&gt; [1] 713.8138</span></span>
<span id="cb89-14"><a href="#cb89-14" tabindex="-1"></a></span>
<span id="cb89-15"><a href="#cb89-15" tabindex="-1"></a><span class="co"># calculate t-statistic</span></span>
<span id="cb89-16"><a href="#cb89-16" tabindex="-1"></a>t_stat <span class="ot">&lt;-</span> (ybar <span class="sc">-</span> <span class="dv">50000</span>) <span class="sc">/</span> se</span>
<span id="cb89-17"><a href="#cb89-17" tabindex="-1"></a>t_stat</span>
<span id="cb89-18"><a href="#cb89-18" tabindex="-1"></a><span class="co">#&gt; [1] 12.97742</span></span></code></pre></div>
<p>This clearly exceeds 1.96 (or any common critical value) which implies that we would reject the null hypothesis that mean income is equal to $50,000.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" tabindex="-1"></a><span class="co"># calculate p-value</span></span>
<span id="cb90-2"><a href="#cb90-2" tabindex="-1"></a>p_val <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">abs</span>(t_stat))</span></code></pre></div>
<p>The p-value is essentially equal to 0. This is expected given the value of the t-statistic that we calculated earlier.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" tabindex="-1"></a><span class="co"># 95% confidence interval</span></span>
<span id="cb91-2"><a href="#cb91-2" tabindex="-1"></a>ci_L <span class="ot">&lt;-</span> ybar <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb91-3"><a href="#cb91-3" tabindex="-1"></a>ci_U <span class="ot">&lt;-</span> ybar <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb91-4"><a href="#cb91-4" tabindex="-1"></a><span class="fu">paste0</span>(<span class="st">&quot;[&quot;</span>,<span class="fu">round</span>(ci_L,<span class="dv">1</span>),<span class="st">&quot;,&quot;</span>,<span class="fu">round</span>(ci_U,<span class="dv">1</span>),<span class="st">&quot;]&quot;</span>)</span>
<span id="cb91-5"><a href="#cb91-5" tabindex="-1"></a><span class="co">#&gt; [1] &quot;[57864.4,60662.5]&quot;</span></span></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb92-2"><a href="#cb92-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb92-3"><a href="#cb92-3" tabindex="-1"></a><span class="co"># create a factor variable for going to college</span></span>
<span id="cb92-4"><a href="#cb92-4" tabindex="-1"></a>acs<span class="sc">$</span>col <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(acs<span class="sc">$</span>educ <span class="sc">&gt;=</span> <span class="dv">16</span>, <span class="st">&quot;college&quot;</span>, <span class="st">&quot;non-college&quot;</span>)</span>
<span id="cb92-5"><a href="#cb92-5" tabindex="-1"></a>acs<span class="sc">$</span>col <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(acs<span class="sc">$</span>col)</span>
<span id="cb92-6"><a href="#cb92-6" tabindex="-1"></a>acs<span class="sc">$</span>female <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(acs<span class="sc">$</span>sex<span class="sc">==</span><span class="dv">2</span>)</span>
<span id="cb92-7"><a href="#cb92-7" tabindex="-1"></a>acs<span class="sc">$</span>incwage <span class="ot">&lt;-</span> acs<span class="sc">$</span>incwage<span class="sc">/</span><span class="dv">1000</span></span>
<span id="cb92-8"><a href="#cb92-8" tabindex="-1"></a><span class="fu">datasummary_balance</span>(<span class="sc">~</span> col, <span class="at">data=</span>dplyr<span class="sc">::</span><span class="fu">select</span>(acs, incwage, female, age, col),</span>
<span id="cb92-9"><a href="#cb92-9" tabindex="-1"></a>                    <span class="at">fmt=</span><span class="dv">2</span>)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
college (N=3871)
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
non-college (N=6129)
</div>
</th>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="2">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Diff. in Means
</th>
<th style="text-align:right;">
Std. Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
incwage
</td>
<td style="text-align:right;">
89.69
</td>
<td style="text-align:right;">
96.15
</td>
<td style="text-align:right;">
40.05
</td>
<td style="text-align:right;">
39.01
</td>
<td style="text-align:right;">
-49.65
</td>
<td style="text-align:right;">
1.62
</td>
</tr>
<tr>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
0.51
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
-0.04
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
44.38
</td>
<td style="text-align:right;">
13.43
</td>
<td style="text-align:right;">
42.80
</td>
<td style="text-align:right;">
15.71
</td>
<td style="text-align:right;">
-1.58
</td>
<td style="text-align:right;">
0.29
</td>
</tr>
</tbody>
</table>
</div>
<div id="lab-3-monte-carlo-simulations" class="section level2 hasAnchor" number="4.17">
<h2><span class="header-section-number">4.17</span> Lab 3: Monte Carlo Simulations<a href="#lab-3-monte-carlo-simulations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this lab, we will study the theoretical properties of the estimators that we have been discussing in this chapter.</p>
<p><strong>Monte Carlo simulations</strong> are a useful way to study/understand the properties of an estimation procedure. The basic idea is that, instead of using real data, we are going to use simulated data where we control the data generating process. This will be useful for two reasons. First, we will <em>know</em> what <strong>the truth</strong> is and compare results coming from our estimation procedure to the truth. Second, because we are simulating data, we can actually carry out our thought experiment of repeatedly drawing a sample of some particular size.</p>
<p>For this lab, we are going to make simulated coin flips.</p>
<ol style="list-style-type: decimal">
<li><p>Write a function called <code>flip</code> that takes in an argument <code>p</code> where <code>p</code> stands for the probability of flipping a heads (you can code this as a <code>1</code> and <code>0</code> for tails) and outputs either <code>1</code> or <code>0</code>. Run the code</p>
<pre><code>flip(0.5)</code></pre>
<p><strong>Hint:</strong> It may be helpful to use the <code>R</code> function <code>sample</code>.</p></li>
<li><p>Write a function called <code>generate_sample</code> that takes in the arguments <code>n</code> and <code>p</code> and generates a sample of <code>n</code> coin flips where the probability of flipping heads is <code>p</code>. Run the code</p>
<pre><code>generate_sample(10,0.5)</code></pre></li>
<li><p>Next, over 1000 Monte Carlo simulations (i.e., do the following 1000 times),</p>
<ol style="list-style-type: lower-roman">
<li><p>generate a new sample with 10 observations</p></li>
<li><p>calculate an estimate of <span class="math inline">\(p\)</span></p></li>
</ol>
<p>(<strong>Hint:</strong> you can estimate <span class="math inline">\(p\)</span> by just calculating the average number of heads flipped in a particular simulation)</p>
<ol start="3" style="list-style-type: lower-roman">
<li><p>a t-statistic for the null hypothesis that <span class="math inline">\(p=0.5\)</span></p></li>
<li><p>and record whether or not you reject the null hypothesis that <span class="math inline">\(p=0.5\)</span> in that simulation</p></li>
</ol></li>
</ol>
<p>Then, using all 1000 Monte Carlo simulations, report (i) an estimate of the bias of your estimator, (ii) an estimate of the variance of your estimator, (iii) an estimate of the mean squared error of your estimator, (iv) plot a histogram of the t-statistics across iterations, and (v) report the fraction of times that you reject <span class="math inline">\(H_0\)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Same as #3, but with 50 observations in each simulation. What differences do you notice?</p></li>
<li><p>Same as #3, but with 50 observations and test <span class="math inline">\(H_0:p=0.6\)</span>. What differences do you notice?</p></li>
<li><p>Same as #3, but with 50 observations and test <span class="math inline">\(H_0:p=0.9\)</span>. What differences do you notice?</p></li>
<li><p>Same as #3, but with 1000 observations and test <span class="math inline">\(H_0:p=0.6\)</span>. What differences do you notice?</p></li>
<li><p>Same as #3, but now set <span class="math inline">\(p=0.95\)</span> (so that this is an unfair coin that flips heads 95% of the time) and with 10 observations and test <span class="math inline">\(H_0:p=0.95\)</span>. What differences do you notice?</p></li>
<li><p>Same as #8, but with 50 observations. What differences do you notice?</p></li>
<li><p>Same as #8, but with 1000 observations. What differences do you notice?</p></li>
</ol>
<p><strong>Hint:</strong> Since problems 3-10 ask you to do roughly the same thing over and over, it is probably useful to try to write a function to do all of these but with arguments that allow you to change the number of observations per simulation, the true value of <span class="math inline">\(p\)</span>, and the null hypothesis that you are testing.</p>
</div>
<div id="lab-3-solutions" class="section level2 hasAnchor" number="4.18">
<h2><span class="header-section-number">4.18</span> Lab 3 Solutions<a href="#lab-3-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" tabindex="-1"></a><span class="co"># function to flip a coin with probability p</span></span>
<span id="cb95-2"><a href="#cb95-2" tabindex="-1"></a>flip <span class="ot">&lt;-</span> <span class="cf">function</span>(p) {</span>
<span id="cb95-3"><a href="#cb95-3" tabindex="-1"></a>  <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">size=</span><span class="dv">1</span>, <span class="at">prob=</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>p,p)))</span>
<span id="cb95-4"><a href="#cb95-4" tabindex="-1"></a>}</span>
<span id="cb95-5"><a href="#cb95-5" tabindex="-1"></a></span>
<span id="cb95-6"><a href="#cb95-6" tabindex="-1"></a><span class="co"># test out flip function</span></span>
<span id="cb95-7"><a href="#cb95-7" tabindex="-1"></a><span class="fu">flip</span>(<span class="fl">0.5</span>)</span>
<span id="cb95-8"><a href="#cb95-8" tabindex="-1"></a><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" tabindex="-1"></a><span class="co"># function to generate a sample of size n</span></span>
<span id="cb96-2"><a href="#cb96-2" tabindex="-1"></a>generate_sample <span class="ot">&lt;-</span> <span class="cf">function</span>(n,p) {</span>
<span id="cb96-3"><a href="#cb96-3" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb96-4"><a href="#cb96-4" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb96-5"><a href="#cb96-5" tabindex="-1"></a>    Y[i] <span class="ot">&lt;-</span> <span class="fu">flip</span>(p)</span>
<span id="cb96-6"><a href="#cb96-6" tabindex="-1"></a>  }</span>
<span id="cb96-7"><a href="#cb96-7" tabindex="-1"></a>  Y</span>
<span id="cb96-8"><a href="#cb96-8" tabindex="-1"></a>}</span>
<span id="cb96-9"><a href="#cb96-9" tabindex="-1"></a></span>
<span id="cb96-10"><a href="#cb96-10" tabindex="-1"></a><span class="co"># test out generate_sample function</span></span>
<span id="cb96-11"><a href="#cb96-11" tabindex="-1"></a><span class="fu">generate_sample</span>(<span class="dv">10</span>,<span class="fl">0.5</span>)</span>
<span id="cb96-12"><a href="#cb96-12" tabindex="-1"></a><span class="co">#&gt;  [1] 1 0 0 1 1 1 1 1 0 0</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" tabindex="-1"></a><span class="co"># carry out monte carlo simulations</span></span>
<span id="cb97-2"><a href="#cb97-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb97-3"><a href="#cb97-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb97-4"><a href="#cb97-4" tabindex="-1"></a>nsims <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co"># need to pick large number of monte carlo simulations</span></span>
<span id="cb97-5"><a href="#cb97-5" tabindex="-1"></a>mc_est <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimation results</span></span>
<span id="cb97-6"><a href="#cb97-6" tabindex="-1"></a>mc_var <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimated variance</span></span>
<span id="cb97-7"><a href="#cb97-7" tabindex="-1"></a></span>
<span id="cb97-8"><a href="#cb97-8" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsims) {</span>
<span id="cb97-9"><a href="#cb97-9" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">generate_sample</span>(n,p)</span>
<span id="cb97-10"><a href="#cb97-10" tabindex="-1"></a>  mc_est[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(Y)</span>
<span id="cb97-11"><a href="#cb97-11" tabindex="-1"></a>  mc_var[i] <span class="ot">&lt;-</span> <span class="fu">var</span>(Y)</span>
<span id="cb97-12"><a href="#cb97-12" tabindex="-1"></a>}</span>
<span id="cb97-13"><a href="#cb97-13" tabindex="-1"></a></span>
<span id="cb97-14"><a href="#cb97-14" tabindex="-1"></a><span class="co"># compute bias</span></span>
<span id="cb97-15"><a href="#cb97-15" tabindex="-1"></a>bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(mc_est) <span class="sc">-</span> p</span>
<span id="cb97-16"><a href="#cb97-16" tabindex="-1"></a>bias</span>
<span id="cb97-17"><a href="#cb97-17" tabindex="-1"></a><span class="co">#&gt; [1] 0.0046</span></span>
<span id="cb97-18"><a href="#cb97-18" tabindex="-1"></a></span>
<span id="cb97-19"><a href="#cb97-19" tabindex="-1"></a><span class="co"># compute sampling variance</span></span>
<span id="cb97-20"><a href="#cb97-20" tabindex="-1"></a>var <span class="ot">&lt;-</span> <span class="fu">var</span>(mc_est)</span>
<span id="cb97-21"><a href="#cb97-21" tabindex="-1"></a>var</span>
<span id="cb97-22"><a href="#cb97-22" tabindex="-1"></a><span class="co">#&gt; [1] 0.0240629</span></span>
<span id="cb97-23"><a href="#cb97-23" tabindex="-1"></a></span>
<span id="cb97-24"><a href="#cb97-24" tabindex="-1"></a><span class="co"># compute mean squared error</span></span>
<span id="cb97-25"><a href="#cb97-25" tabindex="-1"></a>mse <span class="ot">&lt;-</span> bias<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var</span>
<span id="cb97-26"><a href="#cb97-26" tabindex="-1"></a>mse</span>
<span id="cb97-27"><a href="#cb97-27" tabindex="-1"></a><span class="co">#&gt; [1] 0.02408406</span></span>
<span id="cb97-28"><a href="#cb97-28" tabindex="-1"></a></span>
<span id="cb97-29"><a href="#cb97-29" tabindex="-1"></a>H0 <span class="ot">&lt;-</span> p</span>
<span id="cb97-30"><a href="#cb97-30" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(mc_est <span class="sc">-</span> H0) <span class="sc">/</span> <span class="fu">sqrt</span>(mc_var)</span>
<span id="cb97-31"><a href="#cb97-31" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">t=</span>t), <span class="fu">aes</span>(<span class="at">x=</span>t)) <span class="sc">+</span></span>
<span id="cb97-32"><a href="#cb97-32" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins=</span><span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb97-33"><a href="#cb97-33" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb97-34"><a href="#cb97-34" tabindex="-1"></a><span class="co">#&gt; Warning: Removed 2 rows containing non-finite values (`stat_bin()`).</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" tabindex="-1"></a></span>
<span id="cb98-2"><a href="#cb98-2" tabindex="-1"></a>rej <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="dv">1</span><span class="sc">*</span>(<span class="fu">abs</span>(t) <span class="sc">&gt;=</span> <span class="fl">1.96</span>))</span>
<span id="cb98-3"><a href="#cb98-3" tabindex="-1"></a>rej</span>
<span id="cb98-4"><a href="#cb98-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.107</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" tabindex="-1"></a><span class="co"># since we are going to do this over and over, let&#39;s write a function to do it</span></span>
<span id="cb99-2"><a href="#cb99-2" tabindex="-1"></a>mc_sim <span class="ot">&lt;-</span> <span class="cf">function</span>(n, p, H0) {</span>
<span id="cb99-3"><a href="#cb99-3" tabindex="-1"></a>  mc_est <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimation results</span></span>
<span id="cb99-4"><a href="#cb99-4" tabindex="-1"></a>  mc_var <span class="ot">&lt;-</span> <span class="fu">c</span>()  <span class="co"># vector to hold estimated variance</span></span>
<span id="cb99-5"><a href="#cb99-5" tabindex="-1"></a></span>
<span id="cb99-6"><a href="#cb99-6" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsims) {</span>
<span id="cb99-7"><a href="#cb99-7" tabindex="-1"></a>    Y <span class="ot">&lt;-</span> <span class="fu">generate_sample</span>(n,p)</span>
<span id="cb99-8"><a href="#cb99-8" tabindex="-1"></a>    mc_est[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(Y)</span>
<span id="cb99-9"><a href="#cb99-9" tabindex="-1"></a>    mc_var[i] <span class="ot">&lt;-</span> <span class="fu">var</span>(Y)</span>
<span id="cb99-10"><a href="#cb99-10" tabindex="-1"></a>  }</span>
<span id="cb99-11"><a href="#cb99-11" tabindex="-1"></a></span>
<span id="cb99-12"><a href="#cb99-12" tabindex="-1"></a>  <span class="co"># compute bias</span></span>
<span id="cb99-13"><a href="#cb99-13" tabindex="-1"></a>  bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(mc_est) <span class="sc">-</span> p</span>
<span id="cb99-14"><a href="#cb99-14" tabindex="-1"></a></span>
<span id="cb99-15"><a href="#cb99-15" tabindex="-1"></a>  <span class="co"># compute sampling variance</span></span>
<span id="cb99-16"><a href="#cb99-16" tabindex="-1"></a>  var <span class="ot">&lt;-</span> <span class="fu">var</span>(mc_est)</span>
<span id="cb99-17"><a href="#cb99-17" tabindex="-1"></a></span>
<span id="cb99-18"><a href="#cb99-18" tabindex="-1"></a>  <span class="co"># compute mean squared error</span></span>
<span id="cb99-19"><a href="#cb99-19" tabindex="-1"></a>  mse <span class="ot">&lt;-</span> bias<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var</span>
<span id="cb99-20"><a href="#cb99-20" tabindex="-1"></a>  </span>
<span id="cb99-21"><a href="#cb99-21" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(mc_est <span class="sc">-</span> H0) <span class="sc">/</span> <span class="fu">sqrt</span>(mc_var)</span>
<span id="cb99-22"><a href="#cb99-22" tabindex="-1"></a>  hist_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">t=</span>t), <span class="fu">aes</span>(<span class="at">x=</span>t)) <span class="sc">+</span></span>
<span id="cb99-23"><a href="#cb99-23" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="at">bins=</span><span class="dv">30</span>) <span class="sc">+</span> </span>
<span id="cb99-24"><a href="#cb99-24" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb99-25"><a href="#cb99-25" tabindex="-1"></a></span>
<span id="cb99-26"><a href="#cb99-26" tabindex="-1"></a>  rej <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="dv">1</span><span class="sc">*</span>(<span class="fu">abs</span>(t) <span class="sc">&gt;=</span> <span class="fl">1.96</span>))</span>
<span id="cb99-27"><a href="#cb99-27" tabindex="-1"></a>  </span>
<span id="cb99-28"><a href="#cb99-28" tabindex="-1"></a>  <span class="co"># print results</span></span>
<span id="cb99-29"><a href="#cb99-29" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;bias: &quot;</span>, <span class="fu">round</span>(bias,<span class="dv">4</span>)))</span>
<span id="cb99-30"><a href="#cb99-30" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;var : &quot;</span>, <span class="fu">round</span>(var,<span class="dv">4</span>)))</span>
<span id="cb99-31"><a href="#cb99-31" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;mse : &quot;</span>, <span class="fu">round</span>(mse,<span class="dv">4</span>)))</span>
<span id="cb99-32"><a href="#cb99-32" tabindex="-1"></a>  <span class="fu">print</span>(hist_plot)</span>
<span id="cb99-33"><a href="#cb99-33" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;rej : &quot;</span>, <span class="fu">round</span>(rej,<span class="dv">4</span>)))</span>
<span id="cb99-34"><a href="#cb99-34" tabindex="-1"></a>}</span>
<span id="cb99-35"><a href="#cb99-35" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb99-36"><a href="#cb99-36" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 0.0014&quot;</span></span>
<span id="cb99-37"><a href="#cb99-37" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.0056&quot;</span></span>
<span id="cb99-38"><a href="#cb99-38" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.0056&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.085&quot;</code></pre>
<ol start="5" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>)</span>
<span id="cb101-2"><a href="#cb101-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: -0.0015&quot;</span></span>
<span id="cb101-3"><a href="#cb101-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.0048&quot;</span></span>
<span id="cb101-4"><a href="#cb101-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.0048&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.339&quot;</code></pre>
<ol start="6" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>)</span>
<span id="cb103-2"><a href="#cb103-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 0.0011&quot;</span></span>
<span id="cb103-3"><a href="#cb103-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.005&quot;</span></span>
<span id="cb103-4"><a href="#cb103-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.005&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 1&quot;</code></pre>
<ol start="7" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">1000</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>)</span>
<span id="cb105-2"><a href="#cb105-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 4e-04&quot;</span></span>
<span id="cb105-3"><a href="#cb105-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 2e-04&quot;</span></span>
<span id="cb105-4"><a href="#cb105-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 2e-04&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-111-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 1&quot;</code></pre>
<ol start="8" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">10</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>)</span>
<span id="cb107-2"><a href="#cb107-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: -0.0058&quot;</span></span>
<span id="cb107-3"><a href="#cb107-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0.005&quot;</span></span>
<span id="cb107-4"><a href="#cb107-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0.005&quot;</span></span>
<span id="cb107-5"><a href="#cb107-5" tabindex="-1"></a><span class="co">#&gt; Warning: Removed 555 rows containing non-finite values (`stat_bin()`).</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-112-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.556&quot;</code></pre>
<ol start="9" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">50</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>)</span>
<span id="cb109-2"><a href="#cb109-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: -0.001&quot;</span></span>
<span id="cb109-3"><a href="#cb109-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 9e-04&quot;</span></span>
<span id="cb109-4"><a href="#cb109-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 9e-04&quot;</span></span>
<span id="cb109-5"><a href="#cb109-5" tabindex="-1"></a><span class="co">#&gt; Warning: Removed 59 rows containing non-finite values (`stat_bin()`).</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-113-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.064&quot;</code></pre>
<ol start="10" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" tabindex="-1"></a><span class="fu">mc_sim</span>(<span class="dv">1000</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>)</span>
<span id="cb111-2"><a href="#cb111-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;bias: 1e-04&quot;</span></span>
<span id="cb111-3"><a href="#cb111-3" tabindex="-1"></a><span class="co">#&gt; [1] &quot;var : 0&quot;</span></span>
<span id="cb111-4"><a href="#cb111-4" tabindex="-1"></a><span class="co">#&gt; [1] &quot;mse : 0&quot;</span></span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
<pre><code>#&gt; [1] &quot;rej : 0.057&quot;</code></pre>
</div>
<div id="coding-questions-1" class="section level2 hasAnchor" number="4.19">
<h2><span class="header-section-number">4.19</span> Coding Questions<a href="#coding-questions-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>For this question, we’ll use the data <code>Airq</code>. The variable <code>rain</code> contains the amount of rainfall in the county in a year (in inches). For this question, we’ll be interested in testing whether or not the mean rainfall across counties in California is 25 inches.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimate the mean rainfall across counties.</p></li>
<li><p>Calculate the standard error of your estimate of rainfall.</p></li>
<li><p>Calculate a t-statistic for <span class="math inline">\(H_0 : \mathbb{E}[Y] = 25\)</span> where <span class="math inline">\(Y\)</span> denotes rainfall. Do you reject <span class="math inline">\(H_0\)</span> at a 5% significance level? Explain.</p></li>
<li><p>Calculate a p-value for <span class="math inline">\(H_0: \mathbb{E}[Y] = 25\)</span>. How should you interpret this?</p></li>
<li><p>Calculate a 95% confidence interval for average rainfall.</p></li>
<li><p>Use the <code>datasummary_balance</code> function from the <code>modelsummary</code> package to report average air quality, value added, rain, population density, and average income, separately by whether or not the county is located in a coastal area.</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-1" class="section level2 hasAnchor" number="4.20">
<h2><span class="header-section-number">4.20</span> Extra Questions<a href="#extra-questions-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>What is the difference between consistency and unbiasedness?</p></li>
<li><p>Suppose you have an estimator that is unbiased. Will it necessarily be consistent? If not, provide an example of an unbiased estimator that is not consistent.</p></li>
<li><p>Suppose you have an estimator that is consistent. Will it necessarily be unbiased? If not, provide an example of a consistent estimator that is not unbiased.</p></li>
<li><p>The Central Limit Theorem says that, <span class="math inline">\(\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbb{E}[Y])\right) \rightarrow N(0,V)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> where <span class="math inline">\(V = \mathrm{var}(Y)\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What happens to <span class="math inline">\(n \left(\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbb{E}[Y])\right)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>? Explain.</p></li>
<li><p>What happens to <span class="math inline">\(n^{1/3} \left(\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbb{E}[Y])\right)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>? Explain.</p></li>
</ol></li>
</ol>

</div>
</div>
<div id="linear-regression" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Topic 5</span> Linear Regression<a href="#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, our interest will shift to conditional expectations, such as <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span> (I’ll write <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span> in a lot of examples in this chapter, but you can think of there being an arbitrary number of <span class="math inline">\(X\)</span>’s).</p>
<p>I’ll refer to <span class="math inline">\(Y\)</span> as the <strong>outcome</strong>. You might also sometimes heae it called the <strong>dependent variable</strong>.</p>
<p>I’ll refer to the <span class="math inline">\(X\)</span>’s as either <strong>covariates</strong> or <strong>regressors</strong> or <strong>characteristics</strong>. You might also hear them called <strong>independent variables</strong> sometimes.</p>
<p>Before we start to get into the details, let us first discuss why we’re interested in conditional expectations. First, if we are interested in <em>making predictions</em>, it will often be the case that the “best” prediction that one can make is the conditional expectation. This should make sense to you — if you want to make a reasonable prediction about what the outcome will be for a new observation that has characteristics <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>, a good way to do it would be to predict that their outcome would be the same as the mean outcome in the population among those that have the same characteristics; that is, <span class="math inline">\(\mathbb{E}[Y|X_1=x_1, X_2=x_2, X_3=x_3]\)</span>.</p>
<p>Next, in economics, we are often interested in how much some outcome of interest changes when a particular covariate changes, holding other covariates constants. To give some examples, we might be interested in the average return of actively managed mutual funds relative to passively managed mutual funds conditional on investing in assets in the same class (e.g., large cap stocks or international bonds). As another example, we might be interested in the effect of an increase in the amount of fertilizer on average crop yield but while holding constant the temperature and precipitation.</p>
<p>How much the outcome, <span class="math inline">\(Y\)</span>, changes on average when one of the covariates, <span class="math inline">\(X_1\)</span>, changes by 1 unit and holding other covariates constant is what we’ll call the <strong>partial effect</strong> of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>. Suppose <span class="math inline">\(X_1\)</span> is binary, then it is given by</p>
<p><span class="math display">\[
  PE(x_2,x_3) = \mathbb{E}[Y | X_1=1, X_2=x_2, X_3=x_3] - \mathbb{E}[Y | X_1=0,X_2=x_2,X_3=x_3]
\]</span>
Notice that the partial effect can depend on <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>. For example, it could be that the effect of active management relative to passive management could be different across different asset classes.</p>
<p>Slightly more generally, if <span class="math inline">\(X_1\)</span> is discrete, so that it can take on several different discrete values, then we define the partial effect as</p>
<p><span class="math display">\[
  PE(x_1,x_2,x_3) = \mathbb{E}[Y | X_1=x_1+1, X_2=x_2, X_3=x_3] - \mathbb{E}[Y | X_1=x_1,X_2=x_2,X_3=x_3]
\]</span>
which now can depend on <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>. This is the average effect of going from <span class="math inline">\(X_1=x_1\)</span> to <span class="math inline">\(X_1=x_1+1\)</span> holding <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> constant.</p>
<p>Finally, consider the case where we are interested in the partial effect of <span class="math inline">\(X_1\)</span> which is continuous (for example, the partial effect of fertilizer input on crop yield). In this case the partial effect is given by the <em>partial derivative</em> of <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span> with respect to <span class="math inline">\(X_1\)</span>.</p>
<p><span class="math display">\[
  PE(x_1,x_2,x_3) = \frac{\partial \, \mathbb{E}[Y|X_1=x_1, X_2=x_2, X_3=x_3]}{\partial \, x_1}
\]</span>
This partial derivative is analogous to what we have been doing before — we are making a small change of <span class="math inline">\(X_1\)</span> while holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant at <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> This is probably the part of the class where we will jump around in the book the most this semester.</p>
<p>The pedagogical approach of the textbook is to introduce the notion of causality very early and to emphasize the requirements on linear regression models in order to deliver causality, while increasing the complexity of the models over several chapters.</p>
<p>This is totally reasonable, but I prefer to start by teaching the mechanics of regressions: how to compute them, how to interpret them (even if you are not able to meet the requirements of causality), and how to use them to make predictions. Then, we’ll have a serious discussion about causality over the last few weeks of the semester.</p>
<p>In practice, this means we’ll cover parts Chapters 4-8 in the textbook now, and then we’ll circle back to some of the issues covered in these chapters again towards the end of the semester.</p>
</div>
<div id="nonparametric-regression-curse-of-dimensionality" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Nonparametric Regression / Curse of Dimensionality<a href="#nonparametric-regression-curse-of-dimensionality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you knew nothing about regressions, it would seem natural to try to estimate <span class="math inline">\(\mathbb{E}[Y|X_1=x_1,X_2=x_2,X_3=x_3]\)</span> by just calculating the average of <span class="math inline">\(Y\)</span> among observations that have values of the regressors equal to <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> (if these are discrete) or that are, in some sense, close to <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> (if these are continuous).</p>
<p>This is actually a pretty attractive idea.</p>
<p>However, you run into the issue that it is practically challenging to do this when the number of regressors starts to get large (i.e., if you have 10 regressors, generally, you would need tons of data to be able to find a suitable number of observations that are “close” to any particular value of the regressors).</p>
<p>Let me give a more concrete example. Suppose that you were trying to estimate mean house price as a function of a house’s characteristics. If the only characteristic of the house that you knew was the number of bedrooms, then it would be pretty easy to just calculate the average house price among houses with 2, 3, 4, etc. bedrooms. Now suppose that you knew both the number of bedrooms and the number of square feet. In this case, if we wanted to estimate mean house prices as a function of these characteristics, we would need to find houses that have the same number of bedrooms and (at least) a similar number of square feet. This starts to “slice” the data that you have more thinly. If you continue with this idea (suppose that you want to estimate mean house price as a function of number of bedrooms, number of bathrooms, number of square feet, what year the house was built in, whether or not it has a basement, what zip code it is located in, etc.) then you will start to stretch your data extremely thin to the point that you may have very few relevant observations (or perhaps no relevant observations) for particular values of the characteristics.</p>
<p>This issue is called the “curse of dimensionality”.</p>
<p>We will focus on linear models for <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span> largely to get around the curse of dimensionality.</p>
<div class="side-comment">
<p>This idea of using observations that are very close in terms of characteristics in order to estimate a conditional expectation is called <strong>nonparametric</strong> econometrics/statistics. You can take entire courses (typically graduate-level) on this topic if you were interested. The reason that it is is called nonparametric is that it doesn’t involve making any functional form assumptions (like linearity) but the cost is that it would typically require many more observations (due to the curse of dimensionality).</p>
</div>
</div>
<div id="linear-regression-models" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Linear Regression Models<a href="#linear-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 4.1</p>
<p>In order to get around the curse of dimensionality that we discussed in the previous section, we will often an impose a <strong>linear model</strong> for the conditional expectation. For example,</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X] = \beta_0 + \beta_1 X
\]</span>
or</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span>
If we know the values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span>, then it is straighforward for us to make predictions. In particular, suppose that we want to predict the outcome for a new observation with characteristics <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>. Our prediction would be</p>
<p><span class="math display">\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 5.1  </strong></span>Suppose that you are studying intergenerational income mobility and that you are interested in predicting a child’s income whose parents’ income was $50,000 and whose mother had 12 years of education. Let <span class="math inline">\(Y\)</span> denote child’s income, <span class="math inline">\(X_1\)</span> denote parents’ income, and <span class="math inline">\(X_2\)</span> denote mother’s education. Further, suppose that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = 20,000 + 0.5 X_1 + 1000 X_2\)</span>.</p>
<p>In this case, you would predict child’s income to be</p>
<p><span class="math display">\[
  20,000 + 0.5 (50,000) + 1000(12) = 57,000
\]</span></p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span></p>
<p>The above model can be equivalently written as
<span class="math display">\[\begin{align*}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\end{align*}\]</span>
where <span class="math inline">\(U\)</span> is called the <strong>error term</strong> and satisfies <span class="math inline">\(\mathbb{E}[U|X_1,X_2,X_3] = 0\)</span>. There will be a few times where this formulation will be useful for us.</p>
</div>
</div>
<div id="computation" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Computation<a href="#computation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Even if we know that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3\)</span>, in general, we do not know the values of the population parameters (the <span class="math inline">\(\beta\)</span>’s). This is analogous to the framework in the previous chapter where we were interested in
the population parameter <span class="math inline">\(\mathbb{E}[Y]\)</span> and estimated it by <span class="math inline">\(\bar{Y}\)</span>.</p>
<p>In this section, we’ll discuss how to estimate <span class="math inline">\((\beta_0,\beta_1,\beta_2,\beta_3)\)</span> using <code>R</code>. We’ll refer to the estimated values of the parameters as <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3)\)</span>. As in the previous section, it will not be the case that the estimated <span class="math inline">\(\hat{\beta}\)</span>’s are exactly equal to the population <span class="math inline">\(\beta\)</span>’s. Later on in this chapter, we will establish properties like consistency (so that, as long as we have a large sample, the estimated <span class="math inline">\(\hat{\beta}\)</span>’s should be “close” to the population <span class="math inline">\(\beta\)</span>’s) and asymptotic normality (so that we can conduct inference).</p>
<p>Also later on in this chapter, we’ll talk about how <code>R</code> itself actually makes these computations.</p>
<p>The main function in <code>R</code> for estimating linear regressions is the <code>lm</code> function (<code>lm</code> stands for linear model). The key things to specify for running a regression in <code>R</code> are a <code>formula</code> argument which tells <code>lm</code> which variables are the outcome and which variables are the regressors and a <code>data</code> argument which tells the <code>lm</code> command what data we are using to estimate the regression. Let’s give an example using the <code>mtcars</code> data.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data=</span>mtcars)</span></code></pre></div>
<p>What this line of code does is to run a regression. The formula is <code>mpg ~ hp + wt</code>. In other words <code>mpg</code> (standing for miles per gallon) is the outcome, and we are running a regression on <code>hp</code> (horse power) and <code>wt</code> (weight). The <code>~</code> symbol is a “tilde”. In order to add regressors, we separate them with a <code>+</code>. The second argument <code>data=mtcars</code> says to use the <code>mtcars</code> data. All of the variables in the formula need to correspond to column names in the data. We saved the results of the regression in a variable called <code>reg</code>. It’s most common to report the results of the regression using the <code>summary</code> command.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" tabindex="-1"></a><span class="fu">summary</span>(reg)</span>
<span id="cb114-2"><a href="#cb114-2" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-3"><a href="#cb114-3" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb114-4"><a href="#cb114-4" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt, data = mtcars)</span></span>
<span id="cb114-5"><a href="#cb114-5" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-6"><a href="#cb114-6" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb114-7"><a href="#cb114-7" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb114-8"><a href="#cb114-8" tabindex="-1"></a><span class="co">#&gt; -3.941 -1.600 -0.182  1.050  5.854 </span></span>
<span id="cb114-9"><a href="#cb114-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-10"><a href="#cb114-10" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb114-11"><a href="#cb114-11" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb114-12"><a href="#cb114-12" tabindex="-1"></a><span class="co">#&gt; (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***</span></span>
<span id="cb114-13"><a href="#cb114-13" tabindex="-1"></a><span class="co">#&gt; hp          -0.03177    0.00903  -3.519  0.00145 ** </span></span>
<span id="cb114-14"><a href="#cb114-14" tabindex="-1"></a><span class="co">#&gt; wt          -3.87783    0.63273  -6.129 1.12e-06 ***</span></span>
<span id="cb114-15"><a href="#cb114-15" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb114-16"><a href="#cb114-16" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb114-17"><a href="#cb114-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-18"><a href="#cb114-18" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.593 on 29 degrees of freedom</span></span>
<span id="cb114-19"><a href="#cb114-19" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8268, Adjusted R-squared:  0.8148 </span></span>
<span id="cb114-20"><a href="#cb114-20" tabindex="-1"></a><span class="co">#&gt; F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12</span></span></code></pre></div>
<p>The main thing that this reports is the estimated parameters. Our estimate of the “Intercept” (i.e., this is <span class="math inline">\(\hat{\beta}_0\)</span>) is in the first row of the table; our estimate is <code>37.227</code>. The estimated coefficient on <code>hp</code> is <code>-0.0318</code>, and the estimated coefficient on <code>wt</code> is <code>-3.878</code>.</p>
<p>You can also see standard errors for each estimated parameter, a t-statistic, and a p-value in the other columns. We will talk about these in more detail in the next section.</p>
<p>For now, we’ll also ignore the information provided at the bottom of the summary.</p>
<p>Now that we have estimated the parameters, we can use these to predict <span class="math inline">\(mpg\)</span> given a value of <span class="math inline">\(hp\)</span> and <span class="math inline">\(wt\)</span>. For example, suppose that you wanted to predict the <span class="math inline">\(mpg\)</span> of a 2500 pound car (note: weight in <span class="math inline">\(mtcars\)</span> is in 1000s of pounds) and 120 horsepower car, you could compute</p>
<p><span class="math display">\[
  37.227 - 0.0318(120) - 3.878(2.5) = 23.716
\]</span>
Alternatively, there is a built-in function in <code>R</code> called <code>predict</code> that can be used to generate predicted values. We just need to specify the values that we would like to get predicted values for by passing in a data frame with the relevant columns though the <code>newdata</code> argument. For example,</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(reg, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">hp=</span><span class="dv">120</span>,<span class="at">wt=</span><span class="fl">2.5</span>))</span>
<span id="cb115-2"><a href="#cb115-2" tabindex="-1"></a><span class="fu">round</span>(pred,<span class="dv">3</span>)</span>
<span id="cb115-3"><a href="#cb115-3" tabindex="-1"></a><span class="co">#&gt;     1 </span></span>
<span id="cb115-4"><a href="#cb115-4" tabindex="-1"></a><span class="co">#&gt; 23.72</span></span></code></pre></div>
<div class="side-comment">
<p>A popular alternative to <code>R</code>’s <code>lm</code> function is the <code>lm_robust</code> function from the <code>estimatr</code> package. This provides different standard errors from the default standard errors provided by <code>lm</code> that are, at least in most applications in economics, typically a better choice — we’ll have a further discussion on this topic when we talk about inference later on in this chapter.</p>
</div>
</div>
<div id="partial-effects" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Partial Effects<a href="#partial-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we discussed in the beginning of this chapter, besides predicting outcomes, a second main goal for us is to think about partial effects of a regressor on the outcome. We’ll consider partial effects over the next few sections.</p>
<p>In the model,
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y | X_1, X_2, X_3]  &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\end{align*}\]</span></p>
<p>If <span class="math inline">\(X_1\)</span> is continuous, then
<span class="math display">\[\begin{align*}
  \beta_1 = \frac{\partial \mathbb{E}[Y|X_1,X_2,X_3]}{\partial X_1}
\end{align*}\]</span></p>
<p>Thus, <span class="math inline">\(\beta_1\)</span> is the partial effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>. In other words, <span class="math inline">\(\beta_1\)</span> should be interpreted as how much <span class="math inline">\(Y\)</span> increases, on average, when <span class="math inline">\(X_1\)</span> increases by one unit holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant. <em>Make sure to get this interpretation right!</em></p>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 5.2  </strong></span>Continuing the same example as above about intergenerational income mobility and where <span class="math inline">\(Y\)</span> denotes child’s income, <span class="math inline">\(X_1\)</span> denotes parents’ income, <span class="math inline">\(X_2\)</span> denotes mother’s education, and</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2] = 20,000 + 0.5 X_1 + 1000 X_2
\]</span>
The partial effect of parents’ income on child’s income is 0.5. This means that, for every one dollar increase in parents’ income, child’s income is 0.5 dollars higher on average holding mother’s education constant.</p>
</div>
<div id="computation-1" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Computation<a href="#computation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s run the same regression as in the previous section, but think about partial effects in this case.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" tabindex="-1"></a>reg1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data=</span>mtcars)</span>
<span id="cb116-2"><a href="#cb116-2" tabindex="-1"></a><span class="fu">summary</span>(reg1)</span>
<span id="cb116-3"><a href="#cb116-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb116-4"><a href="#cb116-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb116-5"><a href="#cb116-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt, data = mtcars)</span></span>
<span id="cb116-6"><a href="#cb116-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb116-7"><a href="#cb116-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb116-8"><a href="#cb116-8" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb116-9"><a href="#cb116-9" tabindex="-1"></a><span class="co">#&gt; -3.941 -1.600 -0.182  1.050  5.854 </span></span>
<span id="cb116-10"><a href="#cb116-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb116-11"><a href="#cb116-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb116-12"><a href="#cb116-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb116-13"><a href="#cb116-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***</span></span>
<span id="cb116-14"><a href="#cb116-14" tabindex="-1"></a><span class="co">#&gt; hp          -0.03177    0.00903  -3.519  0.00145 ** </span></span>
<span id="cb116-15"><a href="#cb116-15" tabindex="-1"></a><span class="co">#&gt; wt          -3.87783    0.63273  -6.129 1.12e-06 ***</span></span>
<span id="cb116-16"><a href="#cb116-16" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb116-17"><a href="#cb116-17" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb116-18"><a href="#cb116-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb116-19"><a href="#cb116-19" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.593 on 29 degrees of freedom</span></span>
<span id="cb116-20"><a href="#cb116-20" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8268, Adjusted R-squared:  0.8148 </span></span>
<span id="cb116-21"><a href="#cb116-21" tabindex="-1"></a><span class="co">#&gt; F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12</span></span></code></pre></div>
<p>The partial effect of horsepower on miles per gallon is -0.032. In other words, we estimate that if horsepower increases by one then, on average, miles per gallon decreases by 0.032 holding weight constant.</p>
<p>The t-statistic and p-value are computed for the null hypothesis that the corresponding coefficient is equal to 0. For example, for <code>hp</code> the t-statistic is equal to <code>-3.519</code> which is greater than 1.96 and indicates that the partial effect of <code>hp</code> is statistically significant at a 5% significance level. The corresponding p-value for <code>hp</code> is <code>0.0145</code> indicating that there is only about a 1.5% chance of getting a t-statistic this extreme if the partial effect of <code>hp</code> were actually 0 (i.e., under <span class="math inline">\(H_0 : \beta_1=0\)</span>).</p>
<div class="practice">
<p><span class="practice">Practice: </span>What is the partial effect of <code>wt</code> in the previous example? Provide a careful interpretation. Is the partial effect of <code>wt</code> statistically significant? Explain. What is the p-value for <code>wt</code>? How do you interpret the p-value?</p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> One horsepower is a very small increase in horsepower, so it might be a good idea to multiply the coefficient by some larger number, say 50. In this case, we could say that we estimate that if horsepower increases by 50 then, on average, miles per gallon decreases by 1.59 (<span class="math inline">\(=50 \times 0.03177\)</span>) holding weight constant. From the above discussion, we know that this effect is statistically different from 0. That said, it is not clear to me if we should interpret this as a large partial effect; I do not know too much about cars, but a 50 horsepower increase seems rather large while a 1.59 decrease in miles per gallon seems relatively small (at least to me).</p>
</div>
</div>
</div>
<div id="binary-regressors" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Binary Regressors<a href="#binary-regressors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 5.3</p>
<p>Let’s continue with the same model as above</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span></p>
<p>If <span class="math inline">\(X_1\)</span> is discrete (let’s say binary):
<span class="math display">\[\begin{align*}
  \beta_1 = \mathbb{E}[Y|X_1=1,X_2,X_3] - \mathbb{E}[Y|X_1=0,X_2,X_3]
\end{align*}\]</span>
<span class="math inline">\(\beta_1\)</span> is still the partial effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> and should be interpreted as how much <span class="math inline">\(Y\)</span> increases, on average, when <span class="math inline">\(X_1\)</span> changes from 0 to 1, holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant.</p>
<p>If <span class="math inline">\(X_1\)</span> can take more than just the values 0 and 1, but is still discrete (an example is a person’s years of education), then</p>
<p><span class="math display">\[
  \beta_1 = \mathbb{E}[Y | X_1=x_1+1, X_2, X_3] - \mathbb{E}[Y|X_1=x_1, X_2, X_3]
\]</span>
which holds for any possible value that <span class="math inline">\(X_1\)</span> could take, so that <span class="math inline">\(\beta_1\)</span> is the effect of a 1 unit increase in <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>, on average, holding constant <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-19" class="example"><strong>Example 5.3  </strong></span>Suppose that you work for an airline and you are interested in predicting the number of passengers for a Saturday morning flight from Atlanta to Memphis. Let <span class="math inline">\(Y\)</span> denote the number of passengers, <span class="math inline">\(X_1\)</span> be equal to 1 for a morning flight and 0 otherwise, and let <span class="math inline">\(X_2\)</span> be equal to 1 for a weekday flight and 0 otherwise. Further suppose that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = 80 + 20 X_1 - 15 X_2\)</span>.</p>
<p>In this case, you would predict,</p>
<p><span class="math display">\[
  80 + 20 (1) - 15 (0) = 100
\]</span>
passengers on the flight.</p>
<p>In addition, the partial effect of being morning flight is equal to 20. This indicates that, on average, morning flights have 20 more passengers than non-morning flights holding whether or not the flight occurs on a weekday constant.</p>
</div>
<div id="computation-2" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Computation<a href="#computation-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to include a binary or discrete covariate in a regression in <code>R</code> is straightforward. The following regression uses the <code>mtcars</code> data and adds a binary regressor, <code>am</code>, indicating whether or not a car has an automatic transmission.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" tabindex="-1"></a>reg2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt <span class="sc">+</span> am, <span class="at">data=</span>mtcars)</span>
<span id="cb117-2"><a href="#cb117-2" tabindex="-1"></a><span class="fu">summary</span>(reg2)</span>
<span id="cb117-3"><a href="#cb117-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-4"><a href="#cb117-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb117-5"><a href="#cb117-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt + am, data = mtcars)</span></span>
<span id="cb117-6"><a href="#cb117-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-7"><a href="#cb117-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb117-8"><a href="#cb117-8" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb117-9"><a href="#cb117-9" tabindex="-1"></a><span class="co">#&gt; -3.4221 -1.7924 -0.3788  1.2249  5.5317 </span></span>
<span id="cb117-10"><a href="#cb117-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-11"><a href="#cb117-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb117-12"><a href="#cb117-12" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb117-13"><a href="#cb117-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 34.002875   2.642659  12.867 2.82e-13 ***</span></span>
<span id="cb117-14"><a href="#cb117-14" tabindex="-1"></a><span class="co">#&gt; hp          -0.037479   0.009605  -3.902 0.000546 ***</span></span>
<span id="cb117-15"><a href="#cb117-15" tabindex="-1"></a><span class="co">#&gt; wt          -2.878575   0.904971  -3.181 0.003574 ** </span></span>
<span id="cb117-16"><a href="#cb117-16" tabindex="-1"></a><span class="co">#&gt; am           2.083710   1.376420   1.514 0.141268    </span></span>
<span id="cb117-17"><a href="#cb117-17" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb117-18"><a href="#cb117-18" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb117-19"><a href="#cb117-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-20"><a href="#cb117-20" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.538 on 28 degrees of freedom</span></span>
<span id="cb117-21"><a href="#cb117-21" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8399, Adjusted R-squared:  0.8227 </span></span>
<span id="cb117-22"><a href="#cb117-22" tabindex="-1"></a><span class="co">#&gt; F-statistic: 48.96 on 3 and 28 DF,  p-value: 2.908e-11</span></span></code></pre></div>
<p>In this example, cars that had an automatic transmission got about 2 more miles per gallon than cars that had an automatic transmission on average, holding horsepower and weight constant (though the p-value is only 0.14).</p>
</div>
</div>
<div id="nonlinear-regression-functions" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Nonlinear Regression Functions<a href="#nonlinear-regression-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 8.1, 8.2</p>
<p>Also, please read all of SW Ch. 8</p>
<p>So far, the the partial effects that we have been interested in have corresponded to a particular parameter in the regression, usually <span class="math inline">\(\beta_1\)</span>. I think this can sometimes be a source of confusion as, at least in my view, we are not typically interested in the parameters for their own sake, but rather are interested in partial effects. It just so happens that in some leading cases, they coincide.</p>
<p>In addition, while the <span class="math inline">\(\beta\)</span>’s in the sort of models we have considered so far are easy to interpret, in some cases, it might be <em>restrictive</em> to think that the partial effects are the same across different values of the covariates.</p>
<p>In this section, we’ll see the first of several cases where partial effects do not coincide with a particular parameter.</p>
<p>Suppose that</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 + \beta_4 X_3
\]</span></p>
<p>Let’s start with making predictions using this model. If you know the values of <span class="math inline">\(\beta_0,\beta_1,\beta_2,\beta_3,\)</span> and <span class="math inline">\(\beta_4\)</span>, then to get a prediction, you would still just plug in the values of the regressors that you’d like to get a prediction for (including <span class="math inline">\(x_1^2\)</span>).</p>
<p>Next, in this model, the partial effect of <span class="math inline">\(X_1\)</span> is given by</p>
<p><span class="math display">\[
  \frac{\partial \, \mathbb{E}[Y|X_1,X_2,X_3]}{\partial \, X_1} = \beta_1 + 2\beta_2 X_1
\]</span>
In other words, the partial effect of <span class="math inline">\(X_1\)</span> depends on the value that <span class="math inline">\(X_1\)</span> takes.</p>
<p>In this case, it is sometimes useful to report the partial effect for some different values of <span class="math inline">\(X_1\)</span>. In other cases, it is useful to report the <strong>average partial effect</strong> (APE) which is the mean of the partial effects across the distribution of the covariates. In this case, the APE is given by</p>
<p><span class="math display">\[
  APE = \beta_1 + 2 \beta_2 \mathbb{E}[X_1]
\]</span>
and, once you have estimated the regression, you can compute an estimate of <span class="math inline">\(APE\)</span> by</p>
<p><span class="math display">\[
  \widehat{APE} = \hat{\beta}_1 + 2 \hat{\beta}_2 \bar{X}_1
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-20" class="example"><strong>Example 5.4  </strong></span>Let’s continue our example on intergenerational income mobility where <span class="math inline">\(Y\)</span> denotes child’s income, <span class="math inline">\(X_1\)</span> denotes parents’ income, and <span class="math inline">\(X_2\)</span> denotes mother’s education. Now, suppose that</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2] = 15,000 + 0.7 X_1 - 0.000002 X_1^2 + 800 X_2
\]</span>
Then, predicted child’s income when parents’ income is equal to $50,000 is given by</p>
<p><span class="math display">\[
  15,000 + 0.7 (50,000) - 0.000002 (50,000)^2 + 800 (12) = 54,600
\]</span>
In addition, the partial effect of parents’ income is given by</p>
<p><span class="math display">\[
  0.7 - 0.000004 X_1
\]</span>
Let’s compute a few different partial effects for different values of parents’ income</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(X_1\)</span></th>
<th align="center">PE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">20,000</td>
<td align="center">0.62</td>
</tr>
<tr class="even">
<td align="center">50,000</td>
<td align="center">0.50</td>
</tr>
<tr class="odd">
<td align="center">100,000</td>
<td align="center">0.30</td>
</tr>
</tbody>
</table>
<p>which indicates that the partial effect of parents’ income is decreasing — i.e., the effect of additional parents’ income is largest for children whose parents have the lowest income and gets smaller for those whose parents have high incomes.</p>
<p>Finally, if you wanted to compute the <span class="math inline">\(APE\)</span>, you would just plug in <span class="math inline">\(\mathbb{E}[X_1]\)</span> (or <span class="math inline">\(\bar{X}_1\)</span>) into the expression for the partial effect.</p>
</div>
<div id="computation-3" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Computation<a href="#computation-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Including a quadratic (or other higher order term) in <code>R</code> is relatively straightforward. Let’s just do an example.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" tabindex="-1"></a>reg3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> <span class="fu">I</span>(hp<span class="sc">^</span><span class="dv">2</span>), <span class="at">data=</span>mtcars)</span>
<span id="cb118-2"><a href="#cb118-2" tabindex="-1"></a><span class="fu">summary</span>(reg3)</span>
<span id="cb118-3"><a href="#cb118-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb118-4"><a href="#cb118-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb118-5"><a href="#cb118-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + I(hp^2), data = mtcars)</span></span>
<span id="cb118-6"><a href="#cb118-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb118-7"><a href="#cb118-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb118-8"><a href="#cb118-8" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb118-9"><a href="#cb118-9" tabindex="-1"></a><span class="co">#&gt; -4.5512 -1.6027 -0.6977  1.5509  8.7213 </span></span>
<span id="cb118-10"><a href="#cb118-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb118-11"><a href="#cb118-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb118-12"><a href="#cb118-12" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb118-13"><a href="#cb118-13" tabindex="-1"></a><span class="co">#&gt; (Intercept)  4.041e+01  2.741e+00  14.744 5.23e-15 ***</span></span>
<span id="cb118-14"><a href="#cb118-14" tabindex="-1"></a><span class="co">#&gt; hp          -2.133e-01  3.488e-02  -6.115 1.16e-06 ***</span></span>
<span id="cb118-15"><a href="#cb118-15" tabindex="-1"></a><span class="co">#&gt; I(hp^2)      4.208e-04  9.844e-05   4.275 0.000189 ***</span></span>
<span id="cb118-16"><a href="#cb118-16" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb118-17"><a href="#cb118-17" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb118-18"><a href="#cb118-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb118-19"><a href="#cb118-19" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.077 on 29 degrees of freedom</span></span>
<span id="cb118-20"><a href="#cb118-20" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7561, Adjusted R-squared:  0.7393 </span></span>
<span id="cb118-21"><a href="#cb118-21" tabindex="-1"></a><span class="co">#&gt; F-statistic: 44.95 on 2 and 29 DF,  p-value: 1.301e-09</span></span></code></pre></div>
<p>The only thing that is new here is <code>I(hp^2)</code>. The <code>I</code> function stands for inhibit (you can read the documentation using <code>?I</code>). For us, this is not too important. You can understand it like this: there is no variable names <code>hp^2</code> in the data, but if we put the name of a variable that is in the data (here: <code>hp</code>) then we can apply a function to it (here: squaring it) before including it as a regressor.</p>
<p>Interestingly, here it seems there are nonlinear effects of horsepower on miles per gallon. Let’s just quickly report the estimated partial effects for a few different values of horsepower.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" tabindex="-1"></a>hp_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">200</span>,<span class="dv">300</span>)</span>
<span id="cb119-2"><a href="#cb119-2" tabindex="-1"></a><span class="co"># there might be a native function in r</span></span>
<span id="cb119-3"><a href="#cb119-3" tabindex="-1"></a><span class="co"># to compute these partial effects; I just</span></span>
<span id="cb119-4"><a href="#cb119-4" tabindex="-1"></a><span class="co"># don&#39;t know it.</span></span>
<span id="cb119-5"><a href="#cb119-5" tabindex="-1"></a>pe <span class="ot">&lt;-</span> <span class="cf">function</span>(hp) {</span>
<span id="cb119-6"><a href="#cb119-6" tabindex="-1"></a>  <span class="co"># partial effect is b1 + 2b2*hp</span></span>
<span id="cb119-7"><a href="#cb119-7" tabindex="-1"></a>  pes <span class="ot">&lt;-</span> <span class="fu">coef</span>(reg3)[<span class="dv">2</span>] <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">coef</span>(reg3)[<span class="dv">3</span>]<span class="sc">*</span>hp</span>
<span id="cb119-8"><a href="#cb119-8" tabindex="-1"></a>  <span class="co"># print using a data frame</span></span>
<span id="cb119-9"><a href="#cb119-9" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">hp=</span>hp, <span class="at">pe=</span><span class="fu">round</span>(pes,<span class="dv">3</span>))</span>
<span id="cb119-10"><a href="#cb119-10" tabindex="-1"></a>}</span>
<span id="cb119-11"><a href="#cb119-11" tabindex="-1"></a><span class="fu">pe</span>(hp_vec)</span>
<span id="cb119-12"><a href="#cb119-12" tabindex="-1"></a><span class="co">#&gt;    hp     pe</span></span>
<span id="cb119-13"><a href="#cb119-13" tabindex="-1"></a><span class="co">#&gt; 1 100 -0.129</span></span>
<span id="cb119-14"><a href="#cb119-14" tabindex="-1"></a><span class="co">#&gt; 2 200 -0.045</span></span>
<span id="cb119-15"><a href="#cb119-15" tabindex="-1"></a><span class="co">#&gt; 3 300  0.039</span></span></code></pre></div>
<p>which suggests that the partial effect of horsepower on miles per gallon is large (though negative) at small values of horsepower and decreasing up to essentially no effect at larger values of horsepower.</p>
</div>
</div>
<div id="interpreting-interaction-terms" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Interpreting Interaction Terms<a href="#interpreting-interaction-terms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 8.3</p>
<p>Another way to allow for partial effects that vary across different values of the regressors is to include <strong>interaction terms</strong>.</p>
<p>Consider the following regression model</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \beta_4 X_3
\]</span></p>
<p>The term <span class="math inline">\(X_1 X_2\)</span> is called the interaction term. In this model, the partial effect of <span class="math inline">\(X_1\)</span> is given by</p>
<p><span class="math display">\[
  \frac{\partial \, \mathbb{E}[Y|X_1,X_2,X_3]}{\partial \, X_1} = \beta_1 + \beta_3 X_2
\]</span></p>
<p>In this model, the effect of <span class="math inline">\(X_1\)</span> varies with <span class="math inline">\(X_2\)</span>. As in the previous section, you could report the partial effect for different values of <span class="math inline">\(X_2\)</span> or consider <span class="math inline">\(APE = \beta_1 + \beta_3 \mathbb{E}[X_2]\)</span>.</p>
<p>There are a couple of other things worth pointing out for interaction terms</p>
<ul>
<li><p>It is very common for one of the interaction terms, say, <span class="math inline">\(X_2\)</span> to be a binary variable. This gives a way to easily test if the effect of <span class="math inline">\(X_1\)</span> is the same across the two “groups” defined by <span class="math inline">\(X_2\)</span>. For example, suppose you wanted to check if the partial effect of education was the same for men and women. You could run a regression like</p>
<p><span class="math display">\[
    Wage = \beta_0 + \beta_1 Education + \beta_2 Female + \beta_3 Education \cdot Female + U
  \]</span></p>
<p>From the previous discussion, the partial effect of education is given by</p>
<p><span class="math display">\[
    \beta_1 + \beta_3 Female
  \]</span></p>
<p>Thus, the partial effect education for men is given by <span class="math inline">\(\beta_1\)</span>, and the partial effect of education for women is given by <span class="math inline">\(\beta_1 + \beta_3\)</span>. Thus, if you want to test if the partial effect of education differs for men and women, you can just test if <span class="math inline">\(\beta_3=0\)</span>. If <span class="math inline">\(\beta_3&gt;0\)</span>, it suggests a higher partial effect of education for women, and if <span class="math inline">\(\beta_3 &lt; 0\)</span>, it suggests a lower partial effect of education for women.</p></li>
<li><p>Another interesting case is when <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are both binary. In this case, a model that includes an interaction term is called a <strong>saturated model</strong>. It is called this because it is actually nonparametric. In particular, notice that in the model <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>,</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[Y|X_1=0,X_2=0] &amp;= \beta_0 \\
    \mathbb{E}[Y|X_1=1,X_2=0] &amp;= \beta_0 + \beta_1 \\
    \mathbb{E}[Y|X_1=0,X_2=1] &amp;= \beta_0 + \beta_2 \\
    \mathbb{E}[Y|X_1=1,X_2=1] &amp;= \beta_0 + \beta_1 + \beta_2 + \beta_3
  \end{aligned}
  \]</span></p>
<p>This exhausts all possible combinations of the regressors and means that you can recover each possible value of the conditional expectation from the parameters of the model.</p>
<p>It would be possible to write down a saturated model in cases with more than two binary regressors (or even discrete regressors) — you would just need to include more interaction terms. The key thing is that there be no continuous regressors. That said, as you start to add more and more discrete regressors and their interactions, you will effectively start to run into the curse of dimensionality issues that we discussed earlier.</p>
<p>As an example, consider our earlier example of flights from Atlanta to Memphis where <span class="math inline">\(Y\)</span> denoted the number of passengers, <span class="math inline">\(X_1\)</span> was equal to 1 for a a morning flight and 0 otherwise, and <span class="math inline">\(X_2\)</span> was equal to one for a weekday flight and 0 otherwise. Suppose that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = 90 - 15 X_1 - 5 X_2 + 25 X_1 X_2\)</span>. Then,</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[Y|X_1=0,X_2=0] &amp;= 90 \quad &amp; \textrm{non-morning, weekend} \\
    \mathbb{E}[Y|X_1=1,X_2=0] &amp;= 90 - 15 = 75 \quad &amp; \textrm{morning, weekend} \\
    \mathbb{E}[Y|X_1=0,X_2=1] &amp;= 90 - 5 = 85 \quad  &amp; \textrm{non-morning, weekday} \\
    \mathbb{E}[Y|X_1=1,X_2=1] &amp;= 90 - 15 - 5 + 25 = 100 \quad &amp; \textrm{morning, weekend}
  \end{aligned}
  \]</span></p></li>
</ul>
<div id="computation-4" class="section level3 hasAnchor" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Computation<a href="#computation-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Including interaction terms in regressions in <code>R</code> is straightforward. Using the <code>mtcars</code> data, we can do it as follows</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" tabindex="-1"></a>reg4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt <span class="sc">+</span> am <span class="sc">+</span> hp<span class="sc">*</span>am, <span class="at">data=</span>mtcars)</span>
<span id="cb120-2"><a href="#cb120-2" tabindex="-1"></a><span class="fu">summary</span>(reg4)</span>
<span id="cb120-3"><a href="#cb120-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb120-4"><a href="#cb120-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb120-5"><a href="#cb120-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt + am + hp * am, data = mtcars)</span></span>
<span id="cb120-6"><a href="#cb120-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb120-7"><a href="#cb120-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb120-8"><a href="#cb120-8" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb120-9"><a href="#cb120-9" tabindex="-1"></a><span class="co">#&gt; -3.435 -1.510 -0.697  1.284  5.245 </span></span>
<span id="cb120-10"><a href="#cb120-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb120-11"><a href="#cb120-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb120-12"><a href="#cb120-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb120-13"><a href="#cb120-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 33.34196    2.79711  11.920 2.89e-12 ***</span></span>
<span id="cb120-14"><a href="#cb120-14" tabindex="-1"></a><span class="co">#&gt; hp          -0.02918    0.01449  -2.014  0.05407 .  </span></span>
<span id="cb120-15"><a href="#cb120-15" tabindex="-1"></a><span class="co">#&gt; wt          -3.05617    0.94036  -3.250  0.00309 ** </span></span>
<span id="cb120-16"><a href="#cb120-16" tabindex="-1"></a><span class="co">#&gt; am           3.55141    2.35742   1.506  0.14355    </span></span>
<span id="cb120-17"><a href="#cb120-17" tabindex="-1"></a><span class="co">#&gt; hp:am       -0.01129    0.01466  -0.770  0.44809    </span></span>
<span id="cb120-18"><a href="#cb120-18" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb120-19"><a href="#cb120-19" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb120-20"><a href="#cb120-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb120-21"><a href="#cb120-21" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.556 on 27 degrees of freedom</span></span>
<span id="cb120-22"><a href="#cb120-22" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8433, Adjusted R-squared:  0.8201 </span></span>
<span id="cb120-23"><a href="#cb120-23" tabindex="-1"></a><span class="co">#&gt; F-statistic: 36.33 on 4 and 27 DF,  p-value: 1.68e-10</span></span></code></pre></div>
<p>The interaction term in the results is in the row that starts with <code>hp:am</code>. These estimates suggest that, while horsepower does seem to decrease miles per gallon controlling for weight and whether or not the car has an automatic transmission, the effect of horsepower does not seem to vary much by whether or not the car has an automatic transmission (at least not in a big enough way that we can detect it with the data that we have).</p>
</div>
</div>
<div id="elasticities" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Elasticities<a href="#elasticities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 8.2</p>
<p>Economists are often interested in <strong>elasticities</strong>, that is, the percentage change in <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> changes by 1%.</p>
<p>Recall that the definition of percentage change of moving from, say, <span class="math inline">\(x_{old}\)</span> to <span class="math inline">\(x_{new}\)</span> is given by</p>
<p><span class="math display">\[
  \textrm{\% change} = \frac{x_{new} - x_{old}}{x_{old}} \times 100
\]</span></p>
<p>Elasticities are closely connected to natural logarithms; following the most common notation in economics, we’ll refer to the natural logarithm using the notation: <span class="math inline">\(\log\)</span>. Further, recall that the derivative of the <span class="math inline">\(\log\)</span> function is given by</p>
<p><span class="math display">\[
  \frac{d \, \log(x)}{d \, x} = \frac{1}{x} \implies d\, \log(x) = \frac{d \, x}{x}
\]</span>
which further implies that</p>
<p><span class="math display">\[
  \Delta \log(x) := \log(x_{new}) - \log(x_{old}) \approx \frac{x_{new} - x_{old}}{x_{old}}
\]</span>
and, thus, that</p>
<p><span class="math display">\[
  100 \cdot \Delta \log(x) \approx \textrm{\% change}
\]</span>
where the approximation is better when <span class="math inline">\(x_{new}\)</span> and <span class="math inline">\(x_{old}\)</span> are close to each other.</p>
<p>Now, we’ll use these properties of logarithms in order to interpret several linear models</p>
<ul>
<li><p>For simplicity, I am going to not include an error term or extra covariates, but you should continue to interpret parameter estimates as “on average” and “holding other regressors constant” (if there are other regressors in the model).</p></li>
<li><p><strong>Log-Log</strong> Model</p>
<p><span class="math display">\[
  \log(Y) = \beta_0 + \beta_1 \log(X)
  \]</span></p>
<p>In this case,</p>
<p><span class="math display">\[
  \begin{aligned}
  \beta_1 &amp;= \frac{ d \, \log(Y) }{d \, \log(X)} \\
  &amp;= \frac{ d \, \log(Y) \cdot 100 }{d \, \log(X) \cdot 100} \\
  &amp;\approx \frac{ \% \Delta Y}{ \% \Delta X}
  \end{aligned}
  \]</span></p>
<p>All that to say, in a regression of the log of an outcome on the log of a regressor, you should interpret the corresponding coefficient as the average percentage change in the outcome when the regressor changes by 1%. The log-log model is sometimes called a <strong>constant elasticity</strong> model.</p></li>
<li><p><strong>Log-Level</strong> model</p>
<p><span class="math display">\[
  \log(Y) = \beta_0 + \beta_1 X
  \]</span></p>
<p>In this case,</p>
<p><span class="math display">\[
  \begin{aligned}
  \beta_1 &amp;= \frac{ d \, \log(Y) }{d \, X} \\
  \implies 100 \beta_1 &amp;= \frac{ d \, \log(Y) \cdot 100 }{d \, X} \\
  \implies 100 \beta_1 &amp;\approx \frac{ \% \Delta Y}{ d \, X}
  \end{aligned}
  \]</span></p>
<p>Thus, in a regression of the log of an outcome on the <em>level</em> of a regressor, you should multiply the corresponding coefficient by 100 and interpret it as the average percentage change in the outcome when the regressor changes by 1 unit.</p></li>
<li><p><strong>Level-Log</strong> model</p>
<p><span class="math display">\[
    Y = \beta_0 + \beta_1 \log(X)
  \]</span></p>
<p>In this case,</p>
<p><span class="math display">\[
    \begin{aligned}
    \beta_1 &amp;= \frac{d\, Y}{d \, \log(X)} \\
    \implies \frac{\beta_1}{100} &amp;= \frac{d \, Y}{d \, \log(X) \cdot 100} \\
    \implies \frac{\beta_1}{100} &amp;\approx \frac{d \, Y}{\% \Delta X}
    \end{aligned}
  \]</span></p>
<p>Thus, in a regression of the level of an outcome on the log of a regressor, you should divide the corresponding coefficient by 100 and interpret it as the average change in the outcome when the regressor changes by 1%.</p></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-21" class="example"><strong>Example 5.5  </strong></span>Let’s continue the same example on intergenerational income mobility where <span class="math inline">\(Y\)</span> denotes child’s income, <span class="math inline">\(X_1\)</span> denotes parents’ income and <span class="math inline">\(X_2\)</span> denotes mother’s education. We’ll consider how to interpret several different models.</p>
<p><span class="math display">\[
  \log(Y) = 8.8 + 0.4 \log(X_1) + 0.008 X_2 + U
\]</span>
In this model, we estimate that, on average, when parents’ income increases by 1%, child’s income increases by 0.4% holding mother’s education constant.</p>
<p>Next, consider,
<span class="math display">\[
  \log(Y) = 8.9 + 0.00004 X_1 + 0.007 X_2 + U
\]</span>
In this model, we estimate that, on average, when parents’ income increases by $1, child’s income increases by 0.004% (alternatively, when parents’ income increase by $1000, child’s income increases by 4%) holding mother’s education constant.</p>
<p>Finally, consider</p>
<p><span class="math display">\[
  Y = -1,680,000 + 160,000 \log(X_1) + 900 X_2 + U
\]</span>
In this case, we estimate that, on average, when parents’ income increases by 1%, child’s income increases by $1,600 holding mother’s education constant.</p>
</div>
<div id="computation-5" class="section level3 hasAnchor" number="5.8.1">
<h3><span class="header-section-number">5.8.1</span> Computation<a href="#computation-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimating models that include logarithms in <code>R</code> is straightforward.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" tabindex="-1"></a>reg5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(mpg) <span class="sc">~</span> <span class="fu">log</span>(hp) <span class="sc">+</span> wt, <span class="at">data=</span>mtcars) </span>
<span id="cb121-2"><a href="#cb121-2" tabindex="-1"></a>reg6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(mpg) <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data=</span>mtcars) </span>
<span id="cb121-3"><a href="#cb121-3" tabindex="-1"></a>reg7 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">log</span>(hp) <span class="sc">+</span> wt, <span class="at">data=</span>mtcars)</span></code></pre></div>
<p>Let’s show the results all at once using the <code>modelsummary</code> function from the <code>modelsummary</code> package.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb122-2"><a href="#cb122-2" tabindex="-1"></a></span>
<span id="cb122-3"><a href="#cb122-3" tabindex="-1"></a>model_list <span class="ot">&lt;-</span> <span class="fu">list</span>(reg5, reg6, reg7)</span>
<span id="cb122-4"><a href="#cb122-4" tabindex="-1"></a><span class="fu">modelsummary</span>(model_list)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
 (1)
</th>
<th style="text-align:center;">
  (2)
</th>
<th style="text-align:center;">
  (3)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:center;">
4.832
</td>
<td style="text-align:center;">
3.829
</td>
<td style="text-align:center;">
59.571
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.222)
</td>
<td style="text-align:center;">
(0.069)
</td>
<td style="text-align:center;">
(4.977)
</td>
</tr>
<tr>
<td style="text-align:left;">
log(hp)
</td>
<td style="text-align:center;">
−0.266
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
−5.922
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.056)
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
(1.266)
</td>
</tr>
<tr>
<td style="text-align:left;">
wt
</td>
<td style="text-align:center;">
−0.179
</td>
<td style="text-align:center;">
−0.201
</td>
<td style="text-align:center;">
−3.286
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.027)
</td>
<td style="text-align:center;">
(0.027)
</td>
<td style="text-align:center;">
(0.615)
</td>
</tr>
<tr>
<td style="text-align:left;">
hp
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
−0.002
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;box-shadow: 0px 1.5px">
</td>
<td style="text-align:center;box-shadow: 0px 1.5px">
</td>
<td style="text-align:center;box-shadow: 0px 1.5px">
(0.000)
</td>
<td style="text-align:center;box-shadow: 0px 1.5px">
</td>
</tr>
<tr>
<td style="text-align:left;">
Num.Obs.
</td>
<td style="text-align:center;">
32
</td>
<td style="text-align:center;">
32
</td>
<td style="text-align:center;">
32
</td>
</tr>
<tr>
<td style="text-align:left;">
R2
</td>
<td style="text-align:center;">
0.885
</td>
<td style="text-align:center;">
0.869
</td>
<td style="text-align:center;">
0.859
</td>
</tr>
<tr>
<td style="text-align:left;">
R2 Adj.
</td>
<td style="text-align:center;">
0.877
</td>
<td style="text-align:center;">
0.860
</td>
<td style="text-align:center;">
0.849
</td>
</tr>
<tr>
<td style="text-align:left;">
AIC
</td>
<td style="text-align:center;">
−49.0
</td>
<td style="text-align:center;">
−44.8
</td>
<td style="text-align:center;">
150.0
</td>
</tr>
<tr>
<td style="text-align:left;">
BIC
</td>
<td style="text-align:center;">
−43.1
</td>
<td style="text-align:center;">
−38.9
</td>
<td style="text-align:center;">
155.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Log.Lik.
</td>
<td style="text-align:center;">
28.501
</td>
<td style="text-align:center;">
26.395
</td>
<td style="text-align:center;">
−71.017
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:center;">
111.812
</td>
<td style="text-align:center;">
96.232
</td>
<td style="text-align:center;">
88.442
</td>
</tr>
<tr>
<td style="text-align:left;">
RMSE
</td>
<td style="text-align:center;">
0.10
</td>
<td style="text-align:center;">
0.11
</td>
<td style="text-align:center;">
2.23
</td>
</tr>
</tbody>
</table>
<ul>
<li><p>In the first model, we estimate that, on average, a 1% increase in horsepower decreases miles per gallon by 0.266% holding weight constant.</p></li>
<li><p>In the second model, we estimate that, on average, a 1 unit increase in horsepower decreases miles per gallon by 0.2% holding weight constant.</p></li>
<li><p>In the third model, we estimate that, on average, a 1% increase in horsepower decreases miles per gallon by .059 holding weight constant.</p></li>
</ul>
</div>
</div>
<div id="omitted-variable-bias" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Omitted Variable Bias<a href="#omitted-variable-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 6.1</p>
<p>Suppose that we are interested in the following regression model</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1, X_2, Q] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 Q
\]</span>
and, in particular, we are interested in the the partial effect</p>
<p><span class="math display">\[
  \frac{ \partial \, \mathbb{E}[Y|X_1,X_2,Q]}{\partial \, X_1} = \beta_1
\]</span>
But we are faced with the issue that we do not observe <span class="math inline">\(Q\)</span> (which implies that we cannot control for it in the regression)</p>
<p>Recall that we can equivalently write</p>
<p><span class="math display" id="eq:ovb-y">\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 Q + U \tag{5.1}
\]</span>
where <span class="math inline">\(\mathbb{E}[U|X_1,X_2,Q]=0\)</span>.</p>
<p>Now, for simplicity, suppose that</p>
<p><span class="math display">\[
  \mathbb{E}[Q | X_1, X_2] = \gamma_0 + \gamma_1 X_1 + \gamma_2 X_2
\]</span></p>
<p>Now, let’s consider the idea of just running a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (and just not including <span class="math inline">\(Q\)</span>); in other words, consider the regression
<span class="math display">\[
  \mathbb{E}[Y|X_1,X_2] = \delta_0 + \delta_1 X_1 + \delta_2 X_2
\]</span>
We are interested in the question of whether or not we can recover <span class="math inline">\(\beta_1\)</span> if we do this. If we consider this “feasible” regression, notice if we plug in the expression for <span class="math inline">\(Y\)</span> from Equation <a href="#eq:ovb-y">(5.1)</a>,</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathbb{E}[Y|X_1,X_2] &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \mathbb{E}[Q|X_1,X_2] \\
  &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (\gamma_0 + \gamma_1 X_1 + \gamma_2 X_2) \\
  &amp;= \underbrace{(\beta_0 + \beta_3 \gamma_0)}_{\delta_0} + \underbrace{(\beta_1 + \beta_3 \gamma_1)}_{\delta_1} X_1 + \underbrace{(\beta_2 + \beta_3 \gamma_2)}_{\delta_2} X_2
  \end{aligned}
\]</span></p>
<p>In other words, if we run the feasible regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\delta_1\)</span> (the coefficient on <span class="math inline">\(X_1\)</span>) is not equal to <span class="math inline">\(\beta_1\)</span>; rather, it is equal to <span class="math inline">\((\beta_1 + \beta_3 \gamma_1)\)</span>.</p>
<p>That you are not generally able to recover <span class="math inline">\(\beta_1\)</span> in this case is called <strong>omitted variable bias</strong></p>
<p>There are two cases where you will recover <span class="math inline">\(\delta_1 = \beta_1\)</span> though which occur when <span class="math inline">\(\beta_3 \gamma_1 = 0\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\beta_3=0\)</span>. This would be the case where <span class="math inline">\(Q\)</span> has no effect on <span class="math inline">\(Y\)</span></p></li>
<li><p><span class="math inline">\(\gamma_1=0\)</span>. This would be the case where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Q\)</span> are uncorrelated after controlling for <span class="math inline">\(X_2\)</span>.</p></li>
</ul>
<p>Interestingly, there may be some case where you can “sign” the bias; i.e., figure out if <span class="math inline">\(\beta_3 \gamma_1\)</span> is positive or negative. For example, you might have theoretical reasons to suspect that <span class="math inline">\(\gamma_1 &gt; 0\)</span> and <span class="math inline">\(\beta_3 &gt; 0\)</span>. In this case,</p>
<p><span class="math display">\[
  \delta_1 = \beta_1 + \textrm{something positive}
\]</span>
which implies that <span class="math inline">\(\delta_1\)</span> (i.e., running a regression that ignores <span class="math inline">\(Q\)</span>) would cause us to tend to over-estimate <span class="math inline">\(\beta_1\)</span>.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span></p>
<ul>
<li><p>The book talks about omitted variable bias in the context of causality (this is probably the leading case), but we have not talked about causality yet. The same issues arise if we just say that we have some <em>regression of interest</em> but are unable to estimate it because some covariates are unobserved.</p></li>
<li><p>The relationship to causality (which is not so important for now), is that under certain conditions, we may have a particular partial effect that we would be willing to interpret as being the “causal effect”, but if we are unable to control for some variables that would lead to this interpretation, then we get to the issues pointed out in the textbook.</p></li>
</ul>
</div>
</div>
<div id="how-to-estimate-the-parameters-in-a-regression-model" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> How to estimate the parameters in a regression model<a href="#how-to-estimate-the-parameters-in-a-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 4.2, 6.3</p>
<p>Let’s start with the simple linear regression model (i.e., where there is just one regressor):</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X + U
\]</span>
with <span class="math inline">\(\mathbb{E}[U|X]=0\)</span>. This model holds for every observation in the data, so we can write</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + U_i
\]</span>
The question for this section is: How can we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? First, notice that, any choice that we make for an estimate of <span class="math inline">\(\beta_0\)</span> of <span class="math inline">\(\beta_1\)</span> amounts to picking a line. Our strategy will be to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> by choosing values of them that result in the “best fit” of a line to the available data.</p>
<p>This begs the question: How do you choose the line that best fits the data? Let me give you an example</p>
<p><img src="ols_lines.jpg"></p>
<p>It’s clear from the figure that the blue line “fits better” than the green line or the red line. If you take a second and think about the reason why this is the case, you will notice that the reason why you know that it fits better is because the points <em>tend to be closer</em> to the blue line than to the red line or the green line.</p>
<p>In the figure, I drew three additional lines that are labeled <span class="math inline">\(U_i^b\)</span>, <span class="math inline">\(U_i^g\)</span>, and <span class="math inline">\(U_i^r\)</span> which are just the difference between the blue line, the green line, and the red line and the corresponding data point in the figure. That is,</p>
<p><span class="math display">\[
  \begin{aligned}
  U_i^b &amp;= Y_i - b_0^b - b_1^b X_i \\
  U_i^g &amp;= Y_i - b_0^g - b_1^g X_i \\
  U_i^r &amp;= Y_i - b_0^r - b_1^r X_i
  \end{aligned}
\]</span>
where, for example, <span class="math inline">\(b_0^b\)</span> and <span class="math inline">\(b_1^b\)</span> are the intercept and slope of the blue line, <span class="math inline">\(b_0^g\)</span> and <span class="math inline">\(b_1^g\)</span> are the intercept and slope of the green line, etc. Clearly, the blue line fits this data point than the others, but we need to deal with a couple of issues to formalize this thinking. First, <span class="math inline">\(U_i^b\)</span> and <span class="math inline">\(U_i^r\)</span> are both less than 0 (because both of those lines sit above the data point) indicating that we can’t just choose the line where <span class="math inline">\(U_i\)</span> is the smallest — for this point, that strategy would result in us liking the red line the most. Instead, we need a measure of distance that turns negative values of <span class="math inline">\(U_i\)</span> into positive values and is larger for big negative values of <span class="math inline">\(U_i\)</span> too. We’ll use the same quadratic distance that we’ve used before to deal with this issue; that is, we’ll define the distance between a line and the point as <span class="math inline">\({U_i^b}^2\)</span> for the blue line, <span class="math inline">\({U_i^g}^2\)</span> and <span class="math inline">\({U_i^r}^2\)</span> for the green and red lines.</p>
<p>Second, the above discussion computes the distance between the line and the data for a single point. We want to extend this argument to all the data points. And we can do that by computing the average distance between the line and the points across all points. That is given by</p>
<p><span class="math display">\[
  \frac{1}{n}\sum_{i=1}^n {U_i^b}^2, \quad \frac{1}{n}\sum_{i=1}^n {U_i^g}^2, \quad \frac{1}{n}\sum_{i=1}^n {U_i^r}^2
\]</span>
for the blue line, green line, and red line. These are actually numbers that we can compute.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" tabindex="-1"></a><span class="co"># intercept and slope of each line</span></span>
<span id="cb123-2"><a href="#cb123-2" tabindex="-1"></a><span class="co"># (I just picked these)</span></span>
<span id="cb123-3"><a href="#cb123-3" tabindex="-1"></a>int_blue <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb123-4"><a href="#cb123-4" tabindex="-1"></a>slope_blue <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb123-5"><a href="#cb123-5" tabindex="-1"></a>int_green <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb123-6"><a href="#cb123-6" tabindex="-1"></a>slope_green <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">1.5</span></span>
<span id="cb123-7"><a href="#cb123-7" tabindex="-1"></a>int_red <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb123-8"><a href="#cb123-8" tabindex="-1"></a>slope_red <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb123-9"><a href="#cb123-9" tabindex="-1"></a></span>
<span id="cb123-10"><a href="#cb123-10" tabindex="-1"></a><span class="co"># compute &quot;errors&quot;</span></span>
<span id="cb123-11"><a href="#cb123-11" tabindex="-1"></a>Ub <span class="ot">&lt;-</span> Y <span class="sc">-</span> int_blue <span class="sc">-</span> slope_blue<span class="sc">*</span>X</span>
<span id="cb123-12"><a href="#cb123-12" tabindex="-1"></a>Ug <span class="ot">&lt;-</span> Y <span class="sc">-</span> int_green <span class="sc">-</span> slope_green<span class="sc">*</span>X</span>
<span id="cb123-13"><a href="#cb123-13" tabindex="-1"></a>Ur <span class="ot">&lt;-</span> Y <span class="sc">-</span> int_red <span class="sc">-</span> slope_red<span class="sc">*</span>X</span>
<span id="cb123-14"><a href="#cb123-14" tabindex="-1"></a></span>
<span id="cb123-15"><a href="#cb123-15" tabindex="-1"></a><span class="co"># compute distances</span></span>
<span id="cb123-16"><a href="#cb123-16" tabindex="-1"></a>dist_blue <span class="ot">&lt;-</span> <span class="fu">mean</span>(Ub<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb123-17"><a href="#cb123-17" tabindex="-1"></a>dist_green <span class="ot">&lt;-</span> <span class="fu">mean</span>(Ug<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb123-18"><a href="#cb123-18" tabindex="-1"></a>dist_red <span class="ot">&lt;-</span> <span class="fu">mean</span>(Ur<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb123-19"><a href="#cb123-19" tabindex="-1"></a></span>
<span id="cb123-20"><a href="#cb123-20" tabindex="-1"></a><span class="co"># report distances</span></span>
<span id="cb123-21"><a href="#cb123-21" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(dist_blue, dist_green, dist_red), <span class="dv">3</span>)</span>
<span id="cb123-22"><a href="#cb123-22" tabindex="-1"></a><span class="co">#&gt;   dist_blue dist_green dist_red</span></span>
<span id="cb123-23"><a href="#cb123-23" tabindex="-1"></a><span class="co">#&gt; 1     0.255      5.828   14.728</span></span></code></pre></div>
<p>This corroborates our earlier intuition — the blue line appears to fit the data much better than the other two lines.</p>
<p>It turns out that we can use the same sort of idea as above to choose not just among three possible lines to fit the data, but to choose among <em>all possible lines</em>. More generally than we have been doing, let’s write</p>
<p><span class="math display">\[
  Y_i = b_0 + b_1 X_i + U_i(b_0,b_1)
\]</span>
In other words, for any values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that we would like to plug in (say like for the green line or red line in the figure above), we can define <span class="math inline">\(U_i(b_0,b_1)\)</span> to be whatever is leftover between the line and the actual data.</p>
<p>We can choose the line (by choosing values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>) that minimizes the average distance between the line and the points. In other words, we will set</p>
<p><span class="math display">\[
  \begin{aligned}
  (\hat{\beta}_0, \hat{\beta}_1) &amp;= \underset{b_0,b_1}{\textrm{argmin}} \frac{1}{n} \sum_{i=1}^n U_i(b_0,b_1)^2 \\
  &amp;= \underset{b_0,b_1}{\textrm{argmin}} \frac{1}{n} \sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2
  \end{aligned}
\]</span>
This is a complicated looking mathematical expression, but I think the intuition is pretty easy to understand. It says that we want to find the values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that make the distance between the points and the line as small as possible on average. Whatever these values are, that is what we are going to set <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> — our estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> — to be.</p>
<p>How can you do this? One idea is to do it numerically — you could try out a ton of different combinations of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> and pick which one fits the best. Your computer could probably actually do this, but it would become quite a hard problem as we added more and more regressors.</p>
<p>It turns out that we can actually find a solution for the values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that minimize the above expression using calculus.</p>
<p>You probably recall how to minimize a function. You take its derivative, set it equal to 0, and solve that equation. [It’s actually a little more complicated than that because you need to ensure that you’re finding a minimum rather than a maximum, but the above equation is actually quadratic, so it will have a minimum.]</p>
<p>That’s all that we will do here — just the thing we need to take the derivative of looks complicated. Actually, the <span class="math inline">\(\frac{1}{n}\)</span> and <span class="math inline">\(\sum\)</span> terms will just hang around. For the rest though, we need to use the <em>chain rule</em>; that is, we need to take the derivative of the outside and then multiply by the derivative of the inside. We also need to take the derivative both with respect to <span class="math inline">\(b_0\)</span> and with respect to <span class="math inline">\(b_1\)</span>.</p>
<p>Let’s start by taking the derivative with respect to <span class="math inline">\(b_0\)</span> and setting it equal to 0.</p>
<p><span class="math display" id="eq:foc-bet0">\[
  -\frac{2}{n}\sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0 \tag{5.2}
\]</span>
where the <span class="math inline">\(2\)</span> comes from taking the deriviative of the squared part and the negative sign at the beginning comes from taking the derivative of <span class="math inline">\(-b_0\)</span> on the inside. I also replaced <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> here since <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are the values that will solve this equation.</p>
<p>Next, let’s take the derivative with respect to <span class="math inline">\(b_1\)</span>:
<span class="math display" id="eq:foc-bet1">\[
  -\frac{2}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) X_i = 0 \tag{5.3}
\]</span><br />
where the 2 comes from taking the derivative of the squared part, and the negative sign at the beginning and <span class="math inline">\(X_i\)</span> at the end come from taking the derivative of the inside with respect to <span class="math inline">\(b_1\)</span>.</p>
<p>All we have to do now is to solve these Equations <a href="#eq:foc-bet0">(5.2)</a> and <a href="#eq:foc-bet1">(5.3)</a> for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Before we do it, let me just mention that we are about to do some fairly challenging algebra, but that is all it is. Conceptually, it is just the same as solving a system of two equations with two unknowns that you probably did at some point in high school.</p>
<p>Starting with the first equation,</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp; \phantom{\implies} -\frac{2}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) &amp;= 0 \\
  &amp; \implies \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) &amp;= 0 \\
  &amp; \implies \bar{Y} - \hat{\beta}_0 - \hat{\beta}_1 \bar{X} &amp;= 0 \\
  &amp; \implies \boxed{\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}}
  \end{aligned}
\]</span>
where the second line holds by dividing both sides by <span class="math inline">\(-2\)</span>, the third line holds by pushing the summation through the sums/differences and simplifying terms, and the last line holds by rearranging to solve for <span class="math inline">\(\hat{\beta}_0\)</span>.</p>
<p>Now, let’s use the above result and Equation <a href="#eq:foc-bet1">(5.3)</a> to solve for <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp; \phantom{\implies} -\frac{2}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1) X_i &amp;= 0 \\
  &amp; \implies  \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1) X_i) X_i &amp;= 0 \\
  &amp; \implies  \frac{1}{n} \sum_{i=1}^n (Y_i - (\bar{Y} - \hat{\beta}_1 \bar{X}) - \hat{\beta}_1 X_i) X_i &amp;= 0 \\
  &amp; \implies  \frac{1}{n} \sum_{i=1}^n X_i Y_i - \bar{Y} \frac{1}{n} \sum_{i=1}^n X_i + \hat{\beta}_1 \bar{X} \frac{1}{n} \sum_{i=1}^n X_i - \hat{\beta}_1 \frac{1}{n} \sum_{i=1}^n X_i^2 &amp;= 0  \\
\end{aligned}
\]</span>
Let me explain each line and then we will keep going. The second line holds because the <span class="math inline">\(-2\)</span> can just be canceled on each side, the third line holds by plugging in the expression we derived for <span class="math inline">\(\hat{\beta}_0\)</span> above, and the last line holds by splitting up the summation and bringing out terms that do not change across <span class="math inline">\(i\)</span>. Let’s keep going</p>
<p><span class="math display">\[
\begin{aligned}
  \implies  \frac{1}{n} \sum_{i=1}^n X_i Y_i - \bar{X}\bar{Y} &amp;= \hat{\beta}_1\left(\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2 \right) \\
  \implies  \hat{\beta}_1 &amp;= \frac{\frac{1}{n} \sum_{i=1}^n X_i Y_i - \bar{X}\bar{Y}}{\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2} \\
  \implies  &amp; \boxed{\hat{\beta}_1 = \frac{\widehat{\mathrm{cov}}(X,Y)}{\widehat{\mathrm{var}}(X)}}
  \end{aligned}
\]</span>
where the first line holds by using the definition of <span class="math inline">\(\bar{X}\)</span> and moving some terms to the other side. The second equality holds by dividing both sides by <span class="math inline">\(\left(\frac{1}{n} \displaystyle \sum_{i=1}^n X_i^2 - \bar{X}^2 \right)\)</span>, and the last equality holds by the definition of sample covariance and variance.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> I’d like to point out one more time that our estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are generally not exactly equal to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. The figure earlier in this section was generated using simulated data where I set <span class="math inline">\(\beta_0=2\)</span> and <span class="math inline">\(\beta_1=1\)</span>. However, we estimate <span class="math inline">\(\hat{\beta}_0 = 1.75\)</span> and <span class="math inline">\(\hat{\beta}_1 = 0.95\)</span> using the simulated data in that figure. These lines are close to each other, but not exactly the same.</p>
<p>Further, recall that we defined the <strong>error term</strong> <span class="math inline">\(U_i\)</span> from</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + U_i
\]</span>
which is the difference between individual <span class="math inline">\(i\)</span>’s outcome and the <em>population</em> regression line.</p>
<p>Similarly, we define the <strong>residual</strong> <span class="math inline">\(\hat{U}_i\)</span> as</p>
<p><span class="math display">\[
  Y_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{U}_i
\]</span>
so that <span class="math inline">\(\hat{U}_i\)</span> is the difference between individual <span class="math inline">\(i\)</span>’s outcome and the estimated regression line. Moreover, since generally <span class="math inline">\(\beta_0 \neq \hat{\beta}_0\)</span> and <span class="math inline">\(\beta_1 \neq \hat{\beta}_1\)</span>, generally <span class="math inline">\(U_i \neq \hat{U}_i\)</span>.</p>
</div>
<div id="computation-6" class="section level3 hasAnchor" number="5.10.1">
<h3><span class="header-section-number">5.10.1</span> Computation<a href="#computation-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before moving on, let’s just confirm that the formulas that we derived are actually what <code>R</code> uses in order to estimates the parameters in a regression.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" tabindex="-1"></a><span class="co"># using lm</span></span>
<span id="cb124-2"><a href="#cb124-2" tabindex="-1"></a>reg8 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data=</span>mtcars)</span>
<span id="cb124-3"><a href="#cb124-3" tabindex="-1"></a><span class="fu">summary</span>(reg8)</span>
<span id="cb124-4"><a href="#cb124-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb124-5"><a href="#cb124-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb124-6"><a href="#cb124-6" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp, data = mtcars)</span></span>
<span id="cb124-7"><a href="#cb124-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb124-8"><a href="#cb124-8" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb124-9"><a href="#cb124-9" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb124-10"><a href="#cb124-10" tabindex="-1"></a><span class="co">#&gt; -5.7121 -2.1122 -0.8854  1.5819  8.2360 </span></span>
<span id="cb124-11"><a href="#cb124-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb124-12"><a href="#cb124-12" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb124-13"><a href="#cb124-13" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb124-14"><a href="#cb124-14" tabindex="-1"></a><span class="co">#&gt; (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***</span></span>
<span id="cb124-15"><a href="#cb124-15" tabindex="-1"></a><span class="co">#&gt; hp          -0.06823    0.01012  -6.742 1.79e-07 ***</span></span>
<span id="cb124-16"><a href="#cb124-16" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb124-17"><a href="#cb124-17" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb124-18"><a href="#cb124-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb124-19"><a href="#cb124-19" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.863 on 30 degrees of freedom</span></span>
<span id="cb124-20"><a href="#cb124-20" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 </span></span>
<span id="cb124-21"><a href="#cb124-21" tabindex="-1"></a><span class="co">#&gt; F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07</span></span>
<span id="cb124-22"><a href="#cb124-22" tabindex="-1"></a></span>
<span id="cb124-23"><a href="#cb124-23" tabindex="-1"></a><span class="co"># using our formulas</span></span>
<span id="cb124-24"><a href="#cb124-24" tabindex="-1"></a>bet1 <span class="ot">&lt;-</span> <span class="fu">cov</span>(mtcars<span class="sc">$</span>mpg, mtcars<span class="sc">$</span>hp) <span class="sc">/</span> <span class="fu">var</span>(mtcars<span class="sc">$</span>hp)</span>
<span id="cb124-25"><a href="#cb124-25" tabindex="-1"></a></span>
<span id="cb124-26"><a href="#cb124-26" tabindex="-1"></a>bet0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(mtcars<span class="sc">$</span>mpg) <span class="sc">-</span> bet1<span class="sc">*</span><span class="fu">mean</span>(mtcars<span class="sc">$</span>hp)</span>
<span id="cb124-27"><a href="#cb124-27" tabindex="-1"></a></span>
<span id="cb124-28"><a href="#cb124-28" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb124-29"><a href="#cb124-29" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">bet0=</span>bet0, <span class="at">bet1=</span>bet1)</span>
<span id="cb124-30"><a href="#cb124-30" tabindex="-1"></a><span class="co">#&gt;       bet0        bet1</span></span>
<span id="cb124-31"><a href="#cb124-31" tabindex="-1"></a><span class="co">#&gt; 1 30.09886 -0.06822828</span></span></code></pre></div>
<p>and they are identical.</p>
</div>
<div id="more-than-one-regressor" class="section level3 hasAnchor" number="5.10.2">
<h3><span class="header-section-number">5.10.2</span> More than one regressor<a href="#more-than-one-regressor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s suppose that you want to estimate a more complicated regressions like</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\]</span></p>
<p>or, even more generally,</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k + U
\]</span>
We can still choose the line that best fits the data by solving</p>
<p><span class="math display">\[
  (\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_k) = \underset{b_0,b_1,\ldots,b_k}{\textrm{argmin}} \frac{1}{n} \sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2
\]</span>
To do it, we would need to take the derivative with respect to <span class="math inline">\(b_0, b_1, \ldots, b_k\)</span>. This will give a system of equations with <span class="math inline">\((k+1)\)</span> equations and <span class="math inline">\((k+1)\)</span> unknowns. You can solve this — it actually is quite easy / very similar to what we just did if you know just a little bit of linear algebra, but this is beyond the scope of our course — and it is quite easy for the computer to solve.</p>
</div>
</div>
<div id="inference" class="section level2 hasAnchor" number="5.11">
<h2><span class="header-section-number">5.11</span> Inference<a href="#inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 4.5, 5.1, 5.2, 6.6</p>
<p>We discussed in class the practical issues of inference in linear regression models.</p>
<p>These results rely on arguments building on the Central Limit Theorem (this should not surprise you as it is similar to the case for the asymptotic distribution of <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y]))\)</span> that we discussed earlier in the semester.</p>
<p>In this section, I sketch these types of arguments for you. This material is advanced, but I suggest that you study this material.</p>
<p>We are going to show that, in the simple linear regression model,
<span class="math display">\[\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) \rightarrow N(0,V) \quad \textrm{as} \ n \rightarrow \infty
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
  V = \frac{\mathbb{E}[(X-\mathbb{E}[X])^2 U^2]}{\mathrm{var}(X)^2}
\end{align*}\]</span>
and discuss how to use this result to conduct inference.</p>
<p>Let’s start by showing why this result holds.</p>
<p>To start with, recall that
<span class="math display" id="eq:b1">\[\begin{align}
  \hat{\beta}_1 = \frac{\widehat{\mathrm{cov}}(X,Y)}{\widehat{\mathrm{var}}(X)} \tag{5.4}
\end{align}\]</span></p>
<p>Before providing a main result, let’s start with noting the following:</p>
<p><em>Helpful Intermediate Result 1</em>
Notice that
<span class="math display">\[\begin{align*}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})\bar{Y}\Big) &amp;= \bar{Y} \frac{1}{n}\sum_{i=1}^n \Big( X_i-\bar{X} \Big) \\
  &amp;= \bar{Y} \left( \frac{1}{n}\sum_{i=1}^n X_i - \frac{1}{n}\sum_{i=1}^n \bar{X} \right) \\
  &amp;= \bar{Y} \Big(\bar{X} - \bar{X} \Big) \\
  &amp;= 0
\end{align*}\]</span>
where the first equality just pulls <span class="math inline">\(\bar{Y}\)</span> out of the summation (it is a constant with respect to the summation), the second equality pushes the summation through the difference, the first part of the third equality holds by the definition of <span class="math inline">\(\bar{X}\)</span> and the second part holds because it is an average of a constant.</p>
<p>This implies that
<span class="math display" id="eq:hr1">\[\begin{align}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})(Y_i - \bar{Y})\Big) = \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})Y_i\Big) \tag{5.5}
\end{align}\]</span>
and very similar arguments (basically the same arguments in reverse) also imply that
<span class="math display" id="eq:hr2">\[\begin{align}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})X_i\Big) = \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})(X_i - \bar{X})\Big) \tag{5.6}
\end{align}\]</span>
We use both <a href="#eq:hr1">(5.5)</a> and <a href="#eq:hr2">(5.6)</a> below.</p>
<p>Next, consider the numerator in <a href="#eq:b1">(5.4)</a>
<span class="math display">\[\begin{align*}
  \widehat{\mathrm{cov}}(X,Y) &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y}) \\
  &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})Y_i \\
  &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(\beta_0 + \beta_1 X_i + U_i) \\
  &amp;= \underbrace{\beta_0 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})}_{(A)} + \underbrace{\beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) X_i}_{(B)} + \underbrace{\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) U_i}_{(C)}) \\
\end{align*}\]</span>
where the first equality holds by the definition of sample covariance, the second equality holds by <a href="#eq:hr1">(5.5)</a>, the third equality plugs in for <span class="math inline">\(Y_i\)</span>, and the last equality combines terms and passes the summation through the additions/subtractions.</p>
<p>Now, let’s consider each of these in turn.</p>
<p>For (A),
<span class="math display">\[\begin{align*}
  \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} \qquad \textrm{and} \qquad \frac{1}{n} \sum_{i=1}^n \bar{X} = \bar{X}
\end{align*}\]</span>
which implies that this term is equal to 0.</p>
<p>For (B), notice that
<span class="math display">\[\begin{align*}
  \beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) X_i &amp;= \beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) (X_i - \bar{X}) \\
  &amp;= \beta_1 \widehat{\mathrm{var}}(X)
\end{align*}\]</span>
where the first equality holds by <a href="#eq:hr2">(5.6)</a> and the second equality holds by the definition of sample variance.</p>
<p>For (C), well, we’ll just carry that one around for now.</p>
<p>Plugging in the expressions for (A), (B), and (C) back into Equation <a href="#eq:b1">(5.4)</a> implies that
<span class="math display">\[\begin{align*}
  \hat{\beta}_1 = \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{(X_i - \bar{X}) U_i}{\widehat{\mathrm{var}}(X)}
\end{align*}\]</span>
Next, re-arranging terms and multiplying both sides by <span class="math inline">\(\sqrt{n}\)</span> implies that
<span class="math display">\[\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) &amp;= \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \frac{(X_i - \bar{X}) U_i}{\widehat{\mathrm{var}}(X)}\right) \\
  &amp; \approx \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \frac{(X_i - \mathbb{E}[X]) U_i}{\mathrm{var}(X)}\right)
\end{align*}\]</span>
The last line (the approximately one) is kind of a weak argument, but basically you can replace <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\widehat{\mathrm{var}}(X)\)</span> and the effect of this replacement will converge to 0 in large samples (this is the reason for the approximately) — if you want a more complete explanation, sign up for my graduate econometrics class next semester.</p>
<p>Is this helpful? It may not be obvious, but the right hand side of the above equation is actually something that we can apply the Central Limit Theorem to. In particular, maybe it is helpful to define <span class="math inline">\(Z_i = \frac{(X_i - \mathbb{E}[X]) U_i}{\mathrm{var}(X)}\)</span>. We know that we could apply a Central Limit Theorem to <span class="math inline">\(\sqrt{n}\left( \frac{1}{n} \sum_{i=1}^n Z_i \right)\)</span> if (i) <span class="math inline">\(Z_i\)</span> had mean 0, and (ii) it is iid. That it is iid holds immediately from the random sampling assumption. For mean 0,
<span class="math display">\[\begin{align*}
  \mathbb{E}[Z] &amp;= \mathbb{E}\left[ \frac{(X - \mathbb{E}[X]) U}{\mathrm{var}(X)}\right] \\
  &amp;= \frac{1}{\mathrm{var}(X)} \mathbb{E}[(X - \mathbb{E}[X]) U] \\
  &amp;= \frac{1}{\mathrm{var}(X)} \mathbb{E}[(X - \mathbb{E}[X]) \underbrace{\mathbb{E}[U|X]}_{=0}] \\
  &amp;= 0
\end{align*}\]</span>
where the only challenging line here is the third one holds from the Law of Iterated Expectations. This means that we can apply the central limit theorem, and in particular,
<span class="math inline">\(\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n Z_i \right) \rightarrow N(0,V)\)</span> where <span class="math inline">\(V=\mathrm{var}(Z) = \mathbb{E}[Z^2]\)</span> (where the 2nd equality here holds because <span class="math inline">\(Z\)</span> has mean 0). Now, just substituting back in for <span class="math inline">\(Z\)</span> implies that
<span class="math display">\[\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) \rightarrow N(0,V)
\end{align*}\]</span>
where
<span class="math display" id="eq:V">\[\begin{align}
  V &amp;= \mathbb{E}\left[ \left( \frac{(X - \mathbb{E}[X]) U}{\mathrm{var}(X)} \right)^2 \right] \nonumber \\
  &amp;= \mathbb{E}\left[ \frac{(X - \mathbb{E}[X])^2 U^2}{\mathrm{var}(X)^2}\right] \tag{5.7}
\end{align}\]</span>
which is what we were aiming for.</p>
<p>Given this result, all our previous work on standard errors, t-statistics, p-values, and confidence intervals applies. First, let me mention the way that you would estimate <span class="math inline">\(V\)</span> (same as always, just replace the population quantities with corresponding sample quantities).</p>
<p><span class="math display">\[
  \hat{V} = \frac{ \frac{1}{n} \displaystyle \sum_{i=1}^n (X_i - \bar{X})^2 \hat{U}_i^2}{\widehat{\mathrm{var}}(X)^2}
\]</span></p>
<p>where <span class="math inline">\(\hat{U}_i\)</span> are the residuals.</p>
<p>Now, standard errors are just the same as before (the only difference is that <span class="math inline">\(\hat{V}\)</span> itself has changed)</p>
<p><span class="math display">\[
\begin{aligned}
  \textrm{s.e.}(\hat{\beta}) &amp;= \frac{\sqrt{\hat{V}}}{\sqrt{n}}
\end{aligned}
\]</span></p>
<p>By far the most common null hypothesis is <span class="math inline">\(H_0: \beta = 0\)</span>, which suggests the following t-statistic:</p>
<p><span class="math display">\[
  t = \frac{\hat{\beta}}{\textrm{s.e.}(\hat{\beta})}
\]</span>
One can continue to calculate a p-value by</p>
<p><span class="math display">\[
  \textrm{p-value} = 2 \Phi(-|t|)
\]</span>
and a 95% confidence interval is given by</p>
<p><span class="math display">\[
  CI = [\hat{\beta} - 1.96 \textrm{s.e.}(\hat{\beta}), \hat{\beta} + 1.96 \textrm{s.e.}(\hat{\beta})]
\]</span></p>
<div id="computation-7" class="section level3 hasAnchor" number="5.11.1">
<h3><span class="header-section-number">5.11.1</span> Computation<a href="#computation-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s check if what we derived is what we can compute using <code>R</code>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" tabindex="-1"></a><span class="co"># this is the same regression as in the previous section</span></span>
<span id="cb125-2"><a href="#cb125-2" tabindex="-1"></a><span class="fu">summary</span>(reg8)</span>
<span id="cb125-3"><a href="#cb125-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-4"><a href="#cb125-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb125-5"><a href="#cb125-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp, data = mtcars)</span></span>
<span id="cb125-6"><a href="#cb125-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-7"><a href="#cb125-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb125-8"><a href="#cb125-8" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb125-9"><a href="#cb125-9" tabindex="-1"></a><span class="co">#&gt; -5.7121 -2.1122 -0.8854  1.5819  8.2360 </span></span>
<span id="cb125-10"><a href="#cb125-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-11"><a href="#cb125-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb125-12"><a href="#cb125-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb125-13"><a href="#cb125-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***</span></span>
<span id="cb125-14"><a href="#cb125-14" tabindex="-1"></a><span class="co">#&gt; hp          -0.06823    0.01012  -6.742 1.79e-07 ***</span></span>
<span id="cb125-15"><a href="#cb125-15" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb125-16"><a href="#cb125-16" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb125-17"><a href="#cb125-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-18"><a href="#cb125-18" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.863 on 30 degrees of freedom</span></span>
<span id="cb125-19"><a href="#cb125-19" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 </span></span>
<span id="cb125-20"><a href="#cb125-20" tabindex="-1"></a><span class="co">#&gt; F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07</span></span>
<span id="cb125-21"><a href="#cb125-21" tabindex="-1"></a></span>
<span id="cb125-22"><a href="#cb125-22" tabindex="-1"></a><span class="co"># show previous calcuations</span></span>
<span id="cb125-23"><a href="#cb125-23" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">bet0=</span>bet0, <span class="at">bet1=</span>bet1)</span>
<span id="cb125-24"><a href="#cb125-24" tabindex="-1"></a><span class="co">#&gt;       bet0        bet1</span></span>
<span id="cb125-25"><a href="#cb125-25" tabindex="-1"></a><span class="co">#&gt; 1 30.09886 -0.06822828</span></span>
<span id="cb125-26"><a href="#cb125-26" tabindex="-1"></a></span>
<span id="cb125-27"><a href="#cb125-27" tabindex="-1"></a><span class="co"># components of Vhat</span></span>
<span id="cb125-28"><a href="#cb125-28" tabindex="-1"></a>Y <span class="ot">&lt;-</span> mtcars<span class="sc">$</span>mpg</span>
<span id="cb125-29"><a href="#cb125-29" tabindex="-1"></a>X <span class="ot">&lt;-</span> mtcars<span class="sc">$</span>hp</span>
<span id="cb125-30"><a href="#cb125-30" tabindex="-1"></a>Uhat <span class="ot">&lt;-</span> Y <span class="sc">-</span> bet0 <span class="sc">-</span> bet1<span class="sc">*</span>X</span>
<span id="cb125-31"><a href="#cb125-31" tabindex="-1"></a>Xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb125-32"><a href="#cb125-32" tabindex="-1"></a>varX <span class="ot">&lt;-</span> <span class="fu">mean</span>( (X<span class="sc">-</span>Xbar)<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb125-33"><a href="#cb125-33" tabindex="-1"></a>Vhat <span class="ot">&lt;-</span> <span class="fu">mean</span>( (X<span class="sc">-</span>Xbar)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> Uhat<span class="sc">^</span><span class="dv">2</span> ) <span class="sc">/</span> ( varX<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb125-34"><a href="#cb125-34" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(mtcars)</span>
<span id="cb125-35"><a href="#cb125-35" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(Vhat)<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb125-36"><a href="#cb125-36" tabindex="-1"></a>t_stat <span class="ot">&lt;-</span> bet1<span class="sc">/</span>se</span>
<span id="cb125-37"><a href="#cb125-37" tabindex="-1"></a>p_val <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">abs</span>(t_stat))</span>
<span id="cb125-38"><a href="#cb125-38" tabindex="-1"></a>ci_L <span class="ot">&lt;-</span> bet1 <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb125-39"><a href="#cb125-39" tabindex="-1"></a>ci_U <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb125-40"><a href="#cb125-40" tabindex="-1"></a></span>
<span id="cb125-41"><a href="#cb125-41" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb125-42"><a href="#cb125-42" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(se, t_stat, p_val, ci_L, ci_U),<span class="dv">5</span>)</span>
<span id="cb125-43"><a href="#cb125-43" tabindex="-1"></a><span class="co">#&gt;        se   t_stat p_val     ci_L     ci_U</span></span>
<span id="cb125-44"><a href="#cb125-44" tabindex="-1"></a><span class="co">#&gt; 1 0.01313 -5.19644     0 -0.09396 -0.04249</span></span></code></pre></div>
<p>Interestingly, these are not <em>exactly</em> the same as what comes from the <code>lm</code> command. Here’s what the difference is: <code>R</code> makes a simplifying assumption called “homoskedasticity” that simplifies the expression for the variance. This can result in slightly different standard errors (and therefore slightly different t-statistics, p-values, and confidence intervals too) than the ones we calculated.</p>
<p>An alternative package that is popular among economists for estimating regressions and getting “heteroskedasticity robust” standard errors is the <code>estimatr</code> package.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span>
<span id="cb126-2"><a href="#cb126-2" tabindex="-1"></a></span>
<span id="cb126-3"><a href="#cb126-3" tabindex="-1"></a>reg9 <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(mpg <span class="sc">~</span> hp, <span class="at">data=</span>mtcars, <span class="at">se_type=</span><span class="st">&quot;HC0&quot;</span>)</span>
<span id="cb126-4"><a href="#cb126-4" tabindex="-1"></a><span class="fu">summary</span>(reg9)</span>
<span id="cb126-5"><a href="#cb126-5" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb126-6"><a href="#cb126-6" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb126-7"><a href="#cb126-7" tabindex="-1"></a><span class="co">#&gt; lm_robust(formula = mpg ~ hp, data = mtcars, se_type = &quot;HC0&quot;)</span></span>
<span id="cb126-8"><a href="#cb126-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb126-9"><a href="#cb126-9" tabindex="-1"></a><span class="co">#&gt; Standard error type:  HC0 </span></span>
<span id="cb126-10"><a href="#cb126-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb126-11"><a href="#cb126-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb126-12"><a href="#cb126-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper DF</span></span>
<span id="cb126-13"><a href="#cb126-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 30.09886    2.01067  14.970 1.851e-15 25.99252 34.20520 30</span></span>
<span id="cb126-14"><a href="#cb126-14" tabindex="-1"></a><span class="co">#&gt; hp          -0.06823    0.01313  -5.196 1.338e-05 -0.09504 -0.04141 30</span></span>
<span id="cb126-15"><a href="#cb126-15" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb126-16"><a href="#cb126-16" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6024 ,    Adjusted R-squared:  0.5892 </span></span>
<span id="cb126-17"><a href="#cb126-17" tabindex="-1"></a><span class="co">#&gt; F-statistic:    27 on 1 and 30 DF,  p-value: 1.338e-05</span></span></code></pre></div>
<p>The “HC0” standard errors are “heteroskedasticity consistent” standard errors, and you can see that they match what we calculated above.</p>
</div>
</div>
<div id="lab-4-birthweight-and-smoking" class="section level2 hasAnchor" number="5.12">
<h2><span class="header-section-number">5.12</span> Lab 4: Birthweight and Smoking<a href="#lab-4-birthweight-and-smoking" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this lab, we’ll use the data <code>Birthweight_Smoking</code> and study the relationship between infant birthweight and mother’s smoking behavior.</p>
<ol style="list-style-type: decimal">
<li><p>Run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>. How do you interpret the results?</p></li>
<li><p>Use the <code>datasummary_balance</code> function from the <code>modelsummary</code> package to provide summary statistics for each variable in the data separately by smoking status of the mother. Do you notice any interesting patterns?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>, <span class="math inline">\(educ\)</span>, <span class="math inline">\(nprevisit\)</span>, <span class="math inline">\(age\)</span>, and <span class="math inline">\(alcohol\)</span>. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span>? How does its magnitude compare to the result from #1? What do you make of this?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>, the interaction of <span class="math inline">\(smoker\)</span> and <span class="math inline">\(age\)</span> and the other covariates (including <span class="math inline">\(age\)</span>) from #3. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span> and the coefficient on the interaction term?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>, the interaction of <span class="math inline">\(smoker\)</span> and <span class="math inline">\(alcohol\)</span> and the other covariates from #3. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span> and the coefficient on the interaction term?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(age\)</span> and <span class="math inline">\(age^2\)</span>. Plot the predicted value of birthweight as a function of age for ages from 18 to 44. What do you make of this?</p></li>
<li><p>Now run a regression of <span class="math inline">\(\log(birthweight)\)</span> on <span class="math inline">\(smoker\)</span> and the other covariates from #3. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span>?</p></li>
</ol>
</div>
<div id="lab-4-solutions" class="section level2 hasAnchor" number="5.13">
<h2><span class="header-section-number">5.13</span> Lab 4: Solutions<a href="#lab-4-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb127-2"><a href="#cb127-2" tabindex="-1"></a><span class="fu">library</span>(haven)</span>
<span id="cb127-3"><a href="#cb127-3" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb127-4"><a href="#cb127-4" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb127-5"><a href="#cb127-5" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb127-6"><a href="#cb127-6" tabindex="-1"></a></span>
<span id="cb127-7"><a href="#cb127-7" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb127-8"><a href="#cb127-8" tabindex="-1"></a>Birthweight_Smoking <span class="ot">&lt;-</span> <span class="fu">read_dta</span>(<span class="st">&quot;data/birthweight_smoking.dta&quot;</span>)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" tabindex="-1"></a>reg1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker, <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb128-2"><a href="#cb128-2" tabindex="-1"></a><span class="fu">summary</span>(reg1)</span>
<span id="cb128-3"><a href="#cb128-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-4"><a href="#cb128-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb128-5"><a href="#cb128-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker, data = Birthweight_Smoking)</span></span>
<span id="cb128-6"><a href="#cb128-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-7"><a href="#cb128-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb128-8"><a href="#cb128-8" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb128-9"><a href="#cb128-9" tabindex="-1"></a><span class="co">#&gt; -3007.06  -313.06    26.94   366.94  2322.94 </span></span>
<span id="cb128-10"><a href="#cb128-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-11"><a href="#cb128-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb128-12"><a href="#cb128-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb128-13"><a href="#cb128-13" tabindex="-1"></a><span class="co">#&gt; (Intercept)  3432.06      11.87 289.115   &lt;2e-16 ***</span></span>
<span id="cb128-14"><a href="#cb128-14" tabindex="-1"></a><span class="co">#&gt; smoker       -253.23      26.95  -9.396   &lt;2e-16 ***</span></span>
<span id="cb128-15"><a href="#cb128-15" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb128-16"><a href="#cb128-16" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb128-17"><a href="#cb128-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-18"><a href="#cb128-18" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 583.7 on 2998 degrees of freedom</span></span>
<span id="cb128-19"><a href="#cb128-19" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.0286, Adjusted R-squared:  0.02828 </span></span>
<span id="cb128-20"><a href="#cb128-20" tabindex="-1"></a><span class="co">#&gt; F-statistic: 88.28 on 1 and 2998 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We estimate that, on average, smoking reduces an infant’s birthweight by about 250 grams. The estimated effect is strongly statistically significant, and (I am not an expert but) that seems like a large effect of smoking to me.</p>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" tabindex="-1"></a><span class="co"># create smoker factor --- just to make table look nicer</span></span>
<span id="cb129-2"><a href="#cb129-2" tabindex="-1"></a>Birthweight_Smoking<span class="sc">$</span>smoker_factor <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(Birthweight_Smoking<span class="sc">$</span>smoker<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;smoker&quot;</span>, <span class="st">&quot;non-smoker&quot;</span>))</span>
<span id="cb129-3"><a href="#cb129-3" tabindex="-1"></a><span class="fu">datasummary_balance</span>(<span class="sc">~</span>smoker_factor, </span>
<span id="cb129-4"><a href="#cb129-4" tabindex="-1"></a>                    <span class="at">data=</span>dplyr<span class="sc">::</span><span class="fu">select</span>(Birthweight_Smoking, <span class="sc">-</span>smoker),</span>
<span id="cb129-5"><a href="#cb129-5" tabindex="-1"></a>                    <span class="at">fmt=</span><span class="dv">2</span>)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
non-smoker (N=2418)
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
smoker (N=582)
</div>
</th>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="2">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Diff. in Means
</th>
<th style="text-align:right;">
Std. Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
nprevist
</td>
<td style="text-align:right;">
11.19
</td>
<td style="text-align:right;">
3.50
</td>
<td style="text-align:right;">
10.18
</td>
<td style="text-align:right;">
4.23
</td>
<td style="text-align:right;">
-1.01
</td>
<td style="text-align:right;">
0.19
</td>
</tr>
<tr>
<td style="text-align:left;">
alcohol
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre1
</td>
<td style="text-align:right;">
0.83
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
-0.13
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre2
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
0.34
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.41
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre3
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.24
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre0
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
birthweight
</td>
<td style="text-align:right;">
3432.06
</td>
<td style="text-align:right;">
584.62
</td>
<td style="text-align:right;">
3178.83
</td>
<td style="text-align:right;">
580.01
</td>
<td style="text-align:right;">
-253.23
</td>
<td style="text-align:right;">
26.82
</td>
</tr>
<tr>
<td style="text-align:left;">
unmarried
</td>
<td style="text-align:right;">
0.18
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;">
educ
</td>
<td style="text-align:right;">
13.15
</td>
<td style="text-align:right;">
2.21
</td>
<td style="text-align:right;">
11.88
</td>
<td style="text-align:right;">
1.62
</td>
<td style="text-align:right;">
-1.27
</td>
<td style="text-align:right;">
0.08
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
27.27
</td>
<td style="text-align:right;">
5.37
</td>
<td style="text-align:right;">
25.32
</td>
<td style="text-align:right;">
5.06
</td>
<td style="text-align:right;">
-1.95
</td>
<td style="text-align:right;">
0.24
</td>
</tr>
<tr>
<td style="text-align:left;">
drinks
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
0.47
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
1.23
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.05
</td>
</tr>
</tbody>
</table>
<p>The things that stand out to me are:</p>
<ul>
<li><p>Birthweight tends to be notably lower for smokers relative to non-smokers. The difference is about 7.4% lower birthweight for babies whose mothers smoked.</p></li>
<li><p>That said, smoking is also correlated with a number of other things that could be related to lower birthweights. Mothers who smoke went to fewer pre-natal visits on average, were more likely to be unmarried, were more likely to have drink alcohol during their pregnancy, were more likely to be less educated. They also were, on average, somewhat younger than mothers who did not smoke.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" tabindex="-1"></a>reg3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb130-2"><a href="#cb130-2" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb130-3"><a href="#cb130-3" tabindex="-1"></a><span class="fu">summary</span>(reg3)</span>
<span id="cb130-4"><a href="#cb130-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-5"><a href="#cb130-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb130-6"><a href="#cb130-6" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker + educ + nprevist + age + alcohol, </span></span>
<span id="cb130-7"><a href="#cb130-7" tabindex="-1"></a><span class="co">#&gt;     data = Birthweight_Smoking)</span></span>
<span id="cb130-8"><a href="#cb130-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-9"><a href="#cb130-9" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb130-10"><a href="#cb130-10" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb130-11"><a href="#cb130-11" tabindex="-1"></a><span class="co">#&gt; -2728.91  -305.26    24.69   359.63  2220.42 </span></span>
<span id="cb130-12"><a href="#cb130-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-13"><a href="#cb130-13" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb130-14"><a href="#cb130-14" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb130-15"><a href="#cb130-15" tabindex="-1"></a><span class="co">#&gt; (Intercept) 2924.963     74.185  39.428  &lt; 2e-16 ***</span></span>
<span id="cb130-16"><a href="#cb130-16" tabindex="-1"></a><span class="co">#&gt; smoker      -206.507     27.367  -7.546 5.93e-14 ***</span></span>
<span id="cb130-17"><a href="#cb130-17" tabindex="-1"></a><span class="co">#&gt; educ           5.644      5.532   1.020    0.308    </span></span>
<span id="cb130-18"><a href="#cb130-18" tabindex="-1"></a><span class="co">#&gt; nprevist      32.979      2.914  11.318  &lt; 2e-16 ***</span></span>
<span id="cb130-19"><a href="#cb130-19" tabindex="-1"></a><span class="co">#&gt; age            2.360      2.178   1.083    0.279    </span></span>
<span id="cb130-20"><a href="#cb130-20" tabindex="-1"></a><span class="co">#&gt; alcohol      -39.512     76.365  -0.517    0.605    </span></span>
<span id="cb130-21"><a href="#cb130-21" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb130-22"><a href="#cb130-22" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb130-23"><a href="#cb130-23" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-24"><a href="#cb130-24" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 570.3 on 2994 degrees of freedom</span></span>
<span id="cb130-25"><a href="#cb130-25" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07402,    Adjusted R-squared:  0.07247 </span></span>
<span id="cb130-26"><a href="#cb130-26" tabindex="-1"></a><span class="co">#&gt; F-statistic: 47.86 on 5 and 2994 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Here we estimate that smoking reduces an infant’s birthweight by about 200 grams on average holding education, number of pre-natal visits, age, and whether or not the mother consumed alcohol constant. The magnitude of the estimated effect is somewhat smaller than the previous estimate. Due to the discussion in #2 (particularly, that smoking was correlated with a number of other characteristics that are likely associated with lower birthweights), this decrease in the magnitude is not surprising.</p>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" tabindex="-1"></a>reg4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker <span class="sc">+</span> <span class="fu">I</span>(smoker<span class="sc">*</span>age) <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb131-2"><a href="#cb131-2" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb131-3"><a href="#cb131-3" tabindex="-1"></a><span class="fu">summary</span>(reg4)</span>
<span id="cb131-4"><a href="#cb131-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-5"><a href="#cb131-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb131-6"><a href="#cb131-6" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker + I(smoker * age) + educ + </span></span>
<span id="cb131-7"><a href="#cb131-7" tabindex="-1"></a><span class="co">#&gt;     nprevist + age + alcohol, data = Birthweight_Smoking)</span></span>
<span id="cb131-8"><a href="#cb131-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-9"><a href="#cb131-9" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb131-10"><a href="#cb131-10" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb131-11"><a href="#cb131-11" tabindex="-1"></a><span class="co">#&gt; -2722.56  -305.12    23.93   363.43  2244.67 </span></span>
<span id="cb131-12"><a href="#cb131-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-13"><a href="#cb131-13" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb131-14"><a href="#cb131-14" tabindex="-1"></a><span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb131-15"><a href="#cb131-15" tabindex="-1"></a><span class="co">#&gt; (Intercept)     2853.819     77.104  37.013  &lt; 2e-16 ***</span></span>
<span id="cb131-16"><a href="#cb131-16" tabindex="-1"></a><span class="co">#&gt; smoker           231.578    134.854   1.717 0.086036 .  </span></span>
<span id="cb131-17"><a href="#cb131-17" tabindex="-1"></a><span class="co">#&gt; I(smoker * age)  -17.145      5.168  -3.317 0.000919 ***</span></span>
<span id="cb131-18"><a href="#cb131-18" tabindex="-1"></a><span class="co">#&gt; educ               4.895      5.528   0.885 0.375968    </span></span>
<span id="cb131-19"><a href="#cb131-19" tabindex="-1"></a><span class="co">#&gt; nprevist          32.482      2.913  11.151  &lt; 2e-16 ***</span></span>
<span id="cb131-20"><a href="#cb131-20" tabindex="-1"></a><span class="co">#&gt; age                5.528      2.375   2.328 0.019999 *  </span></span>
<span id="cb131-21"><a href="#cb131-21" tabindex="-1"></a><span class="co">#&gt; alcohol          -22.556     76.409  -0.295 0.767864    </span></span>
<span id="cb131-22"><a href="#cb131-22" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb131-23"><a href="#cb131-23" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb131-24"><a href="#cb131-24" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-25"><a href="#cb131-25" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 569.4 on 2993 degrees of freedom</span></span>
<span id="cb131-26"><a href="#cb131-26" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07741,    Adjusted R-squared:  0.07556 </span></span>
<span id="cb131-27"><a href="#cb131-27" tabindex="-1"></a><span class="co">#&gt; F-statistic: 41.85 on 6 and 2993 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We should be careful about the interpretatio here. We have estimated a model like</p>
<p><span class="math display">\[
  \mathbb{E}[Birthweight|Smoker, Age, X] = \beta_0 + \beta_1 Smoker + \beta_2 Smoker \cdot Age + \cdots
\]</span>
Therefore, the partial effect of smoking is given by</p>
<p><span class="math display">\[
  \mathbb{E}[Birthweight | Smoker=1, Age, X] - \mathbb{E}[Birthweight | Smoker=0, Age, X] = \beta_1 + \beta_2 Age
\]</span>
Therefore, the partial effect of smoking depends on <span class="math inline">\(Age\)</span>. For example, for <span class="math inline">\(Age=18\)</span>, the partial effect is <span class="math inline">\(\beta_1 + \beta_2 (18)\)</span>. For <span class="math inline">\(Age=25\)</span>, the partial effect is <span class="math inline">\(\beta_1 + \beta_2 (25)\)</span>, and for <span class="math inline">\(Age=35\)</span>, the partial effect is <span class="math inline">\(\beta_1 + \beta_2 (35)\)</span>. Let’s calculate the partial effect at each of those ages.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" tabindex="-1"></a>bet1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(reg4)[<span class="dv">2</span>]</span>
<span id="cb132-2"><a href="#cb132-2" tabindex="-1"></a>bet2 <span class="ot">&lt;-</span> <span class="fu">coef</span>(reg4)[<span class="dv">3</span>]</span>
<span id="cb132-3"><a href="#cb132-3" tabindex="-1"></a></span>
<span id="cb132-4"><a href="#cb132-4" tabindex="-1"></a>pe_18 <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> bet2<span class="sc">*</span><span class="dv">18</span></span>
<span id="cb132-5"><a href="#cb132-5" tabindex="-1"></a>pe_25 <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> bet2<span class="sc">*</span><span class="dv">25</span></span>
<span id="cb132-6"><a href="#cb132-6" tabindex="-1"></a>pe_35 <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> bet2<span class="sc">*</span><span class="dv">35</span></span>
<span id="cb132-7"><a href="#cb132-7" tabindex="-1"></a></span>
<span id="cb132-8"><a href="#cb132-8" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cbind.data.frame</span>(pe_18, pe_25, pe_35),<span class="dv">2</span>)</span>
<span id="cb132-9"><a href="#cb132-9" tabindex="-1"></a><span class="co">#&gt;         pe_18   pe_25   pe_35</span></span>
<span id="cb132-10"><a href="#cb132-10" tabindex="-1"></a><span class="co">#&gt; smoker -77.04 -197.05 -368.51</span></span></code></pre></div>
<p>This suggests substantially larger effects of smoking on birthweight for older mothers.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" tabindex="-1"></a>reg5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker <span class="sc">+</span> <span class="fu">I</span>(smoker<span class="sc">*</span>alcohol) <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb133-2"><a href="#cb133-2" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb133-3"><a href="#cb133-3" tabindex="-1"></a><span class="fu">summary</span>(reg5)</span>
<span id="cb133-4"><a href="#cb133-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-5"><a href="#cb133-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb133-6"><a href="#cb133-6" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker + I(smoker * alcohol) + educ + </span></span>
<span id="cb133-7"><a href="#cb133-7" tabindex="-1"></a><span class="co">#&gt;     nprevist + age + alcohol, data = Birthweight_Smoking)</span></span>
<span id="cb133-8"><a href="#cb133-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-9"><a href="#cb133-9" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb133-10"><a href="#cb133-10" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb133-11"><a href="#cb133-11" tabindex="-1"></a><span class="co">#&gt; -2728.99  -304.16    24.54   359.92  2222.10 </span></span>
<span id="cb133-12"><a href="#cb133-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-13"><a href="#cb133-13" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb133-14"><a href="#cb133-14" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb133-15"><a href="#cb133-15" tabindex="-1"></a><span class="co">#&gt; (Intercept)         2924.844     74.185  39.426  &lt; 2e-16 ***</span></span>
<span id="cb133-16"><a href="#cb133-16" tabindex="-1"></a><span class="co">#&gt; smoker              -201.852     27.765  -7.270 4.57e-13 ***</span></span>
<span id="cb133-17"><a href="#cb133-17" tabindex="-1"></a><span class="co">#&gt; I(smoker * alcohol) -151.860    152.717  -0.994    0.320    </span></span>
<span id="cb133-18"><a href="#cb133-18" tabindex="-1"></a><span class="co">#&gt; educ                   5.612      5.532   1.014    0.310    </span></span>
<span id="cb133-19"><a href="#cb133-19" tabindex="-1"></a><span class="co">#&gt; nprevist              32.844      2.917  11.260  &lt; 2e-16 ***</span></span>
<span id="cb133-20"><a href="#cb133-20" tabindex="-1"></a><span class="co">#&gt; age                    2.403      2.178   1.103    0.270    </span></span>
<span id="cb133-21"><a href="#cb133-21" tabindex="-1"></a><span class="co">#&gt; alcohol               39.824    110.440   0.361    0.718    </span></span>
<span id="cb133-22"><a href="#cb133-22" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb133-23"><a href="#cb133-23" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb133-24"><a href="#cb133-24" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-25"><a href="#cb133-25" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 570.3 on 2993 degrees of freedom</span></span>
<span id="cb133-26"><a href="#cb133-26" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07432,    Adjusted R-squared:  0.07247 </span></span>
<span id="cb133-27"><a href="#cb133-27" tabindex="-1"></a><span class="co">#&gt; F-statistic: 40.05 on 6 and 2993 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The point estimate suggests that the effect of smoking is larger for women who consume alcohol and smoke than for women who do not drink alcohol. This seems plausible, but our evidence is not very strong here — the estimates are not statistically significant at any conventional significance level (the p-value is equal to 0.32).</p>
<ol start="6" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" tabindex="-1"></a>reg6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> age <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">2</span>), <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb134-2"><a href="#cb134-2" tabindex="-1"></a><span class="fu">summary</span>(reg6)</span>
<span id="cb134-3"><a href="#cb134-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb134-4"><a href="#cb134-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb134-5"><a href="#cb134-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ age + I(age^2), data = Birthweight_Smoking)</span></span>
<span id="cb134-6"><a href="#cb134-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb134-7"><a href="#cb134-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb134-8"><a href="#cb134-8" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb134-9"><a href="#cb134-9" tabindex="-1"></a><span class="co">#&gt; -2949.81  -312.81    30.43   371.03  2452.72 </span></span>
<span id="cb134-10"><a href="#cb134-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb134-11"><a href="#cb134-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb134-12"><a href="#cb134-12" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb134-13"><a href="#cb134-13" tabindex="-1"></a><span class="co">#&gt; (Intercept) 2502.8949   225.6016  11.094  &lt; 2e-16 ***</span></span>
<span id="cb134-14"><a href="#cb134-14" tabindex="-1"></a><span class="co">#&gt; age           58.1670    16.9212   3.438 0.000595 ***</span></span>
<span id="cb134-15"><a href="#cb134-15" tabindex="-1"></a><span class="co">#&gt; I(age^2)      -0.9099     0.3099  -2.936 0.003353 ** </span></span>
<span id="cb134-16"><a href="#cb134-16" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb134-17"><a href="#cb134-17" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb134-18"><a href="#cb134-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb134-19"><a href="#cb134-19" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 589.6 on 2997 degrees of freedom</span></span>
<span id="cb134-20"><a href="#cb134-20" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.009261,   Adjusted R-squared:  0.0086 </span></span>
<span id="cb134-21"><a href="#cb134-21" tabindex="-1"></a><span class="co">#&gt; F-statistic: 14.01 on 2 and 2997 DF,  p-value: 8.813e-07</span></span></code></pre></div>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(reg6, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">age=</span><span class="fu">seq</span>(<span class="dv">18</span>,<span class="dv">40</span>)))</span>
<span id="cb135-2"><a href="#cb135-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">preds=</span>preds, <span class="at">age=</span><span class="fu">seq</span>(<span class="dv">18</span>,<span class="dv">40</span>)), <span class="fu">aes</span>(<span class="at">x=</span>age, <span class="at">y=</span>preds)) <span class="sc">+</span> </span>
<span id="cb135-3"><a href="#cb135-3" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb135-4"><a href="#cb135-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb135-5"><a href="#cb135-5" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb135-6"><a href="#cb135-6" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;predicted values&quot;</span>)</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-138-1.png" width="672" />
The figure suggests that predicted birthweight is increasing in mother’s age up until about age 34 and then decreasing after that.</p>
<ol start="7" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" tabindex="-1"></a>reg7 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">I</span>(<span class="fu">log</span>(birthweight)) <span class="sc">~</span> smoker <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb136-2"><a href="#cb136-2" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb136-3"><a href="#cb136-3" tabindex="-1"></a><span class="fu">summary</span>(reg7)</span>
<span id="cb136-4"><a href="#cb136-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-5"><a href="#cb136-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb136-6"><a href="#cb136-6" tabindex="-1"></a><span class="co">#&gt; lm(formula = I(log(birthweight)) ~ smoker + educ + nprevist + </span></span>
<span id="cb136-7"><a href="#cb136-7" tabindex="-1"></a><span class="co">#&gt;     age + alcohol, data = Birthweight_Smoking)</span></span>
<span id="cb136-8"><a href="#cb136-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-9"><a href="#cb136-9" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb136-10"><a href="#cb136-10" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb136-11"><a href="#cb136-11" tabindex="-1"></a><span class="co">#&gt; -1.96324 -0.07696  0.02435  0.12092  0.50070 </span></span>
<span id="cb136-12"><a href="#cb136-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-13"><a href="#cb136-13" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb136-14"><a href="#cb136-14" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb136-15"><a href="#cb136-15" tabindex="-1"></a><span class="co">#&gt; (Intercept)  7.9402678  0.0270480 293.562  &lt; 2e-16 ***</span></span>
<span id="cb136-16"><a href="#cb136-16" tabindex="-1"></a><span class="co">#&gt; smoker      -0.0635764  0.0099782  -6.372 2.16e-10 ***</span></span>
<span id="cb136-17"><a href="#cb136-17" tabindex="-1"></a><span class="co">#&gt; educ         0.0022169  0.0020171   1.099    0.272    </span></span>
<span id="cb136-18"><a href="#cb136-18" tabindex="-1"></a><span class="co">#&gt; nprevist     0.0129662  0.0010624  12.205  &lt; 2e-16 ***</span></span>
<span id="cb136-19"><a href="#cb136-19" tabindex="-1"></a><span class="co">#&gt; age          0.0003059  0.0007941   0.385    0.700    </span></span>
<span id="cb136-20"><a href="#cb136-20" tabindex="-1"></a><span class="co">#&gt; alcohol     -0.0181053  0.0278428  -0.650    0.516    </span></span>
<span id="cb136-21"><a href="#cb136-21" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb136-22"><a href="#cb136-22" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb136-23"><a href="#cb136-23" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-24"><a href="#cb136-24" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.2079 on 2994 degrees of freedom</span></span>
<span id="cb136-25"><a href="#cb136-25" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07322,    Adjusted R-squared:  0.07167 </span></span>
<span id="cb136-26"><a href="#cb136-26" tabindex="-1"></a><span class="co">#&gt; F-statistic: 47.31 on 5 and 2994 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The estimated coefficient on <span class="math inline">\(smoker\)</span> says that smoking during pregnancy decreases a baby’s birthweight by 6.3%, on average, holding education, number of pre-natal visits, age of the mother, and whether or not the mother consumed alcohol during the pregnancy constant.</p>
</div>
<div id="coding-questions-2" class="section level2 hasAnchor" number="5.14">
<h2><span class="header-section-number">5.14</span> Coding Questions<a href="#coding-questions-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>For this problem, we will use the data <code>intergenerational_mobility</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of child family income (<span class="math inline">\(child\_fincome\)</span>) on parents’ family income (<span class="math inline">\(parent\_fincome\)</span>). How should you interpret the estimated coefficient on parents’ family income? What is the p-value for the coefficient on parents’ family income?</p></li>
<li><p>Run a regression of <span class="math inline">\(\log(child\_fincome)\)</span> on <span class="math inline">\(parent\_fincome\)</span>. How should you interpret the estimated cofficient on <span class="math inline">\(parent\_fincome\)</span>?</p></li>
<li><p>Run a regression of <span class="math inline">\(child\_fincome\)</span> on <span class="math inline">\(\log(parent\_fincome)\)</span>. How should you interpret the estimated coefficient on <span class="math inline">\(\log(parent\_fincome)\)</span>?</p></li>
<li><p>Run a regression of <span class="math inline">\(\log(child\_fincome)\)</span> on <span class="math inline">\(\log(parent\_fincome)\)</span>. How should you interpret the estimated coefficient on <span class="math inline">\(\log(parent\_fincome)\)</span>?</p></li>
</ol></li>
<li><p>For this question, we’ll use the <code>fertilizer_2000</code> data.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of <span class="math inline">\(\log(avyield)\)</span> on <span class="math inline">\(\log(avfert)\)</span>. How do you interpret the estimated coefficient on <span class="math inline">\(\log(avfert)\)</span>?</p></li>
<li><p>Now suppose that you additionally want to control for precipitation and the region that a country is located in. How would you do this? Estimate the model that you propose here, report the results, and interpret the coefficient on <span class="math inline">\(\log(avfert)\)</span>.</p></li>
<li><p>Now suppose that you are interested in whether the effect of fertilizer varies by region that a country is located in (while still controlling for the same covariates as in part (b)). Propose a model that can be used for this purpose. Estimate the model that you proposed, report the results, and discuss whether the effect of fertilizer appears to vary by region or not.</p></li>
</ol></li>
<li><p>For this question, we will use the data <code>mutual_funds</code>. We’ll be interested in whether mutual funds that have higher expense ratios (these are typically actively managed funds) have higher returns relative to mutual funds that have lower expense ratios (e.g., index funds). For this problem, we will use the variables <code>fund_return_3years</code>, <code>investment_type</code>, <code>risk_rating</code>, <code>size_type</code>, <code>fund_net_annual_expense_ratio</code>, <code>asset_cash</code>, <code>asset_stocks</code>, <code>asset_bonds</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Calculate the median <code>fund_net_annual_expense_ratio</code>.</p></li>
<li><p>Use the <code>datasummary_balance</code> function from the <code>modelsummary</code> package to report summary statistics for <code>fund_return_3year</code>, <code>fund_net_annual_expense_ratio</code>, <code>risk_rating</code>, <code>asset_cash</code>, <code>asset_stocks</code>, <code>asset_bonds</code> based on whether their expense ratio is above or below the median. Do you notice any interesting patterns?</p></li>
<li><p>Run a regression of <code>fund_return_3years</code> on <code>fund_net_annual_expense_ratio</code>. How do you interpret the results?</p></li>
<li><p>Now, additionally control for <code>investment_type</code>, <code>risk_rating</code>, and <code>size_type</code> <strong>Hint:</strong> think carefully about what type of variables each of these are and how they should enter the model. How do these results compare to the ones from part c?</p></li>
<li><p>Now, add the variables <code>assets_cash</code>, <code>assets_stocks</code>, and <code>assets_bonds</code> to the model from part d. How do you interpret these results? Compare and interpret the differences between parts c, d, and e.</p></li>
</ol></li>
<li><p>For this question, we’ll use the data <code>Lead_Mortality</code> to study the effect of lead pipes on infant mortality in 1900.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of infant mortality (<code>infrate</code>) on whether or not a city had lead pipes (<code>lead</code>) and interpret/discuss the results.</p></li>
<li><p>It turns out that the amount of lead in drinking water depends on how acidic the water is, with more acidic water leaching more of the lead (so that there is more exposure to lead with more acidic water). To measure acidity, we’ll use the pH of the water in a particular city (<code>ph</code>); recall that, a lower value of pH indicates higher acidity. Run a regression of infant mortality on whether or not a city has lead pipes, the pH of its water, and the interaction between having lead pipes and pH. Report your results. What is the estimated partial effect of having lead pipes from this model?</p></li>
<li><p>Given the results in part b, calculate an estimate of the average partial effect of having lead pipes on infant mortality.</p></li>
<li><p>Given the results in part b, how much does the partial effect of having lead pipes differ for cities that have a pH of 6.5 relative to a pH of 7.5?</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-2" class="section level2 hasAnchor" number="5.15">
<h2><span class="header-section-number">5.15</span> Extra Questions<a href="#extra-questions-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Suppose you run the following regression
<span class="math display">\[\begin{align*}
  Earnings = \beta_0 + \beta_1 Education + U
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|Education] = 0\)</span>. How do you interpret <span class="math inline">\(\beta_1\)</span> here?</p></li>
<li><p>Suppose you run the following regression
<span class="math display">\[\begin{align*}
  Earnings = \beta_0 + \beta_1 Education + \beta_2 Experience + \beta_3 Female + U
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|Education, Experience, Female] = 0\)</span>. How do you interpret <span class="math inline">\(\beta_1\)</span> here?</p></li>
<li><p>Suppose you are interested in testing whether an extra year of education increases earnings by the same amount for men and women.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Propose a regression and strategy for this sort of test.</p></li>
<li><p>Suppose you also want to control for experience in conducting this test, how would do it?</p></li>
</ol></li>
<li><p>Suppose you run the following regression
<span class="math display">\[\begin{align*}
  \log(Earnings) = \beta_0 + \beta_1 Education + \beta_2 Experience + \beta_3 Female + U
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|Education, Experience, Female] = 0\)</span>. How do you interpret <span class="math inline">\(\beta_1\)</span> here?</p></li>
<li><p>A common extra condition (though somewhat old-fashioned) is to impose <em>homoskedasticity</em>. Homoskedasticity says that <span class="math inline">\(\mathbb{E}[U^2|X] = \sigma^2\)</span> (i.e., the variance of the error term does not change across different values of <span class="math inline">\(X\)</span>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Under homoskedasticity, the expression for <span class="math inline">\(V\)</span> in <a href="#eq:V">(5.7)</a> simplifies. Provide a new expression for <span class="math inline">\(V\)</span> under homoskedasticity. <strong>Hint:</strong> you will need to use the law of iterated expectations.</p></li>
<li><p>Using this expression for <span class="math inline">\(V\)</span>, explain how to calculate standard errors for an estimate of <span class="math inline">\(\beta_1\)</span> in a simple linear regression.</p></li>
<li><p>Explain how to construct a t-statistic for testing <span class="math inline">\(H_0: \beta_1=0\)</span> under homoskedasticity.</p></li>
<li><p>Explain how to contruct a p-value for <span class="math inline">\(\beta_1\)</span> under homoskedasticity.</p></li>
<li><p>Explain how to construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> under homoskedasticity.</p></li>
</ol></li>
</ol>
</div>
<div id="answers-to-some-extra-questions" class="section level2 hasAnchor" number="5.16">
<h2><span class="header-section-number">5.16</span> Answers to Some Extra Questions<a href="#answers-to-some-extra-questions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Answer to Question 2</strong></p>
<p><span class="math inline">\(\beta_1\)</span> is how much <span class="math inline">\(Earnings\)</span> increase on average when <span class="math inline">\(Education\)</span> increases by one year holding <span class="math inline">\(Experience\)</span> and <span class="math inline">\(Female\)</span> constant.</p>
<p><strong>Answer to Question 3</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Run the regression
<span class="math display">\[\begin{align*}
     Earnings &amp;= \beta_0 + \beta_1 Education + \beta_2 Female + \beta_3 Education \times Female + U
\end{align*}\]</span>
and test (e.g., calculate a t-statistic and check if it is greater than 1.96 in absolute value) if <span class="math inline">\(\beta_3=0\)</span>.</p></li>
<li><p>You can run the following regression:
<span class="math display">\[\begin{align*}
   Earnings &amp;= \beta_0 + \beta_1 Education + \beta_2 Female \\
   &amp; \hspace{25pt} + \beta_3 Education \times Female + \beta_4 Experience + U
\end{align*}\]</span>
Here, you would still be interested in <span class="math inline">\(\beta_3\)</span>. If you thought that the return to experience varied for men and women, you might also include an interaction term involving <span class="math inline">\(Experience\)</span> and <span class="math inline">\(Female\)</span>.</p></li>
</ol>
<p><strong>Partial Answer to Question 5</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Starting from <a href="#eq:V">(5.7)</a></li>
</ol>
<p><span class="math display">\[\begin{align*}
    V &amp;= \mathbb{E}\left[ \frac{(X - \mathbb{E}[X])^2 U^2}{\mathrm{var}(X)^2} \right] \\
    &amp;= \frac{1}{\mathrm{var}(X)^2} \mathbb{E}[(X-\mathbb{E}[X])^2 U^2] \\
    &amp;= \frac{1}{\mathrm{var}(X)^2} \mathbb{E}\big[(X-\mathbb{E}[X])^2 \mathbb{E}[U^2|X] \big] \\
    &amp;= \frac{1}{\mathrm{var}(X)^2} \mathbb{E}[(X-\mathbb{E}[X])^2 \sigma^2 ] \\
    &amp;= \frac{\sigma^2}{\mathrm{var}(X)^2} \mathbb{E}[(X-\mathbb{E}[X])^2] \\
    &amp;= \frac{\sigma^2}{\mathrm{var}(X)^2} \mathrm{var}(X) \\
    &amp;= \frac{\sigma^2}{\mathrm{var}(X)}
  \end{align*}\]</span></p>
<p>where</p>
<ul>
<li><p>the second equality holds because <span class="math inline">\(\mathrm{var}(X)^2\)</span> is non-random and can come out of the expectation,</p></li>
<li><p>the third equality uses the law of iterated expectations,</p></li>
<li><p>the fourth equality holds by the condition of homoskedasticity,</p></li>
<li><p>the fifth equality holds because <span class="math inline">\(\sigma^2\)</span> is non-random and can come out of the expectation,</p></li>
<li><p>the sixth equality holds by the definition of variance, and</p></li>
<li><p>the last equality holds by canceling <span class="math inline">\(\mathrm{var}(X)\)</span> in the numerator with one of the <span class="math inline">\(\mathrm{var}(X)\)</span>’s in the denominator.</p></li>
</ul>

</div>
</div>
<div id="prediction" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Topic 6</span> Prediction<a href="#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this part of the class, we’ll learn how to use regressions and “machine learning” in order to predict outcomes of interest using available, related data. One key issue when it comes to making predictions is that we typically have lots of possible models that we could use to make the predictions. Choosing which of these models works best, or <strong>model selection</strong>, is therefore often an important step when it comes to making good predictions.</p>
<p>Another important issue with prediction is a focus on making out-of-sample predictions. We can usually make better within-sample predictions just by making the model more complicated, but this often leads to <strong>over-fitting</strong> — predictions that are “too specific” to our particular data.</p>
<div id="measures-of-regression-fit" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Measures of Regression Fit<a href="#measures-of-regression-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ll start this chapter by learning about measures of how well a regression fits the data. Consider the following figure</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-140-1.png" width="50%" /><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-140-2.png" width="50%" />
These are exactly the same two regression lines. But we probably have the sense that the regression line in the second figure “fits better” than in the first figure, and, furthermore, that this is likely to result in better predictions of whatever the second outcome is relative to the first one. We’ll formalize this below.</p>
<div id="tss-ess-ssr" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> TSS, ESS, SSR<a href="#tss-ess-ssr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 6.4</p>
<p>Let’s start by defining some quantities. These will be useful for quantifying how well the model fits the data.</p>
<ul>
<li><p>Total Sum of Squares (TSS) — measures total variation in the data</p>
<p><span class="math display">\[
    TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2
  \]</span></p></li>
<li><p>Explained Sum of Squares (ESS) — measures variation explained by the model</p>
<p><span class="math display">\[
    ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2
  \]</span></p></li>
<li><p>Sum of Squared Residuals (SSR) — measures “leftover” variation in the data that is not explained by the model</p>
<p><span class="math display">\[
    SSR = \sum_{i=1}^n \hat{U}_i^2 = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
  \]</span></p></li>
</ul>
<p><strong>Properties</strong></p>
<ul>
<li><p>A first useful property is</p>
<p><span class="math display">\[
    TSS = ESS + SSR
  \]</span></p>
<p>We will not prove this property though it is a useful exercise and not actually that challenging.</p></li>
<li><p>Another useful property is that <span class="math inline">\(TSS\)</span>, <span class="math inline">\(ESS\)</span>, and <span class="math inline">\(SSR\)</span> are all positive — this holds because they all involve sums of squared quantities.</p></li>
</ul>
</div>
<div id="r2" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> <span class="math inline">\(R^2\)</span><a href="#r2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 6.4</p>
<p><span class="math inline">\(R^2\)</span> is probably the most common measure of regression fit. It is defined as</p>
<p><span class="math display">\[
  R^2 := \frac{ESS}{TSS}
\]</span></p>
<p>so that <span class="math inline">\(R^2\)</span> is the fraction of the variation in <span class="math inline">\(Y\)</span> explained by the model. Notice that <span class="math inline">\(R^2\)</span> is always between 0 and 1 which holds by the two properties of <span class="math inline">\(TSS\)</span>, <span class="math inline">\(ESS\)</span>, and <span class="math inline">\(SSR\)</span> listed in the previous section.</p>
</div>
</div>
<div id="model-selection" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Model Selection<a href="#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 7.5 (note: we cover substantially more details than the textbook about model selection)</p>
<p>Often, when we want to use data to make predictions, there are multiple possible models that we could estimate to make the predictions. For example, suppose that we are interested in predicting house prices and we have access to data about the number of square feet, whether or not the house has a basement, the number of bedrooms, and the number of bathrooms. All of these are probably good variables to include in a model to predict house prices (though it is less clear if we should include quadratic terms, interaction terms, or other higher order terms). But suppose we also observe some variables that are probably irrelevant (e.g., whether the street number is odd or even) or not clear whether they matter or not (e.g., number of ceiling fans or paint color). It is not clear which variables we should include in the model. An alternative way to think about this is that there would be a large number of possible models that we could estimate here, and it is not immediately obvious which one will predict the best.</p>
<p>From the previous example, it seems that in many realistic applications, there are a large number of possible models that we could estimate to make predictions. How should we choose which one to use?</p>
<p>One idea is to just throw all the data that we have into the model. In many applications, this may not be a good idea. I’ll provide a specific example below.</p>
<p>Before doing that, I want to distinguish between two concepts. Typically, what we mean we say that a model is good at making predictions is that it is a good at making <strong>out-of-sample</strong> predictions. In other words, some new observation shows up (e.g., a new house comes on the market) with certain characteristics — we would like to be using a model that makes good predictions for this house. This is a distinct concept from <strong>in-sample</strong> fit of the model — how well a model predicts for the data that it is estimated on. In the context of prediction, one very common problem is <strong>over-fitting</strong>. Over-fitting is the problem of getting predictions that are <em>too specific</em> to the particular data that you have, but then that don’t work well for new data.</p>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 6.1  </strong></span>Suppose that you are interested in predicting height of students at UGA. You consider estimating three models</p>
<p><span class="math display">\[
  \begin{aligned}
    Height &amp;= \beta_0 + \beta_1 Male + U \\
    Height &amp;= \beta_0 + \beta_1 Male + \beta_2 Year + U \\
    Height &amp;= \beta_0 + \beta_1 Male + \beta_2 Year + \beta_3 Birthday + U
  \end{aligned}
\]</span>
where <span class="math inline">\(Year\)</span> is what year in school a student is in (e.g., Freshman, Sophomore, etc.), and <span class="math inline">\(Birthday\)</span> is what day of the year a student was born on.</p>
<p>You also notice that <span class="math inline">\(R^2\)</span> is greatest for the third model, in the middle for the second model, and smallest for the first model.</p>
<p>The third model likely suffers from over-fitting. In particular, knowing a student’s birthday may really help you predict height in-sample. For example, suppose that the center on the basketball team is in your sample, and his birthday is July 15. Knowing this, you will likely be able to make a much better prediction for this observation in your data using Model 3 which <span class="math inline">\(\implies\)</span> that the <span class="math inline">\(R^2\)</span> will be much higher for this model. But this won’t help you predict well out-of-sample. If a new person shows up whose birthday is July 15, they are unlikely to also be a center on the basketball team which further implies that your prediction of their height is likely to be very poor.</p>
</div>
<p>The previous example is about over-fitting with the idea that better in-sample predictions can actually lead to worse out-of-sample predictions.</p>
<div id="limitations-of-r2" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Limitations of <span class="math inline">\(R^2\)</span><a href="#limitations-of-r2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 6.4</p>
<p><span class="math inline">\(R^2\)</span> is a measure of in-sample fit. In fact, you can always increase <span class="math inline">\(R^2\)</span> by adding new variables into a model. To see this, notice that</p>
<p><span class="math display">\[
  \begin{aligned}
    R^2 &amp;= \frac{ESS}{TSS} \\
    &amp;= 1 - \frac{SSR}{TSS}
  \end{aligned}
\]</span>
When you add a new variable to a model, <span class="math inline">\(SSR\)</span> cannot increase (see explanation below). Since <span class="math inline">\(SSR\)</span> cannot increase, it means that <span class="math inline">\(R^2\)</span> can only increase (or at a minimum remain unchanged) when a new regressor is added to the model.</p>
<p>This means that, if you were to choose a model by <span class="math inline">\(R^2\)</span>, it would always choose the “kitchen-sink” (include everything) model. But, as we discussed above, this is likely to lead to severe over-fitting problems. This suggests using alternative approaches to choose a model for the purposed of prediction.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> It might not be obvious why <span class="math inline">\(SSR\)</span> necessarily decreases when a covariate is added to the model. Here is an explanation. Suppose that we estimate two models (I’ll include an <span class="math inline">\(i\)</span> subscript here and just use <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\delta\)</span> to remind you that the parameters are not equal to each other across models)</p>
<p><span class="math display">\[
  \begin{aligned}
  Y_i &amp;= \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \hat{U}_{1i} \\
  Y_i &amp;= \hat{\delta}_0 + \hat{\delta}_1 X_{1i} + \hat{\delta}_2 X_{2i} + \hat{U}_{2i}
  \end{aligned}
\]</span>
Recall that, for both models, we estimate the parameters by minimizing the sum of squared residuals (<span class="math inline">\(SSR\)</span>). Notice that the second model is equivalent to the first model when <span class="math inline">\(\hat{\delta}_2=0\)</span>. In that case, <span class="math inline">\(\hat{\delta}_0 = \hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\delta}_1 = \hat{\beta}_1\)</span>, the residuals would be exactly the same which implies that <span class="math inline">\(SSR\)</span> would be exactly the same across each model. Now, for the second model, if we estimate some any value of <span class="math inline">\(\hat{\delta}_2 \neq 0\)</span>, the reason why we would estimate that is because it makes the <span class="math inline">\(SSR\)</span> of the second model even lower.</p>
<p>This discussion means that, if we estimate <span class="math inline">\(\hat{\delta}_2=0\)</span>, then <span class="math inline">\(SSR\)</span> wil be the same across the first and second model, and that if we estimate any other value for <span class="math inline">\(\hat{\delta}_2\)</span>, then <span class="math inline">\(SSR\)</span> for the second model will be lower than for the first model.</p>
</div>
</div>
<div id="adjusted-r2" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Adjusted <span class="math inline">\(R^2\)</span><a href="#adjusted-r2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 6.4</p>
<p>Given the above discussion, we would like to have a way to choose a model that doesn’t necessarily always select the most complicated model.</p>
<p>A main way to do this is to add a <strong>penalty</strong> to adding more regressors to the model. One version of this is called <strong>adjusted <span class="math inline">\(R^2\)</span></strong>, which we will write as <span class="math inline">\(\bar{R}^2\)</span>. It is defined as</p>
<p><span class="math display">\[
  \bar{R}^2 := 1 - \underbrace{\frac{(n-1)}{(n-k)}}_{\textrm{penalty}} \frac{SSR}{TSS}
\]</span>
where <span class="math inline">\(k\)</span> is the number of regressors included in the model (note: we also count the intercept as a regressor here; so for example, in the regression <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>, <span class="math inline">\(k=3\)</span>). Notice, if the penalty term were equal to 1, then this would just be <span class="math inline">\(R^2\)</span>.</p>
<p>Now, let’s think about what happens to <span class="math inline">\(\bar{R}^2\)</span> when you add a new regressor to the model.</p>
<ul>
<li><p><span class="math inline">\(SSR \downarrow \implies \bar{R}^2 \uparrow\)</span></p></li>
<li><p><span class="math inline">\((n-k-1) \downarrow \implies \bar{R}^2 \downarrow\)</span></p></li>
</ul>
<p>Thus, there is a “benefit” and a “cost” to adding a new regressor. If you are choosing among several models based on <span class="math inline">\(\bar{R}^2\)</span>, you would choose the model that has the highest <span class="math inline">\(\bar{R}^2\)</span>. And the logic is this: if the regressor greatly decreases <span class="math inline">\(SSR\)</span>, then the “benefit” of adding that regressor will tend to outweigh the “cost”. On the other hand, if the regressor has little effect on <span class="math inline">\(SSR\)</span>, then the “benefit” of adding that regressor will tend to be smaller than the “cost”.</p>
</div>
<div id="aic-bic" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> AIC, BIC<a href="#aic-bic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are other approaches to model selection that follow a similar idea. Two of the most common ones are the Akaike Information Criteria (AIC) and the Bayesian Information Criteria (BIC). These tend to be somewhat better choices for model selection than <span class="math inline">\(\bar{R}^2\)</span>.</p>
<p>These are given by</p>
<ul>
<li><span class="math inline">\(AIC = 2k + n \log(SSR)\)</span></li>
<li><span class="math inline">\(BIC = k \log(n) + n \log(SSR)\)</span></li>
</ul>
<p>For both AIC and BIC, you would choose the model that minimizes these.</p>
</div>
<div id="cross-validation" class="section level3 hasAnchor" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> Cross-Validation<a href="#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another common way to choose a model is called <strong>cross-validation</strong>. The idea is to mimic the out-of-sample prediction problem using the data that we have access to.</p>
<p>In particular, one simple version of this is to split your data into two parts: these are usually called the <strong>training sample</strong> and the <strong>testing sample</strong>. Then, estimate all models under consideration using the training sample. Given the estimated values of the parameters, then predict the outcomes for observations in the testing sample; and compare these predictions to the actual outcomes in the testing sample. Choose the model that minimizes the squared prediction error in the testing sample.</p>
<p>Cross-validation is a somewhat more complicated version of the same idea. Basically, we will <em>repeatedly</em> split the data into a training sample and a testing sample, and get predictions for all observations in our data (instead of just for a held-out testing sample).</p>
<p>Here is an algorithm for cross-validation:</p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Split the data into J <strong>folds</strong></p></li>
<li><p>For the jth fold, do the following:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the model using all observations <em>not in</em> the Jth fold (<span class="math inline">\(\implies\)</span> we obtain estimates <span class="math inline">\(\hat{\beta}_0^j, \hat{\beta}_1^j, \ldots, \hat{\beta}_k^j\)</span>)</p></li>
<li><p>Predict outcomes for observations in the Jth fold using the estimated model from part (1):
<span class="math display">\[\begin{align*}
    \tilde{Y}_{ij} = \hat{\beta}_0^j + \hat{\beta}_1^j X_{1ij} + \cdots + \hat{\beta_k^j} X_{kij}
  \end{align*}\]</span></p></li>
<li><p>Compute the prediction error:
<span class="math display">\[\begin{align*}
    \tilde{U}_{ij} = Y_{ij} - \tilde{Y}_{ij}
  \end{align*}\]</span>
(this is the difference between actual outcomes for individuals in the Jth fold and their predicted outcome based on the model from part (1))</p></li>
</ol></li>
<li><p>Do steps 1-3 for all <span class="math inline">\(J\)</span> folds. This gives a prediction error <span class="math inline">\(\tilde{U}_i\)</span> for each observation in the data</p></li>
<li><p>compute the cross validation criteria (mean squared prediction error):
<span class="math display">\[\begin{align*}
CV = \frac{1}{n} \sum_{i=1}^n \tilde{U}_{i}^2
  \end{align*}\]</span></p></li>
<li><p>choose the model that produces the smallest value of <span class="math inline">\(CV\)</span>.</p></li>
</ul></li>
</ul>
<p>Cross-validation is a good way to choose a model, but it can sometimes be computationally challenging (because you are estimating lots of models) — particularly, if the models under consideration are very complicated or there a large number of observations.</p>
<p>Above, we split the data into <span class="math inline">\(J\)</span> folds. This introduces some randomness into our model selection criteria. In particular, if you split the data in a different way from how I split the data, then we could choose different models. This is a somewhat undesirable feature of a model selection criteria. One way to get around this is to set <span class="math inline">\(J=n\)</span>. This makes it so that every observation has its own fold. This is called <strong>leave-one-out cross-validation</strong>. In this case, there is no randomness in the model selection criteria. The drawback is that it is more computational — in this case, you need to estimate the model <span class="math inline">\(n\)</span> times rather than <span class="math inline">\(J\)</span> times.</p>
</div>
<div id="model-averaging" class="section level3 hasAnchor" number="6.2.5">
<h3><span class="header-section-number">6.2.5</span> Model Averaging<a href="#model-averaging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the case where you are considering a large number of possible models, it is pretty common that a number of models will, by any of the above model selection criteria, be expected to perform very similarly when making predictions.</p>
<p>In this case, one strategy that usually does well in terms of making out-of-sample predictions is <em>model averaging</em>.</p>
<p>Suppose you have <span class="math inline">\(M\)</span> different models, and that each model can produce a predicted value for <span class="math inline">\(Y_i\)</span> — let’s call the predicted value from model <span class="math inline">\(m\)</span>, <span class="math inline">\(\hat{Y}_i^m\)</span>. Model averaging would involve obtaining a new predicted value, call it <span class="math inline">\(\hat{Y}_i\)</span> by computing
<span class="math display">\[\begin{align*}
  \hat{Y}_i = \frac{1}{M} \sum_{m=1}^M \hat{Y}_i^m
\end{align*}\]</span></p>
<ul>
<li>Usually, you would throw out models that you know predict poorly and only average together ones that perform reasonably well.</li>
</ul>
</div>
<div id="computation-8" class="section level3 hasAnchor" number="6.2.6">
<h3><span class="header-section-number">6.2.6</span> Computation<a href="#computation-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(R^2\)</span> and <span class="math inline">\(\bar{R}^2\)</span> are both reported as output from <code>R</code>’s <code>lm</code> command.</p>
<p>It is straightforward (and a useful exercise) to compute <span class="math inline">\(TSS, ESS,\)</span> and <span class="math inline">\(SSR\)</span> directly. It is also straightforward to calculate <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span> — there are probably packages that will do this for you, but they are so easy that I suggest just calculating on your own.</p>
<p>The same applies to cross validation. I suspect that there are packages available that will do this for you, but I think these are useful coding exercises and not all that difficult. Also, if you do this yourself, it removes any kinds of “black box” issues from downloading <code>R</code> code where it may not be clear exactly what it is doing.</p>
</div>
</div>
<div id="machine-learning" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Machine Learning<a href="#machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 14.1, 14.2, 14.6</p>
<p>Some extra resources on estimating Lasso and Ridge regressions in R:</p>
<ul>
<li><p><a href="https://www.statology.org/lasso-regression-in-r/">glmnet tutorial</a></p></li>
<li><p><a href="https://cran.r-project.org/web/packages/glmnetUtils/vignettes/intro.html">glmnetUtils vignette</a></p></li>
</ul>
<p>“Big” Data typically means one of two things:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(n\)</span> — the number of observations is extremely large</p></li>
<li><p><span class="math inline">\(k\)</span> — the number of regressors is very large</p></li>
</ol>
<p><span class="math inline">\(n\)</span> being large is more of a computer science problem. That said, there are economists who think about these issues, but I think it is still the case that it is relatively uncommon for an economist to have <em>so much</em> data that they have trouble computing their estimators.</p>
<p>We’ll focus on the second issue — where <span class="math inline">\(k\)</span> is large. A main reason that this may occur in applications is that we may be unsure of the functional form for <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span>. Should it include higher order terms like <span class="math inline">\(X_1^2\)</span> or <span class="math inline">\(X_1^3\)</span>? Should it include interactions like <span class="math inline">\(X_1 X_2\)</span>. Once you start proceeding this way, even if you only have 5-10 actual covariates, <span class="math inline">\(k\)</span> can become quite large.</p>
<p>As in the case of the mean, perhaps we can improve predictions by introducing some bias while simultaneously decreasing the variance of our predictions.</p>
<p>Let’s suppose that we have some function that can take in regressors and makes predictions of the outcome; we’ll call this function <span class="math inline">\(\hat{f}\)</span> (where the “hat” indicates that we’ll typically estimate this function). An example would just be a regression where, for example, <span class="math inline">\(\hat{f}(X) = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_3\)</span>. One way to evaluate how well <span class="math inline">\(\hat{f}\)</span> makes predictions is by considering its <strong>mean squared prediction error</strong>:</p>
<p><span class="math display">\[
  MSPE := \mathbb{E}\left[ (Y - \hat{f}(X))^2 \right]
\]</span>
<span class="math inline">\(MSPE\)</span> quantifies the mean “distance” between our predictions and actual outcomes.</p>
<p>Recall that, if we could just pick any function to minimize <span class="math inline">\(MSPE\)</span>, we would set <span class="math inline">\(\hat{f}(X) = \mathbb{E}[Y|X]\)</span>, but generally we do not just know what <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is. We can “decompose” MSPE into several underlying conditions</p>
<p><span class="math display">\[
  \begin{aligned}
  MSPE &amp;= \mathbb{E}\left[ \left( (Y - \mathbb{E}[Y|X]) - (\hat{f}(X) - \mathbb{E}[\hat{f}(X)|X]) - (\mathbb{E}[\hat{f}(X)|X] - \mathbb{E}[Y|X]) \right)^2 \right] \\
  &amp;= \mathbb{E}\left[ (Y-\mathbb{E}[Y|X])^2 \right] + \mathbb{E}\left[ (\hat{f}(X) - \mathbb{E}[\hat{f}(X)|X])^2\right] + \mathbb{E}\left[ (\mathbb{E}[\hat{f}(X)|X] - \mathbb{E}[Y|X])^2 \right] \\
  &amp;= \mathrm{var}(U) + \mathbb{E}[\mathrm{var}(\hat{f}(X)|X)] + \mathbb{E}\left[\textrm{Bias}(\hat{f}(X))^2\right]
  \end{aligned}
\]</span>
where, to understand this decomposition, the first equality just adds and subtracts <span class="math inline">\(\mathbb{E}[Y|X]\)</span> and <span class="math inline">\(\mathbb{E}[\hat{f}(X)|X]\)</span>. The second equality squares the long expression from the first equality and cancels some terms. The third equality holds where <span class="math inline">\(U := Y-\mathbb{E}[Y|X]\)</span>, by the definition of (conditional) variance, and we call the last bias because it is the difference between the mean of our prediction function <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(\mathbb{E}[Y|X]\)</span>.</p>
<p>You can think of the term <span class="math inline">\(\mathrm{var}(U)\)</span> as being irreducible prediction error — even if we knew <span class="math inline">\(\mathbb{E}[Y|X]\)</span>, we wouldn’t get every prediction exactly right. But the other two terms come from having to estimate <span class="math inline">\(\hat{f}\)</span>. The term <span class="math inline">\(\mathbb{E}[\mathrm{var}(\hat{f}(X)X)]\)</span> is the average variance of our predictions across different values of <span class="math inline">\(X\)</span>. The term <span class="math inline">\(\mathbb{E}\left[\textrm{Bias}(\hat{f}(X))^2\right]\)</span> is the squared bias of our predictions averaged across different values of <span class="math inline">\(X\)</span>. Many machine learning estimators have the property of being biased (so that the last term is not equal to 0), but having reduced variance relative to OLS estimators.</p>
<p>To do this, we’ll estimate the parameters of the model in a similar way to what we have done before except that we’ll add a <strong>penalty</strong> term that gets larger as parameter estimates move further away from 0. In particular, we consider estimates of the form</p>
<p><span class="math display">\[
  (\hat{\beta}_1, \hat{\beta}_2, \ldots, \hat{\beta}_k) = \underset{b_1,\ldots,b_k}{\textrm{argmin}} \sum_{i=1}^n  (Y_i - b_1 X_{1i} - \cdots - b_k X_{ki})^2 + \textrm{penalty}
\]</span></p>
<p>Lasso and ridge regression, which are the two approaches to machine learning that we will talk about below, amount to different choices of the penalty term.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> Generally, you should “standardize” the regressors before implementing Lasso or Ridge regression. The <code>glmnet</code> package does this for you by default.</p>
</div>
<div id="lasso" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Lasso<a href="#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 14.3</p>
<p>For Lasso, the penalty term is given by</p>
<p><span class="math display">\[
  \lambda \sum_{j=1}^k |b_j|
\]</span>
The absolute value on each of the <span class="math inline">\(b_j\)</span> terms means that the penalty function gets larger for larger values of any <span class="math inline">\(b_j\)</span>. Since we are trying to minimize the objective function, this means that there is a “cost” to choosing a larger value of <span class="math inline">\(b_j\)</span> relative to OLS. Thus, Lasso estimates tend to be “shrunk” towards 0.</p>
<p><span class="math inline">\(\lambda\)</span> is called a <strong>tuning parameter</strong> — larger values of <span class="math inline">\(\lambda\)</span> imply that the penalty term is more important. Notice that, if you send <span class="math inline">\(\lambda \rightarrow \infty\)</span>, then the penalty term will be so large that you would set all the parameters to be equal to 0. On the other hand, if you set <span class="math inline">\(\lambda=0\)</span>, then you will get the OLS estimates (because there will be no penalty in this case). In general, it is hard to know what is the “right” value for <span class="math inline">\(\lambda\)</span>, and it is typically chosen using cross validation (this will be done automatically for you using the <code>glmnet</code> package).</p>
</div>
<div id="ridge-regression" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Ridge Regression<a href="#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 14.4</p>
<p>For Ridge, the penalty term is given by</p>
<p><span class="math display">\[
  \lambda \sum_{j=1}^k b_j^2
\]</span>
Much of the discussion from Lasso applies here. The only difference is that the form of the penalty is different here: <span class="math inline">\(b_j^2\)</span> instead of <span class="math inline">\(|b_j|\)</span>. Relative to the Lasso penalty, the Ridge penalty “dislikes more” very large values of <span class="math inline">\(b_j\)</span>.</p>
<p><strong>Comparing Lasso and Ridge</strong></p>
<ul>
<li><p>Both <strong>shrink</strong> the estimated parameters towards 0. This tends to introduce bias into our predictions but comes with the benefit of reducing the variance of the predictions.</p></li>
<li><p>Interestingly, both Lasso and Ridge can be implemented when <span class="math inline">\(k &gt; n\)</span> (i.e., if you have more regressors than observations). This is in contrast to to OLS, where the parameters cannot be estimated in this case.</p></li>
<li><p>Both are generally not very computationally intensive. For ridge regression, we can in fact derive an explicit expression for the estimated <span class="math inline">\(\beta\)</span>’s. We cannot do this with Lasso; a full discussion of how Lasso actually estimates the parameters is beyond the scope of our course, but, suffice it to say, that you can generally compute Lasso estimates quickly.</p></li>
<li><p>Lasso also performs “model selection” — that is, if you use the Lasso, many of the estimated parameters will be set equal to 0. This can sometimes be an advantage. Ridge (like OLS) will generally produce non-zero estimates of all parameters in the model.</p></li>
</ul>
</div>
<div id="computation-9" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Computation<a href="#computation-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Computing Lasso and Ridge regressions is somewhat more complicated than most other things that we have computed this semester. The main <code>R</code> package for Lasso and Ridge regressions is the <code>glmnet</code> package. For some reason, the syntax of the package is somewhat different from, for example, the <code>lm</code> command. In my view, it is often easier to use the <code>glmnetUtils</code> package, which seems to be just a wrapper for <code>glmnet</code> but with functions that are analogous to <code>lm</code>.</p>
<p>I’m just going to sketch here how you would use these functions to estimate a Lasso or Ridge regression. In the lab later on in this chapter, we’ll do a more concrete example. Suppose that you have access to a training dataset called <code>train</code> and a testing dataset called <code>test</code>, you can use the <code>glmnetUtils</code> package in the following way:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" tabindex="-1"></a><span class="fu">library</span>(glmnetUtils)</span>
<span id="cb137-2"><a href="#cb137-2" tabindex="-1"></a>lasso_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data=</span>train, <span class="at">use.model.frame=</span><span class="cn">TRUE</span>) <span class="co"># or whatever formula you want to use</span></span>
<span id="cb137-3"><a href="#cb137-3" tabindex="-1"></a><span class="fu">coef</span>(lasso_model) <span class="co"># if you are interested in estimated coefficients</span></span>
<span id="cb137-4"><a href="#cb137-4" tabindex="-1"></a><span class="fu">predict</span>(lasso_model, <span class="at">newdata=</span>test) <span class="co"># get predictions</span></span>
<span id="cb137-5"><a href="#cb137-5" tabindex="-1"></a></span>
<span id="cb137-6"><a href="#cb137-6" tabindex="-1"></a>ridge_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, </span>
<span id="cb137-7"><a href="#cb137-7" tabindex="-1"></a>                         <span class="at">data=</span>train, </span>
<span id="cb137-8"><a href="#cb137-8" tabindex="-1"></a>                         <span class="at">alpha=</span><span class="dv">0</span>,</span>
<span id="cb137-9"><a href="#cb137-9" tabindex="-1"></a>                         <span class="at">use.model.frame=</span><span class="cn">TRUE</span>)</span>
<span id="cb137-10"><a href="#cb137-10" tabindex="-1"></a><span class="fu">coef</span>(ridge_model)</span>
<span id="cb137-11"><a href="#cb137-11" tabindex="-1"></a><span class="fu">predict</span>(ridge_model, <span class="at">newdata=</span>test)</span></code></pre></div>
<p>It’s worth making a couple of comments about this</p>
<ul>
<li><p>In terms of code, the only difference between the Lasso and Ridge, is that for Ridge, we added the extra argument <code>alpha=0</code>. The default value of <code>alpha</code> is 1, and that leads to a Lasso regression (which is why didn’t need to specify it for our Lasso estimates). If you are interested, you can read more details about this in the documentation for <code>glmnet</code> using <code>?glmnet</code>.</p></li>
<li><p>Generally, if you are going to use Lasso or Ridge, then you would have many more regressors to include than just <code>X1 + X2 + X3</code>. One common way that you get more regressors is when you start to thinking about including higher order terms or interaction terms. One way to do this quickly is to use the <code>poly</code> function inside the formula. For example,</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" tabindex="-1"></a>lasso_model2 <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Y <span class="sc">~</span> <span class="fu">poly</span>(X1, X2, X3, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">raw=</span><span class="cn">TRUE</span>), </span>
<span id="cb138-2"><a href="#cb138-2" tabindex="-1"></a>                          <span class="at">data=</span>train, </span>
<span id="cb138-3"><a href="#cb138-3" tabindex="-1"></a>                          <span class="at">use.model.frame=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>would include all interactions between <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> as well as squared terms for each; if you wanted to include more interactions and cubic terms, you could specify <code>degree=3</code>.</p></li>
</ul>
</div>
</div>
<div id="lab-5-predicting-diamond-prices" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Lab 5: Predicting Diamond Prices<a href="#lab-5-predicting-diamond-prices" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this lab, we will try our hand at predicted diamond prices. We will use the data set <code>diamond_train</code> (which contains around 40,000 observations) and then see how well we can predict data from the <code>diamond_test</code> data.</p>
<ol style="list-style-type: decimal">
<li><p>Estimate a model for <span class="math inline">\(price\)</span> on <span class="math inline">\(carat\)</span>, <span class="math inline">\(cut\)</span>, and <span class="math inline">\(clarity\)</span>. Report <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\bar{R}^2\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> for this model.</p></li>
<li><p>Estimate a model for <span class="math inline">\(price\)</span> on <span class="math inline">\(carat\)</span>, <span class="math inline">\(cut\)</span>, <span class="math inline">\(clarity\)</span>, <span class="math inline">\(depth\)</span>, <span class="math inline">\(table\)</span>, <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span>. Report <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\bar{R}^2\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> for this model.</p></li>
<li><p>Choose any model that you would like for <span class="math inline">\(price\)</span> and report <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\bar{R}^2\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> for this model. We’ll see if your model can predict better than either of the first two.</p></li>
<li><p>Use 10-fold cross validation to report an estimate of mean squared prediction error for each of the models from 1-3.</p></li>
<li><p>Based on your responses to parts 1-4, which model do you think will predict the best?</p></li>
<li><p>Use <code>diamond_test</code> to calculate (out-of-sample) mean squared prediction error for each of the three models from 1-3. Which model performs the best out-of-sample? How does this compare to your answer from 5.</p></li>
<li><p>Use the Lasso and Ridge regression on <code>diamond_train</code> data. Evaluate the predictions from each of these models by computing (out-of-sample) mean squared prediction error. How well did these models predict relative to each other and relative the models from 1-3.</p></li>
</ol>
</div>
<div id="lab-5-solutions" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Lab 5: Solutions<a href="#lab-5-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/diamond_train.RData&quot;</span>)</span>
<span id="cb139-2"><a href="#cb139-2" tabindex="-1"></a></span>
<span id="cb139-3"><a href="#cb139-3" tabindex="-1"></a><span class="co"># formulas</span></span>
<span id="cb139-4"><a href="#cb139-4" tabindex="-1"></a>formla1 <span class="ot">&lt;-</span> price <span class="sc">~</span> carat <span class="sc">+</span> <span class="fu">as.factor</span>(cut) <span class="sc">+</span> <span class="fu">as.factor</span>(clarity)</span>
<span id="cb139-5"><a href="#cb139-5" tabindex="-1"></a>formla2 <span class="ot">&lt;-</span> price <span class="sc">~</span> carat <span class="sc">+</span> <span class="fu">as.factor</span>(cut) <span class="sc">+</span> <span class="fu">as.factor</span>(clarity) <span class="sc">+</span> depth <span class="sc">+</span> table <span class="sc">+</span> x <span class="sc">+</span> y <span class="sc">+</span> x</span>
<span id="cb139-6"><a href="#cb139-6" tabindex="-1"></a>formla3 <span class="ot">&lt;-</span> price <span class="sc">~</span> (carat <span class="sc">+</span> <span class="fu">as.factor</span>(cut) <span class="sc">+</span> <span class="fu">as.factor</span>(clarity) <span class="sc">+</span> depth <span class="sc">+</span> table <span class="sc">+</span> x <span class="sc">+</span> y <span class="sc">+</span> x)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb139-7"><a href="#cb139-7" tabindex="-1"></a></span>
<span id="cb139-8"><a href="#cb139-8" tabindex="-1"></a><span class="co"># estimate each model</span></span>
<span id="cb139-9"><a href="#cb139-9" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla1, <span class="at">data=</span>diamond_train)</span>
<span id="cb139-10"><a href="#cb139-10" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla2, <span class="at">data=</span>diamond_train)</span>
<span id="cb139-11"><a href="#cb139-11" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla3, <span class="at">data=</span>diamond_train)</span>
<span id="cb139-12"><a href="#cb139-12" tabindex="-1"></a></span>
<span id="cb139-13"><a href="#cb139-13" tabindex="-1"></a>mod_sel <span class="ot">&lt;-</span> <span class="cf">function</span>(formla) {</span>
<span id="cb139-14"><a href="#cb139-14" tabindex="-1"></a>  mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla, <span class="at">data=</span>diamond_train)</span>
<span id="cb139-15"><a href="#cb139-15" tabindex="-1"></a>  r.squared <span class="ot">&lt;-</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span>
<span id="cb139-16"><a href="#cb139-16" tabindex="-1"></a>  adj.r.squared <span class="ot">&lt;-</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>adj.r.squared</span>
<span id="cb139-17"><a href="#cb139-17" tabindex="-1"></a></span>
<span id="cb139-18"><a href="#cb139-18" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(diamond_train)</span>
<span id="cb139-19"><a href="#cb139-19" tabindex="-1"></a>  uhat <span class="ot">&lt;-</span> <span class="fu">resid</span>(mod)</span>
<span id="cb139-20"><a href="#cb139-20" tabindex="-1"></a>  ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>(uhat<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb139-21"><a href="#cb139-21" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">coef</span>(mod))</span>
<span id="cb139-22"><a href="#cb139-22" tabindex="-1"></a>  aic <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>k <span class="sc">+</span> n<span class="sc">*</span><span class="fu">log</span>(ssr)</span>
<span id="cb139-23"><a href="#cb139-23" tabindex="-1"></a>  bic <span class="ot">&lt;-</span> k<span class="sc">*</span><span class="fu">log</span>(n) <span class="sc">+</span> n<span class="sc">*</span><span class="fu">log</span>(ssr)</span>
<span id="cb139-24"><a href="#cb139-24" tabindex="-1"></a></span>
<span id="cb139-25"><a href="#cb139-25" tabindex="-1"></a>  <span class="co"># show results</span></span>
<span id="cb139-26"><a href="#cb139-26" tabindex="-1"></a>  result <span class="ot">&lt;-</span> tidyr<span class="sc">::</span><span class="fu">tibble</span>(r.squared, adj.r.squared, aic, bic)</span>
<span id="cb139-27"><a href="#cb139-27" tabindex="-1"></a>  <span class="fu">return</span>(result)</span>
<span id="cb139-28"><a href="#cb139-28" tabindex="-1"></a>}</span>
<span id="cb139-29"><a href="#cb139-29" tabindex="-1"></a></span>
<span id="cb139-30"><a href="#cb139-30" tabindex="-1"></a>res1 <span class="ot">&lt;-</span> <span class="fu">mod_sel</span>(formla1)</span>
<span id="cb139-31"><a href="#cb139-31" tabindex="-1"></a>res2 <span class="ot">&lt;-</span> <span class="fu">mod_sel</span>(formla2)</span>
<span id="cb139-32"><a href="#cb139-32" tabindex="-1"></a>res3 <span class="ot">&lt;-</span> <span class="fu">mod_sel</span>(formla3)</span>
<span id="cb139-33"><a href="#cb139-33" tabindex="-1"></a></span>
<span id="cb139-34"><a href="#cb139-34" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rbind.data.frame</span>(res1, res2, res3),<span class="dv">3</span>)</span>
<span id="cb139-35"><a href="#cb139-35" tabindex="-1"></a><span class="co">#&gt; # A tibble: 3 × 4</span></span>
<span id="cb139-36"><a href="#cb139-36" tabindex="-1"></a><span class="co">#&gt;   r.squared adj.r.squared      aic      bic</span></span>
<span id="cb139-37"><a href="#cb139-37" tabindex="-1"></a><span class="co">#&gt;       &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb139-38"><a href="#cb139-38" tabindex="-1"></a><span class="co">#&gt; 1     0.898         0.898 1104188. 1104301.</span></span>
<span id="cb139-39"><a href="#cb139-39" tabindex="-1"></a><span class="co">#&gt; 2     0.9           0.9   1103499. 1103647.</span></span>
<span id="cb139-40"><a href="#cb139-40" tabindex="-1"></a><span class="co">#&gt; 3     0.928         0.928 1089372. 1090328.</span></span>
<span id="cb139-41"><a href="#cb139-41" tabindex="-1"></a></span>
<span id="cb139-42"><a href="#cb139-42" tabindex="-1"></a><span class="co"># k-fold cross validation</span></span>
<span id="cb139-43"><a href="#cb139-43" tabindex="-1"></a></span>
<span id="cb139-44"><a href="#cb139-44" tabindex="-1"></a><span class="co"># setup data</span></span>
<span id="cb139-45"><a href="#cb139-45" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb139-46"><a href="#cb139-46" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(diamond_train)</span>
<span id="cb139-47"><a href="#cb139-47" tabindex="-1"></a>diamond_train<span class="sc">$</span>fold <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="at">size=</span>n, <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb139-48"><a href="#cb139-48" tabindex="-1"></a>diamond_train<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n</span>
<span id="cb139-49"><a href="#cb139-49" tabindex="-1"></a></span>
<span id="cb139-50"><a href="#cb139-50" tabindex="-1"></a>cv_mod_sel <span class="ot">&lt;-</span> <span class="cf">function</span>(formla) {</span>
<span id="cb139-51"><a href="#cb139-51" tabindex="-1"></a>  u.squared <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, n)</span>
<span id="cb139-52"><a href="#cb139-52" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb139-53"><a href="#cb139-53" tabindex="-1"></a>    this.train <span class="ot">&lt;-</span> <span class="fu">subset</span>(diamond_train, fold <span class="sc">!=</span> i)</span>
<span id="cb139-54"><a href="#cb139-54" tabindex="-1"></a>    this.test <span class="ot">&lt;-</span> <span class="fu">subset</span>(diamond_train, fold <span class="sc">==</span> i)</span>
<span id="cb139-55"><a href="#cb139-55" tabindex="-1"></a>    this.id <span class="ot">&lt;-</span> this.test<span class="sc">$</span>id</span>
<span id="cb139-56"><a href="#cb139-56" tabindex="-1"></a>    cv_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla,</span>
<span id="cb139-57"><a href="#cb139-57" tabindex="-1"></a>              <span class="at">data=</span>this.train)</span>
<span id="cb139-58"><a href="#cb139-58" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv_reg, <span class="at">newdata=</span>this.test)</span>
<span id="cb139-59"><a href="#cb139-59" tabindex="-1"></a>    u <span class="ot">&lt;-</span> this.test<span class="sc">$</span>price <span class="sc">-</span> pred</span>
<span id="cb139-60"><a href="#cb139-60" tabindex="-1"></a>    u.squared[this.id] <span class="ot">&lt;-</span> u<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb139-61"><a href="#cb139-61" tabindex="-1"></a>  }</span>
<span id="cb139-62"><a href="#cb139-62" tabindex="-1"></a>  cv <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u.squared))</span>
<span id="cb139-63"><a href="#cb139-63" tabindex="-1"></a>  <span class="fu">return</span>(cv)</span>
<span id="cb139-64"><a href="#cb139-64" tabindex="-1"></a>}</span>
<span id="cb139-65"><a href="#cb139-65" tabindex="-1"></a></span>
<span id="cb139-66"><a href="#cb139-66" tabindex="-1"></a>cv_res1 <span class="ot">&lt;-</span> <span class="fu">cv_mod_sel</span>(formla1)</span>
<span id="cb139-67"><a href="#cb139-67" tabindex="-1"></a>cv_res2 <span class="ot">&lt;-</span> <span class="fu">cv_mod_sel</span>(formla2)</span>
<span id="cb139-68"><a href="#cb139-68" tabindex="-1"></a>cv_res3 <span class="ot">&lt;-</span> <span class="fu">cv_mod_sel</span>(formla3)</span>
<span id="cb139-69"><a href="#cb139-69" tabindex="-1"></a></span>
<span id="cb139-70"><a href="#cb139-70" tabindex="-1"></a>res1 <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(res1, <span class="at">cv=</span>cv_res1)</span>
<span id="cb139-71"><a href="#cb139-71" tabindex="-1"></a>res2 <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(res2, <span class="at">cv=</span>cv_res2)</span>
<span id="cb139-72"><a href="#cb139-72" tabindex="-1"></a>res3 <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(res3, <span class="at">cv=</span>cv_res3)</span>
<span id="cb139-73"><a href="#cb139-73" tabindex="-1"></a></span>
<span id="cb139-74"><a href="#cb139-74" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rbind.data.frame</span>(res1, res2, res3),<span class="dv">3</span>)</span>
<span id="cb139-75"><a href="#cb139-75" tabindex="-1"></a><span class="co">#&gt;   r.squared adj.r.squared     aic     bic       cv</span></span>
<span id="cb139-76"><a href="#cb139-76" tabindex="-1"></a><span class="co">#&gt; 1     0.898         0.898 1104188 1104301 1366.021</span></span>
<span id="cb139-77"><a href="#cb139-77" tabindex="-1"></a><span class="co">#&gt; 2     0.900         0.900 1103499 1103647 1397.104</span></span>
<span id="cb139-78"><a href="#cb139-78" tabindex="-1"></a><span class="co">#&gt; 3     0.928         0.928 1089372 1090328 2247.940</span></span>
<span id="cb139-79"><a href="#cb139-79" tabindex="-1"></a></span>
<span id="cb139-80"><a href="#cb139-80" tabindex="-1"></a><span class="co"># lasso and ridge</span></span>
<span id="cb139-81"><a href="#cb139-81" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb139-82"><a href="#cb139-82" tabindex="-1"></a><span class="fu">library</span>(glmnetUtils)</span>
<span id="cb139-83"><a href="#cb139-83" tabindex="-1"></a></span>
<span id="cb139-84"><a href="#cb139-84" tabindex="-1"></a>lasso_res <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(formla3, <span class="at">data=</span>diamond_train, <span class="at">alpha=</span><span class="dv">1</span>)</span>
<span id="cb139-85"><a href="#cb139-85" tabindex="-1"></a>ridge_res <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(formla3, <span class="at">data=</span>diamond_train, <span class="at">alpha=</span><span class="dv">0</span>)</span>
<span id="cb139-86"><a href="#cb139-86" tabindex="-1"></a></span>
<span id="cb139-87"><a href="#cb139-87" tabindex="-1"></a><span class="co"># out of sample predictions</span></span>
<span id="cb139-88"><a href="#cb139-88" tabindex="-1"></a></span>
<span id="cb139-89"><a href="#cb139-89" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/diamond_test.RData&quot;</span>)</span>
<span id="cb139-90"><a href="#cb139-90" tabindex="-1"></a></span>
<span id="cb139-91"><a href="#cb139-91" tabindex="-1"></a><span class="co"># compute prediction errors with test data</span></span>
<span id="cb139-92"><a href="#cb139-92" tabindex="-1"></a>u1 <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(mod1, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb139-93"><a href="#cb139-93" tabindex="-1"></a>u2 <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb139-94"><a href="#cb139-94" tabindex="-1"></a>u3 <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(mod3, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb139-95"><a href="#cb139-95" tabindex="-1"></a>u_lasso <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(lasso_res, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb139-96"><a href="#cb139-96" tabindex="-1"></a>u_ridge <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(ridge_res, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb139-97"><a href="#cb139-97" tabindex="-1"></a></span>
<span id="cb139-98"><a href="#cb139-98" tabindex="-1"></a><span class="co"># compute root mean squared prediction errors</span></span>
<span id="cb139-99"><a href="#cb139-99" tabindex="-1"></a>rmspe1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb139-100"><a href="#cb139-100" tabindex="-1"></a>rmspe2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb139-101"><a href="#cb139-101" tabindex="-1"></a>rmspe3 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u3<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb139-102"><a href="#cb139-102" tabindex="-1"></a>rmspe_lasso <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u_lasso<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb139-103"><a href="#cb139-103" tabindex="-1"></a>rmspe_ridge <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u_ridge<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb139-104"><a href="#cb139-104" tabindex="-1"></a></span>
<span id="cb139-105"><a href="#cb139-105" tabindex="-1"></a><span class="co"># report results</span></span>
<span id="cb139-106"><a href="#cb139-106" tabindex="-1"></a>rmspe <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">mod1=</span>rmspe1, <span class="at">mod2=</span>rmspe2, <span class="at">mod3=</span>rmspe3, <span class="at">lasso=</span>rmspe_lasso, <span class="at">ridge=</span>rmspe_ridge)</span>
<span id="cb139-107"><a href="#cb139-107" tabindex="-1"></a><span class="fu">round</span>(rmspe,<span class="dv">3</span>)</span>
<span id="cb139-108"><a href="#cb139-108" tabindex="-1"></a><span class="co">#&gt;      mod1    mod2    mod3  lasso   ridge</span></span>
<span id="cb139-109"><a href="#cb139-109" tabindex="-1"></a><span class="co">#&gt; 1 856.014 785.171 616.996 683.36 794.276</span></span></code></pre></div>
</div>
<div id="extra-questions-3" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Extra Questions<a href="#extra-questions-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>What are some drawbacks of using <span class="math inline">\(R^2\)</span> as a model selection tool?</p></li>
<li><p>For AIC and BIC, we choose the model that minimizes these (rather than maximizes them). Why?</p></li>
<li><p>Does AIC or BIC tend to pick “more complicated” models? What is the reason for this?</p></li>
<li><p>Suppose you are interested in predicting some outcome <span class="math inline">\(Y\)</span> and have access to covariates <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>. You estimate the following two models
<span class="math display">\[\begin{align*}
  Y &amp;= 30 + 4 X_1 - 2 X_2 - 10 X_3, \qquad R^2=0.5, AIC=3421 \\
  Y &amp;= 9 + 2 X_1 - 3 X_2 - 2 X_3 + 2 X_1^2 + 1 X_2^2 - 4 X_3^2 + 2 X_1 X_2, \qquad R^2 = 0.75, AIC=4018
\end{align*}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Which model seems to be predicting better? Explain why.</p></li>
<li><p>Using the model that is predicting better, what would be your prediction for <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1=10, X_2=1, X_3=5\)</span>?</p></li>
</ol></li>
<li><p>In Lasso and Ridge regressions, it is common to “standardize” the regressors before estimating the model (e.g., the <code>glmnet</code> does this automatically for you). What is the reason for doing this?</p></li>
<li><p>In Lasso and Ridge regressions, the penalty term lead to “shrinking” the estimated parameters in the model towards 0. This tends to introduce bias while reducing variance. Why can introducing bias while reducing variance potentially lead to better predictions? Does this argument always apply or just apply in some cases? Explain.</p></li>
<li><p>In Lasso and Ridge regressions, the penalty term depends on the tuning parameter <span class="math inline">\(\lambda\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>How is this tuning parameter often chosen in practice? Why does it make sense to choose it in this way?</p></li>
<li><p>What would happen to the estimated coefficients when <span class="math inline">\(\lambda=0\)</span>?</p></li>
<li><p>What would happen to the estimated coefficients as <span class="math inline">\(\lambda \rightarrow \infty\)</span>?</p></li>
</ol></li>
</ol>
</div>
<div id="answers-to-some-extra-questions-1" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Answers to Some Extra Questions<a href="#answers-to-some-extra-questions-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Answer to Question 3</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>The first model appears to be predicting better because the AIC is lower. [Also, notice that <span class="math inline">\(R^2\)</span> is higher in the second model, but this is by construction because it includes extra terms relative to the first model which implies that it will fit at least as well in-sample as the first model, but may be suffering from over-fitting.]</p></li>
<li><p><span class="math display">\[\begin{align*}
   \hat{Y} &amp;= 30 + 4 (10) - 2 (1) - 10 (5) \\
   &amp;= 18
\end{align*}\]</span></p></li>
</ol>
<p><strong>Answer to Question 6</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>The tuning parameter is often chosen via cross validation. It makes sense to choose it this way because this is effectively choosing a value of <span class="math inline">\(\lambda\)</span> that is making good pseudo-out-of-sample predictions. As we will see below, if you make bad choices of <span class="math inline">\(\lambda\)</span>, that could result in very poor predictions.</p></li>
<li><p>When <span class="math inline">\(\lambda=0\)</span>, there would effectively be no penalty term and, therefore, the estimated parameters would coincide with the OLS estimates.</p></li>
<li><p>When <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the penalty term would overwhelm the term corresponding to minimizing SSR. This would result in setting all the estimated parameters to be equal to 0. This extreme approach is likely to lead to very poor predictions.</p></li>
</ol>

</div>
</div>
<div id="binary-outcome-models" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Topic 7</span> Binary Outcome Models<a href="#binary-outcome-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In addition to referenced material below, please read all of SW Ch. 11</p>
<p>You may or may not have noticed this, but all the outcomes that we have considered so far have involved a continuous outcome. But lots of economic variables are discrete (we’ll mainly focus on binary outcomes). Examples:</p>
<ul>
<li><p>Labor force participation</p></li>
<li><p>Graduating from college</p></li>
</ul>
<p>The question is: Do our linear regression tools still apply to this case? In other words, does</p>
<p><span class="math display">\[
  \mathbb{E}[Y | X_1, X_2, X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span>
still make sense?</p>
<p>We will see that, in many cases, it is more natural to use a <strong>nonlinear model</strong> when the outcome is binary. And, actually, nonlinear models are fairly common in economics. This section will thus also provide an introduction to estimating (and understanding) nonlinear models. In this section, we will not necessarily be so interested in prediction (though you can make predictions using the techniques we discuss below), but I find this a good spot to teach about binary outcome models (after we talk about prediction mainly emphasizing linear models and before we conclude the course talking about causality).</p>
<ul>
<li>Note: we have already included binary regressors and know how to interpret these, so this section is about binary outcomes rather than binary regressors</li>
</ul>
<div id="linear-probability-model" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Linear Probability Model<a href="#linear-probability-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 11.1</p>
<p>Let’s continue to consider</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span></p>
<p>when <span class="math inline">\(Y\)</span> is binary. Of course, you can still run this regression.</p>
<p>One thing that is helpful to notice before we really get started here is that when <span class="math inline">\(Y\)</span> is binary (so that either <span class="math inline">\(Y=0\)</span> or <span class="math inline">\(Y=1\)</span>)</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathbb{E}[Y] &amp;= \sum_{y \in \mathcal{Y}} y \mathrm{P}(Y=y) \\
  &amp;= 0 \mathrm{P}(Y=0) + 1 \mathrm{P}(Y=1) \\
  &amp;= \mathrm{P}(Y=1)
  \end{aligned}
\]</span>
And exactly the same sort of argument implies that, when <span class="math inline">\(Y\)</span> is binary, <span class="math inline">\(\mathbb{E}[Y|X] = \mathrm{P}(Y=1|X)\)</span>. Thus, if we believe the model in the first part of this section, this result implies that</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1|X_1,X_2,X_3) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span>
For this reason, the model in this section is called the <strong>linear probabilty model</strong>. Moreover, this further implies that we should interpret</p>
<p><span class="math display">\[
  \beta_1 = \frac{\partial \, \mathrm{P}(Y=1|X_1,X_2,X_3)}{\partial \, X_1}
\]</span>
as a partial effect. That is, <span class="math inline">\(\beta_1\)</span> is how much the probability that <span class="math inline">\(Y=1\)</span> changes when <span class="math inline">\(X_1\)</span> increases by one unit, holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant. This is good (and simple), but there are some drawbacks:</p>
<ol style="list-style-type: decimal">
<li><p>It’s possible to get non-sensical predictions (predicted probabilities that are less than 0 or greater than 1) with a linear probability model.</p></li>
<li><p>A related problem is that the linear probability model implies constant partial effects. That is, the effect of a change in one regressor always changes the probability of <span class="math inline">\(Y=1\)</span> (holding other regressors constant) by the same amount. It may not be obvious that this is a disadvantage, but it is.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-23" class="example"><strong>Example 7.1  </strong></span>Let <span class="math inline">\(Y=1\)</span> if an individual participates in the labor force. Further let <span class="math inline">\(X_1=1\)</span> if an individual is male and 0 otherwise, <span class="math inline">\(X_2\)</span> denote an individual’s age, and <span class="math inline">\(X_3=1\)</span> for college graduates and 0 otherwise.</p>
<p>Additionally, suppose that <span class="math inline">\(\beta_0=0.4, \beta_1=0.2, \beta_2=0.01, \beta_3=0.1\)</span>.</p>
<p>Let’s calculate the probability of being in the labor force for a 40 year old woman who is not a college graduate. This is given by</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1 | X_1=0, X_2=40, X_3=0) = 0.4 + (0.01)(40) = 0.8
\]</span>
In other words, we’d predict that, given these characteristics, the probability of being in the labor force is 0.8.</p>
<p>Now, let’s calculate the probability of being in the labor force for a 40 year old man who is a college graduate. This is given by</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1|X_1=1, X_2=40, X_3=1) = 0.4 + 0.2 + (0.01)(40) + 0.1 = 1.1
\]</span>
We have calculated that the predicted probability of being in the labor force, given these characteristics, is 1.1 — this makes no sense! Our maximum predicted probabilty should be 1.</p>
<p>The problem of constant partial effect is closely related. Here, labor force participation is increasing in age, but with a binary outcome (by construction) the effect has to die off — for those who are already very likely to participate in the labor force (in this example, older men with a college education, the partial effect of age has to be low because they are already very likely to participate in the labor force).</p>
</div>
<p>We can circumvent both of the main problems with the linear probability model by consider <strong>nonlinear models</strong> for binary outcomes. By far the most common are <strong>probit</strong> and <strong>logit</strong>. We will discuss these next.</p>
</div>
<div id="probit-and-logit" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Probit and Logit<a href="#probit-and-logit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 11.2, 11.3</p>
<p>Let’s start this section with probit. A probit model arises from setting</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1|X_1,X_2,X_3) = \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)
\]</span>
where <span class="math inline">\(\Phi\)</span> is the cdf of a standard normal random variable. This is a nonlinear model due to <span class="math inline">\(\Phi\)</span> making the model nonlinear in parameters.</p>
<p>Using <span class="math inline">\(\Phi\)</span> (or any cdf) here has a useful property that no matter what value the “index” <span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3\)</span> takes on, the cdf is always between 0 and 1. This implies that we cannot get predicted probabilities outside of 0 and 1.</p>
<p>Thus, this circumvents the problems with the linear probability model. That said, there are some things we have to be careful about. First, as usual, we are interested in partial effects rather than the parameters themselves. But partial effects are more complicated here. Notice that</p>
<p><span class="math display">\[
  \begin{aligned}
  \frac{ \partial \, P(Y=1|X_1,X_2,X_3)}{\partial \, X_1} &amp;= \frac{\partial \, \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}{\partial \, X_1} \\
  &amp;= \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1
  \end{aligned}
\]</span>
where <span class="math inline">\(\phi\)</span> is the pdf of a standard normal random variable. And the second equality requires using the chain rule — take the derivative of the “outside” (i.e., <span class="math inline">\(\Phi\)</span>) and then the derivative of the “inside” with respect to <span class="math inline">\(X_1\)</span>. Notice that this partial effect is more complicated that in the case of the linear models that we have mainly considered — it involves <span class="math inline">\(\phi\)</span>, but more importantly it also depends on the values of all the covariates. In other words, the partial effect of <span class="math inline">\(X_1\)</span> can vary across different values of <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>.</p>
<p><strong>Logit</strong> is conceptually similar to probit, but instead of using <span class="math inline">\(\Phi\)</span>, Logit uses the logistic function <span class="math inline">\(\Lambda(z) = \frac{\exp(z)}{1+\exp(z)}\)</span>. The logistic function has the same important properties as <span class="math inline">\(\Phi\)</span>: (i) <span class="math inline">\(\Lambda(z)\)</span> is increasing in <span class="math inline">\(z\)</span>, (ii) <span class="math inline">\(\Lambda(z) \rightarrow 1\)</span> as <span class="math inline">\(z \rightarrow \infty\)</span>, and (iii) <span class="math inline">\(\Lambda(z) \rightarrow 0\)</span> as <span class="math inline">\(z \rightarrow -\infty\)</span>. Thus, in a logit model,</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{P}(Y=1 | X_1, X_2, X_3) &amp;= \Lambda(\beta_0 + \beta_1 X_1 + \beta_2 + \beta_3 X_3) \\
  &amp;= \frac{\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}{1+\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}
  \end{aligned}
\]</span></p>
<p><strong>Estimation</strong></p>
<p>Because probit and logit models are nonlinear, estimation is more complicated than for the linear regression models that we were studying before. In particular, we cannot write down a formula like <span class="math inline">\(\hat{\beta}_1 = \textrm{something}\)</span>.</p>
<p>Instead, probit and logit models are typically esetimated through an approach called <strong>maximum likelihood estimation</strong>. Basically, the computer will solve an optimization problem trying to choose the “most likely” values of the parameters given the data that you have. It turns out that this particular optimization problem is actually quite easy for the computer to solve — even though estimating the parameters is more complicated than for linear regression, it will still feel like <code>R</code> can estimate a probit or logit model pretty much instantly.</p>
</div>
<div id="average-partial-effects" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Average Partial Effects<a href="#average-partial-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the complications with Probit and Logit is that it is not so simple to interpret the estimated parameters.</p>
<p>Remember we are generally interested in partial effects, not the parameters themselves. It just so happens that in many of the linear models that we have considered so far the <span class="math inline">\(\beta\)</span>’s correspond to the partial effect — this means that it is sometimes easy to forget that they are not what we are typically most interested in.</p>
<p>This is helpful framing for thinking about how to interpret the results from a Probit or Logit model.</p>
<p>Let’s focus on the Probit model. In that case,
<span class="math display">\[\begin{align*}
  \mathrm{P}(Y=1|X_1,X_2,X_3) = \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)
\end{align*}\]</span>
where <span class="math inline">\(\Phi\)</span> is the cdf of standard normal random variable.</p>
<p><em>Continuous Case</em>: When <span class="math inline">\(X_1\)</span> is continuous, the partial effect of <span class="math inline">\(X_1\)</span> is given by
<span class="math display">\[\begin{align*}
  \frac{\partial \mathrm{P}(Y=1|X_1,X_2,X_3)}{\partial X_1} = \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1
\end{align*}\]</span>
where <span class="math inline">\(\phi\)</span> is the pdf of a standard normal random variable. This is more complicated than the partial effect in the context of a linear model. It depends on <span class="math inline">\(\phi\)</span> (which looks complicated, but you can just use <code>R</code>’s <code>dnorm</code> command to handle that part). More importantly, the partial effect depends on the values of <span class="math inline">\(X_1,X_2,\)</span> and <span class="math inline">\(X_3\)</span>. [As discussed above, this is likely a good thing in the context of a binary outcome model]. Thus, in order to get a partial effect, we need to put in some values for these. If you have particular values of the covariates that you are interested in, you can definitely do that, but my general suggestion is to report the <em>Average Partial Effect</em>:
<span class="math display">\[\begin{align*}
  APE &amp;= \mathbb{E}\left[ \frac{\partial \mathrm{P}(Y=1|X_1,X_2,X_3)}{\partial X_1} \right] \\
  &amp;= \mathbb{E}\left[ \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1 \right]
\end{align*}\]</span>
which you can estimate by
<span class="math display">\[\begin{align*}
  \widehat{APE} &amp;= \frac{1}{n} \sum_{i=1}^n \phi(\hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_3) \hat{\beta}_1
\end{align*}\]</span>
which amounts to just computing the partial effect at each value of the covariates in your data and then averaging these partial effects together. This can be a bit cumbersome to do in practice, and it is often convenient to use the <code>R</code> package <code>mfx</code> to compute these sorts of average partial effects for you.</p>
<p><em>Discrete/Binary Case</em>: When <span class="math inline">\(X_1\)</span> is discrete (let’s say binary, but extention to discrete is straightforward), the partial effect of <span class="math inline">\(X_1\)</span> is
<span class="math display">\[\begin{align*}
  &amp; \mathrm{P}(Y=1|X_1=1, X_2, X_3) - \mathrm{P}(Y=1|X_1=0, X_2, X_3) \\
  &amp;\hspace{100pt} = \Phi(\beta_0 + \beta_1 + \beta_2 X_2 + \beta_3 X_3) - \Phi(\beta_0 + \beta_2 X_2 + \beta_3 X_3)
\end{align*}\]</span>
Notice that <span class="math inline">\(\beta_1\)</span> does not show up in the last term. As above, the partial effect depends on the values of <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> which suggests reporting an <span class="math inline">\(APE\)</span> as above (follows the same steps, just replacing the partial effect, as in the continuous case above)</p>
<ul>
<li>Extensions to Logit are virtually identical, just replace <span class="math inline">\(\Phi\)</span> with <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\phi\)</span> with <span class="math inline">\(\lambda\)</span>.</li>
</ul>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> The parameters from LPM, Probit, and Logit could be quite different (in fact, they are quite different by construction), but APE’s are often very similar.</p>
</div>
</div>
<div id="lab-6-estimating-binary-outcome-models" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Lab 6: Estimating Binary Outcome Models<a href="#lab-6-estimating-binary-outcome-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this lab, I’ll demonstrate how to estimate and interpret binary outcome models using the <code>titanic_training</code> data. The outcome for this data is <code>Survived</code> which is a binary variable indicating whether a particular passenger survived the Titanic wreck. We’ll estimate models that also include passenger class (<code>Pclass</code>), passenger’s sex, and passenger’s age.</p>
<p>The main <code>R</code> function for estimating binary outcome models is the <code>glm</code> function (this stands for “generalized linear model”). The syntax is very similar to the syntax for the <code>lm</code> command, so much of what we do below will feel feel familiar. We’ll also use the <code>probitmfx</code> and <code>logitmfx</code> functions from the <code>mfx</code> package to compute partial effects.</p>
<p><strong>Linear Probability Model</strong></p>
<p>We’ll start by estimating a linear probability model.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb140-2"><a href="#cb140-2" tabindex="-1"></a>titanic_train <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/titanic_training.csv&quot;</span>)</span>
<span id="cb140-3"><a href="#cb140-3" tabindex="-1"></a></span>
<span id="cb140-4"><a href="#cb140-4" tabindex="-1"></a><span class="co"># linear probability model</span></span>
<span id="cb140-5"><a href="#cb140-5" tabindex="-1"></a>lpm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> </span>
<span id="cb140-6"><a href="#cb140-6" tabindex="-1"></a>            Sex <span class="sc">+</span> Age, </span>
<span id="cb140-7"><a href="#cb140-7" tabindex="-1"></a>          <span class="at">data=</span>titanic_train)</span>
<span id="cb140-8"><a href="#cb140-8" tabindex="-1"></a><span class="fu">summary</span>(lpm)</span>
<span id="cb140-9"><a href="#cb140-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb140-10"><a href="#cb140-10" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb140-11"><a href="#cb140-11" tabindex="-1"></a><span class="co">#&gt; lm(formula = Survived ~ as.factor(Pclass) + Sex + Age, data = titanic_train)</span></span>
<span id="cb140-12"><a href="#cb140-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb140-13"><a href="#cb140-13" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb140-14"><a href="#cb140-14" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb140-15"><a href="#cb140-15" tabindex="-1"></a><span class="co">#&gt; -1.05638 -0.26294 -0.07656  0.21103  0.98057 </span></span>
<span id="cb140-16"><a href="#cb140-16" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb140-17"><a href="#cb140-17" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb140-18"><a href="#cb140-18" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb140-19"><a href="#cb140-19" tabindex="-1"></a><span class="co">#&gt; (Intercept)         1.065516   0.061481  17.331  &lt; 2e-16 ***</span></span>
<span id="cb140-20"><a href="#cb140-20" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2 -0.148575   0.050593  -2.937 0.003472 ** </span></span>
<span id="cb140-21"><a href="#cb140-21" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3 -0.349809   0.046462  -7.529 2.44e-13 ***</span></span>
<span id="cb140-22"><a href="#cb140-22" tabindex="-1"></a><span class="co">#&gt; Sexmale            -0.490605   0.037330 -13.143  &lt; 2e-16 ***</span></span>
<span id="cb140-23"><a href="#cb140-23" tabindex="-1"></a><span class="co">#&gt; Age                -0.004570   0.001317  -3.471 0.000564 ***</span></span>
<span id="cb140-24"><a href="#cb140-24" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb140-25"><a href="#cb140-25" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb140-26"><a href="#cb140-26" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb140-27"><a href="#cb140-27" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.3915 on 495 degrees of freedom</span></span>
<span id="cb140-28"><a href="#cb140-28" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.3747, Adjusted R-squared:  0.3696 </span></span>
<span id="cb140-29"><a href="#cb140-29" tabindex="-1"></a><span class="co">#&gt; F-statistic: 74.14 on 4 and 495 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Let’s quickly interpret a couple of these parameters. Recall that these can all be directly interpreted as partial effects on the probability of surviving the Titanic wreck. For example, these estimates indicate that third class passengers were about 33% less likely to survive on average than first class passengers controlling for passenger’s sex and age.</p>
<p>The other thing that jumps out is passenger’s sex. These estimates indicate that men were, on average, 49% less likely to survive the Titanic wreck than women after controlling for passenger class, and age.</p>
<p>Before we move on, let’s compute a couple of predicted probabilities.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" tabindex="-1"></a><span class="co"># young, first class, female</span></span>
<span id="cb141-2"><a href="#cb141-2" tabindex="-1"></a>pred_df1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Pclass=</span><span class="dv">1</span>, <span class="at">Sex=</span><span class="st">&quot;female&quot;</span>, <span class="at">Age=</span><span class="dv">25</span>, <span class="at">Embarked=</span><span class="st">&quot;S&quot;</span>)</span>
<span id="cb141-3"><a href="#cb141-3" tabindex="-1"></a></span>
<span id="cb141-4"><a href="#cb141-4" tabindex="-1"></a><span class="co"># old, third class, male</span></span>
<span id="cb141-5"><a href="#cb141-5" tabindex="-1"></a>pred_df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Pclass=</span><span class="dv">3</span>, <span class="at">Sex=</span><span class="st">&quot;male&quot;</span>, <span class="at">Age=</span><span class="dv">55</span>, <span class="at">Embarked=</span><span class="st">&quot;S&quot;</span>)</span>
<span id="cb141-6"><a href="#cb141-6" tabindex="-1"></a>pred_df <span class="ot">&lt;-</span> <span class="fu">rbind.data.frame</span>(pred_df1, pred_df2)</span>
<span id="cb141-7"><a href="#cb141-7" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">predict</span>(lpm, <span class="at">newdata=</span>pred_df), <span class="dv">3</span>)</span>
<span id="cb141-8"><a href="#cb141-8" tabindex="-1"></a><span class="co">#&gt;      1      2 </span></span>
<span id="cb141-9"><a href="#cb141-9" tabindex="-1"></a><span class="co">#&gt;  0.951 -0.026</span></span></code></pre></div>
<p>This illustrates that there were very large differences in survival probabilities. It also demonstrates that the linear probability model can deliver non-sensical predictions — we predict the survival probability of a 55 year old, male, third-class passenger to be -3%.</p>
<p><strong>Estimating Probit and Logit Models</strong></p>
<p>Let’s estimate Probit and Logit models using the same specifications. First, Probit:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" tabindex="-1"></a><span class="co"># probit</span></span>
<span id="cb142-2"><a href="#cb142-2" tabindex="-1"></a>probit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age <span class="sc">+</span> <span class="fu">as.factor</span>(Embarked), </span>
<span id="cb142-3"><a href="#cb142-3" tabindex="-1"></a>              <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&quot;probit&quot;</span>), </span>
<span id="cb142-4"><a href="#cb142-4" tabindex="-1"></a>              <span class="at">data=</span>titanic_train)</span>
<span id="cb142-5"><a href="#cb142-5" tabindex="-1"></a><span class="fu">summary</span>(probit)</span>
<span id="cb142-6"><a href="#cb142-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-7"><a href="#cb142-7" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb142-8"><a href="#cb142-8" tabindex="-1"></a><span class="co">#&gt; glm(formula = Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), </span></span>
<span id="cb142-9"><a href="#cb142-9" tabindex="-1"></a><span class="co">#&gt;     family = binomial(link = &quot;probit&quot;), data = titanic_train)</span></span>
<span id="cb142-10"><a href="#cb142-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-11"><a href="#cb142-11" tabindex="-1"></a><span class="co">#&gt; Deviance Residuals: </span></span>
<span id="cb142-12"><a href="#cb142-12" tabindex="-1"></a><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span id="cb142-13"><a href="#cb142-13" tabindex="-1"></a><span class="co">#&gt; -2.5145  -0.7433  -0.4397   0.6560   2.3799  </span></span>
<span id="cb142-14"><a href="#cb142-14" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-15"><a href="#cb142-15" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb142-16"><a href="#cb142-16" tabindex="-1"></a><span class="co">#&gt;                        Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb142-17"><a href="#cb142-17" tabindex="-1"></a><span class="co">#&gt; (Intercept)            5.418219 146.954198   0.037  0.97059    </span></span>
<span id="cb142-18"><a href="#cb142-18" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2    -0.445235   0.197398  -2.256  0.02410 *  </span></span>
<span id="cb142-19"><a href="#cb142-19" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3    -1.145788   0.188508  -6.078 1.22e-09 ***</span></span>
<span id="cb142-20"><a href="#cb142-20" tabindex="-1"></a><span class="co">#&gt; Sexmale               -1.464201   0.136512 -10.726  &lt; 2e-16 ***</span></span>
<span id="cb142-21"><a href="#cb142-21" tabindex="-1"></a><span class="co">#&gt; Age                   -0.015764   0.005009  -3.147  0.00165 ** </span></span>
<span id="cb142-22"><a href="#cb142-22" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)C  -3.443295 146.954199  -0.023  0.98131    </span></span>
<span id="cb142-23"><a href="#cb142-23" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)Q  -3.464431 146.954544  -0.024  0.98119    </span></span>
<span id="cb142-24"><a href="#cb142-24" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)S  -3.662911 146.954180  -0.025  0.98011    </span></span>
<span id="cb142-25"><a href="#cb142-25" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb142-26"><a href="#cb142-26" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb142-27"><a href="#cb142-27" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-28"><a href="#cb142-28" tabindex="-1"></a><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb142-29"><a href="#cb142-29" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-30"><a href="#cb142-30" tabindex="-1"></a><span class="co">#&gt;     Null deviance: 678.28  on 499  degrees of freedom</span></span>
<span id="cb142-31"><a href="#cb142-31" tabindex="-1"></a><span class="co">#&gt; Residual deviance: 468.38  on 492  degrees of freedom</span></span>
<span id="cb142-32"><a href="#cb142-32" tabindex="-1"></a><span class="co">#&gt; AIC: 484.38</span></span>
<span id="cb142-33"><a href="#cb142-33" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-34"><a href="#cb142-34" tabindex="-1"></a><span class="co">#&gt; Number of Fisher Scoring iterations: 12</span></span></code></pre></div>
<p>Now, Logit:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" tabindex="-1"></a><span class="co"># logit</span></span>
<span id="cb143-2"><a href="#cb143-2" tabindex="-1"></a>logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age <span class="sc">+</span> <span class="fu">as.factor</span>(Embarked), </span>
<span id="cb143-3"><a href="#cb143-3" tabindex="-1"></a>              <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&quot;logit&quot;</span>), </span>
<span id="cb143-4"><a href="#cb143-4" tabindex="-1"></a>              <span class="at">data=</span>titanic_train)</span>
<span id="cb143-5"><a href="#cb143-5" tabindex="-1"></a><span class="fu">summary</span>(logit)</span>
<span id="cb143-6"><a href="#cb143-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb143-7"><a href="#cb143-7" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb143-8"><a href="#cb143-8" tabindex="-1"></a><span class="co">#&gt; glm(formula = Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), </span></span>
<span id="cb143-9"><a href="#cb143-9" tabindex="-1"></a><span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = titanic_train)</span></span>
<span id="cb143-10"><a href="#cb143-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb143-11"><a href="#cb143-11" tabindex="-1"></a><span class="co">#&gt; Deviance Residuals: </span></span>
<span id="cb143-12"><a href="#cb143-12" tabindex="-1"></a><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span id="cb143-13"><a href="#cb143-13" tabindex="-1"></a><span class="co">#&gt; -2.4680  -0.7289  -0.4336   0.6471   2.3689  </span></span>
<span id="cb143-14"><a href="#cb143-14" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb143-15"><a href="#cb143-15" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb143-16"><a href="#cb143-16" tabindex="-1"></a><span class="co">#&gt;                        Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb143-17"><a href="#cb143-17" tabindex="-1"></a><span class="co">#&gt; (Intercept)           14.646649 535.411272   0.027  0.97818    </span></span>
<span id="cb143-18"><a href="#cb143-18" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2    -0.793003   0.342058  -2.318  0.02043 *  </span></span>
<span id="cb143-19"><a href="#cb143-19" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3    -2.055828   0.335386  -6.130  8.8e-10 ***</span></span>
<span id="cb143-20"><a href="#cb143-20" tabindex="-1"></a><span class="co">#&gt; Sexmale               -2.461563   0.241113 -10.209  &lt; 2e-16 ***</span></span>
<span id="cb143-21"><a href="#cb143-21" tabindex="-1"></a><span class="co">#&gt; Age                   -0.028436   0.008764  -3.245  0.00118 ** </span></span>
<span id="cb143-22"><a href="#cb143-22" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)C -11.262004 535.411276  -0.021  0.98322    </span></span>
<span id="cb143-23"><a href="#cb143-23" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)Q -11.229232 535.411568  -0.021  0.98327    </span></span>
<span id="cb143-24"><a href="#cb143-24" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)S -11.593053 535.411259  -0.022  0.98273    </span></span>
<span id="cb143-25"><a href="#cb143-25" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb143-26"><a href="#cb143-26" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb143-27"><a href="#cb143-27" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb143-28"><a href="#cb143-28" tabindex="-1"></a><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb143-29"><a href="#cb143-29" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb143-30"><a href="#cb143-30" tabindex="-1"></a><span class="co">#&gt;     Null deviance: 678.28  on 499  degrees of freedom</span></span>
<span id="cb143-31"><a href="#cb143-31" tabindex="-1"></a><span class="co">#&gt; Residual deviance: 467.01  on 492  degrees of freedom</span></span>
<span id="cb143-32"><a href="#cb143-32" tabindex="-1"></a><span class="co">#&gt; AIC: 483.01</span></span>
<span id="cb143-33"><a href="#cb143-33" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb143-34"><a href="#cb143-34" tabindex="-1"></a><span class="co">#&gt; Number of Fisher Scoring iterations: 12</span></span></code></pre></div>
<p>It’s rather hard to interpret the parameters in both of these models (let’s defer this to the next section). But it is worth mentioning that all of the estimated coefficients have the same sign for the linear probability model, the probit model, and the logit model, and the same set of regressors have statistically significant effects across models (and the t-statistics/p-values are very similar across models).</p>
<p>Now, let’s calculate the same predicted probabilities as we did for the linear probability model above:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" tabindex="-1"></a><span class="co"># for probit</span></span>
<span id="cb144-2"><a href="#cb144-2" tabindex="-1"></a><span class="fu">predict</span>(probit, <span class="at">newdata=</span>pred_df, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb144-3"><a href="#cb144-3" tabindex="-1"></a><span class="co">#&gt;          1          2 </span></span>
<span id="cb144-4"><a href="#cb144-4" tabindex="-1"></a><span class="co">#&gt; 0.91327666 0.04256272</span></span>
<span id="cb144-5"><a href="#cb144-5" tabindex="-1"></a></span>
<span id="cb144-6"><a href="#cb144-6" tabindex="-1"></a><span class="co"># for logit</span></span>
<span id="cb144-7"><a href="#cb144-7" tabindex="-1"></a><span class="fu">predict</span>(logit, <span class="at">newdata=</span>pred_df, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb144-8"><a href="#cb144-8" tabindex="-1"></a><span class="co">#&gt;          1          2 </span></span>
<span id="cb144-9"><a href="#cb144-9" tabindex="-1"></a><span class="co">#&gt; 0.91235117 0.04618584</span></span></code></pre></div>
<p>Before we interpret the result, notice that we add the argument <code>type=response</code> (this indicates that we want to get a predicted probability).</p>
<p>Here (for both models), we estimate that a 25 year old woman traveling first class has a 91% probability of survival (this is slightly smaller than the prediction from the linear probability model). On the other hand, we estimate that a 55 year old man traveling 3rd class has a 4.3% (from probit) or 4.6% (from logit) probability of survival. While these probabilities are still quite low, unlike the estimates from the linear probability model, they are at least positive.</p>
<p><strong>Average Partial Effects</strong></p>
<p>To conclude this section, let’s calculate average partial effects for each model. First, for probit:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" tabindex="-1"></a><span class="fu">library</span>(mfx)</span>
<span id="cb145-2"><a href="#cb145-2" tabindex="-1"></a>probit_ape <span class="ot">&lt;-</span> <span class="fu">probitmfx</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age, </span>
<span id="cb145-3"><a href="#cb145-3" tabindex="-1"></a>                        <span class="at">data=</span>titanic_train, </span>
<span id="cb145-4"><a href="#cb145-4" tabindex="-1"></a>                        <span class="at">atmean=</span><span class="cn">FALSE</span>)</span>
<span id="cb145-5"><a href="#cb145-5" tabindex="-1"></a>probit_ape</span>
<span id="cb145-6"><a href="#cb145-6" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb145-7"><a href="#cb145-7" tabindex="-1"></a><span class="co">#&gt; probitmfx(formula = Survived ~ as.factor(Pclass) + Sex + Age, </span></span>
<span id="cb145-8"><a href="#cb145-8" tabindex="-1"></a><span class="co">#&gt;     data = titanic_train, atmean = FALSE)</span></span>
<span id="cb145-9"><a href="#cb145-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-10"><a href="#cb145-10" tabindex="-1"></a><span class="co">#&gt; Marginal Effects:</span></span>
<span id="cb145-11"><a href="#cb145-11" tabindex="-1"></a><span class="co">#&gt;                         dF/dx  Std. Err.        z     P&gt;|z|    </span></span>
<span id="cb145-12"><a href="#cb145-12" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2 -0.1303971  0.0424994  -3.0682 0.0021534 ** </span></span>
<span id="cb145-13"><a href="#cb145-13" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3 -0.3438262  0.0470404  -7.3092 2.688e-13 ***</span></span>
<span id="cb145-14"><a href="#cb145-14" tabindex="-1"></a><span class="co">#&gt; Sexmale            -0.4895845  0.0412840 -11.8590 &lt; 2.2e-16 ***</span></span>
<span id="cb145-15"><a href="#cb145-15" tabindex="-1"></a><span class="co">#&gt; Age                -0.0042614  0.0012934  -3.2948 0.0009851 ***</span></span>
<span id="cb145-16"><a href="#cb145-16" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb145-17"><a href="#cb145-17" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb145-18"><a href="#cb145-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-19"><a href="#cb145-19" tabindex="-1"></a><span class="co">#&gt; dF/dx is for discrete change for the following variables:</span></span>
<span id="cb145-20"><a href="#cb145-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-21"><a href="#cb145-21" tabindex="-1"></a><span class="co">#&gt; [1] &quot;as.factor(Pclass)2&quot; &quot;as.factor(Pclass)3&quot; &quot;Sexmale&quot;</span></span></code></pre></div>
<p>Now, for logit:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" tabindex="-1"></a>logit_ape <span class="ot">&lt;-</span> <span class="fu">logitmfx</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age, </span>
<span id="cb146-2"><a href="#cb146-2" tabindex="-1"></a>                        <span class="at">data=</span>titanic_train, </span>
<span id="cb146-3"><a href="#cb146-3" tabindex="-1"></a>                        <span class="at">atmean=</span><span class="cn">FALSE</span>)</span>
<span id="cb146-4"><a href="#cb146-4" tabindex="-1"></a>logit_ape</span>
<span id="cb146-5"><a href="#cb146-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb146-6"><a href="#cb146-6" tabindex="-1"></a><span class="co">#&gt; logitmfx(formula = Survived ~ as.factor(Pclass) + Sex + Age, </span></span>
<span id="cb146-7"><a href="#cb146-7" tabindex="-1"></a><span class="co">#&gt;     data = titanic_train, atmean = FALSE)</span></span>
<span id="cb146-8"><a href="#cb146-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb146-9"><a href="#cb146-9" tabindex="-1"></a><span class="co">#&gt; Marginal Effects:</span></span>
<span id="cb146-10"><a href="#cb146-10" tabindex="-1"></a><span class="co">#&gt;                         dF/dx  Std. Err.        z     P&gt;|z|    </span></span>
<span id="cb146-11"><a href="#cb146-11" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2 -0.1304383  0.0420444  -3.1024  0.001920 ** </span></span>
<span id="cb146-12"><a href="#cb146-12" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3 -0.3542499  0.0474154  -7.4712 7.947e-14 ***</span></span>
<span id="cb146-13"><a href="#cb146-13" tabindex="-1"></a><span class="co">#&gt; Sexmale            -0.4859890  0.0413167 -11.7625 &lt; 2.2e-16 ***</span></span>
<span id="cb146-14"><a href="#cb146-14" tabindex="-1"></a><span class="co">#&gt; Age                -0.0044098  0.0014205  -3.1045  0.001906 ** </span></span>
<span id="cb146-15"><a href="#cb146-15" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb146-16"><a href="#cb146-16" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb146-17"><a href="#cb146-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb146-18"><a href="#cb146-18" tabindex="-1"></a><span class="co">#&gt; dF/dx is for discrete change for the following variables:</span></span>
<span id="cb146-19"><a href="#cb146-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb146-20"><a href="#cb146-20" tabindex="-1"></a><span class="co">#&gt; [1] &quot;as.factor(Pclass)2&quot; &quot;as.factor(Pclass)3&quot; &quot;Sexmale&quot;</span></span></code></pre></div>
<p>A couple of things to notice:</p>
<ul>
<li><p>The average partial effects are extremely similar across models. For example, across all three models, the average partial effect of being <code>male</code> is to reduce the probability of survival by 49% controlling for the other variables in the model. The other average partial effects are quite similar across models as well.</p></li>
<li><p>Both <code>probitmfx</code> and <code>logitmfx</code> functions took in an argument for <code>atmean</code>. We set it equal to <code>FALSE</code>. If you set it equal to <code>TRUE</code>, you will compute a different kind of partial effect. You can check <code>?probitmfx</code> or <code>?logitmfx</code> for more details.</p></li>
</ul>
</div>
<div id="coding-questions-3" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Coding Questions<a href="#coding-questions-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>For this problem, we will use the data <code>mroz</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimate a probit model where the outcome is whether or not the wife is in the labor force (<code>inlf</code>) using the the number of kids less than 6 (<code>kidslt6</code>) and the number of kids who are 6 or older living in the household (<code>kidsge6</code>). Calculate the average partial effect of each variable. What do you notice?</p></li>
<li><p>Now, add the variables <code>age</code>, <code>educ</code>, and <code>city</code> to the model. Calculate the average partial effects of <code>kidslt6</code> and <code>kidsge6</code>. How do you interpret these? How do they compare to the answers from part a?</p></li>
<li><p>Estimate a linear probability model and logit model using the same specification as in part b. For each one, how do the estimated coefficients compare to the ones from part b? Compute average partial effects for each model. How do these compare to the ones from part b?</p></li>
</ol></li>
<li><p>For this problem, we will use the <code>Fair</code> data.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The variable <code>nbaffairs</code> contains the number of self-reported affairs that an individual has had in the previous year. Create a variable <code>had_affair</code> that is equal to 1 if an individual had any affair in the past year and that is equal to 0 otherwise. What fraction of individuals in the data have had an affair in the past year?</p></li>
<li><p>Estimate a logit model where the outcome is <code>had_affair</code> and the regressor is whether or not the person has a child (<code>child</code>). Calculate the average partial effect of having a child on having an affair. How do you interpret the results?</p></li>
<li><p>Now add <code>sex</code>, <code>age</code>, <code>education</code>, and <code>occupation</code> to the model. Calculate the average partial effect of each variable. How do you interpret the results? <strong>Hint:</strong> Make sure to treat the categorical variables in the model as categorical rather than as numeric.</p></li>
<li><p>In addition to the variables in part c, add the variables years married (<code>ym</code>) and <code>religious</code> to the mode. Calculate the average partial effect of each variable. How do you interpret the results? <strong>Hint:</strong> Make sure to treat the categorical variables in the model as categorical rather than as numeric.</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-4" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Extra Questions<a href="#extra-questions-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Suppose you try to estimate a linear probability model, probit model, and logit model using the same specifications. You notice that the estimated coefficients are substantially different from each other. Does this mean that something has gone wrong? Explain.</li>
</ol>

</div>
</div>
<div id="causal-inference" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Topic 8</span> Causal Inference<a href="#causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>For the remainder of the semester, we will talk about methods for understanding the <strong>causal effect</strong> of one variable on another.</p>
<p>Let’s start with an example. Suppose we were interested in understanding the causal effect of attending college on earnings. We know a lot about calculating averages, so let’s consider calculating average earnings of those who went to college and comparing that to the average earnings of those who didn’t go to college. Let’s use the data that we used all the way back in Chapter 2 to make this calculation.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;us_data.RData&quot;</span>)</span>
<span id="cb147-2"><a href="#cb147-2" tabindex="-1"></a></span>
<span id="cb147-3"><a href="#cb147-3" tabindex="-1"></a>col_earnings <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">subset</span>(us_data, educ <span class="sc">&gt;=</span> <span class="dv">16</span>)<span class="sc">$</span>incwage)</span>
<span id="cb147-4"><a href="#cb147-4" tabindex="-1"></a>non_col_earnings <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">subset</span>(us_data, educ <span class="sc">&lt;</span> <span class="dv">16</span>)<span class="sc">$</span>incwage)</span>
<span id="cb147-5"><a href="#cb147-5" tabindex="-1"></a></span>
<span id="cb147-6"><a href="#cb147-6" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(col_earnings, </span>
<span id="cb147-7"><a href="#cb147-7" tabindex="-1"></a>                 non_col_earnings, </span>
<span id="cb147-8"><a href="#cb147-8" tabindex="-1"></a>                 <span class="at">diff=</span>(col_earnings<span class="sc">-</span>non_col_earnings)),<span class="dv">3</span>)</span>
<span id="cb147-9"><a href="#cb147-9" tabindex="-1"></a><span class="co">#&gt;   col_earnings non_col_earnings     diff</span></span>
<span id="cb147-10"><a href="#cb147-10" tabindex="-1"></a><span class="co">#&gt; 1     87306.33         40557.28 46749.05</span></span></code></pre></div>
<p>This seems to be a huge difference. We are used to calling this difference a partial effect of going to college, but should we call it <em>the causal effect</em> or an average causal effect across individuals? Probably no. The main reason is that earnings of individuals who went to college may have been different from individuals that didn’t go to college even if no one went to college. Another way to say this is that there are a number of other things besides college that affect a person’s earnings (age, race, IQ, “ability”, “hardworking-ness”, luck, etc.). If these are also correlated with whether or not a person goes to college (you would think that at least some of these are), then that would make it hard to interpret our simple difference-in-means as a causal effect.</p>
<p>Before we move on, let me give some more examples of causal questions that may be of interest to economists:</p>
<ul>
<li><p>Causal effects of economic policies</p>
<ul>
<li><p>What was the causal effect of a country raising its interest rate on GDP or employment?</p></li>
<li><p>What was the causal effect of a change in the minimum wage on employment?</p></li>
<li><p>What was the causal effect of changing voter ID laws on the number of ballots cast?</p></li>
</ul></li>
<li><p>Causal effects of individual/firm choices</p>
<ul>
<li><p>What was the causal effect of a price increase on quantity demanded?</p></li>
<li><p>What was the causal effect of changing a product attribute on some outcome of interest (e.g., changing the font type on clicks on Google ads)?</p></li>
<li><p>What was the causal effect of a new cholesterol medication on cholesterol levels?</p></li>
<li><p>What was the causal effect of a job training program on wages?</p></li>
</ul></li>
</ul>
<p>You could easily come up with many others too. A large fraction of the questions that researchers in economics try to address are ultimately about sorting out these sorts of causal effects.</p>
<p>For terminology, we’ll refer to the variable that we are interested in understanding its causal effect (going to college in the earlier example) as the <strong>treatment</strong>. For simplicity, we will mostly focus on the case where the treatment is binary. We will use <span class="math inline">\(D_i\)</span> to denote the treatment, so that <span class="math inline">\(D_i=1\)</span> if individual <span class="math inline">\(i\)</span> participates in the treatment and <span class="math inline">\(D_i=0\)</span> if individual <span class="math inline">\(i\)</span> does not participate in the treatment.</p>
<p>Example: SW 13.3</p>
<div id="potential-outcomes" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Potential Outcomes<a href="#potential-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 13.1</p>
<p>A powerful tool for thinking about causal effects is <strong>counterfactual reasoning</strong>. For individuals that participate in the treatment, we observe what their outcome is given that they participated in the treatment. But we don’t observe what their outcome would have been if they had not participated in the treatment. For example, among those that went to college, we don’t observe what their earnings would have been if they had not gone to college. For those that don’t participate in the treatment, we face the opposite problem — we don’t observe what their outcome would have been if they had participated in the treatment.</p>
<p>We’ll relate causal inference to the problem of trying to figure out what outcomes individuals that participated in the treatment would have experienced if they had <em>not</em> participated in the treatment (at least on average) and/or figuring out what outcomes individuals that did not participated in the treatment would have experienced if they <em>had</em> participated in the treatment.</p>
<p>To do this, let’s introduce somewhat more formal notation/terminology.</p>
<p><strong>Treated potential outcome</strong>: <span class="math inline">\(Y_i(1)\)</span>, the outcome an individual <em>would experience</em> if they participated in the treatment</p>
<p><strong>Untreated potential outcome</strong>: <span class="math inline">\(Y_i(0)\)</span>, the outcome an individual <em>would experience</em> if they did not participate in the treatment</p>
<p>For individuals that participate in the treatment, we observe <span class="math inline">\(Y_i(1)\)</span> (but not <span class="math inline">\(Y_i(0)\)</span>). For individuals that do not participate in the treatment, we observe <span class="math inline">\(Y_i(0)\)</span> (but not <span class="math inline">\(Y_i(1)\)</span>). Another way to write this is that the observed outcome, <span class="math inline">\(Y_i\)</span> is given by
<span class="math display">\[\begin{align*}
  Y_i = D_i Y_i(1) + (1-D_i) Y_i(0)
\end{align*}\]</span></p>
<p>We can think about the individual-level effect of participating in the treatment:
<span class="math display">\[\begin{align*}
  TE_i = Y_i(1) - Y_i(0)
\end{align*}\]</span></p>
<p>Considering the difference between treated and untreated potential outcomes is a very natural (and, I think, helpful) way to think about causality. The causal effect of the treatment is the difference between the outcome that an individual would experience if they participate in the treatment relative to what they would experience if they did not participate in the treatment.</p>
<p>This notation also makes it clear that we are allowing for <strong>treatment effect heterogenity</strong> — the effect of participating in the treatment can vary across different individuals.</p>
<p>That said, most researchers essentially give up on trying to figure out individual level treatment effects. It is not so much that these are not interesting, more it is just that these are very hard to figure out. Take, for example, going to college, and suppose we are interested in the causal effect of going to college on a person’s earnings. I went to college, so I know what my <span class="math inline">\(Y(1)\)</span> is, but I don’t know what my <span class="math inline">\(Y(0)\)</span> is — and, I’d even have a hard time coming with a good guess as to what it might be.</p>
</div>
<div id="parameters-of-interest" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Parameters of Interest<a href="#parameters-of-interest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Instead of going for individual-level effects of participating in the treatment, most researchers instead go for more aggregated parameters. The two most common ones are the Average Treatment Effect (ATE) and Average Treatment Effect on the Treated (ATT).</p>
<p><span class="math display">\[\begin{align*}
  ATE = \mathbb{E}[Y(1) - Y(0)] \qquad \textrm{and} \qquad ATT = \mathbb{E}[Y(1)-Y(0) | D=1]
\end{align*}\]</span>
<span class="math inline">\(ATE\)</span> is the difference between treated potential outcomes and untreated potential outcomes, on average, and for the entire population. <span class="math inline">\(ATT\)</span> is the difference between treated and untreated potential outcomes, on average, conditional on being in the treated group.</p>
<p>We will mostly focus on <span class="math inline">\(ATT\)</span>.</p>
<p>It is worth considering the challenges for learning about <span class="math inline">\(ATT\)</span>. In particular, notice that we can write
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[Y(1)|D=1] - \mathbb{E}[Y(0)|D=1]
\end{align*}\]</span>
and consider these term separately</p>
<ul>
<li><p><span class="math inline">\(\mathbb{E}[Y(1)|D=1]\)</span> is the average treated potential outcome among the treated group. But we observe treated potential outcomes for the treated group <span class="math inline">\(\implies \mathbb{E}[Y(1)|D=1] = \mathbb{E}[Y|D=1]\)</span>. In other words, if we want to estimate this component of the <span class="math inline">\(ATT\)</span>, we can just look right at the data and compute the average outcome experienced by individuals in the treated group.</p></li>
<li><p><span class="math inline">\(\mathbb{E}[Y(0)|D=1]\)</span> is the average untreated potential outcome among the treated group. This is (potentially much) more challenging than the first term because we do not observe untreated potential outcomes among the treated group. But, in order to learn about the <span class="math inline">\(ATT\)</span>, we will have to <em>somehow</em> deal with this term. I will provide a number of strategies below, but it is important to remember that this is a major challenge, and their may not be a good solution.</p></li>
</ul>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> I think it is also worth clearly pointing out that, while I am a big believer in the power/usefulness of using data to try to answer questions in economics, the above discussion suggests that there are a number of questions that we may just not be able to answer. In economics jargon, this amounts to an <strong>identification problem</strong> — in other words, there may be competing theories of the world which the available data is not able to distinguish among. I probably do not emphasize this issue enough in our class, but it is something that you should remember — there may be a large number of causal questions that we’d be interested in answering, but where it is not possible to answer them (at least given the information that we have available).</p>
</div>
</div>
<div id="experiments" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Experiments<a href="#experiments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An experiment is often called the “gold standard” for causal inference. In particular, here, we are thinking about the case where participation in the treatment is randomly assigned — something like: people who show up to possibly participate in the treatment, someone flips a coin, and if the coin comes up heads then the person participates in the treatment or, if tails, they do not participate in the treatment.</p>
<p>Random assignment means that participating in the treatment is independent of potential outcomes, by construction. We can write this in math as
<span class="math display">\[\begin{align*}
  (Y(1), Y(0)) \perp D
\end{align*}\]</span>
For our purposes, this also implies that
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y(0)|D=1] = \mathbb{E}[Y(0)|D=0] = \mathbb{E}[Y|D=0]
\end{align*}\]</span>
In other words, under random assignment, the average untreated potential among the treated group is equal to the average untreated potential outcome among the untreated group (this is the first equality). This is helpful because untreated potential outcomes are observed for those in the untreated group (this is the second equality).</p>
<p>Thus, under random assignment,
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[Y|D=1] - \mathbb{E}[Y|D=0]
\end{align*}\]</span>
In other words, the <span class="math inline">\(ATT\)</span> is just the difference in (population) average outcomes among the treated group relative to average outcomes among the untreated group.</p>
<p>The natural way to estimate the ATT under random assignment is
<span class="math display">\[\begin{align*}
  \widehat{ATT} = \bar{Y}_{D=1} - \bar{Y}_{D=0}
\end{align*}\]</span>
i.e., as we have done many times before, in order to estimate the parameter of interest, we just replace population averages with sample averages.</p>
<div id="estimating-att-with-a-regression" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Estimating ATT with a Regression<a href="#estimating-att-with-a-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is also often convenient to introduce a regression based estimator of the ATT. This is primarily convenient as it will allow us to leverage all the things we already know about regressions, and, particularly, we it will immediately provide us with standard errors, t-statistics, etc.</p>
<p>In order to do this, let’s introduce the following assumption:</p>
<p><strong>Treatment Effect Homogeneity</strong>: <span class="math inline">\(Y_i(1) - Y_i(0) = \alpha\)</span> (and <span class="math inline">\(\alpha\)</span> does not vary across individuals).</p>
<p>This is a potentially quite unrealistic assumption; I’ll make some additional comments about it below, but, for now, let’s just go with it.</p>
<p>Notice that we can also write
<span class="math display">\[\begin{align*}
  Y_i(0) = \beta_0 + U_i
\end{align*}\]</span>
where <span class="math inline">\(\mathbb{E}[U|D=0] = \mathbb{E}[U|D=1] = 0\)</span> (this holds under random assignment since random assignment implies that treated and untreated individuals do not have systematically different untreated potential outcomes)</p>
<p>Recalling the definition of the observed outcome, notice that
<span class="math display">\[\begin{align*}
  Y_i &amp;= D_i Y_i(1) + (1-D_i) Y_i(0) \\
  &amp;= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\
  &amp;= \alpha D_i + \beta_0 + U_i
\end{align*}\]</span>
where the second equality just involves re-arranging terms, and the third equality holds under treatment effect homogeneity and using the expression for <span class="math inline">\(Y_i(0)\)</span> in the previous equation.</p>
<p>This suggests running a regression of the observed <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(D_i\)</span> and interpreting the estimated version of <span class="math inline">\(\alpha\)</span> as an estimate of the causal effect of participating in the treatment (and you can pick up standard errors, etc. from the regression output) — this is very convenient.</p>
<p>The previous discussion invoked the extra condition of treatment effect homogeneity. I want to point out some things related to this now. In the above regression model, we can alternatively (and equivalently) write it as
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y|D] = \beta_0 + \alpha D
\end{align*}\]</span>
Now plug in particular values for <span class="math inline">\(D\)</span>:
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y|D=0] = \beta_0 \qquad \textrm{and} \qquad \mathbb{E}[Y|D=1] = \beta_0 + \alpha
\end{align*}\]</span>
Subtracting the second equation from the first implies that
<span class="math display">\[\begin{align*}
  \alpha = \mathbb{E}[Y|D=1] - \mathbb{E}[Y|D=0]
\end{align*}\]</span>
but notice that this is exactly what the <span class="math inline">\(ATT\)</span> is equal to under random assignment. Thus, it is worth pointing out that, although we imposed the assumption of treatment effect homogeneity to arrive at the regression equation, our regression is “robust” to treatment effect heterogeneity.</p>
</div>
<div id="internal-and-external-validity" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Internal and External Validity<a href="#internal-and-external-validity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SW 13.2</p>
<p>Although experiments, when they are available, are considered the gold standard of causal inference, there are some ways they can go wrong.</p>
<p>An experiment is said to be <strong>internally valid</strong> if inferences about causal effects are valid for the population being studied.</p>
<p>An experiment is said to be <strong>externally valid</strong> if inferences about causal effects can be generalized from the study setting to other populations/time periods/settings.</p>
<p>Generally, its easier for an experiment to be internally valid than externally valid, but there are a number of “threats” to both internal and external validity.</p>
<p>Threats to internal validity:</p>
<ul>
<li><p>Failure to randomize — individuals weren’t actually randomized into the treatment. This seems trivial, but some famous examples include experiments that assigned treatment participation by the first letter of a person’s last name. It turns out that this can be correlated with a number of other things (violating the random assignment assumption)</p>
<ul>
<li>As a side-comment, random treatment assignment implies that treatment should be uncorrelated with other covariates (e.g., race, sex, etc.) and this can be used as a way to check if the treatment was really randomly assigned.</li>
</ul></li>
<li><p>Failure to follow treatment protocol — Individuals assigned to participate in the treatment don’t participate or vice versa.</p>
<ul>
<li>This is actually a pretty common problem. A leading example would be in the context of an experimental drug where patients who were assigned the placebo somehow to convince the doctor to give them the real drug.</li>
</ul></li>
<li><p>Attrition — subjects non-randomly drop out of the sample</p></li>
<li><p>Placebo effects — Participating in an experiment can cause changes in behavior/outcomes.</p>
<ul>
<li>This is a main reason that medical experiments are often “double-blind”</li>
</ul></li>
<li><p>Small sample sizes — Experiments can be expensive to run and therefore often include only a small number of observations, and, therefore, may be less able to detect small effects of a treatment</p></li>
<li><p>Experiments can sometimes be unethical</p>
<ul>
<li>The 2019 Nobel prize in economics was awarded for experiments in developing countries. Some of these experiments were somewhat controversial (e.g., randomly assigning some schools extra resources and randomly giving glasses to some students but not others).</li>
</ul></li>
</ul>
<p>Threats to External Validity:</p>
<ul>
<li><p>Nonrepresentative samples — The population that is willing to participate in an experiment many be substantially different from the overall population.</p>
<ul>
<li>A closely related problem is that the economic environment may change across time and/or space which means that effects of some treatment might be different in different time periods or locations</li>
</ul></li>
<li><p>Nonrepresentative program/policy — The policy in an experiment may be different from other policies being considered</p>
<ul>
<li>For example, the Perry Preschool Project was an experiment that provided intensive pre-school program that seems to have had large and long-lasting effects on participants. It is not clear if these results should apply to less intensive programs like Head Start.</li>
</ul></li>
<li><p>General equilibrium effects — Expanding policies may alter the “state of the world” in a way that an experiment wouldn’t (e.g., the effects of a local minimum wage change could be different from a country-wide minimum wage change)</p></li>
</ul>
</div>
<div id="example-project-star" class="section level3 hasAnchor" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> Example: Project STAR<a href="#example-project-star" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Project STAR was a an experiment in Tennessee in 1980s on the effects of smaller class sizes on student performance where students and teachers were randomly assigned to be in a small class, a regular class, or a regular class with an aide. In this example, we’ll study the causal effect of being in a small class on reading test scores.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Star&quot;</span>, <span class="at">package=</span><span class="st">&quot;Ecdat&quot;</span>)</span>
<span id="cb148-2"><a href="#cb148-2" tabindex="-1"></a></span>
<span id="cb148-3"><a href="#cb148-3" tabindex="-1"></a><span class="co"># limit data to small class or regular class</span></span>
<span id="cb148-4"><a href="#cb148-4" tabindex="-1"></a><span class="co"># this drops the other category of regular class size with an aide</span></span>
<span id="cb148-5"><a href="#cb148-5" tabindex="-1"></a>Star <span class="ot">&lt;-</span> <span class="fu">subset</span>(Star, classk <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;small.class&quot;</span>, <span class="st">&quot;regular&quot;</span>))</span>
<span id="cb148-6"><a href="#cb148-6" tabindex="-1"></a></span>
<span id="cb148-7"><a href="#cb148-7" tabindex="-1"></a><span class="co"># regression for reading scores</span></span>
<span id="cb148-8"><a href="#cb148-8" tabindex="-1"></a>reading <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(treadssk) <span class="sc">~</span> classk, <span class="at">data=</span>Star)</span>
<span id="cb148-9"><a href="#cb148-9" tabindex="-1"></a><span class="fu">summary</span>(reading)</span>
<span id="cb148-10"><a href="#cb148-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb148-11"><a href="#cb148-11" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb148-12"><a href="#cb148-12" tabindex="-1"></a><span class="co">#&gt; lm(formula = log(treadssk) ~ classk, data = Star)</span></span>
<span id="cb148-13"><a href="#cb148-13" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb148-14"><a href="#cb148-14" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb148-15"><a href="#cb148-15" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb148-16"><a href="#cb148-16" tabindex="-1"></a><span class="co">#&gt; -0.31960 -0.04873 -0.00607  0.03929  0.36877 </span></span>
<span id="cb148-17"><a href="#cb148-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb148-18"><a href="#cb148-18" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb148-19"><a href="#cb148-19" tabindex="-1"></a><span class="co">#&gt;                   Estimate Std. Error  t value Pr(&gt;|t|)    </span></span>
<span id="cb148-20"><a href="#cb148-20" tabindex="-1"></a><span class="co">#&gt; (Intercept)       6.072176   0.001568 3871.502  &lt; 2e-16 ***</span></span>
<span id="cb148-21"><a href="#cb148-21" tabindex="-1"></a><span class="co">#&gt; classksmall.class 0.013324   0.002302    5.788  7.7e-09 ***</span></span>
<span id="cb148-22"><a href="#cb148-22" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb148-23"><a href="#cb148-23" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb148-24"><a href="#cb148-24" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb148-25"><a href="#cb148-25" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.07014 on 3731 degrees of freedom</span></span>
<span id="cb148-26"><a href="#cb148-26" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.0089, Adjusted R-squared:  0.008634 </span></span>
<span id="cb148-27"><a href="#cb148-27" tabindex="-1"></a><span class="co">#&gt; F-statistic:  33.5 on 1 and 3731 DF,  p-value: 7.699e-09</span></span></code></pre></div>
<p>You’ll notice that this is a very simple analysis, but that we have access to an experiment means that we do need to do anything complicated — we can just use a regression to compare the differences between test scores of students in small classes relative to large tests. These results say that small class sizes increase reading test scores of students by about 1.3%.</p>
<p>Let me also make a few comments about internal and external validity.</p>
<ul>
<li><p>I don’t have too much to say about internal validity. As far as I know, Project STAR is considered internally valid, but it would be worth reading more about it in order to confirm that.</p></li>
<li><p>Thinking about external validity though is quite interesting. For one thing, this experiment is quite old. Are the results still valid now? It is not clear; one could imagine that better teaching technologies now could possibly make having a small class less important. Another thing that is interesting is that this experiment was for very young students (I think students in kindergarten). Do these results imply that, say, high school or college students would also benefit from smaller class sizes? Again, this is not clear. It is a relevant piece of information, but it also would require a seemingly large amount of extrapolation to say that these results should inform policy decisions about class sizes in the present day and for different age groups.</p></li>
</ul>
</div>
</div>
<div id="unconfoundedness" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Unconfoundedness<a href="#unconfoundedness" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 6.8, SW Ch. 9</p>
<p>Unfortunately, we rarely have access to experimental data in applications or the ability to run experiments to evaluate causal effects programs/policies that we’d be interested in studying. For example, it’s hard to imagine convincing a large number of countries to randomly assign their interest rate, minimum wage, or immigration policies though these are all policies that economists would be interested in thinking about the causal effect of.</p>
<p>For the remainder of this chapter, we’ll discuss common approaches to causal inference when the treatment is not randomly assigned. We’ll start with what is probably the most common approach: <strong>unconfoundedness</strong>.</p>
<p><strong>Unconfoundedness Assumption</strong>:
<span class="math display">\[\begin{align*}
  (Y(1),Y(0)) \perp D | X
\end{align*}\]</span>
You can think of this as saying that, among individuals with the same covariates <span class="math inline">\(X\)</span>, they have the same distributions of potential outcomes regardless of whether or not they participate in the treatment. Note that the distribution of <span class="math inline">\(X\)</span> is still allowed to be different between the treated and untreated groups. In other words, after you condition on covariates, there is nothing special (in terms of the distributions of potential outcomes) about the group that participates in the treatment relative to the group that doesn’t participate in the treatment.</p>
<ul>
<li>This is potentially a strong assumption. In order to believe this assumption, you need to believe that untreated individuals with the same characteristics can deliver, on average, the outcome that individuals in the treated group would have experienced if they had not participated in the treatment. In math, you can write this as
<span class="math display">\[\begin{align*}
    \mathbb{E}[Y(0) | X, D=1] = \mathbb{E}[Y(0) | X, D=0]
  \end{align*}\]</span></li>
</ul>
<p>If you are willing to believe this assumption, then you can recover the <span class="math inline">\(ATT\)</span>. There are a few different ways that you could implement this. Probably the most common (and convenient) way is to link this condition to a regression (just like we did in the previous section on experiments).</p>
<p>To do this, let’s continue to make the treatment effect homogeneity assumption as above. In addition, let’s assume a linear model for untreated potential outcomes
<span class="math display">\[\begin{align*}
  Y(0) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\end{align*}\]</span>
and unconfoundedness implies that <span class="math inline">\(\mathbb{E}[U|X_1,X_2,X_3,D] = 0\)</span> (the conditioning on <span class="math inline">\(D\)</span> is the unconfoundedness part). Now, recalling the definition of the observed outcome, we can write
<span class="math display">\[\begin{align*}
  Y_i &amp;= D_i Y_i(1) + (1-D_i) Y_i(0) \\
  &amp;= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\
  &amp;= D_i \alpha + \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U_i
\end{align*}\]</span>
which suggests running the regression of observed <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1,X_2,X_3,\)</span> and <span class="math inline">\(D\)</span> and interpreting the estimate of <span class="math inline">\(\alpha\)</span> as the causal effect of participating in the treatment. In practice, this will be very similar to what we have done before — so the process would not be hard, but convincing someone (or even yourself) that unconfoundedness holds will be the bigger issue here.</p>
<p>As a final comment, the assumption of treatment effect homogeneity is not quite so innocuous here. It turns out that you can show that, in the presence of treatment effect heterogeneity, <span class="math inline">\(\alpha\)</span> will be equal to a weighted average of individual treatment effects, but the weights can sometimes be “strange”. There are methods that are robust to treatment effect heterogeneity (they are beyond the scope of the current class, but they are not “way” more difficult than what we are doing here). That said, in my experience, the regression estimators (under treatment effect homogeneity) tend to deliver similar estimates to alternative estimators that are robust to treatment effect heterogeneity at least in the setup considered in this section.</p>
</div>
<div id="panel-data-approaches" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Panel Data Approaches<a href="#panel-data-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW All of Ch. 10 and 13.4</p>
<p>In the previous section, we invoked the assumption of unconfoundedness and were in the setup where <span class="math inline">\(X\)</span> was fully observed. But suppose instead that you thought this alternative version of unconfoundedness held
<span class="math display">\[\begin{align*}
  (Y(1),Y(0)) \perp D | (X,W)
\end{align*}\]</span>
where <span class="math inline">\(X\)</span> were observed random variables, but <span class="math inline">\(W\)</span> were not observed. Following exactly the same argument as in the previous section, this would lead to a regression like
<span class="math display">\[\begin{align*}
  Y_i = \alpha D_i + \beta_0 + \beta_1 X_i + \beta_2 W_i + U_i
\end{align*}\]</span>
(I’m just including one <span class="math inline">\(X\)</span> and one <span class="math inline">\(W\)</span> for simplicity, but you can easily imagine the case where there are more.) If <span class="math inline">\(W\)</span> were observed, then we could just run this regression, but since <span class="math inline">\(W\)</span> is not observed, we run into the problem of omitted variable bias (i.e., if we just ignore <span class="math inline">\(W\)</span>, we won’t be estimating the causal effect <span class="math inline">\(\alpha\)</span>)</p>
<p>In this section, we’ll consider the case where a researcher has access to a different type of data called <strong>panel data</strong>. Panel data is data that follows the same individual (or firm, etc.) over time. In this case, it is often helpful to index variables by time. For example, <span class="math inline">\(Y_{it}\)</span> is the outcome for individual <span class="math inline">\(i\)</span> in time period <span class="math inline">\(t\)</span>. <span class="math inline">\(X_{it}\)</span> is the value of a regressor for individual <span class="math inline">\(i\)</span> in time period <span class="math inline">\(t\)</span> and <span class="math inline">\(D_{it}\)</span> is the value of the treatment for individual <span class="math inline">\(i\)</span> in time period <span class="math inline">\(t\)</span>. If some variable doesn’t vary over time (e.g., a regressor like race), we won’t use a <span class="math inline">\(t\)</span> subscript.</p>
<p>Panel data potentially gives us a way around the problem of not observing some variables that we would like to condition on in the model. This is particularly likely to be the case when <span class="math inline">\(W\)</span> does not vary over time. Let’s start with there are exactly two time periods of panel data. In that case, we can write
<span class="math display">\[\begin{align*}
  Y_{it} = \alpha D_{it} + \beta_0 + \beta_1 X_{it} + \beta_2 W_i + U_{it}
\end{align*}\]</span>
where we consider the case where <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> both change over time. Then, defining <span class="math inline">\(\Delta Y_{it} = Y_{it} - Y_{it-1}\)</span> (and using similar notation for other variables), notice that
<span class="math display">\[\begin{align*}
  \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \Delta U_{it}
\end{align*}\]</span>
which, importantly, no longer involves the unobserved <span class="math inline">\(W_i\)</span> and suggests running the above regression and interpreting the estimated version of <span class="math inline">\(\alpha\)</span> as an estimate of the causal effect of participating in the treatment.</p>
<ul>
<li><p><strong>Time fixed effects</strong> — The previous regression did not include an intercept. It is common in applied work to allow for the intercept to vary over time (i.e., so that <span class="math inline">\(\beta_0 = \beta_{0,t}\)</span>) which allows for “aggregate shocks” such as recessions or common trends in outcomes over time. In practice, this amounts to just including an intercept in the previous regression, for example,</p>
<p><span class="math display">\[
    \Delta Y_{it} = \underbrace{\theta_t} + \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \Delta U_{it}
  \]</span></p></li>
</ul>
<p>Often, there may be many omitted, time invariant variables. In practice, these are usually just lumped into a single <strong>individual fixed effect</strong> — even if there are many time invariant, unobserved variables, we can difference them all out at the same time
<span class="math display">\[\begin{align*}
  Y_{it} &amp;= \alpha D_{it} + \beta_{0,t} + \beta_1 X_{it} + \underbrace{\beta_2 W_{1i} + \beta_3 W_{2i} + \beta_4 W_{3i}} + U_{it} \\
  &amp;= \alpha D_{it} + \beta_{0,t} + \underbrace{\eta_i} + U_{it}
\end{align*}\]</span>
and we can follow the same strategies as above.</p>
<p>Another case that is common in practice is when there are more than two time periods. This case is similar to the previous one except there are multiple ways to eliminate the unobserved fixed effect. The two most common are the</p>
<ul>
<li><p><strong>Within estimator</strong></p>
<p>To motivate this approach, notice that, if, for each individual, we average their outcomes over time, the we get
<span class="math display">\[\begin{align*}
    \bar{Y}_i = \alpha \bar{D}_i + \beta_1 \bar{X}_i + (\textrm{time fixed effects}) + \bar{U}_i
  \end{align*}\]</span>
(where I have just written “time fixed effects” to indicate that these are transformed version of original fixed but still show up here.) Subtracting this equation from the expression for <span class="math inline">\(Y_{it}\)</span> gives
<span class="math display">\[\begin{align*}
    Y_{it} - \bar{Y}_i = \alpha (D_{it} - \bar{D}_i) + \beta_1 (X_{it} - \bar{X}_i) + (\textrm{time fixed effects}) + U_{it} - \bar{U}_i
  \end{align*}\]</span></p>
<p>This is a feasible regression to estimate (everything is observed here). This is called a within estimator because the terms <span class="math inline">\(\bar{Y}_i\)</span>, <span class="math inline">\(\bar{D}_i\)</span>, and <span class="math inline">\(\bar{X}_i\)</span> are the within-individual averages-over-time of the corresponding variable.</p></li>
<li><p><strong>First differences</strong></p>
<p>Another approach to eliminating the unobserved fixed effects is to directly consider <span class="math inline">\(\Delta Y_{it}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \Delta U_{it}
  \end{align*}\]</span></p>
<p>This is the same expression as we had before for the two period case. Only here you would include observations from all available time periods on <span class="math inline">\(\Delta Y_{it}, \Delta D_{it}, \Delta X_{it}\)</span> in the regression.</p></li>
</ul>
<p>It’s worth mentioning the cases where a fixed effects strategy can break down:</p>
<ul>
<li><p>Unobserved variables vary over time</p>
<p><span class="math display">\[
    Y_{it} = \alpha D_{it} + \beta_0 + \beta_1 X_{it} + \beta_2 \underbrace{W_{it}} + U_{it}
  \]</span>
In this case,</p>
<p><span class="math display">\[
    \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \beta_2 \underbrace{\Delta W_{it}} + \Delta U_{it}
  \]</span></p>
<p>which still involves the unobserved <span class="math inline">\(W_{it}\)</span>, and implies that the fixed effects regression will contain omitted variable bias.</p></li>
<li><p>The effect of unobserved variables varies over time</p>
<p><span class="math display">\[
    Y_{it} = \alpha D_{it} + \beta_0 + \beta_1 X_{it} + \underbrace{\beta_{2,t}} W_i + U_{it}
  \]</span>
In this case,</p>
<p><span class="math display">\[
    \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \underbrace{(\beta_{2,t} - \beta_{2,t-1})} W_i + \Delta U_{it}
  \]</span></p>
<p>which still involves the unobserved <span class="math inline">\(W_i\)</span> (even though it doesn’t vary over time) and, therefore, the fixed effects regressions we have been considering will contain omitted variable bias.</p></li>
</ul>
<p>Also, the assumption of treatment effect homogeneity can potentially matter a lot in this context. This will particularly be the case when (i) individuals can become treated at different points in time, and (ii) there are treatment effect dynamics (so that the effect of participating in the treatment can vary over time) — both of these are realistic in many applications. This is a main research area of mine and one I am happy to talk way more about.</p>
<div id="difference-in-differences" class="section level3 hasAnchor" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Difference in differences<a href="#difference-in-differences" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The panel data approaches that we have been talking about so far are closely related to a <strong>natural-experiment</strong> type of strategy called <strong>difference in differences</strong> (DID).</p>
<p>One important difference relative to the previous approach is that DID is typically implemented when some units (these are often states or particular locations) implement a policy at some time period while others do not; and, in particular, we observe some periods before any units participate in the treatment.</p>
<p>Let’s think about the case with exactly two time periods: <span class="math inline">\(t\)</span> and <span class="math inline">\(t-1\)</span>. In this case, we’ll suppose that the outcomes that we observe are
<span class="math display">\[\begin{align*}
  Y_{it} &amp;= D_i Y_{it}(1) + (1-D_i) Y_{it}(0) \\
  Y_{it-1} &amp;= Y_{it-1}(0)
\end{align*}\]</span>
In other words, in the second period, we observe treated potential outcomes for treated units and untreated potential outcomes for untreated units (this is just like the cross-sectional case above). But in the first period, we observe untreated potential outcomes for all units — because no one is treated yet.</p>
<p>DID is often motivated by an assumption called the parallel trends assumption:</p>
<p><strong>Parallel Trends Assumption</strong>
<span class="math display">\[\begin{align*}
\mathbb{E}[\Delta Y_t(0) | D=1] = \mathbb{E}[\Delta Y_t(0) | D=0]
\end{align*}\]</span>
This says that the <em>path</em> of outcomes that individuals in the treated group would have experienced if they had not been treated is the same as the path of outcomes that individual in the untreated group actually experienced.</p>
<p>As before, we continue to be interested in
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[Y_t(1) - Y_t(0) | D=1]
\end{align*}\]</span>
Recall that the key identification challenge if for <span class="math inline">\(\mathbb{E}[Y_t(0)|D=1]\)</span> here, and notice that
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y_t(0) | D=1] &amp;= \mathbb{E}[\Delta Y_t(0) | D=1] + \mathbb{E}[Y_{t-1}(0) | D=1] \\
  &amp;= \mathbb{E}[\Delta Y_t(0) | D=0] + \mathbb{E}[Y_{t-1}(0)|D=1] \\
  &amp;= \mathbb{E}[\Delta Y_t | D=0] + \mathbb{E}[Y_{t-1}|D=1]
\end{align*}\]</span>
where the first equality adds and subtracts <span class="math inline">\(\mathbb{E}[Y_{t-1}(0)|D=1]\)</span>, the second equality uses the parallel trends assumption, and the last equality holds because all the potential outcomes in the previous line are actually observed outcome. Plugging this expression into the one for <span class="math inline">\(ATT\)</span> yields:
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[\Delta Y_t | D=1] - \mathbb{E}[\Delta Y_t | D=0]
\end{align*}\]</span>
In other words, under parallel trends, the <span class="math inline">\(ATT\)</span> can be recovered by comparing the path of outcomes that treated units experienced relative to the path of outcomes that untreated units experienced (the latter of which is the path of outcomes that treated units would have experienced if they had not participated in the treatment).</p>
<p>As above, it is often convenient to estimate <span class="math inline">\(ATT\)</span> using a regression. In fact, you can show that (in the case with two periods), <span class="math inline">\(\alpha\)</span> in the following regression is equal to the <span class="math inline">\(ATT\)</span>:
<span class="math display">\[\begin{align*}
  Y_{it} = \alpha D_{it} + \theta_t + \eta_i + v_{it}
\end{align*}\]</span>
where <span class="math inline">\(\mathbb{E}[v_t | D] = 0\)</span>.</p>
</div>
<div id="computation-10" class="section level3 hasAnchor" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> Computation<a href="#computation-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The lab in this chapter uses panel data to think about causal effects, so I won’t provide an extended discussion here but rather just mention the syntax for a few panel data estimators. Let’s suppose that you had a data frame that, for the first few rows, looked like (this is totally made up data)</p>
<pre><code>#&gt;   id year         Y       X1 X2
#&gt; 1  1 2019  87.92934 495.4021  1
#&gt; 2  1 2020 102.77429 495.6269  1
#&gt; 3  1 2021 110.84441 495.4844  0
#&gt; 4  2 2019  76.54302 492.8797  1
#&gt; 5  2 2020 104.29125 496.1825  1
#&gt; 6  2 2021 105.06056 492.0129  1</code></pre>
<p>This is what panel data typically looks like — here, we are following a single individual (who is distinguished by their <code>id</code> variable) over three years (from 2019-2021), there is an outcome <code>Y</code> and potential regressors <code>X1</code> and <code>X2</code>.</p>
<p>There are several packages in <code>R</code> for estimating the fixed effects models that we have been considering. I mainly use <code>plm</code> (for “panel linear models”), so I’ll show you that one and then mention one more.</p>
<p>For <code>plm</code>, if you want to estimate a fixed effects model in first differences, you would use the <code>plm</code> command with the following sort of syntax</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="#cb150-1" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb150-2"><a href="#cb150-2" tabindex="-1"></a><span class="fu">plm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> <span class="fu">as.factor</span>(year), </span>
<span id="cb150-3"><a href="#cb150-3" tabindex="-1"></a>    <span class="at">data=</span>name_of_data,</span>
<span id="cb150-4"><a href="#cb150-4" tabindex="-1"></a>    <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb150-5"><a href="#cb150-5" tabindex="-1"></a>    <span class="at">model=</span><span class="st">&quot;fd&quot;</span>,</span>
<span id="cb150-6"><a href="#cb150-6" tabindex="-1"></a>    <span class="at">index=</span><span class="st">&quot;id&quot;</span>)</span></code></pre></div>
<p>We include here <code>as.factor(year)</code> to include time fixed effects, <code>effect="individual"</code> means to include an individual fixed effect, <code>model="fd"</code> says to estimate the model in first differences, and <code>index="id"</code> means that the individual identifier is in the column “id”.</p>
<p>The code for estimating the model using a within transformation is very similar:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" tabindex="-1"></a><span class="fu">plm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> <span class="fu">as.factor</span>(year), </span>
<span id="cb151-2"><a href="#cb151-2" tabindex="-1"></a>    <span class="at">data=</span>name_of_data,</span>
<span id="cb151-3"><a href="#cb151-3" tabindex="-1"></a>    <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb151-4"><a href="#cb151-4" tabindex="-1"></a>    <span class="at">model=</span><span class="st">&quot;within&quot;</span>,</span>
<span id="cb151-5"><a href="#cb151-5" tabindex="-1"></a>    <span class="at">index=</span><span class="st">&quot;id&quot;</span>)</span></code></pre></div>
<p>The only difference is that <code>model="fd"</code> has been replaced with <code>model="within"</code>.</p>
<p>Let me also just mention that the <code>estimatr</code> package can estimate a fixed effects model using a within transformation. The code for this case would look like</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span>
<span id="cb152-2"><a href="#cb152-2" tabindex="-1"></a></span>
<span id="cb152-3"><a href="#cb152-3" tabindex="-1"></a><span class="fu">lm_robust</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> <span class="fu">as.factor</span>(year), </span>
<span id="cb152-4"><a href="#cb152-4" tabindex="-1"></a>          <span class="at">data=</span>name_of_data,</span>
<span id="cb152-5"><a href="#cb152-5" tabindex="-1"></a>          <span class="at">fixed_effects=</span><span class="sc">~</span>id)</span></code></pre></div>
<p>I think the advantage of using this approach is that it seems straightforward to get the heteroskedasticity robust standard errors (or cluster-robust standard errors) that are popular in economics (as we have done before for heteroskedasticity robust standard errors for a regression with just cross sectional data). But I am not sure how (or if it is possible) to use <code>estimatr</code> to estimate the fixed effects model in first differences.</p>
</div>
</div>
<div id="instrumental-variables" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Instrumental Variables<a href="#instrumental-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW all of chapter 12</p>
<p>In the previous section, I used the word <strong>natural experiment</strong> but didn’t really define it. When an actual experiment is not actually available, a very common strategy used by researchers interested in causal effects is to consider natural experiments — these are not actual experiments, but more like the case where “something weird” happens that makes some individuals more likely to participate in the treatment without otherwise affecting their outcomes. This “something weird” is called an <strong>instrumental variable</strong>.</p>
<p>Let me give you some examples:</p>
<ul>
<li><p>This is not as popular of a topic as it used to be, but many economists used to be interested in the causal effect of military service on earnings. This is challenging because individuals “self-select” into the military (i.e., individuals don’t just randomly choose to join the military, and, while there may be many dimensions of choosing to join the military, probably one dimension is what a person expects the effect to be on their future earnings).</p>
<ul>
<li>A famous example of an instrumental variable in this case is an individual’s Vietname draft lottery number. Here, the idea is that a randomly generated lottery number (by construction) doesn’t have any direct effect on earnings, but it does affect the chances that someone participates in the military. This is therefore a natural experiment and could serve the role of an instrumental variable.</li>
</ul></li>
<li><p>For studying the effect of education on on earnings, researchers have used the day of birth as an instrument for years of education. The idea is that compulsory school laws are set up so that individuals can leave school when they reach a certain age (e.g., 16). But this means that, among students that want to drop out as early as they can, students who have an “early” birthday (usually around October) will have spent less time in school than students who have a “late” birthday (usually around July) at any particular age. This is a kind of natural experiment — comparing earnings of students who drop out at 16 for those who have early birthdays relative to late birthdays.</p></li>
</ul>
<p>Let’s formalize these arguments. Using the same arguments as before, suppose we have a regression that we’d like to run</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \alpha D_i + \underbrace{\beta_1 W_i + U_i}_{V_i}
\]</span>
and interpret our estimate of <span class="math inline">\(\alpha\)</span> as an estimate of the causal effect of participating in the treatment. And where, for simplicity, I am not including any <span class="math inline">\(X\)</span> covariates and where we do not observe <span class="math inline">\(W\)</span>. If <span class="math inline">\(D\)</span> is correlated with <span class="math inline">\(W\)</span>, then just ignoring <span class="math inline">\(W\)</span> and running a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> will result in omitted variable bias so that regression does not recover an estimate of <span class="math inline">\(\alpha\)</span>. To help with the discussion below, we’ll define <span class="math inline">\(V_i\)</span> to be the entire unobservable term, <span class="math inline">\(\beta_1 W_i + U_i\)</span>, in the above equation.</p>
<p>An instrumental variable, which we’ll call <span class="math inline">\(Z\)</span>, needs to satisfy the following two conditions:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathrm{cov}(Z,V) = 0\)</span> — This condition is called the <strong>exclusion restriction</strong>, and it means that the instrument is uncorrelated with the error term in the above equation. In practice, we’d mainly need to make sure that it is uncorrelated with whatever we think is in <span class="math inline">\(W\)</span>.</p></li>
<li><p><span class="math inline">\(\mathrm{cov}(Z,D) \neq 0\)</span> — This condition is called <strong>instrument relevance</strong>, and it means that the instrument needs to actually affect whether or not an individual participates in the treatment. We’ll see why this condition is important momentarily.</p></li>
</ol>
<p>Next, notice that</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{cov}(Z,Y) &amp;= \mathrm{cov}(Z,\beta_0 + \alpha D + V) \\
  &amp;= \alpha \mathrm{cov}(Z,D)
  \end{aligned}
\]</span>
which holds because <span class="math inline">\(\mathrm{cov}(Z,\beta_0) = 0\)</span> (because <span class="math inline">\(\beta_0\)</span> is a constant) and <span class="math inline">\(\mathrm{cov}(Z,V)=0\)</span> by the first condition of <span class="math inline">\(Z\)</span> being a valid instrument. This implies that</p>
<p><span class="math display">\[
  \alpha = \frac{\mathrm{cov}(Z,Y)}{\mathrm{cov}(Z,D)}
\]</span>
That is, if we have a valid instrument, the above formula gives us a path to recovering the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. [Now you can also see why we needed the second condition — otherwise, we could divide by 0 here.]</p>
<p>The intuition for this is the following: changes in the instrument can cause changes in the outcome but only because they can change whether or not an individual participates in the treatment. These changes show up in the numerator. They are scaled by how much changes in the instrument result in changes in the treatment.</p>
<p>If there are other covariates in the model, the formula for <span class="math inline">\(\alpha\)</span> will become more complicated. But you can use the <code>ivreg</code> function in the <code>ivreg</code> package to make these complications for you.</p>
<div id="example-return-to-education" class="section level3 hasAnchor" number="8.6.1">
<h3><span class="header-section-number">8.6.1</span> Example: Return to Education<a href="#example-return-to-education" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this example, we’ll estimate the return to education using whether or not an individual lives close to a college as an instrument for attending college. The idea is that (at least after controlling for some other covariates), the distance that a person lives from a college should not directly affect their earnings but it could affect whether or not they attend college due to it being more or less convenient. I think that the papers that use this sort of an idea primarily have in mind that distance-to-college may affect whether or not a student attends a community college rather than a university.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" tabindex="-1"></a><span class="fu">library</span>(ivreg)</span>
<span id="cb153-2"><a href="#cb153-2" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb153-3"><a href="#cb153-3" tabindex="-1"></a></span>
<span id="cb153-4"><a href="#cb153-4" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;SchoolingReturns&quot;</span>, <span class="at">package=</span><span class="st">&quot;ivreg&quot;</span>)</span>
<span id="cb153-5"><a href="#cb153-5" tabindex="-1"></a></span>
<span id="cb153-6"><a href="#cb153-6" tabindex="-1"></a>lm_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> education <span class="sc">+</span> <span class="fu">poly</span>(experience, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> ethnicity <span class="sc">+</span> smsa <span class="sc">+</span> south,</span>
<span id="cb153-7"><a href="#cb153-7" tabindex="-1"></a>  <span class="at">data =</span> SchoolingReturns)</span>
<span id="cb153-8"><a href="#cb153-8" tabindex="-1"></a></span>
<span id="cb153-9"><a href="#cb153-9" tabindex="-1"></a>iv_reg <span class="ot">&lt;-</span> <span class="fu">ivreg</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> education <span class="sc">+</span> <span class="fu">poly</span>(experience, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> ethnicity <span class="sc">+</span> smsa <span class="sc">+</span> south, </span>
<span id="cb153-10"><a href="#cb153-10" tabindex="-1"></a>  <span class="sc">~</span> nearcollege <span class="sc">+</span> <span class="fu">poly</span>(age, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> ethnicity <span class="sc">+</span> smsa <span class="sc">+</span> south,</span>
<span id="cb153-11"><a href="#cb153-11" tabindex="-1"></a>  <span class="at">data =</span> SchoolingReturns)</span>
<span id="cb153-12"><a href="#cb153-12" tabindex="-1"></a></span>
<span id="cb153-13"><a href="#cb153-13" tabindex="-1"></a>reg_list <span class="ot">&lt;-</span> <span class="fu">list</span>(lm_reg, iv_reg)</span>
<span id="cb153-14"><a href="#cb153-14" tabindex="-1"></a></span>
<span id="cb153-15"><a href="#cb153-15" tabindex="-1"></a><span class="fu">modelsummary</span>(reg_list)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
 (1)
</th>
<th style="text-align:center;">
  (2)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:center;">
4.734
</td>
<td style="text-align:center;">
4.066
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.068)
</td>
<td style="text-align:center;">
(0.608)
</td>
</tr>
<tr>
<td style="text-align:left;">
education
</td>
<td style="text-align:center;">
0.074
</td>
<td style="text-align:center;">
0.133
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.004)
</td>
<td style="text-align:center;">
(0.051)
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(experience, 2, raw = TRUE)1
</td>
<td style="text-align:center;">
0.084
</td>
<td style="text-align:center;">
0.056
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.007)
</td>
<td style="text-align:center;">
(0.026)
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(experience, 2, raw = TRUE)2
</td>
<td style="text-align:center;">
−0.002
</td>
<td style="text-align:center;">
−0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.000)
</td>
<td style="text-align:center;">
(0.001)
</td>
</tr>
<tr>
<td style="text-align:left;">
ethnicityafam
</td>
<td style="text-align:center;">
−0.190
</td>
<td style="text-align:center;">
−0.103
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.018)
</td>
<td style="text-align:center;">
(0.077)
</td>
</tr>
<tr>
<td style="text-align:left;">
smsayes
</td>
<td style="text-align:center;">
0.161
</td>
<td style="text-align:center;">
0.108
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.016)
</td>
<td style="text-align:center;">
(0.050)
</td>
</tr>
<tr>
<td style="text-align:left;">
southyes
</td>
<td style="text-align:center;">
−0.125
</td>
<td style="text-align:center;">
−0.098
</td>
</tr>
<tr>
<td style="text-align:left;box-shadow: 0px 1.5px">
</td>
<td style="text-align:center;box-shadow: 0px 1.5px">
(0.015)
</td>
<td style="text-align:center;box-shadow: 0px 1.5px">
(0.029)
</td>
</tr>
<tr>
<td style="text-align:left;">
Num.Obs.
</td>
<td style="text-align:center;">
3010
</td>
<td style="text-align:center;">
3010
</td>
</tr>
<tr>
<td style="text-align:left;">
R2
</td>
<td style="text-align:center;">
0.291
</td>
<td style="text-align:center;">
0.176
</td>
</tr>
<tr>
<td style="text-align:left;">
R2 Adj.
</td>
<td style="text-align:center;">
0.289
</td>
<td style="text-align:center;">
0.175
</td>
</tr>
<tr>
<td style="text-align:left;">
AIC
</td>
<td style="text-align:center;">
40329.6
</td>
<td style="text-align:center;">
40778.6
</td>
</tr>
<tr>
<td style="text-align:left;">
BIC
</td>
<td style="text-align:center;">
40377.7
</td>
<td style="text-align:center;">
40826.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Log.Lik.
</td>
<td style="text-align:center;">
−1308.702
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:center;">
204.932
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;">
RMSE
</td>
<td style="text-align:center;">
0.37
</td>
<td style="text-align:center;">
0.40
</td>
</tr>
</tbody>
</table>
<p>The main parameter of interest here is the coefficient on <code>education</code>. The IV estimates are noticeably larger than the OLS estimates (<code>0.133</code> relative to <code>0.074</code>). [This is actually quite surprising as you would think that OLS would tend to <em>over-estimate</em> the return to education. This is a very famous example, and there are actually quite a few “explanations” from labor economists about why this sort of result arises.]</p>
</div>
</div>
<div id="regression-discontinuity" class="section level2 hasAnchor" number="8.7">
<h2><span class="header-section-number">8.7</span> Regression Discontinuity<a href="#regression-discontinuity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SW 13.4</p>
<p>The final type of natural experiment that we will talk about is called <strong>regression discontinuity</strong>. The sort of natural experiment is available when there is a <strong>running variable</strong> with a threshold (i.e., cutoff) where individuals above the threshold are treated while individuals below the threshold are not treated. These sorts of thresholds/cutoffs are fairly common.</p>
<p>Here are some examples:</p>
<ul>
<li><p>Cutoffs that make students eligible for a scholarship (e.g., the Hope scholarship)</p></li>
<li><p>Rules about maximum numbers of students allowed in a classroom in a particular school district</p></li>
<li><p>Very close political elections</p></li>
<li><p>Very close union elections</p></li>
<li><p>Thresholds in tax laws</p></li>
</ul>
<p>Then, the idea is to compare outcomes among individuals that “barely” were treated relative to those that “barely” weren’t treated. By construction, this often has properties that are similar to an actual experiment as those that are just above the cutoff should have observed and unobserved characteristics that are the same as those just below the cutoff.</p>
<p>Typically, regression discontinuity designs are implemented using a regression that includes a binary variable for participating in the treatment, the running variable itself, and the interaction between the running variable and the treatment, <em>using only observations that are “close” to the cutoff</em>. [What should be considered “close” to the cutoff is actually a hard choice and there are tons of papers suggesting various approaches to decide what “close” means — we’ll largely avoid this and just pick what we think is close.] The estimated coefficient on the treatment indicator variable is an estimate of the average effect of participating in the treatment among those individuals who are close to the cutoff.</p>
<p>This will become clearer with an example.</p>
<div id="example-causal-effect-of-alcohol-on-driving-deaths" class="section level3 hasAnchor" number="8.7.1">
<h3><span class="header-section-number">8.7.1</span> Example: Causal effect of Alcohol on Driving Deaths<a href="#example-causal-effect-of-alcohol-on-driving-deaths" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we’ll be interested in the causal effect of young adult alcohol consumption on the number of deaths in car accidents.</p>
<p>The idea here will be to compare the number of deaths in car accidents that involve someone who is 21 or just over to the number of deaths in car accidents that involve someone who is just under 21. The reason to make this comparison is that alcohol consumption markedly increases when individuals turn 21 (due to that being the legal drinking age in the U.S.). If alcohol consumption increases car accident deaths, then we should also be able to detect a jump in the number of car accident deaths involving those who are just over 21.</p>
<p>The data that we have consists of age groups by age up to a particular month (<code>agecell</code>) and the number of car accident deaths involving that age group (<code>mva</code>).</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb154-2"><a href="#cb154-2" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;mlda&quot;</span>, <span class="at">package=</span><span class="st">&quot;masteringmetrics&quot;</span>)</span>
<span id="cb154-3"><a href="#cb154-3" tabindex="-1"></a></span>
<span id="cb154-4"><a href="#cb154-4" tabindex="-1"></a><span class="co"># drop some data with missing observations</span></span>
<span id="cb154-5"><a href="#cb154-5" tabindex="-1"></a>mlda <span class="ot">&lt;-</span> mlda[<span class="fu">complete.cases</span>(mlda),]</span>
<span id="cb154-6"><a href="#cb154-6" tabindex="-1"></a></span>
<span id="cb154-7"><a href="#cb154-7" tabindex="-1"></a><span class="co"># create treated variable</span></span>
<span id="cb154-8"><a href="#cb154-8" tabindex="-1"></a>mlda<span class="sc">$</span>D <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(mlda<span class="sc">$</span>agecell <span class="sc">&gt;=</span> <span class="dv">21</span>)</span></code></pre></div>
<p>In regression discontinuity designs, it is very common to show a plot of the data. That’s what we’ll do here.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1" tabindex="-1"></a><span class="fu">ggplot</span>(mlda, <span class="fu">aes</span>(<span class="at">x=</span>agecell, <span class="at">y=</span>mva)) <span class="sc">+</span> </span>
<span id="cb155-2"><a href="#cb155-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb155-3"><a href="#cb155-3" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">21</span>) <span class="sc">+</span> </span>
<span id="cb155-4"><a href="#cb155-4" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;age&quot;</span>) <span class="sc">+</span> </span>
<span id="cb155-5"><a href="#cb155-5" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;moving vehicle accident deaths&quot;</span>) <span class="sc">+</span> </span>
<span id="cb155-6"><a href="#cb155-6" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-159-1.png" width="672" /></p>
<p>This figure at least suggests that the number of car accident deaths does appear to jump at age 21.</p>
<p>Now, let’s run the regression that we talked about earlier, involving a treatment dummy, age, and age interacted with the treatment.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" tabindex="-1"></a>rd_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(mva <span class="sc">~</span> D <span class="sc">+</span> agecell <span class="sc">+</span> agecell<span class="sc">*</span>D, <span class="at">data=</span>mlda)</span>
<span id="cb156-2"><a href="#cb156-2" tabindex="-1"></a><span class="fu">summary</span>(rd_reg)</span>
<span id="cb156-3"><a href="#cb156-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-4"><a href="#cb156-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb156-5"><a href="#cb156-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = mva ~ D + agecell + agecell * D, data = mlda)</span></span>
<span id="cb156-6"><a href="#cb156-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-7"><a href="#cb156-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb156-8"><a href="#cb156-8" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb156-9"><a href="#cb156-9" tabindex="-1"></a><span class="co">#&gt; -2.4124 -0.7774 -0.2913  0.8495  3.2378 </span></span>
<span id="cb156-10"><a href="#cb156-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-11"><a href="#cb156-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb156-12"><a href="#cb156-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb156-13"><a href="#cb156-13" tabindex="-1"></a><span class="co">#&gt; (Intercept)  83.8492     9.3328   8.984 1.63e-11 ***</span></span>
<span id="cb156-14"><a href="#cb156-14" tabindex="-1"></a><span class="co">#&gt; D            28.9450    13.8638   2.088   0.0426 *  </span></span>
<span id="cb156-15"><a href="#cb156-15" tabindex="-1"></a><span class="co">#&gt; agecell      -2.5676     0.4661  -5.508 1.77e-06 ***</span></span>
<span id="cb156-16"><a href="#cb156-16" tabindex="-1"></a><span class="co">#&gt; D:agecell    -1.1624     0.6592  -1.763   0.0848 .  </span></span>
<span id="cb156-17"><a href="#cb156-17" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb156-18"><a href="#cb156-18" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb156-19"><a href="#cb156-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-20"><a href="#cb156-20" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 1.299 on 44 degrees of freedom</span></span>
<span id="cb156-21"><a href="#cb156-21" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7222, Adjusted R-squared:  0.7032 </span></span>
<span id="cb156-22"><a href="#cb156-22" tabindex="-1"></a><span class="co">#&gt; F-statistic: 38.13 on 3 and 44 DF,  p-value: 2.671e-12</span></span></code></pre></div>
<p>These results suggest that alcohol consumption increased car accident deaths (you can see this from the estimated coefficient on <code>D</code>). [The setup in this example is somewhat simplified; if you wanted to be careful about how much alcohol consumption increased car accident deaths, then we would probably need to scale up our estimate by how much alcohol consumption increases on average when people turn 21. Nevertheless, what we have presented above does suggest that alcohol consumption increases car accident deaths.]</p>
<p>Finally, let me show one more plot that is common to report in a regression discontinuity design.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" tabindex="-1"></a><span class="co"># get predicted values for plotting</span></span>
<span id="cb157-2"><a href="#cb157-2" tabindex="-1"></a>mlda<span class="sc">$</span>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(rd_reg)</span>
<span id="cb157-3"><a href="#cb157-3" tabindex="-1"></a></span>
<span id="cb157-4"><a href="#cb157-4" tabindex="-1"></a><span class="co"># make plot</span></span>
<span id="cb157-5"><a href="#cb157-5" tabindex="-1"></a><span class="fu">ggplot</span>(mlda, <span class="fu">aes</span>(<span class="at">x=</span>agecell, <span class="at">y=</span>mva, <span class="at">color=</span><span class="fu">as.factor</span>(D))) <span class="sc">+</span></span>
<span id="cb157-6"><a href="#cb157-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb157-7"><a href="#cb157-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>preds)) <span class="sc">+</span> </span>
<span id="cb157-8"><a href="#cb157-8" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">21</span>) <span class="sc">+</span></span>
<span id="cb157-9"><a href="#cb157-9" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color=</span><span class="st">&quot;treated&quot;</span>, <span class="at">x=</span><span class="st">&quot;age&quot;</span>, <span class="at">y=</span><span class="st">&quot;moving vehicle accident deaths&quot;</span>) <span class="sc">+</span> </span>
<span id="cb157-10"><a href="#cb157-10" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-161-1.png" width="672" /></p>
<p>This shows the two lines that we effectively fit with the regression that included the binary variable for the treatment, the running variable, and their interaction. The “jump” between the red line and the blue line at <code>age=21</code> is our estimated effect of the treatment.</p>
</div>
</div>
<div id="lab-7-drunk-driving-laws" class="section level2 hasAnchor" number="8.8">
<h2><span class="header-section-number">8.8</span> Lab 7: Drunk Driving Laws<a href="#lab-7-drunk-driving-laws" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this lab, we will use the <code>Fatalities</code> data. We will study the causal effect of mandatory jail sentence policies for drunk driving on traffic fatalities. The <code>Fatalities</code> data consists of panel data of traffic fatality death rates, whether or not a state has a mandatory jail sentence policy or not as well as several other variables from 1982-1988. Economic theory suggests that raising the cost of some behavior (in this case, you can think of a mandatory jail sentence as raising the cost of drunk driving) will lead to less of that behavior. That being said, it’s both interesting to test this theory and also consider the magnitude of this effect. That’s what we’ll do in this problem.</p>
<ol style="list-style-type: decimal">
<li><p>This data comes in a somewhat messier format than some of the data that we have used previously. To start with, create a new column in the data called <code>afatal_per_million</code> that is the number of alcohol involved vehicle fatalities per millions people in a state in a particular year. The variable <code>afatal</code> contains the total number of alcohol involved vehicle fatalities, and the variable <code>pop</code> contains the total population in a state.</p></li>
<li><p>Using a subset of the data from 1988, run a regression of <code>afatal_per_million</code> on whether or not a state has a mandatory jail sentence policy <code>jail</code>. How do you interpret the results?</p></li>
<li><p>Using the same subset from part 2, run a regression of <code>afatal_per_million</code> on <code>jail</code>, unemployment rate (<code>unemp</code>), the tax on a case of beer (<code>beertax</code>), the percentage of southern baptists in the state (<code>baptist</code>), the percentage of residents residing in dry counties (<code>dry</code>), the percentage of young drivers in the state, (<code>youngdrivers</code>), and the average miles driven per person in a state (<code>miles</code>). How do you interpret the estimated coefficient on <code>jail</code>? Would you consider this to be a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related fatalities?</p></li>
<li><p>Now, using the full data, let’s estimate a fixed effects model with alcohol related fatalities per million as the outcome and mandatory jail policies as a regressor. Estimate the model using first differences and make sure to include time fixed effects. How do you interpret the results?</p></li>
<li><p>Estimate the same model as in part 4, but using the within estimator instead of first differences. Compare these results to the ones from part 4.</p></li>
<li><p>Using the same within estimator as in part 5, include the same set of covariates from part 3 and interpret the estimated effect of mandatory jail policies. How do these estimates compare to the earlier ones?</p></li>
<li><p>Now, we’ll switch to using a difference in differences approach to estimating the effect of mandatory jail policies. First, we’ll manipulate the data some.</p>
<ol style="list-style-type: lower-alpha">
<li><p>To keep things simple, let’s start by limiting the data to the years 1982 and 1988 and drop the in-between periods.</p></li>
<li><p>Second, let’s calculate the change in alcohol related fatalities per million between 1982 and 1998 and keep the covariates that we have been using from 1982. One way to do this, is to use the <code>pivot_wider</code> function from the <code>tidyr</code>. In the case of panel data, “long format” data means that each row in the data corresponds to a paricular observation <em>and</em> a particular time period. Thus, with long format data, there are <span class="math inline">\(n \times T\)</span> total rows in the data. On the other hand, “wide format” data means that each row holds all the data (across all time periods) for a particular observation. Converting back and forth between long and wide formats is a common data manipulation task. <strong>Hint:</strong> This step is probably unfamiliar, so I’d recommend seeing if you can use <code>?tidyr::pivot_wider</code> to see if you can figure out how to complete this step, but, if not, you can copy this code from the solutions in the next section.</p></li>
<li><p>Finally, drop all states that are already treated in 1982.</p></li>
</ol></li>
<li><p>Using the data that you constructed in part 7, implement the difference in differences regression of the change in alcohol related fatalities per million from 1982 to 1988 on the mandatory jail policy. How do you interpret these results and how do they compare to the previous ones? Now, additionally include the set of covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?</p></li>
<li><p>An alternative to DID, is to include the lagged outcome as a covariate. Using the data constructed in part 7, run a regression of alcohol related fatalities per million in 1988 on the mandatory jail policy and alcohol related fatalities per million in 1982. How do you interpret these results and how do they compare to the previous ones? Now include the additional covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?</p></li>
<li><p>Comment on your results from parts 1-9. Which, if any, of these are you most inclined to interpret as a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related policies?</p></li>
</ol>
</div>
<div id="lab-7-solutions" class="section level2 hasAnchor" number="8.9">
<h2><span class="header-section-number">8.9</span> Lab 7: Solutions<a href="#lab-7-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="#cb158-1" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb158-2"><a href="#cb158-2" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb158-3"><a href="#cb158-3" tabindex="-1"></a></span>
<span id="cb158-4"><a href="#cb158-4" tabindex="-1"></a><span class="fu">data</span>(Fatalities, <span class="at">package=</span><span class="st">&quot;AER&quot;</span>)</span>
<span id="cb158-5"><a href="#cb158-5" tabindex="-1"></a></span>
<span id="cb158-6"><a href="#cb158-6" tabindex="-1"></a>Fatalities<span class="sc">$</span>afatal_per_million <span class="ot">&lt;-</span> <span class="dv">1000000</span> <span class="sc">*</span> (Fatalities<span class="sc">$</span>afatal <span class="sc">/</span> Fatalities<span class="sc">$</span>pop )</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" tabindex="-1"></a>Fatalities88 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Fatalities, year<span class="sc">==</span><span class="dv">1988</span>)</span>
<span id="cb159-2"><a href="#cb159-2" tabindex="-1"></a></span>
<span id="cb159-3"><a href="#cb159-3" tabindex="-1"></a>reg88 <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million <span class="sc">~</span> jail, <span class="at">data=</span>Fatalities88)</span>
<span id="cb159-4"><a href="#cb159-4" tabindex="-1"></a><span class="fu">summary</span>(reg88)</span>
<span id="cb159-5"><a href="#cb159-5" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-6"><a href="#cb159-6" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb159-7"><a href="#cb159-7" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million ~ jail, data = Fatalities88)</span></span>
<span id="cb159-8"><a href="#cb159-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-9"><a href="#cb159-9" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb159-10"><a href="#cb159-10" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb159-11"><a href="#cb159-11" tabindex="-1"></a><span class="co">#&gt; -36.123 -16.622  -1.469   8.642 112.260 </span></span>
<span id="cb159-12"><a href="#cb159-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-13"><a href="#cb159-13" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb159-14"><a href="#cb159-14" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb159-15"><a href="#cb159-15" tabindex="-1"></a><span class="co">#&gt; (Intercept)   59.496      4.273  13.923   &lt;2e-16 ***</span></span>
<span id="cb159-16"><a href="#cb159-16" tabindex="-1"></a><span class="co">#&gt; jailyes        9.155      7.829   1.169    0.248    </span></span>
<span id="cb159-17"><a href="#cb159-17" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb159-18"><a href="#cb159-18" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb159-19"><a href="#cb159-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-20"><a href="#cb159-20" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 24.55 on 45 degrees of freedom</span></span>
<span id="cb159-21"><a href="#cb159-21" tabindex="-1"></a><span class="co">#&gt;   (1 observation deleted due to missingness)</span></span>
<span id="cb159-22"><a href="#cb159-22" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.02949,    Adjusted R-squared:  0.007921 </span></span>
<span id="cb159-23"><a href="#cb159-23" tabindex="-1"></a><span class="co">#&gt; F-statistic: 1.367 on 1 and 45 DF,  p-value: 0.2484</span></span></code></pre></div>
<p>The estimated coefficient on mandatory jail laws is 9.155. We should interpret this as just the difference between alcohol related fatalities per million in states that had mandatory jail laws in 1988 relative to states that did not have them. We cannot reject that there is no difference between states where the policy is in place relative to those that do not have the policy.</p>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" tabindex="-1"></a>reg88_covs <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles, <span class="at">data=</span>Fatalities88)</span>
<span id="cb160-2"><a href="#cb160-2" tabindex="-1"></a><span class="fu">summary</span>(reg88_covs)</span>
<span id="cb160-3"><a href="#cb160-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-4"><a href="#cb160-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb160-5"><a href="#cb160-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + </span></span>
<span id="cb160-6"><a href="#cb160-6" tabindex="-1"></a><span class="co">#&gt;     dry + youngdrivers + miles, data = Fatalities88)</span></span>
<span id="cb160-7"><a href="#cb160-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-8"><a href="#cb160-8" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb160-9"><a href="#cb160-9" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb160-10"><a href="#cb160-10" tabindex="-1"></a><span class="co">#&gt; -39.065  -9.907  -1.690   9.673  82.100 </span></span>
<span id="cb160-11"><a href="#cb160-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-12"><a href="#cb160-12" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb160-13"><a href="#cb160-13" tabindex="-1"></a><span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span id="cb160-14"><a href="#cb160-14" tabindex="-1"></a><span class="co">#&gt; (Intercept)  -29.373536  32.500240  -0.904   0.3717  </span></span>
<span id="cb160-15"><a href="#cb160-15" tabindex="-1"></a><span class="co">#&gt; jailyes        3.120574   6.849271   0.456   0.6512  </span></span>
<span id="cb160-16"><a href="#cb160-16" tabindex="-1"></a><span class="co">#&gt; unemp          4.815081   1.892369   2.544   0.0150 *</span></span>
<span id="cb160-17"><a href="#cb160-17" tabindex="-1"></a><span class="co">#&gt; beertax        2.311850   9.521684   0.243   0.8094  </span></span>
<span id="cb160-18"><a href="#cb160-18" tabindex="-1"></a><span class="co">#&gt; baptist        0.661694   0.527228   1.255   0.2169  </span></span>
<span id="cb160-19"><a href="#cb160-19" tabindex="-1"></a><span class="co">#&gt; dry           -0.026675   0.383956  -0.069   0.9450  </span></span>
<span id="cb160-20"><a href="#cb160-20" tabindex="-1"></a><span class="co">#&gt; youngdrivers  -0.092100 142.804244  -0.001   0.9995  </span></span>
<span id="cb160-21"><a href="#cb160-21" tabindex="-1"></a><span class="co">#&gt; miles          0.006802   0.002822   2.411   0.0207 *</span></span>
<span id="cb160-22"><a href="#cb160-22" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb160-23"><a href="#cb160-23" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb160-24"><a href="#cb160-24" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-25"><a href="#cb160-25" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 19.41 on 39 degrees of freedom</span></span>
<span id="cb160-26"><a href="#cb160-26" tabindex="-1"></a><span class="co">#&gt;   (1 observation deleted due to missingness)</span></span>
<span id="cb160-27"><a href="#cb160-27" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.4742, Adjusted R-squared:  0.3798 </span></span>
<span id="cb160-28"><a href="#cb160-28" tabindex="-1"></a><span class="co">#&gt; F-statistic: 5.024 on 7 and 39 DF,  p-value: 0.0003999</span></span></code></pre></div>
<p>The estimated coefficient on jail is 3.12. It is somewhat smaller than the previous estimate, though neither is statistically significant. We should interpret this as the partial effect of the mandatory jail policy, that is, that we estimate that mandatory jail laws increase the number of alcohol related fatalities per million by 3.12 on average controlling for the unemployment rate, beer tax, the fraction of southern baptists in the state, the fraction of residents in dry counties, the fraction of young drivers, and the average miles driven in the state. We cannot reject that the partial effect of mandatory jail policies is equal to 0.</p>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1" tabindex="-1"></a>fd_reg <span class="ot">&lt;-</span> <span class="fu">plm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> <span class="fu">as.factor</span>(year),</span>
<span id="cb161-2"><a href="#cb161-2" tabindex="-1"></a>              <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb161-3"><a href="#cb161-3" tabindex="-1"></a>              <span class="at">index=</span><span class="st">&quot;state&quot;</span>, <span class="at">model=</span><span class="st">&quot;fd&quot;</span>,</span>
<span id="cb161-4"><a href="#cb161-4" tabindex="-1"></a>              <span class="at">data=</span>Fatalities)</span>
<span id="cb161-5"><a href="#cb161-5" tabindex="-1"></a><span class="fu">summary</span>(fd_reg)</span>
<span id="cb161-6"><a href="#cb161-6" tabindex="-1"></a><span class="co">#&gt; Oneway (individual) effect First-Difference Model</span></span>
<span id="cb161-7"><a href="#cb161-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb161-8"><a href="#cb161-8" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb161-9"><a href="#cb161-9" tabindex="-1"></a><span class="co">#&gt; plm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, </span></span>
<span id="cb161-10"><a href="#cb161-10" tabindex="-1"></a><span class="co">#&gt;     effect = &quot;individual&quot;, model = &quot;fd&quot;, index = &quot;state&quot;)</span></span>
<span id="cb161-11"><a href="#cb161-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb161-12"><a href="#cb161-12" tabindex="-1"></a><span class="co">#&gt; Unbalanced Panel: n = 48, T = 6-7, N = 335</span></span>
<span id="cb161-13"><a href="#cb161-13" tabindex="-1"></a><span class="co">#&gt; Observations used in estimation: 287</span></span>
<span id="cb161-14"><a href="#cb161-14" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb161-15"><a href="#cb161-15" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb161-16"><a href="#cb161-16" tabindex="-1"></a><span class="co">#&gt;      Min.   1st Qu.    Median   3rd Qu.      Max. </span></span>
<span id="cb161-17"><a href="#cb161-17" tabindex="-1"></a><span class="co">#&gt; -51.66677  -5.09887   0.23801   6.28688 119.08976 </span></span>
<span id="cb161-18"><a href="#cb161-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb161-19"><a href="#cb161-19" tabindex="-1"></a><span class="co">#&gt; Coefficients: (1 dropped because of singularities)</span></span>
<span id="cb161-20"><a href="#cb161-20" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t-value Pr(&gt;|t|)   </span></span>
<span id="cb161-21"><a href="#cb161-21" tabindex="-1"></a><span class="co">#&gt; (Intercept)         -2.15376    0.80673 -2.6697 0.008035 **</span></span>
<span id="cb161-22"><a href="#cb161-22" tabindex="-1"></a><span class="co">#&gt; jailyes              2.60763    5.28351  0.4935 0.622016   </span></span>
<span id="cb161-23"><a href="#cb161-23" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1983 -5.28423    1.82330 -2.8982 0.004050 **</span></span>
<span id="cb161-24"><a href="#cb161-24" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1984 -3.58247    2.29451 -1.5613 0.119577   </span></span>
<span id="cb161-25"><a href="#cb161-25" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1985 -5.60800    2.43517 -2.3029 0.022017 * </span></span>
<span id="cb161-26"><a href="#cb161-26" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1986 -0.74192    2.28988 -0.3240 0.746180   </span></span>
<span id="cb161-27"><a href="#cb161-27" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1987 -2.16244    1.80716 -1.1966 0.232476   </span></span>
<span id="cb161-28"><a href="#cb161-28" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb161-29"><a href="#cb161-29" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb161-30"><a href="#cb161-30" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb161-31"><a href="#cb161-31" tabindex="-1"></a><span class="co">#&gt; Total Sum of Squares:    54692</span></span>
<span id="cb161-32"><a href="#cb161-32" tabindex="-1"></a><span class="co">#&gt; Residual Sum of Squares: 51620</span></span>
<span id="cb161-33"><a href="#cb161-33" tabindex="-1"></a><span class="co">#&gt; R-Squared:      0.056171</span></span>
<span id="cb161-34"><a href="#cb161-34" tabindex="-1"></a><span class="co">#&gt; Adj. R-Squared: 0.035946</span></span>
<span id="cb161-35"><a href="#cb161-35" tabindex="-1"></a><span class="co">#&gt; F-statistic: 2.77733 on 6 and 280 DF, p-value: 0.012223</span></span></code></pre></div>
<p>We should interpret the estimated coefficient on <code>jail</code> as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after accounting for time invariant variables whose effects do not change over time. Again, we cannot reject that the effect is equal to 0.</p>
<ol start="5" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" tabindex="-1"></a>within_reg <span class="ot">&lt;-</span> <span class="fu">plm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> <span class="fu">as.factor</span>(year),</span>
<span id="cb162-2"><a href="#cb162-2" tabindex="-1"></a>              <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb162-3"><a href="#cb162-3" tabindex="-1"></a>              <span class="at">index=</span><span class="st">&quot;state&quot;</span>, <span class="at">model=</span><span class="st">&quot;within&quot;</span>,</span>
<span id="cb162-4"><a href="#cb162-4" tabindex="-1"></a>              <span class="at">data=</span>Fatalities)</span>
<span id="cb162-5"><a href="#cb162-5" tabindex="-1"></a><span class="fu">summary</span>(within_reg)</span>
<span id="cb162-6"><a href="#cb162-6" tabindex="-1"></a><span class="co">#&gt; Oneway (individual) effect Within Model</span></span>
<span id="cb162-7"><a href="#cb162-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-8"><a href="#cb162-8" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb162-9"><a href="#cb162-9" tabindex="-1"></a><span class="co">#&gt; plm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, </span></span>
<span id="cb162-10"><a href="#cb162-10" tabindex="-1"></a><span class="co">#&gt;     effect = &quot;individual&quot;, model = &quot;within&quot;, index = &quot;state&quot;)</span></span>
<span id="cb162-11"><a href="#cb162-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-12"><a href="#cb162-12" tabindex="-1"></a><span class="co">#&gt; Unbalanced Panel: n = 48, T = 6-7, N = 335</span></span>
<span id="cb162-13"><a href="#cb162-13" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-14"><a href="#cb162-14" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb162-15"><a href="#cb162-15" tabindex="-1"></a><span class="co">#&gt;        Min.     1st Qu.      Median     3rd Qu.        Max. </span></span>
<span id="cb162-16"><a href="#cb162-16" tabindex="-1"></a><span class="co">#&gt; -95.1937300  -4.9678238   0.0088078   5.1611249  40.6263546 </span></span>
<span id="cb162-17"><a href="#cb162-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-18"><a href="#cb162-18" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb162-19"><a href="#cb162-19" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t-value  Pr(&gt;|t|)    </span></span>
<span id="cb162-20"><a href="#cb162-20" tabindex="-1"></a><span class="co">#&gt; jailyes               8.3327     4.9666  1.6777 0.0945164 .  </span></span>
<span id="cb162-21"><a href="#cb162-21" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1983  -7.9151     2.6936 -2.9384 0.0035734 ** </span></span>
<span id="cb162-22"><a href="#cb162-22" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1984  -8.4863     2.7115 -3.1298 0.0019341 ** </span></span>
<span id="cb162-23"><a href="#cb162-23" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1985 -12.7849     2.7331 -4.6778 4.518e-06 ***</span></span>
<span id="cb162-24"><a href="#cb162-24" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1986 -10.0726     2.7331 -3.6854 0.0002741 ***</span></span>
<span id="cb162-25"><a href="#cb162-25" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1987 -13.5276     2.7115 -4.9890 1.067e-06 ***</span></span>
<span id="cb162-26"><a href="#cb162-26" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1988 -13.6296     2.7279 -4.9964 1.030e-06 ***</span></span>
<span id="cb162-27"><a href="#cb162-27" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb162-28"><a href="#cb162-28" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb162-29"><a href="#cb162-29" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-30"><a href="#cb162-30" tabindex="-1"></a><span class="co">#&gt; Total Sum of Squares:    53854</span></span>
<span id="cb162-31"><a href="#cb162-31" tabindex="-1"></a><span class="co">#&gt; Residual Sum of Squares: 47607</span></span>
<span id="cb162-32"><a href="#cb162-32" tabindex="-1"></a><span class="co">#&gt; R-Squared:      0.116</span></span>
<span id="cb162-33"><a href="#cb162-33" tabindex="-1"></a><span class="co">#&gt; Adj. R-Squared: -0.054487</span></span>
<span id="cb162-34"><a href="#cb162-34" tabindex="-1"></a><span class="co">#&gt; F-statistic: 5.24882 on 7 and 280 DF, p-value: 1.2051e-05</span></span></code></pre></div>
<p>The estimated coefficient on <code>jail</code> has the same interpretation as in the previous problem. The estimated effect here is marginally statistically significant.
6.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1" tabindex="-1"></a>within_reg_covs <span class="ot">&lt;-</span> <span class="fu">plm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles,</span>
<span id="cb163-2"><a href="#cb163-2" tabindex="-1"></a>                       <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb163-3"><a href="#cb163-3" tabindex="-1"></a>                       <span class="at">index=</span><span class="st">&quot;state&quot;</span>, <span class="at">model=</span><span class="st">&quot;within&quot;</span>,</span>
<span id="cb163-4"><a href="#cb163-4" tabindex="-1"></a>                       <span class="at">data=</span>Fatalities)</span>
<span id="cb163-5"><a href="#cb163-5" tabindex="-1"></a><span class="fu">summary</span>(within_reg_covs)</span>
<span id="cb163-6"><a href="#cb163-6" tabindex="-1"></a><span class="co">#&gt; Oneway (individual) effect Within Model</span></span>
<span id="cb163-7"><a href="#cb163-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-8"><a href="#cb163-8" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb163-9"><a href="#cb163-9" tabindex="-1"></a><span class="co">#&gt; plm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + </span></span>
<span id="cb163-10"><a href="#cb163-10" tabindex="-1"></a><span class="co">#&gt;     dry + youngdrivers + miles, data = Fatalities, effect = &quot;individual&quot;, </span></span>
<span id="cb163-11"><a href="#cb163-11" tabindex="-1"></a><span class="co">#&gt;     model = &quot;within&quot;, index = &quot;state&quot;)</span></span>
<span id="cb163-12"><a href="#cb163-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-13"><a href="#cb163-13" tabindex="-1"></a><span class="co">#&gt; Unbalanced Panel: n = 48, T = 6-7, N = 335</span></span>
<span id="cb163-14"><a href="#cb163-14" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-15"><a href="#cb163-15" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb163-16"><a href="#cb163-16" tabindex="-1"></a><span class="co">#&gt;      Min.   1st Qu.    Median   3rd Qu.      Max. </span></span>
<span id="cb163-17"><a href="#cb163-17" tabindex="-1"></a><span class="co">#&gt; -95.62306  -5.69773  -0.56903   4.79219  47.80871 </span></span>
<span id="cb163-18"><a href="#cb163-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-19"><a href="#cb163-19" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb163-20"><a href="#cb163-20" tabindex="-1"></a><span class="co">#&gt;                 Estimate  Std. Error t-value  Pr(&gt;|t|)    </span></span>
<span id="cb163-21"><a href="#cb163-21" tabindex="-1"></a><span class="co">#&gt; jailyes       4.9731e+00  4.9613e+00  1.0024   0.31702    </span></span>
<span id="cb163-22"><a href="#cb163-22" tabindex="-1"></a><span class="co">#&gt; unemp        -1.1340e+00  5.6592e-01 -2.0038   0.04605 *  </span></span>
<span id="cb163-23"><a href="#cb163-23" tabindex="-1"></a><span class="co">#&gt; beertax      -2.7456e+01  1.5080e+01 -1.8207   0.06972 .  </span></span>
<span id="cb163-24"><a href="#cb163-24" tabindex="-1"></a><span class="co">#&gt; baptist       2.5083e+00  4.3324e+00  0.5790   0.56308    </span></span>
<span id="cb163-25"><a href="#cb163-25" tabindex="-1"></a><span class="co">#&gt; dry           4.3092e-01  1.0870e+00  0.3964   0.69208    </span></span>
<span id="cb163-26"><a href="#cb163-26" tabindex="-1"></a><span class="co">#&gt; youngdrivers  2.6357e+02  5.0169e+01  5.2537 2.957e-07 ***</span></span>
<span id="cb163-27"><a href="#cb163-27" tabindex="-1"></a><span class="co">#&gt; miles        -6.8899e-04  7.3182e-04 -0.9415   0.34727    </span></span>
<span id="cb163-28"><a href="#cb163-28" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb163-29"><a href="#cb163-29" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb163-30"><a href="#cb163-30" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-31"><a href="#cb163-31" tabindex="-1"></a><span class="co">#&gt; Total Sum of Squares:    53854</span></span>
<span id="cb163-32"><a href="#cb163-32" tabindex="-1"></a><span class="co">#&gt; Residual Sum of Squares: 48083</span></span>
<span id="cb163-33"><a href="#cb163-33" tabindex="-1"></a><span class="co">#&gt; R-Squared:      0.10717</span></span>
<span id="cb163-34"><a href="#cb163-34" tabindex="-1"></a><span class="co">#&gt; Adj. R-Squared: -0.065023</span></span>
<span id="cb163-35"><a href="#cb163-35" tabindex="-1"></a><span class="co">#&gt; F-statistic: 4.80119 on 7 and 280 DF, p-value: 4.0281e-05</span></span></code></pre></div>
<p>We should interpret the estimated coefficient on <code>jail</code> as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after controlling for the unemployment rate, beer taxes, the fraction of the state that is southern baptist, the fraction of the state that lives in a dry county, the fraction of young drivers in a state, and the average number of miles driven per person in the stata, and accounting for time invariant variables whose effects do not change over time.</p>
<ol start="7" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="#cb164-1" tabindex="-1"></a><span class="co"># part a: convert data to two period panel data</span></span>
<span id="cb164-2"><a href="#cb164-2" tabindex="-1"></a>two_period <span class="ot">&lt;-</span> <span class="fu">subset</span>(Fatalities, year<span class="sc">==</span><span class="dv">1982</span> <span class="sc">|</span> year<span class="sc">==</span><span class="dv">1988</span>)</span>
<span id="cb164-3"><a href="#cb164-3" tabindex="-1"></a><span class="co"># and drop some missing</span></span>
<span id="cb164-4"><a href="#cb164-4" tabindex="-1"></a>two_period <span class="ot">&lt;-</span> <span class="fu">subset</span>(two_period, <span class="sc">!</span><span class="fu">is.na</span>(jail))</span>
<span id="cb164-5"><a href="#cb164-5" tabindex="-1"></a>two_period <span class="ot">&lt;-</span> BMisc<span class="sc">::</span><span class="fu">makeBalancedPanel</span>(two_period, <span class="st">&quot;state&quot;</span>, <span class="st">&quot;year&quot;</span>)</span>
<span id="cb164-6"><a href="#cb164-6" tabindex="-1"></a>two_period<span class="sc">$</span>jail <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(two_period<span class="sc">$</span>jail<span class="sc">==</span><span class="st">&quot;yes&quot;</span>)</span>
<span id="cb164-7"><a href="#cb164-7" tabindex="-1"></a></span>
<span id="cb164-8"><a href="#cb164-8" tabindex="-1"></a><span class="co"># part b: convert into wide format</span></span>
<span id="cb164-9"><a href="#cb164-9" tabindex="-1"></a>wide_df <span class="ot">&lt;-</span> <span class="fu">pivot_wider</span>(two_period, </span>
<span id="cb164-10"><a href="#cb164-10" tabindex="-1"></a>                       <span class="at">id_cols=</span><span class="st">&quot;state&quot;</span>, </span>
<span id="cb164-11"><a href="#cb164-11" tabindex="-1"></a>                       <span class="at">names_from=</span><span class="st">&quot;year&quot;</span>,</span>
<span id="cb164-12"><a href="#cb164-12" tabindex="-1"></a>                       <span class="at">values_from=</span><span class="fu">c</span>(<span class="st">&quot;jail&quot;</span>, <span class="st">&quot;afatal_per_million&quot;</span>))</span>
<span id="cb164-13"><a href="#cb164-13" tabindex="-1"></a></span>
<span id="cb164-14"><a href="#cb164-14" tabindex="-1"></a><span class="co"># add back other covariates from 1982</span></span>
<span id="cb164-15"><a href="#cb164-15" tabindex="-1"></a>wide_df <span class="ot">&lt;-</span> <span class="fu">merge</span>(wide_df, <span class="fu">subset</span>(Fatalities, year<span class="sc">==</span><span class="dv">1982</span>)[,<span class="fu">c</span>(<span class="st">&quot;unemp&quot;</span>, <span class="st">&quot;beertax&quot;</span>, <span class="st">&quot;baptist&quot;</span>, <span class="st">&quot;dry&quot;</span>, <span class="st">&quot;youngdrivers&quot;</span>, <span class="st">&quot;miles&quot;</span>,<span class="st">&quot;state&quot;</span>)], <span class="at">by=</span><span class="st">&quot;state&quot;</span>)</span>
<span id="cb164-16"><a href="#cb164-16" tabindex="-1"></a></span>
<span id="cb164-17"><a href="#cb164-17" tabindex="-1"></a><span class="co"># change in fatal accidents over time</span></span>
<span id="cb164-18"><a href="#cb164-18" tabindex="-1"></a>wide_df<span class="sc">$</span>Dafatal_per_million <span class="ot">&lt;-</span> wide_df<span class="sc">$</span>afatal_per_million_1988 <span class="sc">-</span> wide_df<span class="sc">$</span>afatal_per_million_1982</span>
<span id="cb164-19"><a href="#cb164-19" tabindex="-1"></a></span>
<span id="cb164-20"><a href="#cb164-20" tabindex="-1"></a><span class="co"># part c: drop already treated states</span></span>
<span id="cb164-21"><a href="#cb164-21" tabindex="-1"></a>wide_df <span class="ot">&lt;-</span> <span class="fu">subset</span>(wide_df, jail_1982<span class="sc">==</span><span class="dv">0</span>)</span></code></pre></div>
<ol start="8" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1" tabindex="-1"></a>did <span class="ot">&lt;-</span> <span class="fu">lm</span>(Dafatal_per_million <span class="sc">~</span> jail_1988, <span class="at">data=</span>wide_df)</span>
<span id="cb165-2"><a href="#cb165-2" tabindex="-1"></a><span class="fu">summary</span>(did)</span>
<span id="cb165-3"><a href="#cb165-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-4"><a href="#cb165-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb165-5"><a href="#cb165-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = Dafatal_per_million ~ jail_1988, data = wide_df)</span></span>
<span id="cb165-6"><a href="#cb165-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-7"><a href="#cb165-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb165-8"><a href="#cb165-8" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb165-9"><a href="#cb165-9" tabindex="-1"></a><span class="co">#&gt; -55.652 -10.993   5.033  10.405  76.822 </span></span>
<span id="cb165-10"><a href="#cb165-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-11"><a href="#cb165-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb165-12"><a href="#cb165-12" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span id="cb165-13"><a href="#cb165-13" tabindex="-1"></a><span class="co">#&gt; (Intercept)  -12.585      4.242  -2.966  0.00532 **</span></span>
<span id="cb165-14"><a href="#cb165-14" tabindex="-1"></a><span class="co">#&gt; jail_1988      5.102     11.695   0.436  0.66526   </span></span>
<span id="cb165-15"><a href="#cb165-15" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb165-16"><a href="#cb165-16" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb165-17"><a href="#cb165-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-18"><a href="#cb165-18" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 24.37 on 36 degrees of freedom</span></span>
<span id="cb165-19"><a href="#cb165-19" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.005259,   Adjusted R-squared:  -0.02237 </span></span>
<span id="cb165-20"><a href="#cb165-20" tabindex="-1"></a><span class="co">#&gt; F-statistic: 0.1903 on 1 and 36 DF,  p-value: 0.6653</span></span>
<span id="cb165-21"><a href="#cb165-21" tabindex="-1"></a>did_covs <span class="ot">&lt;-</span> <span class="fu">lm</span>(Dafatal_per_million <span class="sc">~</span> jail_1988 <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles, <span class="at">data=</span>wide_df)</span>
<span id="cb165-22"><a href="#cb165-22" tabindex="-1"></a><span class="fu">summary</span>(did_covs)</span>
<span id="cb165-23"><a href="#cb165-23" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-24"><a href="#cb165-24" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb165-25"><a href="#cb165-25" tabindex="-1"></a><span class="co">#&gt; lm(formula = Dafatal_per_million ~ jail_1988 + unemp + beertax + </span></span>
<span id="cb165-26"><a href="#cb165-26" tabindex="-1"></a><span class="co">#&gt;     baptist + dry + youngdrivers + miles, data = wide_df)</span></span>
<span id="cb165-27"><a href="#cb165-27" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-28"><a href="#cb165-28" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb165-29"><a href="#cb165-29" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb165-30"><a href="#cb165-30" tabindex="-1"></a><span class="co">#&gt; -38.346 -12.383   1.456   9.092  60.585 </span></span>
<span id="cb165-31"><a href="#cb165-31" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-32"><a href="#cb165-32" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb165-33"><a href="#cb165-33" tabindex="-1"></a><span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span id="cb165-34"><a href="#cb165-34" tabindex="-1"></a><span class="co">#&gt; (Intercept)    6.851636  50.643035   0.135   0.8933  </span></span>
<span id="cb165-35"><a href="#cb165-35" tabindex="-1"></a><span class="co">#&gt; jail_1988     -1.853041  10.834391  -0.171   0.8653  </span></span>
<span id="cb165-36"><a href="#cb165-36" tabindex="-1"></a><span class="co">#&gt; unemp          3.725007   1.919862   1.940   0.0618 .</span></span>
<span id="cb165-37"><a href="#cb165-37" tabindex="-1"></a><span class="co">#&gt; beertax        8.300778  10.052007   0.826   0.4154  </span></span>
<span id="cb165-38"><a href="#cb165-38" tabindex="-1"></a><span class="co">#&gt; baptist        0.527893   0.723263   0.730   0.4711  </span></span>
<span id="cb165-39"><a href="#cb165-39" tabindex="-1"></a><span class="co">#&gt; dry           -0.955636   0.546475  -1.749   0.0906 .</span></span>
<span id="cb165-40"><a href="#cb165-40" tabindex="-1"></a><span class="co">#&gt; youngdrivers  89.432017 234.379768   0.382   0.7055  </span></span>
<span id="cb165-41"><a href="#cb165-41" tabindex="-1"></a><span class="co">#&gt; miles         -0.010360   0.005823  -1.779   0.0854 .</span></span>
<span id="cb165-42"><a href="#cb165-42" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb165-43"><a href="#cb165-43" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb165-44"><a href="#cb165-44" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb165-45"><a href="#cb165-45" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 21.9 on 30 degrees of freedom</span></span>
<span id="cb165-46"><a href="#cb165-46" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.3303, Adjusted R-squared:  0.174 </span></span>
<span id="cb165-47"><a href="#cb165-47" tabindex="-1"></a><span class="co">#&gt; F-statistic: 2.114 on 7 and 30 DF,  p-value: 0.07276</span></span></code></pre></div>
<p>If we are willing to believe that, in the absence of the policy, that trends in alcohol related fatalities per million people would have followed the same trends over time for treated and untreated states, then we can interpret these as causal effects. These estimates are broadly similar to the previous ones though the second ones (that include additional covariates) are about the only ones where we ever get a negative estimate for the effect of mandatory jail policies. Like the previous estimates, neither of these estimates are statistically different from 0.</p>
<ol start="9" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="#cb166-1" tabindex="-1"></a>lag_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million_1988 <span class="sc">~</span> jail_1988 <span class="sc">+</span> afatal_per_million_1982, <span class="at">data=</span>wide_df)</span>
<span id="cb166-2"><a href="#cb166-2" tabindex="-1"></a><span class="fu">summary</span>(lag_reg)</span>
<span id="cb166-3"><a href="#cb166-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-4"><a href="#cb166-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb166-5"><a href="#cb166-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982, </span></span>
<span id="cb166-6"><a href="#cb166-6" tabindex="-1"></a><span class="co">#&gt;     data = wide_df)</span></span>
<span id="cb166-7"><a href="#cb166-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-8"><a href="#cb166-8" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb166-9"><a href="#cb166-9" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb166-10"><a href="#cb166-10" tabindex="-1"></a><span class="co">#&gt; -29.120 -12.663  -0.684   6.873  92.390 </span></span>
<span id="cb166-11"><a href="#cb166-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-12"><a href="#cb166-12" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb166-13"><a href="#cb166-13" tabindex="-1"></a><span class="co">#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb166-14"><a href="#cb166-14" tabindex="-1"></a><span class="co">#&gt; (Intercept)              19.0810    10.1089   1.888 0.067401 .  </span></span>
<span id="cb166-15"><a href="#cb166-15" tabindex="-1"></a><span class="co">#&gt; jail_1988                 3.4323    10.3171   0.333 0.741363    </span></span>
<span id="cb166-16"><a href="#cb166-16" tabindex="-1"></a><span class="co">#&gt; afatal_per_million_1982   0.5607     0.1303   4.303 0.000129 ***</span></span>
<span id="cb166-17"><a href="#cb166-17" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb166-18"><a href="#cb166-18" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb166-19"><a href="#cb166-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-20"><a href="#cb166-20" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 21.47 on 35 degrees of freedom</span></span>
<span id="cb166-21"><a href="#cb166-21" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.3462, Adjusted R-squared:  0.3088 </span></span>
<span id="cb166-22"><a href="#cb166-22" tabindex="-1"></a><span class="co">#&gt; F-statistic: 9.266 on 2 and 35 DF,  p-value: 0.0005896</span></span>
<span id="cb166-23"><a href="#cb166-23" tabindex="-1"></a>lag_reg_covs <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million_1988 <span class="sc">~</span> jail_1988 <span class="sc">+</span> afatal_per_million_1982 <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles, <span class="at">data=</span>wide_df)</span>
<span id="cb166-24"><a href="#cb166-24" tabindex="-1"></a><span class="fu">summary</span>(lag_reg_covs)</span>
<span id="cb166-25"><a href="#cb166-25" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-26"><a href="#cb166-26" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb166-27"><a href="#cb166-27" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982 + </span></span>
<span id="cb166-28"><a href="#cb166-28" tabindex="-1"></a><span class="co">#&gt;     unemp + beertax + baptist + dry + youngdrivers + miles, data = wide_df)</span></span>
<span id="cb166-29"><a href="#cb166-29" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-30"><a href="#cb166-30" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb166-31"><a href="#cb166-31" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb166-32"><a href="#cb166-32" tabindex="-1"></a><span class="co">#&gt; -27.840  -8.793  -1.364   5.146  71.409 </span></span>
<span id="cb166-33"><a href="#cb166-33" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-34"><a href="#cb166-34" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb166-35"><a href="#cb166-35" tabindex="-1"></a><span class="co">#&gt;                           Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span id="cb166-36"><a href="#cb166-36" tabindex="-1"></a><span class="co">#&gt; (Intercept)             -17.292595  46.318477  -0.373  0.71161   </span></span>
<span id="cb166-37"><a href="#cb166-37" tabindex="-1"></a><span class="co">#&gt; jail_1988                 0.817453   9.786782   0.084  0.93401   </span></span>
<span id="cb166-38"><a href="#cb166-38" tabindex="-1"></a><span class="co">#&gt; afatal_per_million_1982   0.505371   0.173718   2.909  0.00689 **</span></span>
<span id="cb166-39"><a href="#cb166-39" tabindex="-1"></a><span class="co">#&gt; unemp                     3.189918   1.736443   1.837  0.07647 . </span></span>
<span id="cb166-40"><a href="#cb166-40" tabindex="-1"></a><span class="co">#&gt; beertax                   2.885796   9.236174   0.312  0.75694   </span></span>
<span id="cb166-41"><a href="#cb166-41" tabindex="-1"></a><span class="co">#&gt; baptist                   0.965785   0.668259   1.445  0.15911   </span></span>
<span id="cb166-42"><a href="#cb166-42" tabindex="-1"></a><span class="co">#&gt; dry                      -0.567567   0.509915  -1.113  0.27482   </span></span>
<span id="cb166-43"><a href="#cb166-43" tabindex="-1"></a><span class="co">#&gt; youngdrivers            120.615853 211.026841   0.572  0.57202   </span></span>
<span id="cb166-44"><a href="#cb166-44" tabindex="-1"></a><span class="co">#&gt; miles                    -0.002692   0.005888  -0.457  0.65087   </span></span>
<span id="cb166-45"><a href="#cb166-45" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb166-46"><a href="#cb166-46" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb166-47"><a href="#cb166-47" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-48"><a href="#cb166-48" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 19.7 on 29 degrees of freedom</span></span>
<span id="cb166-49"><a href="#cb166-49" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.5443, Adjusted R-squared:  0.4186 </span></span>
<span id="cb166-50"><a href="#cb166-50" tabindex="-1"></a><span class="co">#&gt; F-statistic: 4.329 on 8 and 29 DF,  p-value: 0.001596</span></span></code></pre></div>
<p>These estimates directly control for alcohol related fatalities per million in the pre-treatment period 1982. These sorts of specifications are less common in economics, but, in my view, it seems like a reasonable approach here. That said, the results are more or less the same as earlier estimates.</p>
<ol start="10" style="list-style-type: decimal">
<li>We don’t have very strong evidence that mandatory jail policies reduced the number traffic fatalities. In my view, probably the best specifications for trying to understand the causal effects are the ones in part 7 (particularly, the ones that include covariates there), but I think that the the results in parts 4-9 are also informative. Broadly, these estimates are more or less similar — none of them are statistically significant and most are positive (which is an unexpected sign).</li>
</ol>
<p>Before we finish, let me mention a few caveats to these results:</p>
<ul>
<li><p>First, I would be very hesitant to interpret these results as definitively saying that mandatory jail policies have no effect on alcohol related traffic fatalities. The main reason to be clear about this is that our standard error are quite large. For example, in the second specification in part 7 (the one I like the most), a 95% confidence interval for our estimate is <span class="math inline">\([-23.1, 19.4]\)</span>. This is a wide confidence interval — the average number of alcohol related traffic fatalities per million across all states and time periods is only 66. So our estimates are basically still compatible with very large reductions in alcohol related traffic fatalities up to large increases in alcohol related traffic fatalities.</p></li>
<li><p>Let me make one more comment about the sign of our results. Many of our point estimates are positive; as we discussed earlier, it is hard to rationalize harsher punishments <em>increasing</em> alcohol related traffic fatalities. I think the main explanation for these results is just that our estimates are pretty noisy and, therefore, more or less “by chance” we are getting estimates that have an unexpected sign. But there are some other possible explanations that are worth mentioning. For one, there are a number of other policies related to drunk driving that occurred in the 1980s (particularly, related to legal drinking age) but perhaps others. It is not clear how these would interact with our estimates, but they could certainly play some role. Besides that, it seems to me that we have a pretty good set of covariates that enter our models, but there could be important covariates that we are missing. For this reason, some expertise in how to model state-level traffic fatalities is actually a very important skill here (actually probably the key skill here!)</p></li>
</ul>
</div>
<div id="coding-questions-4" class="section level2 hasAnchor" number="8.10">
<h2><span class="header-section-number">8.10</span> Coding Questions<a href="#coding-questions-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>For this problem, we will use the data <code>rand_hie</code>. This is data from the RAND health insurance experiment in the 1980s. In the experiment, participants were randomly assigned to get Catastrophic (the least amount of coverage), insurance that came with a Deductible, insurance that came with Cost Sharing (i.e., co-insurance so that an individual pays part of their medical insurance), and Free (so that there is no cost of medical care).</p>
<p>For this problem, we will be interested in whether or not changing the type of health insurance changed the amount of health care utilization and the health status of individuals.</p>
<p>We will focus on the difference between the least amount of health insurance (“Catastrophic”) and the most amount of health insurance (“Free”). In particular, you can start this problem by creating a new dataset as follows:</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1" tabindex="-1"></a>    rand_hie_subset <span class="ot">&lt;-</span> <span class="fu">subset</span>(rand_hie, plan_type <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Catastrophic&quot;</span>, <span class="st">&quot;Free&quot;</span>))</span></code></pre></div>
<p>and use this data to answer the questions below.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Use a regression to estimate the average difference between total medical expenditure (<code>total_med_expenditure</code>) by plan type (<code>plan_type</code>) and report your results. Should you interpret these as average causal effects? Explain.</p></li>
<li><p>Use a regression to estimate the average difference between face to face doctors visits (<code>face_to_face_visits</code>) by plan type (<code>plan_type</code>) and report your results. Should you interpret these as average causal effects? Explain.</p></li>
<li><p>Use a regression to estimate the average difference between the overall health index (<code>health_index</code>) by plan type (<code>plan_type</code>) and report your results. Should you interpret these as average causal effects? Explain.</p></li>
<li><p>How do you interpret the results from parts a-c?</p></li>
</ol></li>
<li><p>For this problem, we will study the causal effect of having more children on women’s labor supply using the data <code>Fertility</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Let’s start by running a regression of the number of hours that a woman typically works per week (<code>work</code>) on whether or not she has more than two children (<code>morekids</code>), her age and <span class="math inline">\(age^2\)</span>, and race/ethnicity (<code>afam</code> and <code>hispanic</code>). Report your results. How do you feel about interpreting the estimated coefficient on <code>morekids</code> as the causal effect of having more than two children? Explain.</p></li>
<li><p>One possible instrument in this setup is the sex composition of the first two children (i.e., whether they are both girls, both boys, or a boy and a girl). The thinking here is that, at least in the United States, parents tend to have a preference for having both a girl and a boy and that, therefore, parents whose first two children have the same sex may be more likely to have a third child than they would have been if they have a girl and a boy. Do you think that using a binary variable for whether or not the first two children have the same sex is a reasonable instrument of for <code>morekids</code> from part a?</p></li>
<li><p>Create a new variable called <code>samesex</code> that is equal to one for families whose first two children have the same sex. Using the same specification as in part a, use <code>samesex</code> as an instrument for <code>morekids</code> and report the results. Provide some discussion about your results.</p></li>
</ol></li>
<li><p>For this question, we will use the <code>AJR</code> data. A deep question in development economics is: Why are some countries much richer than other countries? One explanation for this is that richer countries have different institutions (e.g., property rights, democracy, etc.) that are conducive to growth. Its hard to study these questions though because institutions do not arise randomly — there could be reverse causality so that property rights, democracy, etc. are (perhaps partially) caused by being rich rather than the other way around. Alternatively, other factors (say a country’s geography) could cause both of these. We’ll consider one instrumental variables approach to thinking about this question in this problem.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of the log of per capita GDP (the log of per capita GDP is stored in the variable <code>GDP</code>) on a measure of the protection against expropriation risk (this is a measure of how “good” a country’s institutions are (a larger number indicates “better” institutions) and it is in the variable <code>Exprop</code>). How do you interpret these results? Do you think it would be reasonable to interpret the estimated coefficient on <code>Exprop</code> as the causal effect of institutions on GDP.</p></li>
<li><p>One possible instrument for <code>Exprop</code> is settler mortality (we’ll use the log of this which is available in the variable <code>logMort</code>). Settler mortality is a measure of how dangerous it was for early settlers of a particular location. The idea is that places that have high settler mortality may have set up worse (sometimes called “extractive”) institutions than places that had lower settler mortality. But that settler mortality (from a long time ago) does not have any other direct effect on modern GDP. Provide some discussion about whether settler mortality is a valid instrument for institutions.</p></li>
<li><p>Estimate an IV regression of <code>GDP</code> on <code>Exprop</code> using <code>logMort</code> as an instrument for <code>Exprop</code>. How do you interpret the results? How do these results compare to the ones from part a?</p></li>
</ol></li>
<li><p>For this question, we’ll use the data <code>house</code> to study the causal effect of incumbency on the probability that a member of the House of Representatives gets re-elected.</p>
<ol style="list-style-type: lower-alpha">
<li><p>One way to try to estimate the causal effect of incumbency is to just run a regression where the outcome is <code>democratic_vote_share</code> (this is the same outcome we’ll use below) and where the model includes a dummy variable for whether or not the democratic candidate is an incumbent. What are some limitions of this strategy?</p></li>
<li><p>The <code>house</code> data contains data about the margin of victory (is positive if they won the election and negative if they lost) for Democratic candidates in the current election and data about the Democratic margin of victory in the past election. Explain how you could use this data in a regression discontinuity design to estimate the causal effect of incumbency.</p></li>
<li><p>Use the <code>house</code> data to implement the regression discontinuity design that you proposed in part b. What do you estimate as the causal effect of incumbency?</p></li>
</ol></li>
<li><p>For this problem, we will use the data <code>banks</code>. We will study the causal effect of monetary policy on bank closures during the Great Depression. We’ll consider an interesting natural experiment in Mississippi where half the northern half of the state was in St. Louis’s federal reserve district (District 8) and the southern half of the state was in Atlanta’s federal reserve district (District 6). Atlanta had much looser monetary policy (meaning they substantially increased lending) than St. Louis during the early part of the Great Depression and our interest is in whether looser monetary policy made an difference.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Plot the total number of banks separately for District 6 and District 8 across all available time periods in the data.</p></li>
<li><p>An important event in the South early in the Great Depression was the collapse of Caldwell and Company — the largest banking chain in the South at the time. This happened in November 1930. The Atlanta Fed’s lending markedly increased quickly after this event while St. Louis’s did not. Calculate a DID estimate of the effect of looser monetary policy on the number of banks that are still in business. How do you interpret these results? <strong>Hint:</strong> You can calculate this by taking the difference between the number of banks in District 6 relative to the number of banks in District 8 across all time periods relative to the difference between the number of banks in District 6 relative to District 8 in the first period (July 1, 1929).</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-5" class="section level2 hasAnchor" number="8.11">
<h2><span class="header-section-number">8.11</span> Extra Questions<a href="#extra-questions-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>What is the difference between treatment effect homogeneity and treatment effect heterogeneity?</p></li>
<li><p>Why do most researchers give up on trying to estimate the individual-level effect of participating in a treatment?</p></li>
<li><p>Explain what unconfoundedness means.</p></li>
<li><p>What is the key condition underlying a difference-in-differences approach to learn about the causal effect of some treatment on some outcome?</p></li>
<li><p>What are two key conditions for a valid instrument?</p></li>
<li><p>Suppose you are interested in the causal effect of participating in a union on a person’s income. Consider the following approaches.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose you run the following regression</p>
<p><span class="math display">\[\begin{align*}
   Earnings_i = \beta_0 + \alpha Union_i + \beta_1 Education_i + U_i
\end{align*}\]</span></p>
<p>Would it be reasonable to interpret <span class="math inline">\(\hat{\alpha}\)</span> in this regression as an estimate of the causal effect of participating in a union on earnings? Explain.</p></li>
<li><p>Suppose you have access to panel data and run the following fixed effects regression
<span class="math display">\[\begin{align*}
   Earnings_{it} = \beta_{0,t} + \alpha Union_{it} + \beta_1 Education_{it} + \eta_i + U_{it}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\eta_i\)</span> is an individual fixed effect. Would it be reasonable to interpert <span class="math inline">\(\hat{\alpha}\)</span> in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?</p></li>
<li><p>Going back to the case with cross-sectional data, consider the regression
<span class="math display">\[\begin{align*}
   Earnings_i = \beta_0 + \alpha Union_i + U_i
\end{align*}\]</span>
but using the variable <span class="math inline">\(Z_i = 1\)</span> if birthday is between Jan. 1 and Jun. 30 while <span class="math inline">\(Z_i=0\)</span> otherwise. Would it be reasonable to interpert <span class="math inline">\(\hat{\alpha}\)</span> in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?</p></li>
</ol></li>
<li><p>Suppose that you are interested in the effect of lower college costs on the probability of graduating from college. You have access to student-level data from Georgia where students are eligible for the Hope Scholarship if they can keep their GPA above 3.0.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What strategy can use to exploit this institional setting to learn about the causal effect of lower college costs on the probability of going to college?</p></li>
<li><p>What sort of data would you need in order to implement this strategy?</p></li>
<li><p>Can you think of any ways that the approach that you suggested could go wrong?</p></li>
<li><p>Another researcher reads the results from the approach you have implemented and complains that your results are only specific to students who have grades right around the 3.0 cutoff. Is this a fair criticism?</p></li>
</ol></li>
<li><p>Suppose you are willing to believe versions of unconfoundedness, a linear model for untreated potential outcomes, and treatment effect homogeneity so that you could write
<span class="math display">\[\begin{align*}
  Y_i = \beta_0 + \alpha D_i + \beta_1 X_i + \beta_2 W_i + U_i
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|D,X,W] = 0\)</span> so that you were willing to interpret <span class="math inline">\(\alpha\)</span> in this regression as the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. However, suppose that <span class="math inline">\(W\)</span> is not observed so that you cannot operationalize the above regression.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Since you do not observe <span class="math inline">\(W\)</span>, you are considering just running a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> and interpreting the estimated coefficient on <span class="math inline">\(D\)</span> as the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. Does this seem like a good idea?</p></li>
<li><p>In part (a), we can write a version of the model that you are thinking about estimating as
<span class="math display">\[\begin{align*}
   Y_i = \delta_0 + \delta_1 D_i + \delta_2 X_i + \epsilon_i
\end{align*}\]</span>
Suppose that <span class="math inline">\(\mathbb{E}[\epsilon | D, X] = 0\)</span> and suppose also that
<span class="math display">\[\begin{align*}
W_i = \gamma_0 + \gamma_1 D_i + \gamma_2 X_i + V_i
  \end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[V|D,X]=0\)</span>. Provide an expression for <span class="math inline">\(\delta_1\)</span> in terms of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\gamma\)</span>’s and <span class="math inline">\(\beta\)</span>’s. Explain what this expression means.</p></li>
</ol></li>
<li><p>Suppose you have access to an experiment where some participants were randomly assigned to participate in a job training program and others were randomly assigned not to participate. However, some individuals that were assigned to participate in the treatment decided not to actually participate. Let’s use the following notation: <span class="math inline">\(D=1\)</span> for individuals who actually participated and <span class="math inline">\(D=0\)</span> for individuals who did not participate. <span class="math inline">\(Z=1\)</span> for individuals who were assigned to the treatment and <span class="math inline">\(Z=0\)</span> for individuals assigned not to participate (here, <span class="math inline">\(D\)</span> and <span class="math inline">\(Z\)</span> are not exactly the same because some individuals who were assigned to the treatment did not actually participate).</p>
<p>You are considering several different approaches to dealing with this issue. Discuss which of the following are good or bad ideas:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimating <span class="math inline">\(ATT\)</span> by <span class="math inline">\(\bar{Y}_{D=1} - \bar{Y}_{D=0}\)</span>.</p></li>
<li><p>Run the regression <span class="math inline">\(Y_i = \beta_0 + \alpha D_i + U_i\)</span> using <span class="math inline">\(Z_i\)</span> as an instrument.</p></li>
</ol></li>
<li><p>Suppose you and a friend have conducted an experiment (things went well so that everyone complied with the treatment that they were assigned to, etc.). You interpret the difference <span class="math inline">\(\bar{Y}_{D=1} - \bar{Y}_{D=0}\)</span> as an estimate of the <span class="math inline">\(ATT\)</span>, but your friend says that you should interpret it as an estimate of the <span class="math inline">\(ATE\)</span>. In fact, according to your friend, random treatment assignment implies that <span class="math inline">\(\mathbb{E}[Y(1)] = \mathbb{E}[Y(1)|D=1] = \mathbb{E}[Y|D=1]\)</span> and <span class="math inline">\(\mathbb{E}[Y(0)] = \mathbb{E}[Y(0)|D=0] = \mathbb{E}[Y|D=0]\)</span> which implies that <span class="math inline">\(ATE = \mathbb{E}[Y|D=1] - \mathbb{E}[Y|D=0]\)</span>. Who is right?</p></li>
</ol>
</div>
<div id="answers-to-some-extra-questions-2" class="section level2 hasAnchor" number="8.12">
<h2><span class="header-section-number">8.12</span> Answers to Some Extra Questions<a href="#answers-to-some-extra-questions-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Answer to Question 4</strong></p>
<p>The key condition is the parallel trends assumption that says that, in the absence of participating in the treatment, the <em>path</em> of outcomes that individuals in the treated group is the same, on average, as the path of outcomes that individuals in the untreated group actually experienced.</p>
<p><strong>Answer to Question 9</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>When some individuals do not comply with their treatment assignment, this approach is probably not so great. In particular, notice that the comparison in this part of the problem is among individuals who <em>actually</em> participated in the treatment relative to those who didn’t (the latter group includes both those assigned not to participate in the treatment along with those assigned to participate in the treatment, but ultimately didn’t actually participate). This suggests that this approach would generally lead to biased estimates of the <span class="math inline">\(ATT\)</span>. In the particular context of job training, you can see this would not be such a good idea if, for example, the people who were assigned to the job training program but who did not participate tended to do this because they were able to find a job before the job training program started.</p></li>
<li><p>This approach is likely to be better. By construction, <span class="math inline">\(Z\)</span> is not correlated with <span class="math inline">\(U\)</span> (since <span class="math inline">\(Z\)</span> is randomly assigned). <span class="math inline">\(Z\)</span> is also likely to be positively correlated with <span class="math inline">\(Z\)</span> (in particular, this will be the case if being randomly assigned to treatment increases the probability of being treated). This implies that <span class="math inline">\(Z\)</span> is a valid instrument and should be able to deliver a reasonable estimate of the effect of participating in the treatment.</p></li>
</ol>
<p><strong>Answer to Question 10</strong></p>
<p>While your friend’s explanation is not technically wrong, it seems to me that you are more right than your friend. There is an important issue related to external validity here. The group of people that show up to participate in the experiment could be (and likely are) quite different from the general population. Interpreting the results of the experiment as being an <span class="math inline">\(ATE\)</span> (in the sense of across the entire population) is therefore likely to be incorrect — or at least would require extra assumptions and/or justifications. Interpreting them as an <span class="math inline">\(ATT\)</span> (i.e., as the effect among those who participated in the treatment) is still perfectly reasonable though.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Detailed Course Notes.pdf", "Detailed Course Notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
