<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 5 Prediction | Supplementary Notes and References for ECON 4750</title>
  <meta name="description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 5 Prediction | Supplementary Notes and References for ECON 4750" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 5 Prediction | Supplementary Notes and References for ECON 4750" />
  
  <meta name="twitter:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

<meta name="author" content="Brantly Callaway" />


<meta name="date" content="2021-12-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="causal-inference.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Outline/Notes for ECON 4750</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-is-this"><i class="fa fa-check"></i><b>1.1</b> What is this?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#what-is-this-not"><i class="fa fa-check"></i><b>1.2</b> What is this not?</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#why-did-i-write-this"><i class="fa fa-check"></i><b>1.3</b> Why did I write this?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#additional-references"><i class="fa fa-check"></i><b>1.4</b> Additional References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#goals-for-the-course"><i class="fa fa-check"></i><b>1.5</b> Goals for the Course</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#studying-for-the-class"><i class="fa fa-check"></i><b>1.6</b> Studying for the Class</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#data-used-in-the-course"><i class="fa fa-check"></i><b>1.7</b> Data Used in the Course</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#first-week-of-class"><i class="fa fa-check"></i><b>1.8</b> First Week of Class</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-programming.html"><a href="statistical-programming.html"><i class="fa fa-check"></i><b>2</b> Statistical Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up R</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-programming.html"><a href="statistical-programming.html#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-programming.html"><a href="statistical-programming.html#downloading-r"><i class="fa fa-check"></i><b>2.1.2</b> Downloading R</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio"><i class="fa fa-check"></i><b>2.1.3</b> RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio-development-environment"><i class="fa fa-check"></i><b>2.1.4</b> RStudio Development Environment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-programming.html"><a href="statistical-programming.html#installing-r-packages"><i class="fa fa-check"></i><b>2.2</b> Installing R Packages</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#a-list-of-useful-r-packages"><i class="fa fa-check"></i><b>2.2.1</b> A list of useful R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-programming.html"><a href="statistical-programming.html#r-basics"><i class="fa fa-check"></i><b>2.3</b> R Basics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-programming.html"><a href="statistical-programming.html#objects"><i class="fa fa-check"></i><b>2.3.1</b> Objects</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-programming.html"><a href="statistical-programming.html#workspace"><i class="fa fa-check"></i><b>2.3.2</b> Workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-programming.html"><a href="statistical-programming.html#functions-in-r"><i class="fa fa-check"></i><b>2.4</b> Functions in R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-types"><i class="fa fa-check"></i><b>2.5</b> Data types</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-programming.html"><a href="statistical-programming.html#numeric-vectors"><i class="fa fa-check"></i><b>2.5.1</b> Numeric Vectors</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-programming.html"><a href="statistical-programming.html#vector-arithmetic"><i class="fa fa-check"></i><b>2.5.2</b> Vector arithmetic</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-programming.html"><a href="statistical-programming.html#more-helpful-functions-in-r"><i class="fa fa-check"></i><b>2.5.3</b> More helpful functions in R</a></li>
<li class="chapter" data-level="2.5.4" data-path="statistical-programming.html"><a href="statistical-programming.html#other-types-of-vectors"><i class="fa fa-check"></i><b>2.5.4</b> Other types of vectors</a></li>
<li class="chapter" data-level="2.5.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-frames"><i class="fa fa-check"></i><b>2.5.5</b> Data Frames</a></li>
<li class="chapter" data-level="2.5.6" data-path="statistical-programming.html"><a href="statistical-programming.html#lists"><i class="fa fa-check"></i><b>2.5.6</b> Lists</a></li>
<li class="chapter" data-level="2.5.7" data-path="statistical-programming.html"><a href="statistical-programming.html#matrices"><i class="fa fa-check"></i><b>2.5.7</b> Matrices</a></li>
<li class="chapter" data-level="2.5.8" data-path="statistical-programming.html"><a href="statistical-programming.html#factors"><i class="fa fa-check"></i><b>2.5.8</b> Factors</a></li>
<li class="chapter" data-level="2.5.9" data-path="statistical-programming.html"><a href="statistical-programming.html#understanding-an-object-in-r"><i class="fa fa-check"></i><b>2.5.9</b> Understanding an object in R</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-programming.html"><a href="statistical-programming.html#logicals"><i class="fa fa-check"></i><b>2.6</b> Logicals</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-programming.html"><a href="statistical-programming.html#additional-logical-operators"><i class="fa fa-check"></i><b>2.6.1</b> Additional Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="statistical-programming.html"><a href="statistical-programming.html#programming-basics"><i class="fa fa-check"></i><b>2.7</b> Programming basics</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="statistical-programming.html"><a href="statistical-programming.html#writing-functions"><i class="fa fa-check"></i><b>2.7.1</b> Writing functions</a></li>
<li class="chapter" data-level="2.7.2" data-path="statistical-programming.html"><a href="statistical-programming.html#ifelse"><i class="fa fa-check"></i><b>2.7.2</b> if/else</a></li>
<li class="chapter" data-level="2.7.3" data-path="statistical-programming.html"><a href="statistical-programming.html#for-loops"><i class="fa fa-check"></i><b>2.7.3</b> for loops</a></li>
<li class="chapter" data-level="2.7.4" data-path="statistical-programming.html"><a href="statistical-programming.html#vectorization"><i class="fa fa-check"></i><b>2.7.4</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="statistical-programming.html"><a href="statistical-programming.html#advanced-topics"><i class="fa fa-check"></i><b>2.8</b> Advanced Topics</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="statistical-programming.html"><a href="statistical-programming.html#tidyverse"><i class="fa fa-check"></i><b>2.8.1</b> Tidyverse</a></li>
<li class="chapter" data-level="2.8.2" data-path="statistical-programming.html"><a href="statistical-programming.html#data-visualization"><i class="fa fa-check"></i><b>2.8.2</b> Data Visualization</a></li>
<li class="chapter" data-level="2.8.3" data-path="statistical-programming.html"><a href="statistical-programming.html#reproducible-research"><i class="fa fa-check"></i><b>2.8.3</b> Reproducible Research</a></li>
<li class="chapter" data-level="2.8.4" data-path="statistical-programming.html"><a href="statistical-programming.html#technical-writing-tools"><i class="fa fa-check"></i><b>2.8.4</b> Technical Writing Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-introduction-to-r-programming"><i class="fa fa-check"></i><b>2.9</b> Lab 1: Introduction to R Programming</a></li>
<li class="chapter" data-level="2.10" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-solutions"><i class="fa fa-check"></i><b>2.10</b> Lab 1: Solutions</a></li>
<li class="chapter" data-level="2.11" data-path="statistical-programming.html"><a href="statistical-programming.html#coding-exercises"><i class="fa fa-check"></i><b>2.11</b> Coding Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html"><i class="fa fa-check"></i><b>3</b> Probability and Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-probability"><i class="fa fa-check"></i><b>3.1</b> Topics in Probability</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#data-for-this-chapter"><i class="fa fa-check"></i><b>3.1.1</b> Data for this chapter</a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#random-variables"><i class="fa fa-check"></i><b>3.1.2</b> Random Variables</a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#pdfs-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.3</b> pdfs, pmfs, and cdfs</a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#summation-operator"><i class="fa fa-check"></i><b>3.1.4</b> Summation operator</a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.5</b> Properties of pmfs and cdfs</a></li>
<li class="chapter" data-level="3.1.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.1.6</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="3.1.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#expected-values"><i class="fa fa-check"></i><b>3.1.7</b> Expected Values</a></li>
<li class="chapter" data-level="3.1.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance"><i class="fa fa-check"></i><b>3.1.8</b> Variance</a></li>
<li class="chapter" data-level="3.1.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-and-variance-of-linear-functions"><i class="fa fa-check"></i><b>3.1.9</b> Mean and Variance of Linear Functions</a></li>
<li class="chapter" data-level="3.1.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.1.10</b> Multiple Random Variables</a></li>
<li class="chapter" data-level="3.1.11" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#conditional-expectations"><i class="fa fa-check"></i><b>3.1.11</b> Conditional Expectations</a></li>
<li class="chapter" data-level="3.1.12" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>3.1.12</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="3.1.13" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#covariance"><i class="fa fa-check"></i><b>3.1.13</b> Covariance</a></li>
<li class="chapter" data-level="3.1.14" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#correlation"><i class="fa fa-check"></i><b>3.1.14</b> Correlation</a></li>
<li class="chapter" data-level="3.1.15" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-expectationsvariances-of-sums-of-rvs"><i class="fa fa-check"></i><b>3.1.15</b> Properties of Expectations/Variances of Sums of RVs</a></li>
<li class="chapter" data-level="3.1.16" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#normal-distribution"><i class="fa fa-check"></i><b>3.1.16</b> Normal Distribution</a></li>
<li class="chapter" data-level="3.1.17" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding"><i class="fa fa-check"></i><b>3.1.17</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-statistics"><i class="fa fa-check"></i><b>3.2</b> Topics in Statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#simple-random-sample"><i class="fa fa-check"></i><b>3.2.1</b> Simple Random Sample</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#estimating-mathbbey"><i class="fa fa-check"></i><b>3.2.2</b> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-of-bary"><i class="fa fa-check"></i><b>3.2.3</b> Mean of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance-of-bary"><i class="fa fa-check"></i><b>3.2.4</b> Variance of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-estimators"><i class="fa fa-check"></i><b>3.2.5</b> Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#relative-efficiency"><i class="fa fa-check"></i><b>3.2.6</b> Relative Efficiency</a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-squared-error"><i class="fa fa-check"></i><b>3.2.7</b> Mean Squared Error</a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#large-sample-properties-of-estimators"><i class="fa fa-check"></i><b>3.2.8</b> Large Sample Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#inference-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.9</b> Inference / Hypothesis Testing</a></li>
<li class="chapter" data-level="3.2.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-1"><i class="fa fa-check"></i><b>3.2.10</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-monte-carlo-simulations"><i class="fa fa-check"></i><b>3.3</b> Lab 2: Monte Carlo Simulations</a></li>
<li class="chapter" data-level="3.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-solutions"><i class="fa fa-check"></i><b>3.4</b> Lab 2 Solutions</a></li>
<li class="chapter" data-level="3.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-questions"><i class="fa fa-check"></i><b>3.5</b> Coding Questions</a></li>
<li class="chapter" data-level="3.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#extra-questions"><i class="fa fa-check"></i><b>3.6</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#nonparametric-regression-curse-of-dimensionality"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Regression / Curse of Dimensionality</a></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>4.2</b> Linear Regression Models</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computation"><i class="fa fa-check"></i><b>4.3</b> Computation</a></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#partial-effects"><i class="fa fa-check"></i><b>4.4</b> Partial Effects</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-1"><i class="fa fa-check"></i><b>4.4.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-regression.html"><a href="linear-regression.html#binary-regressors"><i class="fa fa-check"></i><b>4.5</b> Binary Regressors</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-2"><i class="fa fa-check"></i><b>4.5.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-regression.html"><a href="linear-regression.html#nonlinear-regression-functions"><i class="fa fa-check"></i><b>4.6</b> Nonlinear Regression Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-3"><i class="fa fa-check"></i><b>4.6.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-interaction-terms"><i class="fa fa-check"></i><b>4.7</b> Interpreting Interaction Terms</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-4"><i class="fa fa-check"></i><b>4.7.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="linear-regression.html"><a href="linear-regression.html#elasticities"><i class="fa fa-check"></i><b>4.8</b> Elasticities</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-5"><i class="fa fa-check"></i><b>4.8.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="linear-regression.html"><a href="linear-regression.html#omitted-variable-bias"><i class="fa fa-check"></i><b>4.9</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="4.10" data-path="linear-regression.html"><a href="linear-regression.html#how-to-estimate-the-parameters-in-a-regression-model"><i class="fa fa-check"></i><b>4.10</b> How to estimate the parameters in a regression model</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-6"><i class="fa fa-check"></i><b>4.10.1</b> Computation</a></li>
<li class="chapter" data-level="4.10.2" data-path="linear-regression.html"><a href="linear-regression.html#more-than-one-regressor"><i class="fa fa-check"></i><b>4.10.2</b> More than one regressor</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i><b>4.11</b> Inference</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-7"><i class="fa fa-check"></i><b>4.11.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-birthweight-and-smoking"><i class="fa fa-check"></i><b>4.12</b> Lab 3: Birthweight and Smoking</a></li>
<li class="chapter" data-level="4.13" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-solutions"><i class="fa fa-check"></i><b>4.13</b> Lab 3: Solutions</a></li>
<li class="chapter" data-level="4.14" data-path="linear-regression.html"><a href="linear-regression.html#coding-questions-1"><i class="fa fa-check"></i><b>4.14</b> Coding Questions</a></li>
<li class="chapter" data-level="4.15" data-path="linear-regression.html"><a href="linear-regression.html#extra-questions-1"><i class="fa fa-check"></i><b>4.15</b> Extra Questions</a></li>
<li class="chapter" data-level="4.16" data-path="linear-regression.html"><a href="linear-regression.html#answers-to-some-extra-questions"><i class="fa fa-check"></i><b>4.16</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>5</b> Prediction</a>
<ul>
<li class="chapter" data-level="5.1" data-path="prediction.html"><a href="prediction.html#measures-of-regression-fit"><i class="fa fa-check"></i><b>5.1</b> Measures of Regression Fit</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="prediction.html"><a href="prediction.html#tss-ess-ssr"><i class="fa fa-check"></i><b>5.1.1</b> TSS, ESS, SSR</a></li>
<li class="chapter" data-level="5.1.2" data-path="prediction.html"><a href="prediction.html#r2"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="prediction.html"><a href="prediction.html#model-selection"><i class="fa fa-check"></i><b>5.2</b> Model Selection</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="prediction.html"><a href="prediction.html#limitations-of-r2"><i class="fa fa-check"></i><b>5.2.1</b> Limitations of <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.2" data-path="prediction.html"><a href="prediction.html#adjusted-r2"><i class="fa fa-check"></i><b>5.2.2</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.3" data-path="prediction.html"><a href="prediction.html#aic-bic"><i class="fa fa-check"></i><b>5.2.3</b> AIC, BIC</a></li>
<li class="chapter" data-level="5.2.4" data-path="prediction.html"><a href="prediction.html#cross-validation"><i class="fa fa-check"></i><b>5.2.4</b> Cross-Validation</a></li>
<li class="chapter" data-level="5.2.5" data-path="prediction.html"><a href="prediction.html#model-averaging"><i class="fa fa-check"></i><b>5.2.5</b> Model Averaging</a></li>
<li class="chapter" data-level="5.2.6" data-path="prediction.html"><a href="prediction.html#computation-8"><i class="fa fa-check"></i><b>5.2.6</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prediction.html"><a href="prediction.html#machine-learning"><i class="fa fa-check"></i><b>5.3</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="prediction.html"><a href="prediction.html#lasso"><i class="fa fa-check"></i><b>5.3.1</b> Lasso</a></li>
<li class="chapter" data-level="5.3.2" data-path="prediction.html"><a href="prediction.html#ridge-regression"><i class="fa fa-check"></i><b>5.3.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.3.3" data-path="prediction.html"><a href="prediction.html#computation-9"><i class="fa fa-check"></i><b>5.3.3</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="prediction.html"><a href="prediction.html#binary-outcome-models"><i class="fa fa-check"></i><b>5.4</b> Binary Outcome Models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="prediction.html"><a href="prediction.html#linear-probability-model"><i class="fa fa-check"></i><b>5.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="5.4.2" data-path="prediction.html"><a href="prediction.html#probit-and-logit"><i class="fa fa-check"></i><b>5.4.2</b> Probit and Logit</a></li>
<li class="chapter" data-level="5.4.3" data-path="prediction.html"><a href="prediction.html#average-partial-effects"><i class="fa fa-check"></i><b>5.4.3</b> Average Partial Effects</a></li>
<li class="chapter" data-level="5.4.4" data-path="prediction.html"><a href="prediction.html#computation-10"><i class="fa fa-check"></i><b>5.4.4</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prediction.html"><a href="prediction.html#lab-4-predicting-diamond-prices"><i class="fa fa-check"></i><b>5.5</b> Lab 4: Predicting Diamond Prices</a></li>
<li class="chapter" data-level="5.6" data-path="prediction.html"><a href="prediction.html#lab-4-solutions"><i class="fa fa-check"></i><b>5.6</b> Lab 4: Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="prediction.html"><a href="prediction.html#coding-questions-2"><i class="fa fa-check"></i><b>5.7</b> Coding Questions</a></li>
<li class="chapter" data-level="5.8" data-path="prediction.html"><a href="prediction.html#extra-questions-2"><i class="fa fa-check"></i><b>5.8</b> Extra Questions</a></li>
<li class="chapter" data-level="5.9" data-path="prediction.html"><a href="prediction.html#answers-to-some-extra-questions-1"><i class="fa fa-check"></i><b>5.9</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>6</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="causal-inference.html"><a href="causal-inference.html#potential-outcomes"><i class="fa fa-check"></i><b>6.1</b> Potential Outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="causal-inference.html"><a href="causal-inference.html#parameters-of-interest"><i class="fa fa-check"></i><b>6.2</b> Parameters of Interest</a></li>
<li class="chapter" data-level="6.3" data-path="causal-inference.html"><a href="causal-inference.html#experiments"><i class="fa fa-check"></i><b>6.3</b> Experiments</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="causal-inference.html"><a href="causal-inference.html#estimating-att-with-a-regression"><i class="fa fa-check"></i><b>6.3.1</b> Estimating ATT with a Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="causal-inference.html"><a href="causal-inference.html#internal-and-external-validity"><i class="fa fa-check"></i><b>6.3.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="6.3.3" data-path="causal-inference.html"><a href="causal-inference.html#example-project-star"><i class="fa fa-check"></i><b>6.3.3</b> Example: Project STAR</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="causal-inference.html"><a href="causal-inference.html#unconfoundedness"><i class="fa fa-check"></i><b>6.4</b> Unconfoundedness</a></li>
<li class="chapter" data-level="6.5" data-path="causal-inference.html"><a href="causal-inference.html#panel-data-approaches"><i class="fa fa-check"></i><b>6.5</b> Panel Data Approaches</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="causal-inference.html"><a href="causal-inference.html#difference-in-differences"><i class="fa fa-check"></i><b>6.5.1</b> Difference in differences</a></li>
<li class="chapter" data-level="6.5.2" data-path="causal-inference.html"><a href="causal-inference.html#computation-11"><i class="fa fa-check"></i><b>6.5.2</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="causal-inference.html"><a href="causal-inference.html#instrumental-variables"><i class="fa fa-check"></i><b>6.6</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="causal-inference.html"><a href="causal-inference.html#example-return-to-education"><i class="fa fa-check"></i><b>6.6.1</b> Example: Return to Education</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="causal-inference.html"><a href="causal-inference.html#regression-discontinuity"><i class="fa fa-check"></i><b>6.7</b> Regression Discontinuity</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="causal-inference.html"><a href="causal-inference.html#example-causal-effect-of-alcohol-on-driving-deaths"><i class="fa fa-check"></i><b>6.7.1</b> Example: Causal effect of Alcohol on Driving Deaths</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-drunk-driving-laws"><i class="fa fa-check"></i><b>6.8</b> Lab 5: Drunk Driving Laws</a></li>
<li class="chapter" data-level="6.9" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-solutions"><i class="fa fa-check"></i><b>6.9</b> Lab 5: Solutions</a></li>
<li class="chapter" data-level="6.10" data-path="causal-inference.html"><a href="causal-inference.html#coding-questions-3"><i class="fa fa-check"></i><b>6.10</b> Coding Questions</a></li>
<li class="chapter" data-level="6.11" data-path="causal-inference.html"><a href="causal-inference.html#extra-questions-3"><i class="fa fa-check"></i><b>6.11</b> Extra Questions</a></li>
<li class="chapter" data-level="6.12" data-path="causal-inference.html"><a href="causal-inference.html#answers-to-some-extra-questions-2"><i class="fa fa-check"></i><b>6.12</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supplementary Notes and References for ECON 4750</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prediction" class="section level1" number="5">
<h1><span class="header-section-number">Topic 5</span> Prediction</h1>
<p>In this part of the class, we’ll learn how to use regressions and “machine learning” in order to predict outcomes of interest using available, related data. One key issue when it comes to making predictions is that we typically have lots of possible models that we could use to make the predictions. Choosing which of these models works best, or <strong>model selection</strong>, is therefore often an important step when it comes to making good predictions.</p>
<p>Another important issue with prediction is a focus on making out-of-sample predictions. We can usually make better within-sample predictions just by making the model more complicated, but this often leads to <strong>over-fitting</strong> — predictions that are “too specific” to our particular data.</p>
<div id="measures-of-regression-fit" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Measures of Regression Fit</h2>
<p>We’ll start this chapter by learning about measures of how well a regression fits the data. Consider the following figure</p>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-137-1.png" width="50%" /><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-137-2.png" width="50%" />
These are exactly the same two regression lines. But we probably have the sense that the regression line in the second figure “fits better” than in the first figure, and, furthermore, that this is likely to result in better predictions of whatever the second outcome is relative to the first one. We’ll formalize this below.</p>
<div id="tss-ess-ssr" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> TSS, ESS, SSR</h3>
<p>SW 6.4</p>
<p>Let’s start by defining some quantities. These will be useful for quantifying how well the model fits the data.</p>
<ul>
<li><p>Total Sum of Squares (TSS) — measures total variation in the data</p>
<p><span class="math display">\[
    TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2
  \]</span></p></li>
<li><p>Explained Sum of Squares (ESS) — measures variation explained by the model</p>
<p><span class="math display">\[
    ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2
  \]</span></p></li>
<li><p>Sum of Squared Residuals (SSR) — measures “leftover” variation in the data that is not explained by the model</p>
<p><span class="math display">\[
    SSR = \sum_{i=1}^n \hat{U}_i^2 = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
  \]</span></p></li>
</ul>
<p><strong>Properties</strong></p>
<ul>
<li><p>A first useful property is</p>
<p><span class="math display">\[
    TSS = ESS + SSR
  \]</span></p>
<p>We will not prove this property though it is a useful exercise and not actually that challenging.</p></li>
<li><p>Another useful property is that <span class="math inline">\(TSS\)</span>, <span class="math inline">\(ESS\)</span>, and <span class="math inline">\(SSR\)</span> are all positive — this holds because they all involve sums of squared quantities.</p></li>
</ul>
</div>
<div id="r2" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> <span class="math inline">\(R^2\)</span></h3>
<p>SW 6.4</p>
<p><span class="math inline">\(R^2\)</span> is probably the most common measure of regression fit. It is defined as</p>
<p><span class="math display">\[
  R^2 := \frac{ESS}{TSS}
\]</span></p>
<p>so that <span class="math inline">\(R^2\)</span> is the fraction of the variation in <span class="math inline">\(Y\)</span> explained by the model. Notice that <span class="math inline">\(R^2\)</span> is always between 0 and 1 which holds by the two properties of <span class="math inline">\(TSS\)</span>, <span class="math inline">\(ESS\)</span>, and <span class="math inline">\(SSR\)</span> listed in the previous section.</p>
</div>
</div>
<div id="model-selection" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Model Selection</h2>
<p>SW 7.5 (note: we cover substantially more details than the textbook about model selection)</p>
<p>Often, when we want to use data to make predictions, there are multiple possible models that we could estimate to make the predictions. For example, suppose that we are interested in predicting house prices and we have access to data about the number of square feet, whether or not the house has a basement, the number of bedrooms, and the number of bathrooms. All of these are probably good variables to include in a model to predict house prices (though it is less clear if we should include quadratic terms, interaction terms, or other higher order terms). But suppose we also observe some variables that are probably irrelevant (e.g., whether the street number is odd or even) or not clear whether they matter or not (e.g., number of ceiling fans or paint color). It is not clear which variables we should include in the model. An alternative way to think about this is that there would be a large number of possible models that we could estimate here, and it is not immediately obvious which one will predict the best.</p>
<p>From the previous example, it seems that in many realistic applications, there are a large number of possible models that we could estimate to make predictions. How should we choose which one to use?</p>
<p>One idea is to just throw all the data that we have into the model. In many applications, this may not be a good idea. I’ll provide a specific example below.</p>
<p>Before doing that, I want to distinguish between two concepts. Typically, what we mean we say that a model is good at making predictions is that it is a good at making <strong>out-of-sample</strong> predictions. In other words, some new observation shows up (e.g., a new house comes on the market) with certain characteristics — we would like to be using a model that makes good predictions for this house. This is a distinct concept from <strong>in-sample</strong> fit of the model — how well a model predicts for the data that it is estimated on. In the context of prediction, one very common problem is <strong>over-fitting</strong>. Over-fitting is the problem of getting predictions that are <em>too specific</em> to the particular data that you have, but then that don’t work well for new data.</p>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 5.1  </strong></span>Suppose that you are interested in predicting height of students at UGA. You consider estimating three models</p>
<p><span class="math display">\[
  \begin{aligned}
    Height &amp;= \beta_0 + \beta_1 Male + U \\
    Height &amp;= \beta_0 + \beta_1 Male + \beta_2 Year + U \\
    Height &amp;= \beta_0 + \beta_1 Male + \beta_2 Year + \beta_3 Birthday + U
  \end{aligned}
\]</span>
where <span class="math inline">\(Year\)</span> is what year in school a student is in (e.g., Freshman, Sophomore, etc.), and <span class="math inline">\(Birthday\)</span> is what day of the year a student was born on.</p>
<p>You also notice that <span class="math inline">\(R^2\)</span> is greatest for the third model, in the middle for the second model, and smallest for the first model.</p>
<p>The third model likely suffers from over-fitting. In particular, knowing a student’s birthday may really help you predict height in-sample. For example, suppose that the center on the basketball team is in your sample, and his birthday is July 15. Knowing this, you will likely be able to make a much better prediction for this observation in your data using Model 3 which <span class="math inline">\(\implies\)</span> that the <span class="math inline">\(R^2\)</span> will be much higher for this model. But this won’t help you predict well out-of-sample. If a new person shows up whose birthday is July 15, they are unlikely to also be a center on the basketball team which further implies that your prediction of their height is likely to be very poor.</p>
</div>
<p>The previous example is about over-fitting with the idea that better in-sample predictions can actually lead to worse out-of-sample predictions.</p>
<div id="limitations-of-r2" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Limitations of <span class="math inline">\(R^2\)</span></h3>
<p>SW 6.4</p>
<p><span class="math inline">\(R^2\)</span> is a measure of in-sample fit. In fact, you can always increase <span class="math inline">\(R^2\)</span> by adding new variables into a model. To see this, notice that</p>
<p><span class="math display">\[
  \begin{aligned}
    R^2 &amp;= \frac{ESS}{TSS} \\
    &amp;= 1 - \frac{SSR}{TSS}
  \end{aligned}
\]</span>
When you add a new variable to a model, <span class="math inline">\(SSR\)</span> cannot increase (see explanation below). Since <span class="math inline">\(SSR\)</span> cannot increase, it means that <span class="math inline">\(R^2\)</span> can only increase (or at a minimum remain unchanged) when a new regressor is added to the model.</p>
<p>This means that, if you were to choose a model by <span class="math inline">\(R^2\)</span>, it would always choose the “kitchen-sink” (include everything) model. But, as we discussed above, this is likely to lead to severe over-fitting problems. This suggests using alternative approaches to choose a model for the purposed of prediction.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> It might not be obvious why <span class="math inline">\(SSR\)</span> necessarily decreases when a covariate is added to the model. Here is an explanation. Suppose that we estimate two models (I’ll include an <span class="math inline">\(i\)</span> subscript here and just use <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\delta\)</span> to remind you that the parameters are not equal to each other across models)</p>
<p><span class="math display">\[
  \begin{aligned}
  Y_i &amp;= \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \hat{U}_{1i} \\
  Y_i &amp;= \hat{\delta}_0 + \hat{\delta}_1 X_{1i} + \hat{\delta}_2 X_{2i} + \hat{U}_{2i}
  \end{aligned}
\]</span>
Recall that, for both models, we estimate the parameters by minimizing the sum of squared residuals (<span class="math inline">\(SSR\)</span>). Notice that the second model is equivalent to the first model when <span class="math inline">\(\hat{\delta}_2=0\)</span>. In that case, <span class="math inline">\(\hat{\delta}_0 = \hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\delta}_1 = \hat{\beta}_1\)</span>, the residuals would be exactly the same which implies that <span class="math inline">\(SSR\)</span> would be exactly the same across each model. Now, for the second model, if we estimate some any value of <span class="math inline">\(\hat{\delta}_2 \neq 0\)</span>, the reason why we would estimate that is because it makes the <span class="math inline">\(SSR\)</span> of the second model even lower.</p>
<p>This discussion means that, if we estimate <span class="math inline">\(\hat{\delta}_2=0\)</span>, then <span class="math inline">\(SSR\)</span> wil be the same across the first and second model, and that if we estimate any other value for <span class="math inline">\(\hat{\delta}_2\)</span>, then <span class="math inline">\(SSR\)</span> for the second model will be lower than for the first model.</p>
</div>
</div>
<div id="adjusted-r2" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Adjusted <span class="math inline">\(R^2\)</span></h3>
<p>SW 6.4</p>
<p>Given the above discussion, we would like to have a way to choose a model that doesn’t necessarily always select the most complicated model.</p>
<p>A main way to do this is to add a <strong>penalty</strong> to adding more regressors to the model. One version of this is called <strong>adjusted <span class="math inline">\(R^2\)</span></strong>, which we will write as <span class="math inline">\(\bar{R}^2\)</span>. It is defined as</p>
<p><span class="math display">\[
  \bar{R}^2 := 1 - \underbrace{\frac{(n-1)}{(n-k)}}_{\textrm{penalty}} \frac{SSR}{TSS}
\]</span>
where <span class="math inline">\(k\)</span> is the number of regressors included in the model (note: we also count the intercept as a regressor here; so for example, in the regression <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>, <span class="math inline">\(k=3\)</span>). Notice, if the penalty term were equal to 1, then this would just be <span class="math inline">\(R^2\)</span>.</p>
<p>Now, let’s think about what happens to <span class="math inline">\(\bar{R}^2\)</span> when you add a new regressor to the model.</p>
<ul>
<li><p><span class="math inline">\(SSR \downarrow \implies \bar{R}^2 \uparrow\)</span></p></li>
<li><p><span class="math inline">\((n-k-1) \downarrow \implies \bar{R}^2 \downarrow\)</span></p></li>
</ul>
<p>Thus, there is a “benefit” and a “cost” to adding a new regressor. If you are choosing among several models based on <span class="math inline">\(\bar{R}^2\)</span>, you would choose the model that has the highest <span class="math inline">\(\bar{R}^2\)</span>. And the logic is this: if the regressor greatly decreases <span class="math inline">\(SSR\)</span>, then the “benefit” of adding that regressor will tend to outweigh the “cost.” On the other hand, if the regressor has little effect on <span class="math inline">\(SSR\)</span>, then the “benefit” of adding that regressor will tend to be smaller than the “cost.”</p>
</div>
<div id="aic-bic" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> AIC, BIC</h3>
<p>There are other approaches to model selection that follow a similar idea. Two of the most common ones are the Akaike Information Criteria (AIC) and the Bayesian Information Criteria (BIC). These tend to be somewhat better choices for model selection than <span class="math inline">\(\bar{R}^2\)</span>.</p>
<p>These are given by</p>
<ul>
<li><span class="math inline">\(AIC = 2k + n \log(SSR)\)</span></li>
<li><span class="math inline">\(BIC = k \log(n) + n \log(SSR)\)</span></li>
</ul>
<p>For both AIC and BIC, you would choose the model that minimizes these.</p>
</div>
<div id="cross-validation" class="section level3" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Cross-Validation</h3>
<p>Another common way to choose a model is called <strong>cross-validation</strong>. The idea is to mimic the out-of-sample prediction problem using the data that we have access to.</p>
<p>In particular, one simple version of this is to split your data into two parts: these are usually called the <strong>training sample</strong> and the <strong>testing sample</strong>. Then, estimate all models under consideration using the training sample. Given the estimated values of the parameters, then predict the outcomes for observations in the testing sample; and compare these predictions to the actual outcomes in the testing sample. Choose the model that minimizes the squared prediction error in the testing sample.</p>
<p>Cross-validation is a somewhat more complicated version of the same idea. Basically, we will <em>repeatedly</em> split the data into a training sample and a testing sample, and get predictions for all observations in our data (instead of just for a held-out testing sample).</p>
<p>Here is an algorithm for cross-validation:</p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Split the data into J <strong>folds</strong></p></li>
<li><p>For the jth fold, do the following:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the model using all observations <em>not in</em> the Jth fold (<span class="math inline">\(\implies\)</span> we obtain estimates <span class="math inline">\(\hat{\beta}_0^j, \hat{\beta}_1^j, \ldots, \hat{\beta}_k^j\)</span>)</p></li>
<li><p>Predict outcomes for observations in the Jth fold using the estimated model from part (1):
<span class="math display">\[\begin{align*}
    \tilde{Y}_{ij} = \hat{\beta}_0^j + \hat{\beta}_1^j X_{1ij} + \cdots + \hat{\beta_k^j} X_{kij}
  \end{align*}\]</span></p></li>
<li><p>Compute the prediction error:
<span class="math display">\[\begin{align*}
    \tilde{U}_{ij} = Y_{ij} - \tilde{Y}_{ij}
  \end{align*}\]</span>
(this is the difference between actual outcomes for individuals in the Jth fold and their predicted outcome based on the model from part (1))</p></li>
</ol></li>
<li><p>Do steps 1-3 for all <span class="math inline">\(J\)</span> folds. This gives a prediction error <span class="math inline">\(\tilde{U}_i\)</span> for each observation in the data</p></li>
<li><p>compute the cross validation criteria (mean squared prediction error):
<span class="math display">\[\begin{align*}
CV = \frac{1}{n} \sum_{i=1}^n \tilde{U}_{i}^2
  \end{align*}\]</span></p></li>
<li><p>choose the model that produces the smallest value of <span class="math inline">\(CV\)</span>.</p></li>
</ul></li>
</ul>
<p>Cross-validation is a good way to choose a model, but it can sometimes be computationally challenging (because you are estimating lots of models) — particularly, if the models under consideration are very complicated or there a large number of observations.</p>
<p>Above, we split the data into <span class="math inline">\(J\)</span> folds. This introduces some randomness into our model selection criteria. In particular, if you split the data in a different way from how I split the data, then we could choose different models. This is a somewhat undesirable feature of a model selection criteria. One way to get around this is to set <span class="math inline">\(J=n\)</span>. This makes it so that every observation has its own fold. This is called <strong>leave-one-out cross-validation</strong>. In this case, there is no randomness in the model selection criteria. The drawback is that it is more computational — in this case, you need to estimate the model <span class="math inline">\(n\)</span> times rather than <span class="math inline">\(J\)</span> times.</p>
</div>
<div id="model-averaging" class="section level3" number="5.2.5">
<h3><span class="header-section-number">5.2.5</span> Model Averaging</h3>
<p>In the case where you are considering a large number of possible models, it is pretty common that a number of models will, by any of the above model selection criteria, be expected to perform very similarly when making predictions.</p>
<p>In this case, one strategy that usually does well in terms of making out-of-sample predictions is <em>model averaging</em>.</p>
<p>Suppose you have <span class="math inline">\(M\)</span> different models, and that each model can produce a predicted value for <span class="math inline">\(Y_i\)</span> — let’s call the predicted value from model <span class="math inline">\(m\)</span>, <span class="math inline">\(\hat{Y}_i^m\)</span>. Model averaging would involve obtaining a new predicted value, call it <span class="math inline">\(\hat{Y}_i\)</span> by computing
<span class="math display">\[\begin{align*}
  \hat{Y}_i = \frac{1}{M} \sum_{m=1}^M \hat{Y}_i^m
\end{align*}\]</span></p>
<ul>
<li>Usually, you would throw out models that you know predict poorly and only average together ones that perform reasonably well.</li>
</ul>
</div>
<div id="computation-8" class="section level3" number="5.2.6">
<h3><span class="header-section-number">5.2.6</span> Computation</h3>
<p><span class="math inline">\(R^2\)</span> and <span class="math inline">\(\bar{R}^2\)</span> are both reported as output from <code>R</code>’s <code>lm</code> command.</p>
<p>It is straightforward (and a useful exercise) to compute <span class="math inline">\(TSS, ESS,\)</span> and <span class="math inline">\(SSR\)</span> directly. It is also straightforward to calculate <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span> — there are probably packages that will do this for you, but they are so easy that I suggest just calculating on your own.</p>
<p>The same applies to cross validation. I suspect that there are packages available that will do this for you, but I think these are useful coding exercises and not all that difficult. Also, if you do this yourself, it removes any kinds of “black box” issues from downloading <code>R</code> code where it may not be clear exactly what it is doing.</p>
</div>
</div>
<div id="machine-learning" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Machine Learning</h2>
<p>SW 14.1, 14.2, 14.6</p>
<p>Some extra resources on estimating Lasso and Ridge regressions in R:</p>
<ul>
<li><p><a href="https://www.statology.org/lasso-regression-in-r/">glmnet tutorial</a></p></li>
<li><p><a href="https://cran.r-project.org/web/packages/glmnetUtils/vignettes/intro.html">glmnetUtils vignette</a></p></li>
</ul>
<p>“Big” Data typically means one of two things:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(n\)</span> — the number of observations is extremely large</p></li>
<li><p><span class="math inline">\(k\)</span> — the number of regressors is very large</p></li>
</ol>
<p><span class="math inline">\(n\)</span> being large is more of a computer science problem. That said, there are economists who think about these issues, but I think it is still the case that it is relatively uncommon for an economist to have <em>so much</em> data that they have trouble computing their estimators.</p>
<p>We’ll focus on the second issue — where <span class="math inline">\(k\)</span> is large. A main reason that this may occur in applications is that we may be unsure of the functional form for <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span>. Should it include higher order terms like <span class="math inline">\(X_1^2\)</span> or <span class="math inline">\(X_1^3\)</span>? Should it include interactions like <span class="math inline">\(X_1 X_2\)</span>. Once you start proceeding this way, even if you only have 5-10 actual covariates, <span class="math inline">\(k\)</span> can become quite large.</p>
<p>As in the case of the mean, perhaps we can improve predictions by introducing some bias while simultaneously decreasing the variance of our predictions.</p>
<p>Let’s suppose that we have some function that can take in regressors and makes predictions of the outcome; we’ll call this function <span class="math inline">\(\hat{f}\)</span> (where the “hat” indicates that we’ll typically estimate this function). An example would just be a regression where, for example, <span class="math inline">\(\hat{f}(X) = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_3\)</span>. One way to evaluate how well <span class="math inline">\(\hat{f}\)</span> makes predictions is by considering its <strong>mean squared prediction error</strong>:</p>
<p><span class="math display">\[
  MSPE := \mathbb{E}\left[ (Y - \hat{f}(X))^2 \right]
\]</span>
<span class="math inline">\(MSPE\)</span> quantifies the mean “distance” between our predictions and actual outcomes.</p>
<p>Recall that, if we could just pick any function to minimize <span class="math inline">\(MSPE\)</span>, we would set <span class="math inline">\(\hat{f}(X) = \mathbb{E}[Y|X]\)</span>, but generally we do not just know what <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is. We can “decompose” MSPE into several underlying conditions</p>
<p><span class="math display">\[
  \begin{aligned}
  MSPE &amp;= \mathbb{E}\left[ \left( (Y - \mathbb{E}[Y|X]) - (\hat{f}(X) - \mathbb{E}[\hat{f}(X)|X]) - (\mathbb{E}[\hat{f}(X)|X] - \mathbb{E}[Y|X]) \right)^2 \right] \\
  &amp;= \mathbb{E}\left[ (Y-\mathbb{E}[Y|X])^2 \right] + \mathbb{E}\left[ (\hat{f}(X) - \mathbb{E}[\hat{f}(X)|X])^2\right] + \mathbb{E}\left[ (\mathbb{E}[\hat{f}(X)|X] - \mathbb{E}[Y|X])^2 \right] \\
  &amp;= \mathrm{var}(U) + \mathbb{E}[\mathrm{var}(\hat{f}(X)|X)] + \mathbb{E}\left[\textrm{Bias}(\hat{f}(X))^2\right]
  \end{aligned}
\]</span>
where, to understand this decomposition, the first equality just adds and subtracts <span class="math inline">\(\mathbb{E}[Y|X]\)</span> and <span class="math inline">\(\mathbb{E}[\hat{f}(X)|X]\)</span>. The second equality squares the long expression from the first equality and cancels some terms. The third equality holds where <span class="math inline">\(U := Y-\mathbb{E}[Y|X]\)</span>, by the definition of (conditional) variance, and we call the last bias because it is the difference between the mean of our prediction function <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(\mathbb{E}[Y|X]\)</span>.</p>
<p>You can think of the term <span class="math inline">\(\mathrm{var}(U)\)</span> as being irreducible prediction error — even if we knew <span class="math inline">\(\mathbb{E}[Y|X]\)</span>, we wouldn’t get every prediction exactly right. But the other two terms come from having to estimate <span class="math inline">\(\hat{f}\)</span>. The term <span class="math inline">\(\mathbb{E}[\mathrm{var}(\hat{f}(X)X)]\)</span> is the average variance of our predictions across different values of <span class="math inline">\(X\)</span>. The term <span class="math inline">\(\mathbb{E}\left[\textrm{Bias}(\hat{f}(X))^2\right]\)</span> is the squared bias of our predictions averaged across different values of <span class="math inline">\(X\)</span>. Many machine learning estimators have the property of being biased (so that the last term is not equal to 0), but having reduced variance relative to OLS estimators.</p>
<p>To do this, we’ll estimate the parameters of the model in a similar way to what we have done before except that we’ll add a <strong>penalty</strong> term that gets larger as parameter estimates move further away from 0. In particular, we consider estimates of the form</p>
<p><span class="math display">\[
  (\hat{\beta}_1, \hat{\beta}_2, \ldots, \hat{\beta}_k) = \underset{b_1,\ldots,b_k}{\textrm{argmin}} \sum_{i=1}^n  (Y_i - b_1 X_{1i} - \cdots - b_k X_{ki})^2 + \textrm{penalty}
\]</span></p>
<p>Lasso and ridge regression, which are the two approaches to machine learning that we will talk about below, amount to different choices of the penalty term.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> Generally, you should “standardize” the regressors before implementing Lasso or Ridge regression. The <code>glmnet</code> package does this for you by default.</p>
</div>
<div id="lasso" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Lasso</h3>
<p>SW 14.3</p>
<p>For Lasso, the penalty term is given by</p>
<p><span class="math display">\[
  \lambda \sum_{j=1}^k |b_j|
\]</span>
The absolute value on each of the <span class="math inline">\(b_j\)</span> terms means that the penalty function gets larger for larger values of any <span class="math inline">\(b_j\)</span>. Since we are trying to minimize the objective function, this means that there is a “cost” to choosing a larger value of <span class="math inline">\(b_j\)</span> relative to OLS. Thus, Lasso estimates tend to be “shrunk” towards 0.</p>
<p><span class="math inline">\(\lambda\)</span> is called a <strong>tuning parameter</strong> — larger values of <span class="math inline">\(\lambda\)</span> imply that the penalty term is more important. Notice that, if you send <span class="math inline">\(\lambda \rightarrow \infty\)</span>, then the penalty term will be so large that you would set all the parameters to be equal to 0. On the other hand, if you set <span class="math inline">\(\lambda=0\)</span>, then you will get the OLS estimates (because there will be no penalty in this case). In general, it is hard to know what is the “right” value for <span class="math inline">\(\lambda\)</span>, and it is typically chosen using cross validation (this will be done automatically for you using the <code>glmnet</code> package).</p>
</div>
<div id="ridge-regression" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Ridge Regression</h3>
<p>SW 14.4</p>
<p>For Ridge, the penalty term is given by</p>
<p><span class="math display">\[
  \lambda \sum_{j=1}^k b_j^2
\]</span>
Much of the discussion from Lasso applies here. The only difference is that the form of the penalty is different here: <span class="math inline">\(b_j^2\)</span> instead of <span class="math inline">\(|b_j|\)</span>. Relative to the Lasso penalty, the Ridge penalty “dislikes more” very large values of <span class="math inline">\(b_j\)</span>.</p>
<p><strong>Comparing Lasso and Ridge</strong></p>
<ul>
<li><p>Both <strong>shrink</strong> the estimated parameters towards 0. This tends to introduce bias into our predictions but comes with the benefit of reducing the variance of the predictions.</p></li>
<li><p>Interestingly, both Lasso and Ridge can be implemented when <span class="math inline">\(k &gt; n\)</span> (i.e., if you have more regressors than observations). This is in contrast to to OLS, where the parameters cannot be estimated in this case.</p></li>
<li><p>Both are generally not very computationally intensive. For ridge regression, we can in fact derive an explicit expression for the estimated <span class="math inline">\(\beta\)</span>’s. We cannot do this with Lasso; a full discussion of how Lasso actually estimates the parameters is beyond the scope of our course, but, suffice it to say, that you can generally compute Lasso estimates quickly.</p></li>
<li><p>Lasso also performs “model selection” — that is, if you use the Lasso, many of the estimated parameters will be set equal to 0. This can sometimes be an advantage. Ridge (like OLS) will generally produce non-zero estimates of all parameters in the model.</p></li>
</ul>
</div>
<div id="computation-9" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Computation</h3>
<p>Computing Lasso and Ridge regressions is somewhat more complicated than most other things that we have computed this semester. The main <code>R</code> package for Lasso and Ridge regressions is the <code>glmnet</code> package. For some reason, the syntax of the package is somewhat different from, for example, the <code>lm</code> command. In my view, it is often easier to use the <code>glmnetUtils</code> package, which seems to be just a wrapper for <code>glmnet</code> but with functions that are analogous to <code>lm</code>.</p>
<p>I’m just going to sketch here how you would use these functions to estimate a Lasso or Ridge regression. In the lab later on in this chapter, we’ll do a more concrete example. Suppose that you have access to a training dataset called <code>train</code> and a testing dataset called <code>test</code>, you can use the <code>glmnetUtils</code> package in the following way:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="prediction.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnetUtils)</span>
<span id="cb134-2"><a href="prediction.html#cb134-2" aria-hidden="true" tabindex="-1"></a>lasso_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data=</span>train, <span class="at">use.model.frame=</span><span class="cn">TRUE</span>) <span class="co"># or whatever formula you want to use</span></span>
<span id="cb134-3"><a href="prediction.html#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lasso_model) <span class="co"># if you are interested in estimated coefficients</span></span>
<span id="cb134-4"><a href="prediction.html#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lasso_model, <span class="at">newdata=</span>test) <span class="co"># get predictions</span></span>
<span id="cb134-5"><a href="prediction.html#cb134-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-6"><a href="prediction.html#cb134-6" aria-hidden="true" tabindex="-1"></a>ridge_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, </span>
<span id="cb134-7"><a href="prediction.html#cb134-7" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data=</span>train, </span>
<span id="cb134-8"><a href="prediction.html#cb134-8" aria-hidden="true" tabindex="-1"></a>                         <span class="at">alpha=</span><span class="dv">0</span>,</span>
<span id="cb134-9"><a href="prediction.html#cb134-9" aria-hidden="true" tabindex="-1"></a>                         <span class="at">use.model.frame=</span><span class="cn">TRUE</span>)</span>
<span id="cb134-10"><a href="prediction.html#cb134-10" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge_model)</span>
<span id="cb134-11"><a href="prediction.html#cb134-11" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ridge_model, <span class="at">newdata=</span>test)</span></code></pre></div>
<p>It’s worth making a couple of comments about this</p>
<ul>
<li><p>In terms of code, the only difference between the Lasso and Ridge, is that for Ridge, we added the extra argument <code>alpha=0</code>. The default value of <code>alpha</code> is 1, and that leads to a Lasso regression (which is why didn’t need to specify it for our Lasso estimates). If you are interested, you can read more details about this in the documentation for <code>glmnet</code> using <code>?glmnet</code>.</p></li>
<li><p>Generally, if you are going to use Lasso or Ridge, then you would have many more regressors to include than just <code>X1 + X2 + X3</code>. One common way that you get more regressors is when you start to thinking about including higher order terms or interaction terms. One way to do this quickly is to use the <code>poly</code> function inside the formula. For example,</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="prediction.html#cb135-1" aria-hidden="true" tabindex="-1"></a>lasso_model2 <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Y <span class="sc">~</span> <span class="fu">poly</span>(X1, X2, X3, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">raw=</span><span class="cn">TRUE</span>), </span>
<span id="cb135-2"><a href="prediction.html#cb135-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data=</span>train, </span>
<span id="cb135-3"><a href="prediction.html#cb135-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">use.model.frame=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>would include all interactions between <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> as well as squared terms for each; if you wanted to include more interactions and cubic terms, you could specify <code>degree=3</code>.</p></li>
</ul>
</div>
</div>
<div id="binary-outcome-models" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Binary Outcome Models</h2>
<p>In addition to referenced material below, please read all of SW Ch. 11</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> We are not necessarily so interested in prediction (though you can make predictions using the techniques we discuss below), but I find this a good spot to teach about binary outcome models before we conclude the course talking about causality</p>
</div>
<p>You may or may not have noticed this, but all the outcomes that we have considered so far have involved a continuous outcome. But lots of economic variables are discrete (we’ll mainly focus on binary outcomes). Examples:</p>
<ul>
<li><p>Labor force participation</p></li>
<li><p>Graduating from college</p></li>
</ul>
<p>The question is: Do our linear regression tools still apply to this case? In other words, does</p>
<p><span class="math display">\[
  \mathbb{E}[Y | X_1, X_2, X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span>
still make sense?</p>
<ul>
<li>Note: we have already included binary regressors and know how to interpret these, so this section is about binary outcomes rather than binary regressors</li>
</ul>
<div id="linear-probability-model" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Linear Probability Model</h3>
<p>SW 11.1</p>
<p>Let’s continue to consider</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span></p>
<p>when <span class="math inline">\(Y\)</span> is binary. Of course, you can still run this regression.</p>
<p>One thing that is helpful to notice before we really get started here is that when <span class="math inline">\(Y\)</span> is binary (so that either <span class="math inline">\(Y=0\)</span> or <span class="math inline">\(Y=1\)</span>)</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathbb{E}[Y] &amp;= \sum_{y \in \mathcal{Y}} y \mathrm{P}(Y=y) \\
  &amp;= 0 \mathrm{P}(Y=0) + 1 \mathrm{P}(Y=1) \\
  &amp;= \mathrm{P}(Y=1)
  \end{aligned}
\]</span>
And exactly the same sort of argument implies that, when <span class="math inline">\(Y\)</span> is binary, <span class="math inline">\(\mathbb{E}[Y|X] = \mathrm{P}(Y=1|X)\)</span>. Thus, if we believe the model in the first part of this section, this result implies that</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1|X_1,X_2,X_3) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span>
For this reason, the model in this section is called the <strong>linear probabilty model</strong>. Moreover, this further implies that we should interpret</p>
<p><span class="math display">\[
  \beta_1 = \frac{\partial \, \mathrm{P}(Y=1|X_1,X_2,X_3)}{\partial \, X_1}
\]</span>
as a partial effect. That is, <span class="math inline">\(\beta_1\)</span> is how much the probability that <span class="math inline">\(Y=1\)</span> changes when <span class="math inline">\(X_1\)</span> increases by one unit, holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant. This is good (and simple), but there are some drawbacks:</p>
<ol style="list-style-type: decimal">
<li><p>It’s possible to get non-sensical predictions (predicted probabilities that are less than 0 or greater than 1) with a linear probability model.</p></li>
<li><p>A related problem is that the linear probability model implies constant partial effects. That is, the effect of a change in one regressor always changes the probability of <span class="math inline">\(Y=1\)</span> (holding other regressors constant) by the same amount. It may not be obvious that this is a disadvantage, but it is.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-23" class="example"><strong>Example 5.2  </strong></span>Let <span class="math inline">\(Y=1\)</span> if an individual participates in the labor force. Further let <span class="math inline">\(X_1=1\)</span> if an individual is male and 0 otherwise, <span class="math inline">\(X_2\)</span> denote an individual’s age, and <span class="math inline">\(X_3=1\)</span> for college graduates and 0 otherwise.</p>
<p>Additionally, suppose that <span class="math inline">\(\beta_0=0.4, \beta_1=0.2, \beta_2=0.01, \beta_3=0.1\)</span>.</p>
<p>Let’s calculate the probability of being in the labor force for a 40 year old woman who is not a college graduate. This is given by</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1 | X_1=0, X_2=40, X_3=0) = 0.4 + (0.01)(40) = 0.8
\]</span>
In other words, we’d predict that, given these characteristics, the probability of being in the labor force is 0.8.</p>
<p>Now, let’s calculate the probability of being in the labor force for a 40 year old man who is a college graduate. This is given by</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1|X_1=1, X_2=40, X_3=1) = 0.4 + 0.2 + (0.01)(40) + 0.1 = 1.1
\]</span>
We have calculated that the predicted probability of being in the labor force, given these characteristics, is 1.1 — this makes no sense! Our maximum predicted probabilty should be 1.</p>
<p>The problem of constant partial effect is closely related. Here, labor force participation is increasing in age, but with a binary outcome (by construction) the effect has to die off — for those who are already very likely to participate in the labor force (in this example, older men with a college education, the partial effect of age has to be low because they are already very likely to participate in the labor force).</p>
</div>
<p>We can circumvent both of the main problems with the linear probability model by consider <strong>nonlinear models</strong> for binary outcomes. By far the most common are <strong>probit</strong> and <strong>logit</strong>. We will discuss these next.</p>
</div>
<div id="probit-and-logit" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Probit and Logit</h3>
<p>SW 11.2, 11.3</p>
<p>Let’s start this section with probit. A probit model arises from setting</p>
<p><span class="math display">\[
  \mathrm{P}(Y=1|X_1,X_2,X_3) = \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)
\]</span>
where <span class="math inline">\(\Phi\)</span> is the cdf of a standard normal random variable. This is a nonlinear model due to <span class="math inline">\(\Phi\)</span> making the model nonlinear in parameters.</p>
<p>Using <span class="math inline">\(\Phi\)</span> (or any cdf) here has a useful property that no matter what value the “index” <span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3\)</span> takes on, the cdf is always between 0 and 1. This implies that we cannot get predicted probabilities outside of 0 and 1.</p>
<p>Thus, this circumvents the problems with the linear probability model. That said, there are some things we have to be careful about. First, as usual, we are interested in partial effects rather than the parameters themselves. But partial effects are more complicated here. Notice that</p>
<p><span class="math display">\[
  \begin{aligned}
  \frac{ \partial \, P(Y=1|X_1,X_2,X_3)}{\partial \, X_1} &amp;= \frac{\partial \, \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}{\partial \, X_1} \\
  &amp;= \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1
  \end{aligned}
\]</span>
where <span class="math inline">\(\phi\)</span> is the pdf of a standard normal random variable. And the second equality requires using the chain rule — take the derivative of the “outside” (i.e., <span class="math inline">\(\Phi\)</span>) and then the derivative of the “inside” with respect to <span class="math inline">\(X_1\)</span>. Notice that this partial effect is more complicated that in the case of the linear models that we have mainly considered — it involves <span class="math inline">\(\phi\)</span>, but more importantly it also depends on the values of all the covariates. In other words, the partial effect of <span class="math inline">\(X_1\)</span> can vary across different values of <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>.</p>
<p><strong>Logit</strong> is conceptually similar to probit, but instead of using <span class="math inline">\(\Phi\)</span>, Logit uses the logistic function <span class="math inline">\(\Lambda(z) = \frac{\exp(z)}{1+\exp(z)}\)</span>. The logistic function has the same important properties as <span class="math inline">\(\Phi\)</span>: (i) <span class="math inline">\(\Lambda(z)\)</span> is increasing in <span class="math inline">\(z\)</span>, (ii) <span class="math inline">\(\Lambda(z) \rightarrow 1\)</span> as <span class="math inline">\(z \rightarrow \infty\)</span>, and (iii) <span class="math inline">\(\Lambda(z) \rightarrow 0\)</span> as <span class="math inline">\(z \rightarrow -\infty\)</span>. Thus, in a logit model,</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{P}(Y=1 | X_1, X_2, X_3) &amp;= \Lambda(\beta_0 + \beta_1 X_1 + \beta_2 + \beta_3 X_3) \\
  &amp;= \frac{\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}{1+\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}
  \end{aligned}
\]</span></p>
<p><strong>Estimation</strong></p>
<p>Because probit and logit models are nonlinear, estimation is more complicated than for the linear regression models that we were studying before. In particular, we cannot write down a formula like <span class="math inline">\(\hat{\beta}_1 = \textrm{something}\)</span>.</p>
<p>Instead, probit and logit models are typically esetimated through an approach called <strong>maximum likelihood estimation</strong>. Basically, the computer will solve an optimization problem trying to choose the “most likely” values of the parameters given the data that you have. It turns out that this particular optimization problem is actually quite easy for the computer to solve — even though estimating the parameters is more complicated than for linear regression, it will still feel like <code>R</code> can estimate a probit or logit model pretty much instantly.</p>
</div>
<div id="average-partial-effects" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> Average Partial Effects</h3>
<p>One of the complications with Probit and Logit is that it is not so simple to interpret the estimated parameters.</p>
<p>Remember we are generally interested in partial effects, not the parameters themselves. It just so happens that in many of the linear models that we have considered so far the <span class="math inline">\(\beta\)</span>’s correspond to the partial effect — this means that it is sometimes easy to forget that they are not what we are typically most interested in.</p>
<p>This is helpful framing for thinking about how to interpret the results from a Probit or Logit model.</p>
<p>Let’s focus on the Probit model. In that case,
<span class="math display">\[\begin{align*}
  \mathrm{P}(Y=1|X_1,X_2,X_3) = \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)
\end{align*}\]</span>
where <span class="math inline">\(\Phi\)</span> is the cdf of standard normal random variable.</p>
<p><em>Continuous Case</em>: When <span class="math inline">\(X_1\)</span> is continuous, the partial effect of <span class="math inline">\(X_1\)</span> is given by
<span class="math display">\[\begin{align*}
  \frac{\partial \mathrm{P}(Y=1|X_1,X_2,X_3)}{\partial X_1} = \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1
\end{align*}\]</span>
where <span class="math inline">\(\phi\)</span> is the pdf of a standard normal random variable. This is more complicated than the partial effect in the context of a linear model. It depends on <span class="math inline">\(\phi\)</span> (which looks complicated, but you can just use <code>R</code>’s <code>dnorm</code> command to handle that part). More importantly, the partial effect depends on the values of <span class="math inline">\(X_1,X_2,\)</span> and <span class="math inline">\(X_3\)</span>. [As discussed above, this is likely a good thing in the context of a binary outcome model]. Thus, in order to get a partial effect, we need to put in some values for these. If you have particular values of the covariates that you are interested in, you can definitely do that, but my general suggestion is to report the <em>Average Partial Effect</em>:
<span class="math display">\[\begin{align*}
  APE &amp;= \mathbb{E}\left[ \frac{\partial \mathrm{P}(Y=1|X_1,X_2,X_3)}{\partial X_1} \right] \\
  &amp;= \mathbb{E}\left[ \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1 \right]
\end{align*}\]</span>
which you can estimate by
<span class="math display">\[\begin{align*}
  \widehat{APE} &amp;= \frac{1}{n} \sum_{i=1}^n \phi(\hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_3) \hat{\beta}_1
\end{align*}\]</span>
which amounts to just computing the partial effect at each value of the covariates in your data and then averaging these partial effects together. This can be a bit cumbersome to do in practice, and it is often convenient to use the <code>R</code> package <code>mfx</code> to compute these sorts of average partial effects for you.</p>
<p><em>Discrete/Binary Case</em>: When <span class="math inline">\(X_1\)</span> is discrete (let’s say binary, but extention to discrete is straightforward), the partial effect of <span class="math inline">\(X_1\)</span> is
<span class="math display">\[\begin{align*}
  &amp; \mathrm{P}(Y=1|X_1=1, X_2, X_3) - \mathrm{P}(Y=1|X_1=0, X_2, X_3) \\
  &amp;\hspace{100pt} = \Phi(\beta_0 + \beta_1 + \beta_2 X_2 + \beta_3 X_3) - \Phi(\beta_0 + \beta_2 X_2 + \beta_3 X_3)
\end{align*}\]</span>
Notice that <span class="math inline">\(\beta_1\)</span> does not show up in the last term. As above, the partial effect depends on the values of <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> which suggests reporting an <span class="math inline">\(APE\)</span> as above (follows the same steps, just replacing the partial effect, as in the continuous case above)</p>
<ul>
<li>Extensions to Logit are virtually identical, just replace <span class="math inline">\(\Phi\)</span> with <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\phi\)</span> with <span class="math inline">\(\lambda\)</span>.</li>
</ul>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> The parameters from LPM, Probit, and Logit could be quite different (in fact, they are quite different by construction), but APE’s are often very similar.</p>
</div>
</div>
<div id="computation-10" class="section level3" number="5.4.4">
<h3><span class="header-section-number">5.4.4</span> Computation</h3>
<p>In this section, I’ll demonstrate how to estimate and interpret binary outcome models using the <code>titanic_training</code> data. The outcome for this data is <code>Survived</code> which is a binary variable indicating whether a particular passenger survived the Titanic wreck. We’ll estimate models that also include passenger class (<code>Pclass</code>), passenger’s sex, and passenger’s age.</p>
<p>The main <code>R</code> function for estimating binary outcome models is the <code>glm</code> function (this stands for “generalized linear model”). The syntax is very similar to the syntax for the <code>lm</code> command, so much of what we do below will feel feel familiar. We’ll also use the <code>probitmfx</code> and <code>logitmfx</code> functions from the <code>mfx</code> package to compute partial effects.</p>
<p><strong>Linear Probability Model</strong></p>
<p>We’ll start by estimating a linear probability model.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="prediction.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb136-2"><a href="prediction.html#cb136-2" aria-hidden="true" tabindex="-1"></a>titanic_train <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/titanic_training.csv&quot;</span>)</span>
<span id="cb136-3"><a href="prediction.html#cb136-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-4"><a href="prediction.html#cb136-4" aria-hidden="true" tabindex="-1"></a><span class="co"># linear probability model</span></span>
<span id="cb136-5"><a href="prediction.html#cb136-5" aria-hidden="true" tabindex="-1"></a>lpm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> </span>
<span id="cb136-6"><a href="prediction.html#cb136-6" aria-hidden="true" tabindex="-1"></a>            Sex <span class="sc">+</span> Age, </span>
<span id="cb136-7"><a href="prediction.html#cb136-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">data=</span>titanic_train)</span>
<span id="cb136-8"><a href="prediction.html#cb136-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lpm)</span>
<span id="cb136-9"><a href="prediction.html#cb136-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-10"><a href="prediction.html#cb136-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb136-11"><a href="prediction.html#cb136-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = Survived ~ as.factor(Pclass) + Sex + Age, data = titanic_train)</span></span>
<span id="cb136-12"><a href="prediction.html#cb136-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-13"><a href="prediction.html#cb136-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb136-14"><a href="prediction.html#cb136-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb136-15"><a href="prediction.html#cb136-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -1.05638 -0.26294 -0.07656  0.21103  0.98057 </span></span>
<span id="cb136-16"><a href="prediction.html#cb136-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-17"><a href="prediction.html#cb136-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb136-18"><a href="prediction.html#cb136-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb136-19"><a href="prediction.html#cb136-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)         1.065516   0.061481  17.331  &lt; 2e-16 ***</span></span>
<span id="cb136-20"><a href="prediction.html#cb136-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2 -0.148575   0.050593  -2.937 0.003472 ** </span></span>
<span id="cb136-21"><a href="prediction.html#cb136-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3 -0.349809   0.046462  -7.529 2.44e-13 ***</span></span>
<span id="cb136-22"><a href="prediction.html#cb136-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Sexmale            -0.490605   0.037330 -13.143  &lt; 2e-16 ***</span></span>
<span id="cb136-23"><a href="prediction.html#cb136-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Age                -0.004570   0.001317  -3.471 0.000564 ***</span></span>
<span id="cb136-24"><a href="prediction.html#cb136-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb136-25"><a href="prediction.html#cb136-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb136-26"><a href="prediction.html#cb136-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb136-27"><a href="prediction.html#cb136-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.3915 on 495 degrees of freedom</span></span>
<span id="cb136-28"><a href="prediction.html#cb136-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.3747, Adjusted R-squared:  0.3696 </span></span>
<span id="cb136-29"><a href="prediction.html#cb136-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 74.14 on 4 and 495 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Let’s quickly interpret a couple of these parameters. Recall that these can all be directly interpreted as partial effects on the probability of surviving the Titanic wreck. For example, these estimates indicate that third class passengers were about 33% less likely to survive on average than first class passengers controlling for passenger’s sex and age.</p>
<p>The other thing that jumps out is passenger’s sex. These estimates indicate that men were, on average, 49% less likely to survive the Titanic wreck than women after controlling for passenger class, and age.</p>
<p>Before we move on, let’s compute a couple of predicted probabilities.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="prediction.html#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="co"># young, first class, female</span></span>
<span id="cb137-2"><a href="prediction.html#cb137-2" aria-hidden="true" tabindex="-1"></a>pred_df1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Pclass=</span><span class="dv">1</span>, <span class="at">Sex=</span><span class="st">&quot;female&quot;</span>, <span class="at">Age=</span><span class="dv">25</span>, <span class="at">Embarked=</span><span class="st">&quot;S&quot;</span>)</span>
<span id="cb137-3"><a href="prediction.html#cb137-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-4"><a href="prediction.html#cb137-4" aria-hidden="true" tabindex="-1"></a><span class="co"># old, third class, male</span></span>
<span id="cb137-5"><a href="prediction.html#cb137-5" aria-hidden="true" tabindex="-1"></a>pred_df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Pclass=</span><span class="dv">3</span>, <span class="at">Sex=</span><span class="st">&quot;male&quot;</span>, <span class="at">Age=</span><span class="dv">55</span>, <span class="at">Embarked=</span><span class="st">&quot;S&quot;</span>)</span>
<span id="cb137-6"><a href="prediction.html#cb137-6" aria-hidden="true" tabindex="-1"></a>pred_df <span class="ot">&lt;-</span> <span class="fu">rbind.data.frame</span>(pred_df1, pred_df2)</span>
<span id="cb137-7"><a href="prediction.html#cb137-7" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">predict</span>(lpm, <span class="at">newdata=</span>pred_df), <span class="dv">3</span>)</span>
<span id="cb137-8"><a href="prediction.html#cb137-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      1      2 </span></span>
<span id="cb137-9"><a href="prediction.html#cb137-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.951 -0.026</span></span></code></pre></div>
<p>This illustrates that there were very large differences in survival probabilities. It also demonstrates that the linear probability model can deliver non-sensical predictions — we predict the survival probability of a 55 year old, male, third-class passenger to be -3%.</p>
<p><strong>Estimating Probit and Logit Models</strong></p>
<p>Let’s estimate Probit and Logit models using the same specifications. First, Probit:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="prediction.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># probit</span></span>
<span id="cb138-2"><a href="prediction.html#cb138-2" aria-hidden="true" tabindex="-1"></a>probit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age <span class="sc">+</span> <span class="fu">as.factor</span>(Embarked), </span>
<span id="cb138-3"><a href="prediction.html#cb138-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&quot;probit&quot;</span>), </span>
<span id="cb138-4"><a href="prediction.html#cb138-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>titanic_train)</span>
<span id="cb138-5"><a href="prediction.html#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(probit)</span>
<span id="cb138-6"><a href="prediction.html#cb138-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb138-7"><a href="prediction.html#cb138-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb138-8"><a href="prediction.html#cb138-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; glm(formula = Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), </span></span>
<span id="cb138-9"><a href="prediction.html#cb138-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     family = binomial(link = &quot;probit&quot;), data = titanic_train)</span></span>
<span id="cb138-10"><a href="prediction.html#cb138-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb138-11"><a href="prediction.html#cb138-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Deviance Residuals: </span></span>
<span id="cb138-12"><a href="prediction.html#cb138-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span id="cb138-13"><a href="prediction.html#cb138-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2.5145  -0.7433  -0.4397   0.6560   2.3799  </span></span>
<span id="cb138-14"><a href="prediction.html#cb138-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb138-15"><a href="prediction.html#cb138-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb138-16"><a href="prediction.html#cb138-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                        Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb138-17"><a href="prediction.html#cb138-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)            5.418219 146.954198   0.037  0.97059    </span></span>
<span id="cb138-18"><a href="prediction.html#cb138-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2    -0.445235   0.197398  -2.256  0.02410 *  </span></span>
<span id="cb138-19"><a href="prediction.html#cb138-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3    -1.145788   0.188508  -6.078 1.22e-09 ***</span></span>
<span id="cb138-20"><a href="prediction.html#cb138-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Sexmale               -1.464201   0.136512 -10.726  &lt; 2e-16 ***</span></span>
<span id="cb138-21"><a href="prediction.html#cb138-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Age                   -0.015764   0.005009  -3.147  0.00165 ** </span></span>
<span id="cb138-22"><a href="prediction.html#cb138-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)C  -3.443295 146.954199  -0.023  0.98131    </span></span>
<span id="cb138-23"><a href="prediction.html#cb138-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)Q  -3.464431 146.954544  -0.024  0.98119    </span></span>
<span id="cb138-24"><a href="prediction.html#cb138-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)S  -3.662911 146.954180  -0.025  0.98011    </span></span>
<span id="cb138-25"><a href="prediction.html#cb138-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb138-26"><a href="prediction.html#cb138-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb138-27"><a href="prediction.html#cb138-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb138-28"><a href="prediction.html#cb138-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb138-29"><a href="prediction.html#cb138-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb138-30"><a href="prediction.html#cb138-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Null deviance: 678.28  on 499  degrees of freedom</span></span>
<span id="cb138-31"><a href="prediction.html#cb138-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual deviance: 468.38  on 492  degrees of freedom</span></span>
<span id="cb138-32"><a href="prediction.html#cb138-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; AIC: 484.38</span></span>
<span id="cb138-33"><a href="prediction.html#cb138-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb138-34"><a href="prediction.html#cb138-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Number of Fisher Scoring iterations: 12</span></span></code></pre></div>
<p>Now, Logit:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="prediction.html#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># logit</span></span>
<span id="cb139-2"><a href="prediction.html#cb139-2" aria-hidden="true" tabindex="-1"></a>logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age <span class="sc">+</span> <span class="fu">as.factor</span>(Embarked), </span>
<span id="cb139-3"><a href="prediction.html#cb139-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&quot;logit&quot;</span>), </span>
<span id="cb139-4"><a href="prediction.html#cb139-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>titanic_train)</span>
<span id="cb139-5"><a href="prediction.html#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit)</span>
<span id="cb139-6"><a href="prediction.html#cb139-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb139-7"><a href="prediction.html#cb139-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb139-8"><a href="prediction.html#cb139-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; glm(formula = Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), </span></span>
<span id="cb139-9"><a href="prediction.html#cb139-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     family = binomial(link = &quot;logit&quot;), data = titanic_train)</span></span>
<span id="cb139-10"><a href="prediction.html#cb139-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb139-11"><a href="prediction.html#cb139-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Deviance Residuals: </span></span>
<span id="cb139-12"><a href="prediction.html#cb139-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span id="cb139-13"><a href="prediction.html#cb139-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2.4680  -0.7289  -0.4336   0.6471   2.3689  </span></span>
<span id="cb139-14"><a href="prediction.html#cb139-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb139-15"><a href="prediction.html#cb139-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb139-16"><a href="prediction.html#cb139-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                        Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb139-17"><a href="prediction.html#cb139-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)           14.646649 535.411272   0.027  0.97818    </span></span>
<span id="cb139-18"><a href="prediction.html#cb139-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2    -0.793003   0.342058  -2.318  0.02043 *  </span></span>
<span id="cb139-19"><a href="prediction.html#cb139-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3    -2.055828   0.335386  -6.130  8.8e-10 ***</span></span>
<span id="cb139-20"><a href="prediction.html#cb139-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Sexmale               -2.461563   0.241113 -10.209  &lt; 2e-16 ***</span></span>
<span id="cb139-21"><a href="prediction.html#cb139-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Age                   -0.028436   0.008764  -3.245  0.00118 ** </span></span>
<span id="cb139-22"><a href="prediction.html#cb139-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)C -11.262004 535.411277  -0.021  0.98322    </span></span>
<span id="cb139-23"><a href="prediction.html#cb139-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)Q -11.229232 535.411568  -0.021  0.98327    </span></span>
<span id="cb139-24"><a href="prediction.html#cb139-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Embarked)S -11.593053 535.411259  -0.022  0.98273    </span></span>
<span id="cb139-25"><a href="prediction.html#cb139-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb139-26"><a href="prediction.html#cb139-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb139-27"><a href="prediction.html#cb139-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb139-28"><a href="prediction.html#cb139-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb139-29"><a href="prediction.html#cb139-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb139-30"><a href="prediction.html#cb139-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Null deviance: 678.28  on 499  degrees of freedom</span></span>
<span id="cb139-31"><a href="prediction.html#cb139-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual deviance: 467.01  on 492  degrees of freedom</span></span>
<span id="cb139-32"><a href="prediction.html#cb139-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; AIC: 483.01</span></span>
<span id="cb139-33"><a href="prediction.html#cb139-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb139-34"><a href="prediction.html#cb139-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Number of Fisher Scoring iterations: 12</span></span></code></pre></div>
<p>It’s rather hard to interpret the parameters in both of these models (let’s defer this to the next section). But it is worth mentioning that all of the estimated coefficients have the same sign for the linear probability model, the probit model, and the logit model, and the same set of regressors have statistically significant effects across models (and the t-statistics/p-values are very similar across models).</p>
<p>Now, let’s calculate the same predicted probabilities as we did for the linear probability model above:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="prediction.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for probit</span></span>
<span id="cb140-2"><a href="prediction.html#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(probit, <span class="at">newdata=</span>pred_df, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb140-3"><a href="prediction.html#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;          1          2 </span></span>
<span id="cb140-4"><a href="prediction.html#cb140-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.91327666 0.04256272</span></span>
<span id="cb140-5"><a href="prediction.html#cb140-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-6"><a href="prediction.html#cb140-6" aria-hidden="true" tabindex="-1"></a><span class="co"># for logit</span></span>
<span id="cb140-7"><a href="prediction.html#cb140-7" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(logit, <span class="at">newdata=</span>pred_df, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb140-8"><a href="prediction.html#cb140-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;          1          2 </span></span>
<span id="cb140-9"><a href="prediction.html#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.91235117 0.04618584</span></span></code></pre></div>
<p>Before we interpret the result, notice that we add the argument <code>type=response</code> (this indicates that we want to get a predicted probability).</p>
<p>Here (for both models), we estimate that a 25 year old woman traveling first class has a 91% probability of survival (this is slightly smaller than the prediction from the linear probability model). On the other hand, we estimate that a 55 year old man traveling 3rd class has a 4.3% (from probit) or 4.6% (from logit) probability of survival. While these probabilities are still quite low, unlike the estimates from the linear probability model, they are at least positive.</p>
<p><strong>Average Partial Effects</strong></p>
<p>To conclude this section, let’s calculate average partial effects for each model. First, for probit:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="prediction.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mfx)</span>
<span id="cb141-2"><a href="prediction.html#cb141-2" aria-hidden="true" tabindex="-1"></a>probit_ape <span class="ot">&lt;-</span> <span class="fu">probitmfx</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age, </span>
<span id="cb141-3"><a href="prediction.html#cb141-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data=</span>titanic_train, </span>
<span id="cb141-4"><a href="prediction.html#cb141-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">atmean=</span><span class="cn">FALSE</span>)</span>
<span id="cb141-5"><a href="prediction.html#cb141-5" aria-hidden="true" tabindex="-1"></a>probit_ape</span>
<span id="cb141-6"><a href="prediction.html#cb141-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb141-7"><a href="prediction.html#cb141-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; probitmfx(formula = Survived ~ as.factor(Pclass) + Sex + Age, </span></span>
<span id="cb141-8"><a href="prediction.html#cb141-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     data = titanic_train, atmean = FALSE)</span></span>
<span id="cb141-9"><a href="prediction.html#cb141-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb141-10"><a href="prediction.html#cb141-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Marginal Effects:</span></span>
<span id="cb141-11"><a href="prediction.html#cb141-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                         dF/dx  Std. Err.        z     P&gt;|z|    </span></span>
<span id="cb141-12"><a href="prediction.html#cb141-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2 -0.1303971  0.0424994  -3.0682 0.0021534 ** </span></span>
<span id="cb141-13"><a href="prediction.html#cb141-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3 -0.3438262  0.0470404  -7.3092 2.688e-13 ***</span></span>
<span id="cb141-14"><a href="prediction.html#cb141-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Sexmale            -0.4895845  0.0412840 -11.8590 &lt; 2.2e-16 ***</span></span>
<span id="cb141-15"><a href="prediction.html#cb141-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Age                -0.0042614  0.0012934  -3.2948 0.0009851 ***</span></span>
<span id="cb141-16"><a href="prediction.html#cb141-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb141-17"><a href="prediction.html#cb141-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb141-18"><a href="prediction.html#cb141-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb141-19"><a href="prediction.html#cb141-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dF/dx is for discrete change for the following variables:</span></span>
<span id="cb141-20"><a href="prediction.html#cb141-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb141-21"><a href="prediction.html#cb141-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;as.factor(Pclass)2&quot; &quot;as.factor(Pclass)3&quot; &quot;Sexmale&quot;</span></span></code></pre></div>
<p>Now, for logit:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="prediction.html#cb142-1" aria-hidden="true" tabindex="-1"></a>logit_ape <span class="ot">&lt;-</span> <span class="fu">logitmfx</span>(Survived <span class="sc">~</span> <span class="fu">as.factor</span>(Pclass) <span class="sc">+</span> Sex <span class="sc">+</span> Age, </span>
<span id="cb142-2"><a href="prediction.html#cb142-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data=</span>titanic_train, </span>
<span id="cb142-3"><a href="prediction.html#cb142-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">atmean=</span><span class="cn">FALSE</span>)</span>
<span id="cb142-4"><a href="prediction.html#cb142-4" aria-hidden="true" tabindex="-1"></a>logit_ape</span>
<span id="cb142-5"><a href="prediction.html#cb142-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb142-6"><a href="prediction.html#cb142-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; logitmfx(formula = Survived ~ as.factor(Pclass) + Sex + Age, </span></span>
<span id="cb142-7"><a href="prediction.html#cb142-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     data = titanic_train, atmean = FALSE)</span></span>
<span id="cb142-8"><a href="prediction.html#cb142-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-9"><a href="prediction.html#cb142-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Marginal Effects:</span></span>
<span id="cb142-10"><a href="prediction.html#cb142-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                         dF/dx  Std. Err.        z     P&gt;|z|    </span></span>
<span id="cb142-11"><a href="prediction.html#cb142-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)2 -0.1304383  0.0420444  -3.1024  0.001920 ** </span></span>
<span id="cb142-12"><a href="prediction.html#cb142-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(Pclass)3 -0.3542499  0.0474154  -7.4712 7.947e-14 ***</span></span>
<span id="cb142-13"><a href="prediction.html#cb142-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Sexmale            -0.4859890  0.0413167 -11.7625 &lt; 2.2e-16 ***</span></span>
<span id="cb142-14"><a href="prediction.html#cb142-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Age                -0.0044098  0.0014205  -3.1045  0.001906 ** </span></span>
<span id="cb142-15"><a href="prediction.html#cb142-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb142-16"><a href="prediction.html#cb142-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb142-17"><a href="prediction.html#cb142-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-18"><a href="prediction.html#cb142-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dF/dx is for discrete change for the following variables:</span></span>
<span id="cb142-19"><a href="prediction.html#cb142-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb142-20"><a href="prediction.html#cb142-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;as.factor(Pclass)2&quot; &quot;as.factor(Pclass)3&quot; &quot;Sexmale&quot;</span></span></code></pre></div>
<p>A couple of things to notice:</p>
<ul>
<li><p>The average partial effects are extremely similar across models. For example, across all three models, the average partial effect of being <code>male</code> is to reduce the probability of survival by 49% controlling for the other variables in the model. The other average partial effects are quite similar across models as well.</p></li>
<li><p>Both <code>probitmfx</code> and <code>logitmfx</code> functions took in an argument for <code>atmean</code>. We set it equal to <code>FALSE</code>. If you set it equal to <code>TRUE</code>, you will compute a different kind of partial effect. You can check <code>?probitmfx</code> or <code>?logitmfx</code> for more details.</p></li>
</ul>
</div>
</div>
<div id="lab-4-predicting-diamond-prices" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Lab 4: Predicting Diamond Prices</h2>
<p>For this lab, we will try our hand at predicted diamond prices. We will use the data set <code>diamond_train</code> (which contains around 40,000 observations) and then see how well we can predict data from the <code>diamond_test</code> data.</p>
<ol style="list-style-type: decimal">
<li><p>Estimate a model for <span class="math inline">\(price\)</span> on <span class="math inline">\(carat\)</span>, <span class="math inline">\(cut\)</span>, and <span class="math inline">\(clarity\)</span>. Report <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\bar{R}^2\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> for this model.</p></li>
<li><p>Estimate a model for <span class="math inline">\(price\)</span> on <span class="math inline">\(carat\)</span>, <span class="math inline">\(cut\)</span>, <span class="math inline">\(clarity\)</span>, <span class="math inline">\(depth\)</span>, <span class="math inline">\(table\)</span>, <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span>. Report <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\bar{R}^2\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> for this model.</p></li>
<li><p>Choose any model that you would like for <span class="math inline">\(price\)</span> and report <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\bar{R}^2\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> for this model. We’ll see if your model can predict better than either of the first two.</p></li>
<li><p>Use 10-fold cross validation to report an estimate of mean squared prediction error for each of the models from 1-3.</p></li>
<li><p>Based on your responses to parts 1-4, which model do you think will predict the best?</p></li>
<li><p>Use <code>diamond_test</code> to calculate (out-of-sample) mean squared prediction error for each of the three models from 1-3. Which model performs the best out-of-sample? How does this compare to your answer from 5.</p></li>
<li><p>Use the Lasso and Ridge regression on <code>diamond_train</code> data. Evaluate the predictions from each of these models by computing (out-of-sample) mean squared prediction error. How well did these models predict relative to each other and relative the models from 1-3.</p></li>
</ol>
</div>
<div id="lab-4-solutions" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Lab 4: Solutions</h2>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="prediction.html#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/diamond_train.RData&quot;</span>)</span>
<span id="cb143-2"><a href="prediction.html#cb143-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-3"><a href="prediction.html#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="co"># formulas</span></span>
<span id="cb143-4"><a href="prediction.html#cb143-4" aria-hidden="true" tabindex="-1"></a>formla1 <span class="ot">&lt;-</span> price <span class="sc">~</span> carat <span class="sc">+</span> <span class="fu">as.factor</span>(cut) <span class="sc">+</span> <span class="fu">as.factor</span>(clarity)</span>
<span id="cb143-5"><a href="prediction.html#cb143-5" aria-hidden="true" tabindex="-1"></a>formla2 <span class="ot">&lt;-</span> price <span class="sc">~</span> carat <span class="sc">+</span> <span class="fu">as.factor</span>(cut) <span class="sc">+</span> <span class="fu">as.factor</span>(clarity) <span class="sc">+</span> depth <span class="sc">+</span> table <span class="sc">+</span> x <span class="sc">+</span> y <span class="sc">+</span> x</span>
<span id="cb143-6"><a href="prediction.html#cb143-6" aria-hidden="true" tabindex="-1"></a>formla3 <span class="ot">&lt;-</span> price <span class="sc">~</span> (carat <span class="sc">+</span> <span class="fu">as.factor</span>(cut) <span class="sc">+</span> <span class="fu">as.factor</span>(clarity) <span class="sc">+</span> depth <span class="sc">+</span> table <span class="sc">+</span> x <span class="sc">+</span> y <span class="sc">+</span> x)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb143-7"><a href="prediction.html#cb143-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-8"><a href="prediction.html#cb143-8" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate each model</span></span>
<span id="cb143-9"><a href="prediction.html#cb143-9" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla1, <span class="at">data=</span>diamond_train)</span>
<span id="cb143-10"><a href="prediction.html#cb143-10" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla2, <span class="at">data=</span>diamond_train)</span>
<span id="cb143-11"><a href="prediction.html#cb143-11" aria-hidden="true" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla3, <span class="at">data=</span>diamond_train)</span>
<span id="cb143-12"><a href="prediction.html#cb143-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-13"><a href="prediction.html#cb143-13" aria-hidden="true" tabindex="-1"></a>mod_sel <span class="ot">&lt;-</span> <span class="cf">function</span>(formla) {</span>
<span id="cb143-14"><a href="prediction.html#cb143-14" aria-hidden="true" tabindex="-1"></a>  mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla, <span class="at">data=</span>diamond_train)</span>
<span id="cb143-15"><a href="prediction.html#cb143-15" aria-hidden="true" tabindex="-1"></a>  r.squared <span class="ot">&lt;-</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span>
<span id="cb143-16"><a href="prediction.html#cb143-16" aria-hidden="true" tabindex="-1"></a>  adj.r.squared <span class="ot">&lt;-</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>adj.r.squared</span>
<span id="cb143-17"><a href="prediction.html#cb143-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-18"><a href="prediction.html#cb143-18" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(diamond_train)</span>
<span id="cb143-19"><a href="prediction.html#cb143-19" aria-hidden="true" tabindex="-1"></a>  uhat <span class="ot">&lt;-</span> <span class="fu">resid</span>(mod)</span>
<span id="cb143-20"><a href="prediction.html#cb143-20" aria-hidden="true" tabindex="-1"></a>  ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>(uhat<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb143-21"><a href="prediction.html#cb143-21" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">coef</span>(mod))</span>
<span id="cb143-22"><a href="prediction.html#cb143-22" aria-hidden="true" tabindex="-1"></a>  aic <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>k <span class="sc">+</span> n<span class="sc">*</span><span class="fu">log</span>(ssr)</span>
<span id="cb143-23"><a href="prediction.html#cb143-23" aria-hidden="true" tabindex="-1"></a>  bic <span class="ot">&lt;-</span> k<span class="sc">*</span><span class="fu">log</span>(n) <span class="sc">+</span> n<span class="sc">*</span><span class="fu">log</span>(ssr)</span>
<span id="cb143-24"><a href="prediction.html#cb143-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-25"><a href="prediction.html#cb143-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show results</span></span>
<span id="cb143-26"><a href="prediction.html#cb143-26" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">&lt;-</span> tidyr<span class="sc">::</span><span class="fu">tibble</span>(r.squared, adj.r.squared, aic, bic)</span>
<span id="cb143-27"><a href="prediction.html#cb143-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(result)</span>
<span id="cb143-28"><a href="prediction.html#cb143-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb143-29"><a href="prediction.html#cb143-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-30"><a href="prediction.html#cb143-30" aria-hidden="true" tabindex="-1"></a>res1 <span class="ot">&lt;-</span> <span class="fu">mod_sel</span>(formla1)</span>
<span id="cb143-31"><a href="prediction.html#cb143-31" aria-hidden="true" tabindex="-1"></a>res2 <span class="ot">&lt;-</span> <span class="fu">mod_sel</span>(formla2)</span>
<span id="cb143-32"><a href="prediction.html#cb143-32" aria-hidden="true" tabindex="-1"></a>res3 <span class="ot">&lt;-</span> <span class="fu">mod_sel</span>(formla3)</span>
<span id="cb143-33"><a href="prediction.html#cb143-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-34"><a href="prediction.html#cb143-34" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rbind.data.frame</span>(res1, res2, res3),<span class="dv">3</span>)</span>
<span id="cb143-35"><a href="prediction.html#cb143-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; # A tibble: 3 × 4</span></span>
<span id="cb143-36"><a href="prediction.html#cb143-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   r.squared adj.r.squared      aic      bic</span></span>
<span id="cb143-37"><a href="prediction.html#cb143-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb143-38"><a href="prediction.html#cb143-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1     0.898         0.898 1104188. 1104301.</span></span>
<span id="cb143-39"><a href="prediction.html#cb143-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2     0.9           0.9   1103499. 1103647.</span></span>
<span id="cb143-40"><a href="prediction.html#cb143-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3     0.928         0.928 1089372. 1090328.</span></span>
<span id="cb143-41"><a href="prediction.html#cb143-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-42"><a href="prediction.html#cb143-42" aria-hidden="true" tabindex="-1"></a><span class="co"># k-fold cross validation</span></span>
<span id="cb143-43"><a href="prediction.html#cb143-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-44"><a href="prediction.html#cb143-44" aria-hidden="true" tabindex="-1"></a><span class="co"># setup data</span></span>
<span id="cb143-45"><a href="prediction.html#cb143-45" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb143-46"><a href="prediction.html#cb143-46" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(diamond_train)</span>
<span id="cb143-47"><a href="prediction.html#cb143-47" aria-hidden="true" tabindex="-1"></a>diamond_train<span class="sc">$</span>fold <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="at">size=</span>n, <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb143-48"><a href="prediction.html#cb143-48" aria-hidden="true" tabindex="-1"></a>diamond_train<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n</span>
<span id="cb143-49"><a href="prediction.html#cb143-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-50"><a href="prediction.html#cb143-50" aria-hidden="true" tabindex="-1"></a>cv_mod_sel <span class="ot">&lt;-</span> <span class="cf">function</span>(formla) {</span>
<span id="cb143-51"><a href="prediction.html#cb143-51" aria-hidden="true" tabindex="-1"></a>  u.squared <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, n)</span>
<span id="cb143-52"><a href="prediction.html#cb143-52" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb143-53"><a href="prediction.html#cb143-53" aria-hidden="true" tabindex="-1"></a>    this.train <span class="ot">&lt;-</span> <span class="fu">subset</span>(diamond_train, fold <span class="sc">!=</span> i)</span>
<span id="cb143-54"><a href="prediction.html#cb143-54" aria-hidden="true" tabindex="-1"></a>    this.test <span class="ot">&lt;-</span> <span class="fu">subset</span>(diamond_train, fold <span class="sc">==</span> i)</span>
<span id="cb143-55"><a href="prediction.html#cb143-55" aria-hidden="true" tabindex="-1"></a>    this.id <span class="ot">&lt;-</span> this.test<span class="sc">$</span>id</span>
<span id="cb143-56"><a href="prediction.html#cb143-56" aria-hidden="true" tabindex="-1"></a>    cv_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(formla,</span>
<span id="cb143-57"><a href="prediction.html#cb143-57" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>this.train)</span>
<span id="cb143-58"><a href="prediction.html#cb143-58" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv_reg, <span class="at">newdata=</span>this.test)</span>
<span id="cb143-59"><a href="prediction.html#cb143-59" aria-hidden="true" tabindex="-1"></a>    u <span class="ot">&lt;-</span> this.test<span class="sc">$</span>price <span class="sc">-</span> pred</span>
<span id="cb143-60"><a href="prediction.html#cb143-60" aria-hidden="true" tabindex="-1"></a>    u.squared[this.id] <span class="ot">&lt;-</span> u<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb143-61"><a href="prediction.html#cb143-61" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb143-62"><a href="prediction.html#cb143-62" aria-hidden="true" tabindex="-1"></a>  cv <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u.squared))</span>
<span id="cb143-63"><a href="prediction.html#cb143-63" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(cv)</span>
<span id="cb143-64"><a href="prediction.html#cb143-64" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb143-65"><a href="prediction.html#cb143-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-66"><a href="prediction.html#cb143-66" aria-hidden="true" tabindex="-1"></a>cv_res1 <span class="ot">&lt;-</span> <span class="fu">cv_mod_sel</span>(formla1)</span>
<span id="cb143-67"><a href="prediction.html#cb143-67" aria-hidden="true" tabindex="-1"></a>cv_res2 <span class="ot">&lt;-</span> <span class="fu">cv_mod_sel</span>(formla2)</span>
<span id="cb143-68"><a href="prediction.html#cb143-68" aria-hidden="true" tabindex="-1"></a>cv_res3 <span class="ot">&lt;-</span> <span class="fu">cv_mod_sel</span>(formla3)</span>
<span id="cb143-69"><a href="prediction.html#cb143-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-70"><a href="prediction.html#cb143-70" aria-hidden="true" tabindex="-1"></a>res1 <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(res1, <span class="at">cv=</span>cv_res1)</span>
<span id="cb143-71"><a href="prediction.html#cb143-71" aria-hidden="true" tabindex="-1"></a>res2 <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(res2, <span class="at">cv=</span>cv_res2)</span>
<span id="cb143-72"><a href="prediction.html#cb143-72" aria-hidden="true" tabindex="-1"></a>res3 <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(res3, <span class="at">cv=</span>cv_res3)</span>
<span id="cb143-73"><a href="prediction.html#cb143-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-74"><a href="prediction.html#cb143-74" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rbind.data.frame</span>(res1, res2, res3),<span class="dv">3</span>)</span>
<span id="cb143-75"><a href="prediction.html#cb143-75" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   r.squared adj.r.squared     aic     bic       cv</span></span>
<span id="cb143-76"><a href="prediction.html#cb143-76" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1     0.898         0.898 1104188 1104301 1366.021</span></span>
<span id="cb143-77"><a href="prediction.html#cb143-77" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2     0.900         0.900 1103499 1103647 1397.104</span></span>
<span id="cb143-78"><a href="prediction.html#cb143-78" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3     0.928         0.928 1089372 1090328 2247.940</span></span>
<span id="cb143-79"><a href="prediction.html#cb143-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-80"><a href="prediction.html#cb143-80" aria-hidden="true" tabindex="-1"></a><span class="co"># lasso and ridge</span></span>
<span id="cb143-81"><a href="prediction.html#cb143-81" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb143-82"><a href="prediction.html#cb143-82" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnetUtils)</span>
<span id="cb143-83"><a href="prediction.html#cb143-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-84"><a href="prediction.html#cb143-84" aria-hidden="true" tabindex="-1"></a>lasso_res <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(formla3, <span class="at">data=</span>diamond_train, <span class="at">alpha=</span><span class="dv">1</span>)</span>
<span id="cb143-85"><a href="prediction.html#cb143-85" aria-hidden="true" tabindex="-1"></a>ridge_res <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(formla3, <span class="at">data=</span>diamond_train, <span class="at">alpha=</span><span class="dv">0</span>)</span>
<span id="cb143-86"><a href="prediction.html#cb143-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-87"><a href="prediction.html#cb143-87" aria-hidden="true" tabindex="-1"></a><span class="co"># out of sample predictions</span></span>
<span id="cb143-88"><a href="prediction.html#cb143-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-89"><a href="prediction.html#cb143-89" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/diamond_test.RData&quot;</span>)</span>
<span id="cb143-90"><a href="prediction.html#cb143-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-91"><a href="prediction.html#cb143-91" aria-hidden="true" tabindex="-1"></a><span class="co"># compute prediction errors with test data</span></span>
<span id="cb143-92"><a href="prediction.html#cb143-92" aria-hidden="true" tabindex="-1"></a>u1 <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(mod1, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb143-93"><a href="prediction.html#cb143-93" aria-hidden="true" tabindex="-1"></a>u2 <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb143-94"><a href="prediction.html#cb143-94" aria-hidden="true" tabindex="-1"></a>u3 <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(mod3, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb143-95"><a href="prediction.html#cb143-95" aria-hidden="true" tabindex="-1"></a>u_lasso <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(lasso_res, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb143-96"><a href="prediction.html#cb143-96" aria-hidden="true" tabindex="-1"></a>u_ridge <span class="ot">&lt;-</span> diamond_test<span class="sc">$</span>price <span class="sc">-</span> <span class="fu">predict</span>(ridge_res, <span class="at">newdata=</span>diamond_test)</span>
<span id="cb143-97"><a href="prediction.html#cb143-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-98"><a href="prediction.html#cb143-98" aria-hidden="true" tabindex="-1"></a><span class="co"># compute root mean squared prediction errors</span></span>
<span id="cb143-99"><a href="prediction.html#cb143-99" aria-hidden="true" tabindex="-1"></a>rmspe1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb143-100"><a href="prediction.html#cb143-100" aria-hidden="true" tabindex="-1"></a>rmspe2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u2<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb143-101"><a href="prediction.html#cb143-101" aria-hidden="true" tabindex="-1"></a>rmspe3 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u3<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb143-102"><a href="prediction.html#cb143-102" aria-hidden="true" tabindex="-1"></a>rmspe_lasso <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u_lasso<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb143-103"><a href="prediction.html#cb143-103" aria-hidden="true" tabindex="-1"></a>rmspe_ridge <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(u_ridge<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb143-104"><a href="prediction.html#cb143-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-105"><a href="prediction.html#cb143-105" aria-hidden="true" tabindex="-1"></a><span class="co"># report results</span></span>
<span id="cb143-106"><a href="prediction.html#cb143-106" aria-hidden="true" tabindex="-1"></a>rmspe <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">mod1=</span>rmspe1, <span class="at">mod2=</span>rmspe2, <span class="at">mod3=</span>rmspe3, <span class="at">lasso=</span>rmspe_lasso, <span class="at">ridge=</span>rmspe_ridge)</span>
<span id="cb143-107"><a href="prediction.html#cb143-107" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rmspe,<span class="dv">3</span>)</span>
<span id="cb143-108"><a href="prediction.html#cb143-108" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      mod1    mod2    mod3  lasso   ridge</span></span>
<span id="cb143-109"><a href="prediction.html#cb143-109" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 856.014 785.171 616.996 683.36 794.276</span></span></code></pre></div>
</div>
<div id="coding-questions-2" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Coding Questions</h2>
<ol style="list-style-type: decimal">
<li><p>For this problem, we will use the data <code>mroz</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimate a probit model where the outcome is whether or not the wife is in the labor force (<code>inlf</code>) using the the number of kids less than 6 (<code>kidslt6</code>) and the number of kids who are 6 or older living in the household (<code>kidsge6</code>). Calculate the average partial effect of each variable. What do you notice?</p></li>
<li><p>Now, add the variables <code>age</code>, <code>educ</code>, and <code>city</code> to the model. Calculate the average partial effects of <code>kidslt6</code> and <code>kidsge6</code>. How do you interpret these? How do they compare to the answers from part a?</p></li>
<li><p>Estimate a linear probability model and logit model using the same specification as in part b. For each one, how do the estimated coefficients compare to the ones from part b? Compute average partial effects for each model. How do these compare to the ones from part b?</p></li>
</ol></li>
<li><p>For this problem, we will use the <code>Fair</code> data.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The variable <code>nbaffairs</code> contains the number of self-reported affairs that an individual has had in the previous year. Create a variable <code>had_affair</code> that is equal to 1 if an individual had any affair in the past year and that is equal to 0 otherwise. What fraction of individuals in the data have had an affair in the past year?</p></li>
<li><p>Estimate a logit model where the outcome is <code>had_affair</code> and the regressor is whether or not the person has a child (<code>child</code>). Calculate the average partial effect of having a child on having an affair. How do you interpret the results?</p></li>
<li><p>Now add <code>sex</code>, <code>age</code>, <code>education</code>, and <code>occupation</code> to the model. Calculate the average partial effect of each variable. How do you interpret the results? <strong>Hint:</strong> Make sure to treat the categorical variables in the model as categorical rather than as numeric.</p></li>
<li><p>In addition to the variables in part c, add the variables years married (<code>ym</code>) and <code>religious</code> to the mode. Calculate the average partial effect of each variable. How do you interpret the results? <strong>Hint:</strong> Make sure to treat the categorical variables in the model as categorical rather than as numeric.</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-2" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Extra Questions</h2>
<ol style="list-style-type: decimal">
<li><p>What are some drawbacks of using <span class="math inline">\(R^2\)</span> as a model selection tool?</p></li>
<li><p>For AIC and BIC, we choose the model that minimizes these (rather than maximizes them). Why?</p></li>
<li><p>Does AIC or BIC tend to pick “more complicated” models? What is the reason for this?</p></li>
<li><p>Suppose you are interested in predicting some outcome <span class="math inline">\(Y\)</span> and have access to covariates <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>. You estimate the following two models
<span class="math display">\[\begin{align*}
  Y &amp;= 30 + 4 X_1 - 2 X_2 - 10 X_3, \qquad R^2=0.5, AIC=3421 \\
  Y &amp;= 9 + 2 X_1 - 3 X_2 - 2 X_3 + 2 X_1^2 + 1 X_2^2 - 4 X_3^2 + 2 X_1 X_2, \qquad R^2 = 0.75, AIC=4018
\end{align*}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Which model seems to be predicting better? Explain why.</p></li>
<li><p>Using the model that is predicting better, what would be your prediction for <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1=10, X_2=1, X_3=5\)</span>?</p></li>
</ol></li>
<li><p>In Lasso and Ridge regressions, it is common to “standardize” the regressors before estimating the model (e.g., the <code>glmnet</code> does this automatically for you). What is the reason for doing this?</p></li>
<li><p>In Lasso and Ridge regressions, the penalty term lead to “shrinking” the estimated parameters in the model towards 0. This tends to introduce bias while reducing variance. Why can introducing bias while reducing variance potentially lead to better predictions? Does this argument always apply or just apply in some cases? Explain.</p></li>
<li><p>In Lasso and Ridge regressions, the penalty term depends on the tuning parameter <span class="math inline">\(\lambda\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>How is this tuning parameter often chosen in practice? Why does it make sense to choose it in this way?</p></li>
<li><p>What would happen to the estimated coefficients when <span class="math inline">\(\lambda=0\)</span>?</p></li>
<li><p>What would happen to the estimated coefficients as <span class="math inline">\(\lambda \rightarrow \infty\)</span>?</p></li>
</ol></li>
<li><p>Suppose you try to estimate a linear probability model, probit model, and logit model using the same specifications. You notice that the estimated coefficients are substantially different from each other. Does this mean that something has gone wrong?</p></li>
</ol>
</div>
<div id="answers-to-some-extra-questions-1" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Answers to Some Extra Questions</h2>
<p><strong>Answer to Question 3</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>The first model appears to be predicting better because the AIC is lower. [Also, notice that <span class="math inline">\(R^2\)</span> is higher in the second model, but this is by construction because it includes extra terms relative to the first model which implies that it will fit at least as well in-sample as the first model, but may be suffering from over-fitting.]</p></li>
<li><p><span class="math display">\[\begin{align*}
   \hat{Y} &amp;= 30 + 4 (10) - 2 (1) - 10 (5) \\
   &amp;= 18
 \end{align*}\]</span></p></li>
</ol>
<p><strong>Answer to Question 6</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>The tuning parameter is often chosen via cross validation. It makes sense to choose it this way because this is effectively choosing a value of <span class="math inline">\(\lambda\)</span> that is making good pseudo-out-of-sample predictions. As we will see below, if you make bad choices of <span class="math inline">\(\lambda\)</span>, that could result in very poor predictions.</p></li>
<li><p>When <span class="math inline">\(\lambda=0\)</span>, there would effectively be no penalty term and, therefore, the estimated parameters would coincide with the OLS estimates.</p></li>
<li><p>When <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the penalty term would overwhelm the term corresponding to minimizing SSR. This would result in setting all the estimated parameters to be equal to 0. This extreme approach is likely to lead to very poor predictions.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="causal-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Detailed Course Notes.pdf", "Detailed Course Notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
