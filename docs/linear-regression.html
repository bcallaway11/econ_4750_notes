<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 4 Linear Regression | Supplementary Notes and References for ECON 4750</title>
  <meta name="description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 4 Linear Regression | Supplementary Notes and References for ECON 4750" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 4 Linear Regression | Supplementary Notes and References for ECON 4750" />
  
  <meta name="twitter:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

<meta name="author" content="Brantly Callaway" />


<meta name="date" content="2021-12-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability-and-statistics.html"/>
<link rel="next" href="prediction.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Outline/Notes for ECON 4750</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-is-this"><i class="fa fa-check"></i><b>1.1</b> What is this?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#what-is-this-not"><i class="fa fa-check"></i><b>1.2</b> What is this not?</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#why-did-i-write-this"><i class="fa fa-check"></i><b>1.3</b> Why did I write this?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#additional-references"><i class="fa fa-check"></i><b>1.4</b> Additional References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#goals-for-the-course"><i class="fa fa-check"></i><b>1.5</b> Goals for the Course</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#studying-for-the-class"><i class="fa fa-check"></i><b>1.6</b> Studying for the Class</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#data-used-in-the-course"><i class="fa fa-check"></i><b>1.7</b> Data Used in the Course</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#first-week-of-class"><i class="fa fa-check"></i><b>1.8</b> First Week of Class</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-programming.html"><a href="statistical-programming.html"><i class="fa fa-check"></i><b>2</b> Statistical Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up R</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-programming.html"><a href="statistical-programming.html#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-programming.html"><a href="statistical-programming.html#downloading-r"><i class="fa fa-check"></i><b>2.1.2</b> Downloading R</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio"><i class="fa fa-check"></i><b>2.1.3</b> RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio-development-environment"><i class="fa fa-check"></i><b>2.1.4</b> RStudio Development Environment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-programming.html"><a href="statistical-programming.html#installing-r-packages"><i class="fa fa-check"></i><b>2.2</b> Installing R Packages</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#a-list-of-useful-r-packages"><i class="fa fa-check"></i><b>2.2.1</b> A list of useful R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-programming.html"><a href="statistical-programming.html#r-basics"><i class="fa fa-check"></i><b>2.3</b> R Basics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-programming.html"><a href="statistical-programming.html#objects"><i class="fa fa-check"></i><b>2.3.1</b> Objects</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-programming.html"><a href="statistical-programming.html#workspace"><i class="fa fa-check"></i><b>2.3.2</b> Workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-programming.html"><a href="statistical-programming.html#functions-in-r"><i class="fa fa-check"></i><b>2.4</b> Functions in R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-types"><i class="fa fa-check"></i><b>2.5</b> Data types</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-programming.html"><a href="statistical-programming.html#numeric-vectors"><i class="fa fa-check"></i><b>2.5.1</b> Numeric Vectors</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-programming.html"><a href="statistical-programming.html#vector-arithmetic"><i class="fa fa-check"></i><b>2.5.2</b> Vector arithmetic</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-programming.html"><a href="statistical-programming.html#more-helpful-functions-in-r"><i class="fa fa-check"></i><b>2.5.3</b> More helpful functions in R</a></li>
<li class="chapter" data-level="2.5.4" data-path="statistical-programming.html"><a href="statistical-programming.html#other-types-of-vectors"><i class="fa fa-check"></i><b>2.5.4</b> Other types of vectors</a></li>
<li class="chapter" data-level="2.5.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-frames"><i class="fa fa-check"></i><b>2.5.5</b> Data Frames</a></li>
<li class="chapter" data-level="2.5.6" data-path="statistical-programming.html"><a href="statistical-programming.html#lists"><i class="fa fa-check"></i><b>2.5.6</b> Lists</a></li>
<li class="chapter" data-level="2.5.7" data-path="statistical-programming.html"><a href="statistical-programming.html#matrices"><i class="fa fa-check"></i><b>2.5.7</b> Matrices</a></li>
<li class="chapter" data-level="2.5.8" data-path="statistical-programming.html"><a href="statistical-programming.html#factors"><i class="fa fa-check"></i><b>2.5.8</b> Factors</a></li>
<li class="chapter" data-level="2.5.9" data-path="statistical-programming.html"><a href="statistical-programming.html#understanding-an-object-in-r"><i class="fa fa-check"></i><b>2.5.9</b> Understanding an object in R</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-programming.html"><a href="statistical-programming.html#logicals"><i class="fa fa-check"></i><b>2.6</b> Logicals</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-programming.html"><a href="statistical-programming.html#additional-logical-operators"><i class="fa fa-check"></i><b>2.6.1</b> Additional Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="statistical-programming.html"><a href="statistical-programming.html#programming-basics"><i class="fa fa-check"></i><b>2.7</b> Programming basics</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="statistical-programming.html"><a href="statistical-programming.html#writing-functions"><i class="fa fa-check"></i><b>2.7.1</b> Writing functions</a></li>
<li class="chapter" data-level="2.7.2" data-path="statistical-programming.html"><a href="statistical-programming.html#ifelse"><i class="fa fa-check"></i><b>2.7.2</b> if/else</a></li>
<li class="chapter" data-level="2.7.3" data-path="statistical-programming.html"><a href="statistical-programming.html#for-loops"><i class="fa fa-check"></i><b>2.7.3</b> for loops</a></li>
<li class="chapter" data-level="2.7.4" data-path="statistical-programming.html"><a href="statistical-programming.html#vectorization"><i class="fa fa-check"></i><b>2.7.4</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="statistical-programming.html"><a href="statistical-programming.html#advanced-topics"><i class="fa fa-check"></i><b>2.8</b> Advanced Topics</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="statistical-programming.html"><a href="statistical-programming.html#tidyverse"><i class="fa fa-check"></i><b>2.8.1</b> Tidyverse</a></li>
<li class="chapter" data-level="2.8.2" data-path="statistical-programming.html"><a href="statistical-programming.html#data-visualization"><i class="fa fa-check"></i><b>2.8.2</b> Data Visualization</a></li>
<li class="chapter" data-level="2.8.3" data-path="statistical-programming.html"><a href="statistical-programming.html#reproducible-research"><i class="fa fa-check"></i><b>2.8.3</b> Reproducible Research</a></li>
<li class="chapter" data-level="2.8.4" data-path="statistical-programming.html"><a href="statistical-programming.html#technical-writing-tools"><i class="fa fa-check"></i><b>2.8.4</b> Technical Writing Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-introduction-to-r-programming"><i class="fa fa-check"></i><b>2.9</b> Lab 1: Introduction to R Programming</a></li>
<li class="chapter" data-level="2.10" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-solutions"><i class="fa fa-check"></i><b>2.10</b> Lab 1: Solutions</a></li>
<li class="chapter" data-level="2.11" data-path="statistical-programming.html"><a href="statistical-programming.html#coding-exercises"><i class="fa fa-check"></i><b>2.11</b> Coding Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html"><i class="fa fa-check"></i><b>3</b> Probability and Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-probability"><i class="fa fa-check"></i><b>3.1</b> Topics in Probability</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#data-for-this-chapter"><i class="fa fa-check"></i><b>3.1.1</b> Data for this chapter</a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#random-variables"><i class="fa fa-check"></i><b>3.1.2</b> Random Variables</a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#pdfs-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.3</b> pdfs, pmfs, and cdfs</a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#summation-operator"><i class="fa fa-check"></i><b>3.1.4</b> Summation operator</a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.5</b> Properties of pmfs and cdfs</a></li>
<li class="chapter" data-level="3.1.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.1.6</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="3.1.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#expected-values"><i class="fa fa-check"></i><b>3.1.7</b> Expected Values</a></li>
<li class="chapter" data-level="3.1.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance"><i class="fa fa-check"></i><b>3.1.8</b> Variance</a></li>
<li class="chapter" data-level="3.1.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-and-variance-of-linear-functions"><i class="fa fa-check"></i><b>3.1.9</b> Mean and Variance of Linear Functions</a></li>
<li class="chapter" data-level="3.1.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.1.10</b> Multiple Random Variables</a></li>
<li class="chapter" data-level="3.1.11" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#conditional-expectations"><i class="fa fa-check"></i><b>3.1.11</b> Conditional Expectations</a></li>
<li class="chapter" data-level="3.1.12" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>3.1.12</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="3.1.13" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#covariance"><i class="fa fa-check"></i><b>3.1.13</b> Covariance</a></li>
<li class="chapter" data-level="3.1.14" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#correlation"><i class="fa fa-check"></i><b>3.1.14</b> Correlation</a></li>
<li class="chapter" data-level="3.1.15" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-expectationsvariances-of-sums-of-rvs"><i class="fa fa-check"></i><b>3.1.15</b> Properties of Expectations/Variances of Sums of RVs</a></li>
<li class="chapter" data-level="3.1.16" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#normal-distribution"><i class="fa fa-check"></i><b>3.1.16</b> Normal Distribution</a></li>
<li class="chapter" data-level="3.1.17" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding"><i class="fa fa-check"></i><b>3.1.17</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-statistics"><i class="fa fa-check"></i><b>3.2</b> Topics in Statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#simple-random-sample"><i class="fa fa-check"></i><b>3.2.1</b> Simple Random Sample</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#estimating-mathbbey"><i class="fa fa-check"></i><b>3.2.2</b> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-of-bary"><i class="fa fa-check"></i><b>3.2.3</b> Mean of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance-of-bary"><i class="fa fa-check"></i><b>3.2.4</b> Variance of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-estimators"><i class="fa fa-check"></i><b>3.2.5</b> Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#relative-efficiency"><i class="fa fa-check"></i><b>3.2.6</b> Relative Efficiency</a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-squared-error"><i class="fa fa-check"></i><b>3.2.7</b> Mean Squared Error</a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#large-sample-properties-of-estimators"><i class="fa fa-check"></i><b>3.2.8</b> Large Sample Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#inference-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.9</b> Inference / Hypothesis Testing</a></li>
<li class="chapter" data-level="3.2.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-1"><i class="fa fa-check"></i><b>3.2.10</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-monte-carlo-simulations"><i class="fa fa-check"></i><b>3.3</b> Lab 2: Monte Carlo Simulations</a></li>
<li class="chapter" data-level="3.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-solutions"><i class="fa fa-check"></i><b>3.4</b> Lab 2 Solutions</a></li>
<li class="chapter" data-level="3.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-questions"><i class="fa fa-check"></i><b>3.5</b> Coding Questions</a></li>
<li class="chapter" data-level="3.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#extra-questions"><i class="fa fa-check"></i><b>3.6</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#nonparametric-regression-curse-of-dimensionality"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Regression / Curse of Dimensionality</a></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>4.2</b> Linear Regression Models</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computation"><i class="fa fa-check"></i><b>4.3</b> Computation</a></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#partial-effects"><i class="fa fa-check"></i><b>4.4</b> Partial Effects</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-1"><i class="fa fa-check"></i><b>4.4.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-regression.html"><a href="linear-regression.html#binary-regressors"><i class="fa fa-check"></i><b>4.5</b> Binary Regressors</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-2"><i class="fa fa-check"></i><b>4.5.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-regression.html"><a href="linear-regression.html#nonlinear-regression-functions"><i class="fa fa-check"></i><b>4.6</b> Nonlinear Regression Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-3"><i class="fa fa-check"></i><b>4.6.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-interaction-terms"><i class="fa fa-check"></i><b>4.7</b> Interpreting Interaction Terms</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-4"><i class="fa fa-check"></i><b>4.7.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="linear-regression.html"><a href="linear-regression.html#elasticities"><i class="fa fa-check"></i><b>4.8</b> Elasticities</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-5"><i class="fa fa-check"></i><b>4.8.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="linear-regression.html"><a href="linear-regression.html#omitted-variable-bias"><i class="fa fa-check"></i><b>4.9</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="4.10" data-path="linear-regression.html"><a href="linear-regression.html#how-to-estimate-the-parameters-in-a-regression-model"><i class="fa fa-check"></i><b>4.10</b> How to estimate the parameters in a regression model</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-6"><i class="fa fa-check"></i><b>4.10.1</b> Computation</a></li>
<li class="chapter" data-level="4.10.2" data-path="linear-regression.html"><a href="linear-regression.html#more-than-one-regressor"><i class="fa fa-check"></i><b>4.10.2</b> More than one regressor</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i><b>4.11</b> Inference</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-7"><i class="fa fa-check"></i><b>4.11.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-birthweight-and-smoking"><i class="fa fa-check"></i><b>4.12</b> Lab 3: Birthweight and Smoking</a></li>
<li class="chapter" data-level="4.13" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-solutions"><i class="fa fa-check"></i><b>4.13</b> Lab 3: Solutions</a></li>
<li class="chapter" data-level="4.14" data-path="linear-regression.html"><a href="linear-regression.html#coding-questions-1"><i class="fa fa-check"></i><b>4.14</b> Coding Questions</a></li>
<li class="chapter" data-level="4.15" data-path="linear-regression.html"><a href="linear-regression.html#extra-questions-1"><i class="fa fa-check"></i><b>4.15</b> Extra Questions</a></li>
<li class="chapter" data-level="4.16" data-path="linear-regression.html"><a href="linear-regression.html#answers-to-some-extra-questions"><i class="fa fa-check"></i><b>4.16</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>5</b> Prediction</a>
<ul>
<li class="chapter" data-level="5.1" data-path="prediction.html"><a href="prediction.html#measures-of-regression-fit"><i class="fa fa-check"></i><b>5.1</b> Measures of Regression Fit</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="prediction.html"><a href="prediction.html#tss-ess-ssr"><i class="fa fa-check"></i><b>5.1.1</b> TSS, ESS, SSR</a></li>
<li class="chapter" data-level="5.1.2" data-path="prediction.html"><a href="prediction.html#r2"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="prediction.html"><a href="prediction.html#model-selection"><i class="fa fa-check"></i><b>5.2</b> Model Selection</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="prediction.html"><a href="prediction.html#limitations-of-r2"><i class="fa fa-check"></i><b>5.2.1</b> Limitations of <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.2" data-path="prediction.html"><a href="prediction.html#adjusted-r2"><i class="fa fa-check"></i><b>5.2.2</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.3" data-path="prediction.html"><a href="prediction.html#aic-bic"><i class="fa fa-check"></i><b>5.2.3</b> AIC, BIC</a></li>
<li class="chapter" data-level="5.2.4" data-path="prediction.html"><a href="prediction.html#cross-validation"><i class="fa fa-check"></i><b>5.2.4</b> Cross-Validation</a></li>
<li class="chapter" data-level="5.2.5" data-path="prediction.html"><a href="prediction.html#model-averaging"><i class="fa fa-check"></i><b>5.2.5</b> Model Averaging</a></li>
<li class="chapter" data-level="5.2.6" data-path="prediction.html"><a href="prediction.html#computation-8"><i class="fa fa-check"></i><b>5.2.6</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prediction.html"><a href="prediction.html#machine-learning"><i class="fa fa-check"></i><b>5.3</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="prediction.html"><a href="prediction.html#lasso"><i class="fa fa-check"></i><b>5.3.1</b> Lasso</a></li>
<li class="chapter" data-level="5.3.2" data-path="prediction.html"><a href="prediction.html#ridge-regression"><i class="fa fa-check"></i><b>5.3.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.3.3" data-path="prediction.html"><a href="prediction.html#computation-9"><i class="fa fa-check"></i><b>5.3.3</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="prediction.html"><a href="prediction.html#binary-outcome-models"><i class="fa fa-check"></i><b>5.4</b> Binary Outcome Models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="prediction.html"><a href="prediction.html#linear-probability-model"><i class="fa fa-check"></i><b>5.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="5.4.2" data-path="prediction.html"><a href="prediction.html#probit-and-logit"><i class="fa fa-check"></i><b>5.4.2</b> Probit and Logit</a></li>
<li class="chapter" data-level="5.4.3" data-path="prediction.html"><a href="prediction.html#average-partial-effects"><i class="fa fa-check"></i><b>5.4.3</b> Average Partial Effects</a></li>
<li class="chapter" data-level="5.4.4" data-path="prediction.html"><a href="prediction.html#computation-10"><i class="fa fa-check"></i><b>5.4.4</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prediction.html"><a href="prediction.html#lab-4-predicting-diamond-prices"><i class="fa fa-check"></i><b>5.5</b> Lab 4: Predicting Diamond Prices</a></li>
<li class="chapter" data-level="5.6" data-path="prediction.html"><a href="prediction.html#lab-4-solutions"><i class="fa fa-check"></i><b>5.6</b> Lab 4: Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="prediction.html"><a href="prediction.html#coding-questions-2"><i class="fa fa-check"></i><b>5.7</b> Coding Questions</a></li>
<li class="chapter" data-level="5.8" data-path="prediction.html"><a href="prediction.html#extra-questions-2"><i class="fa fa-check"></i><b>5.8</b> Extra Questions</a></li>
<li class="chapter" data-level="5.9" data-path="prediction.html"><a href="prediction.html#answers-to-some-extra-questions-1"><i class="fa fa-check"></i><b>5.9</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>6</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="causal-inference.html"><a href="causal-inference.html#potential-outcomes"><i class="fa fa-check"></i><b>6.1</b> Potential Outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="causal-inference.html"><a href="causal-inference.html#parameters-of-interest"><i class="fa fa-check"></i><b>6.2</b> Parameters of Interest</a></li>
<li class="chapter" data-level="6.3" data-path="causal-inference.html"><a href="causal-inference.html#experiments"><i class="fa fa-check"></i><b>6.3</b> Experiments</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="causal-inference.html"><a href="causal-inference.html#estimating-att-with-a-regression"><i class="fa fa-check"></i><b>6.3.1</b> Estimating ATT with a Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="causal-inference.html"><a href="causal-inference.html#internal-and-external-validity"><i class="fa fa-check"></i><b>6.3.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="6.3.3" data-path="causal-inference.html"><a href="causal-inference.html#example-project-star"><i class="fa fa-check"></i><b>6.3.3</b> Example: Project STAR</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="causal-inference.html"><a href="causal-inference.html#unconfoundedness"><i class="fa fa-check"></i><b>6.4</b> Unconfoundedness</a></li>
<li class="chapter" data-level="6.5" data-path="causal-inference.html"><a href="causal-inference.html#panel-data-approaches"><i class="fa fa-check"></i><b>6.5</b> Panel Data Approaches</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="causal-inference.html"><a href="causal-inference.html#difference-in-differences"><i class="fa fa-check"></i><b>6.5.1</b> Difference in differences</a></li>
<li class="chapter" data-level="6.5.2" data-path="causal-inference.html"><a href="causal-inference.html#computation-11"><i class="fa fa-check"></i><b>6.5.2</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="causal-inference.html"><a href="causal-inference.html#instrumental-variables"><i class="fa fa-check"></i><b>6.6</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="causal-inference.html"><a href="causal-inference.html#example-return-to-education"><i class="fa fa-check"></i><b>6.6.1</b> Example: Return to Education</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="causal-inference.html"><a href="causal-inference.html#regression-discontinuity"><i class="fa fa-check"></i><b>6.7</b> Regression Discontinuity</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="causal-inference.html"><a href="causal-inference.html#example-causal-effect-of-alcohol-on-driving-deaths"><i class="fa fa-check"></i><b>6.7.1</b> Example: Causal effect of Alcohol on Driving Deaths</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-drunk-driving-laws"><i class="fa fa-check"></i><b>6.8</b> Lab 5: Drunk Driving Laws</a></li>
<li class="chapter" data-level="6.9" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-solutions"><i class="fa fa-check"></i><b>6.9</b> Lab 5: Solutions</a></li>
<li class="chapter" data-level="6.10" data-path="causal-inference.html"><a href="causal-inference.html#coding-questions-3"><i class="fa fa-check"></i><b>6.10</b> Coding Questions</a></li>
<li class="chapter" data-level="6.11" data-path="causal-inference.html"><a href="causal-inference.html#extra-questions-3"><i class="fa fa-check"></i><b>6.11</b> Extra Questions</a></li>
<li class="chapter" data-level="6.12" data-path="causal-inference.html"><a href="causal-inference.html#answers-to-some-extra-questions-2"><i class="fa fa-check"></i><b>6.12</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supplementary Notes and References for ECON 4750</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1" number="4">
<h1><span class="header-section-number">Topic 4</span> Linear Regression</h1>
<p>In this chapter, our interest will shift to conditional expectations, such as <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span> (I’ll write <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span> in a lot of examples in this chapter, but you can think of there being an arbitrary number of <span class="math inline">\(X\)</span>’s).</p>
<p>I’ll refer to <span class="math inline">\(Y\)</span> as the <strong>outcome</strong>. You might also sometimes heae it called the <strong>dependent variable</strong>.</p>
<p>I’ll refer to the <span class="math inline">\(X\)</span>’s as either <strong>covariates</strong> or <strong>regressors</strong> or <strong>characteristics</strong>. You might also hear them called <strong>independent variables</strong> sometimes.</p>
<p>Before we start to get into the details, let us first discuss why we’re interested in conditional expectations. First, if we are interested in <em>making predictions</em>, it will often be the case that the “best” prediction that one can make is the conditional expectation. This should make sense to you — if you want to make a reasonable prediction about what the outcome will be for a new observation that has characteristics <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>, a good way to do it would be to predict that their outcome would be the same as the mean outcome in the population among those that have the same characteristics; that is, <span class="math inline">\(\mathbb{E}[Y|X_1=x_1, X_2=x_2, X_3=x_3]\)</span>.</p>
<p>Next, in economics, we are often interested in how much some outcome of interest changes when a particular covariate changes, holding other covariates constants. To give some examples, we might be interested in the average return of actively managed mutual funds relative to passively managed mutual funds conditional on investing in assets in the same class (e.g., large cap stocks or international bonds). As another example, we might be interested in the effect of an increase in the amount of fertilizer on average crop yield but while holding constant the temperature and precipitation.</p>
<p>How much the outcome, <span class="math inline">\(Y\)</span>, changes on average when one of the covariates, <span class="math inline">\(X_1\)</span>, changes by 1 unit and holding other covariates constant is what we’ll call the <strong>partial effect</strong> of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>. Suppose <span class="math inline">\(X_1\)</span> is binary, then it is given by</p>
<p><span class="math display">\[
  PE(x_2,x_3) = \mathbb{E}[Y | X_1=1, X_2=x_2, X_3=x_3] - \mathbb{E}[Y | X_1=0,X_2=x_2,X_3=x_3]
\]</span>
Notice that the partial effect can depend on <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>. For example, it could be that the effect of active management relative to passive management could be different across different asset classes.</p>
<p>Slightly more generally, if <span class="math inline">\(X_1\)</span> is discrete, so that it can take on several different discrete values, then we define the partial effect as</p>
<p><span class="math display">\[
  PE(x_1,x_2,x_3) = \mathbb{E}[Y | X_1=x_1+1, X_2=x_2, X_3=x_3] - \mathbb{E}[Y | X_1=x_1,X_2=x_2,X_3=x_3]
\]</span>
which now can depend on <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>. This is the average effect of going from <span class="math inline">\(X_1=x_1\)</span> to <span class="math inline">\(X_1=x_1+1\)</span> holding <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> constant.</p>
<p>Finally, consider the case where we are interested in the partial effect of <span class="math inline">\(X_1\)</span> which is continuous (for example, the partial effect of fertilizer input on crop yield). In this case the partial effect is given by the <em>partial derivative</em> of <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span> with respect to <span class="math inline">\(X_1\)</span>.</p>
<p><span class="math display">\[
  PE(x_1,x_2,x_3) = \frac{\partial \, \mathbb{E}[Y|X_1=x_1, X_2=x_2, X_3=x_3]}{\partial \, x_1}
\]</span>
This partial derivative is analogous to what we have been doing before — we are making a small change of <span class="math inline">\(X_1\)</span> while holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant at <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> This is probably the part of the class where we will jump around in the book the most this semester.</p>
<p>The pedagogical approach of the textbook is to introduce the notion of causality very early and to emphasize the requirements on linear regression models in order to deliver causality, while increasing the complexity of the models over several chapters.</p>
<p>This is totally reasonable, but I prefer to start by teaching the mechanics of regressions: how to compute them, how to interpret them (even if you are not able to meet the requirements of causality), and how to use them to make predictions. Then, we’ll have a serious discussion about causality over the last few weeks of the semester.</p>
<p>In practice, this means we’ll cover parts Chapters 4-8 in the textbook now, and then we’ll circle back to some of the issues covered in these chapters again towards the end of the semester.</p>
</div>
<div id="nonparametric-regression-curse-of-dimensionality" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Nonparametric Regression / Curse of Dimensionality</h2>
<p>If you knew nothing about regressions, it would seem natural to try to estimate <span class="math inline">\(\mathbb{E}[Y|X_1=x_1,X_2=x_2,X_3=x_3]\)</span> by just calculating the average of <span class="math inline">\(Y\)</span> among observations that have values of the regressors equal to <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> (if these are discrete) or that are, in some sense, close to <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> (if these are continuous).</p>
<p>This is actually a pretty attractive idea.</p>
<p>However, you run into the issue that it is practically challenging to do this when the number of regressors starts to get large (i.e., if you have 10 regressors, generally, you would need tons of data to be able to find a suitable number of observations that are “close” to any particular value of the regressors).</p>
<p>Let me give a more concrete example. Suppose that you were trying to estimate mean house price as a function of a house’s characteristics. If the only characteristic of the house that you knew was the number of bedrooms, then it would be pretty easy to just calculate the average house price among houses with 2, 3, 4, etc. bedrooms. Now suppose that you knew both the number of bedrooms and the number of square feet. In this case, if we wanted to estimate mean house prices as a function of these characteristics, we would need to find houses that have the same number of bedrooms and (at least) a similar number of square feet. This starts to “slice” the data that you have more thinly. If you continue with this idea (suppose that you want to estimate mean house price as a function of number of bedrooms, number of bathrooms, number of square feet, what year the house was built in, whether or not it has a basement, what zip code it is located in, etc.) then you will start to stretch your data extremely thin to the point that you may have very few relevant observations (or perhaps no relevant observations) for particular values of the characteristics.</p>
<p>This issue is called the “curse of dimensionality.”</p>
<p>We will focus on linear models for <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3]\)</span> largely to get around the curse of dimensionality.</p>
<div class="side-comment">
<p>This idea of using observations that are very close in terms of characteristics in order to estimate a conditional expectation is called <strong>nonparametric</strong> econometrics/statistics. You can take entire courses (typically graduate-level) on this topic if you were interested. The reason that it is is called nonparametric is that it doesn’t involve making any functional form assumptions (like linearity) but the cost is that it would typically require many more observations (due to the curse of dimensionality).</p>
</div>
</div>
<div id="linear-regression-models" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Linear Regression Models</h2>
<p>SW 4.1</p>
<p>In order to get around the curse of dimensionality that we discussed in the previous section, we will often an impose a <strong>linear model</strong> for the conditional expectation. For example,</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X] = \beta_0 + \beta_1 X
\]</span>
or</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span>
If we know the values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span>, then it is straighforward for us to make predictions. In particular, suppose that we want to predict the outcome for a new observation with characteristics <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>. Our prediction would be</p>
<p><span class="math display">\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 4.1  </strong></span>Suppose that you are studying intergenerational income mobility and that you are interested in predicting a child’s income whose parents’ income was $50,000 and whose mother had 12 years of education. Let <span class="math inline">\(Y\)</span> denote child’s income, <span class="math inline">\(X_1\)</span> denote parents’ income, and <span class="math inline">\(X_2\)</span> denote mother’s education. Further, suppose that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = 20,000 + 0.5 X_1 + 1000 X_2\)</span>.</p>
<p>In this case, you would predict child’s income to be</p>
<p><span class="math display">\[
  20,000 + 0.5 (50,000) + 1000(12) = 57,000
\]</span></p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span></p>
<p>The above model can be equivalently written as
<span class="math display">\[\begin{align*}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\end{align*}\]</span>
where <span class="math inline">\(U\)</span> is called the <strong>error term</strong> and satisfies <span class="math inline">\(\mathbb{E}[U|X_1,X_2,X_3] = 0\)</span>. There will be a few times where this formulation will be useful for us.</p>
</div>
</div>
<div id="computation" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Computation</h2>
<p>Even if we know that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3\)</span>, in general, we do not know the values of the population parameters (the <span class="math inline">\(\beta\)</span>’s). This is analogous to the framework in the previous chapter where we were interested in
the population parameter <span class="math inline">\(\mathbb{E}[Y]\)</span> and estimated it by <span class="math inline">\(\bar{Y}\)</span>.</p>
<p>In this section, we’ll discuss how to estimate <span class="math inline">\((\beta_0,\beta_1,\beta_2,\beta_3)\)</span> using <code>R</code>. We’ll refer to the estimated values of the parameters as <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3)\)</span>. As in the previous section, it will not be the case that the estimated <span class="math inline">\(\hat{\beta}\)</span>’s are exactly equal to the population <span class="math inline">\(\beta\)</span>’s. Later on in this chapter, we will establish properties like consistency (so that, as long as we have a large sample, the estimated <span class="math inline">\(\hat{\beta}\)</span>’s should be “close” to the population <span class="math inline">\(\beta\)</span>’s) and asymptotic normality (so that we can conduct inference).</p>
<p>Also later on in this chapter, we’ll talk about how <code>R</code> itself actually makes these computations.</p>
<p>The main function in <code>R</code> for estimating linear regressions is the <code>lm</code> function (<code>lm</code> stands for linear model). The key things to specify for running a regression in <code>R</code> are a <code>formula</code> argument which tells <code>lm</code> which variables are the outcome and which variables are the regressors and a <code>data</code> argument which tells the <code>lm</code> command what data we are using to estimate the regression. Let’s give an example using the <code>mtcars</code> data.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="linear-regression.html#cb110-1" aria-hidden="true" tabindex="-1"></a>reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data=</span>mtcars)</span></code></pre></div>
<p>What this line of code does is to run a regression. The formula is <code>mpg ~ hp + wt</code>. In other words <code>mpg</code> (standing for miles per gallon) is the outcome, and we are running a regression on <code>hp</code> (horse power) and <code>wt</code> (weight). The <code>~</code> symbol is a “tilde.” In order to add regressors, we separate them with a <code>+</code>. The second argument <code>data=mtcars</code> says to use the <code>mtcars</code> data. All of the variables in the formula need to correspond to column names in the data. We saved the results of the regression in a variable called <code>reg</code>. It’s most common to report the results of the regression using the <code>summary</code> command.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="linear-regression.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg)</span>
<span id="cb111-2"><a href="linear-regression.html#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb111-3"><a href="linear-regression.html#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb111-4"><a href="linear-regression.html#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt, data = mtcars)</span></span>
<span id="cb111-5"><a href="linear-regression.html#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb111-6"><a href="linear-regression.html#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb111-7"><a href="linear-regression.html#cb111-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb111-8"><a href="linear-regression.html#cb111-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -3.941 -1.600 -0.182  1.050  5.854 </span></span>
<span id="cb111-9"><a href="linear-regression.html#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb111-10"><a href="linear-regression.html#cb111-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb111-11"><a href="linear-regression.html#cb111-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb111-12"><a href="linear-regression.html#cb111-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***</span></span>
<span id="cb111-13"><a href="linear-regression.html#cb111-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.03177    0.00903  -3.519  0.00145 ** </span></span>
<span id="cb111-14"><a href="linear-regression.html#cb111-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; wt          -3.87783    0.63273  -6.129 1.12e-06 ***</span></span>
<span id="cb111-15"><a href="linear-regression.html#cb111-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb111-16"><a href="linear-regression.html#cb111-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb111-17"><a href="linear-regression.html#cb111-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb111-18"><a href="linear-regression.html#cb111-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.593 on 29 degrees of freedom</span></span>
<span id="cb111-19"><a href="linear-regression.html#cb111-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8268, Adjusted R-squared:  0.8148 </span></span>
<span id="cb111-20"><a href="linear-regression.html#cb111-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12</span></span></code></pre></div>
<p>The main thing that this reports is the estimated parameters. Our estimate of the “Intercept” (i.e., this is <span class="math inline">\(\hat{\beta}_0\)</span>) is in the first row of the table; our estimate is <code>37.227</code>. The estimated coefficient on <code>hp</code> is <code>-0.0318</code>, and the estimated coefficient on <code>wt</code> is <code>-3.878</code>.</p>
<p>You can also see standard errors for each estimated parameter, a t-statistic, and a p-value in the other columns. We will talk about these in more detail in the next section.</p>
<p>For now, we’ll also ignore the information provided at the bottom of the summary.</p>
<p>Now that we have estimated the parameters, we can use these to predict <span class="math inline">\(mpg\)</span> given a value of <span class="math inline">\(hp\)</span> and <span class="math inline">\(wt\)</span>. For example, suppose that you wanted to predict the <span class="math inline">\(mpg\)</span> of a 2500 pound car (note: weight in <span class="math inline">\(mtcars\)</span> is in 1000s of pounds) and 120 horsepower car, you could compute</p>
<p><span class="math display">\[
  37.227 - 0.0318(120) - 3.878(2.5) = 23.716
\]</span>
Alternatively, there is a built-in function in <code>R</code> called <code>predict</code> that can be used to generate predicted values. We just need to specify the values that we would like to get predicted values for by passing in a data frame with the relevant columns though the <code>newdata</code> argument. For example,</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="linear-regression.html#cb112-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(reg, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">hp=</span><span class="dv">120</span>,<span class="at">wt=</span><span class="fl">2.5</span>))</span>
<span id="cb112-2"><a href="linear-regression.html#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(pred,<span class="dv">3</span>)</span>
<span id="cb112-3"><a href="linear-regression.html#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     1 </span></span>
<span id="cb112-4"><a href="linear-regression.html#cb112-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 23.72</span></span></code></pre></div>
<div class="side-comment">
<p>A popular alternative to <code>R</code>’s <code>lm</code> function is the <code>lm_robust</code> function from the <code>estimatr</code> package. This provides different standard errors from the default standard errors provided by <code>lm</code> that are, at least in most applications in economics, typically a better choice — we’ll have a further discussion on this topic when we talk about inference later on in this chapter.</p>
</div>
</div>
<div id="partial-effects" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Partial Effects</h2>
<p>As we discussed in the beginning of this chapter, besides predicting outcomes, a second main goal for us is to think about partial effects of a regressor on the outcome. We’ll consider partial effects over the next few sections.</p>
<p>In the model,
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y | X_1, X_2, X_3]  &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\end{align*}\]</span></p>
<p>If <span class="math inline">\(X_1\)</span> is continuous, then
<span class="math display">\[\begin{align*}
  \beta_1 = \frac{\partial \mathbb{E}[Y|X_1,X_2,X_3]}{\partial X_1}
\end{align*}\]</span></p>
<p>Thus, <span class="math inline">\(\beta_1\)</span> is the partial effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>. In other words, <span class="math inline">\(\beta_1\)</span> should be interpreted as how much <span class="math inline">\(Y\)</span> increases, on average, when <span class="math inline">\(X_1\)</span> increases by one unit holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant. <em>Make sure to get this interpretation right!</em></p>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 4.2  </strong></span>Continuing the same example as above about intergenerational income mobility and where <span class="math inline">\(Y\)</span> denotes child’s income, <span class="math inline">\(X_1\)</span> denotes parents’ income, <span class="math inline">\(X_2\)</span> denotes mother’s education, and</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2] = 20,000 + 0.5 X_1 + 1000 X_2
\]</span>
The partial effect of parents’ income on child’s income is 0.5. This means that, for every one dollar increase in parents’ income, child’s income is 0.5 dollars higher on average holding mother’s education constant.</p>
</div>
<div id="computation-1" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Computation</h3>
<p>Let’s run the same regression as in the previous section, but think about partial effects in this case.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="linear-regression.html#cb113-1" aria-hidden="true" tabindex="-1"></a>reg1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data=</span>mtcars)</span>
<span id="cb113-2"><a href="linear-regression.html#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg1)</span>
<span id="cb113-3"><a href="linear-regression.html#cb113-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb113-4"><a href="linear-regression.html#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb113-5"><a href="linear-regression.html#cb113-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt, data = mtcars)</span></span>
<span id="cb113-6"><a href="linear-regression.html#cb113-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb113-7"><a href="linear-regression.html#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb113-8"><a href="linear-regression.html#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb113-9"><a href="linear-regression.html#cb113-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -3.941 -1.600 -0.182  1.050  5.854 </span></span>
<span id="cb113-10"><a href="linear-regression.html#cb113-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb113-11"><a href="linear-regression.html#cb113-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb113-12"><a href="linear-regression.html#cb113-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb113-13"><a href="linear-regression.html#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***</span></span>
<span id="cb113-14"><a href="linear-regression.html#cb113-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.03177    0.00903  -3.519  0.00145 ** </span></span>
<span id="cb113-15"><a href="linear-regression.html#cb113-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; wt          -3.87783    0.63273  -6.129 1.12e-06 ***</span></span>
<span id="cb113-16"><a href="linear-regression.html#cb113-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb113-17"><a href="linear-regression.html#cb113-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb113-18"><a href="linear-regression.html#cb113-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb113-19"><a href="linear-regression.html#cb113-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.593 on 29 degrees of freedom</span></span>
<span id="cb113-20"><a href="linear-regression.html#cb113-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8268, Adjusted R-squared:  0.8148 </span></span>
<span id="cb113-21"><a href="linear-regression.html#cb113-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12</span></span></code></pre></div>
<p>The partial effect of horsepower on miles per gallon is -0.032. In other words, we estimate that if horsepower increases by one then, on average, miles per gallon decreases by 0.032 holding weight constant.</p>
<p>The t-statistic and p-value are computed for the null hypothesis that the corresponding coefficient is equal to 0. For example, for <code>hp</code> the t-statistic is equal to <code>-3.519</code> which is greater than 1.96 and indicates that the partial effect of <code>hp</code> is statistically significant at a 5% significance level. The corresponding p-value for <code>hp</code> is <code>0.0145</code> indicating that there is only about a 1.5% chance of getting a t-statistic this extreme if the partial effect of <code>hp</code> were actually 0 (i.e., under <span class="math inline">\(H_0 : \beta_1=0\)</span>).</p>
<div class="practice">
<p><span class="practice">Practice: </span>What is the partial effect of <code>wt</code> in the previous example? Provide a careful interpretation. Is the partial effect of <code>wt</code> statistically significant? Explain. What is the p-value for <code>wt</code>? How do you interpret the p-value?</p>
</div>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> One horsepower is a very small increase in horsepower, so it might be a good idea to multiply the coefficient by some larger number, say 50. In this case, we could say that we estimate that if horsepower increases by 50 then, on average, miles per gallon decreases by 1.59 (<span class="math inline">\(=50 \times 0.03177\)</span>) holding weight constant. From the above discussion, we know that this effect is statistically different from 0. That said, it is not clear to me if we should interpret this as a large partial effect; I do not know too much about cars, but a 50 horsepower increase seems rather large while a 1.59 decrease in miles per gallon seems relatively small (at least to me).</p>
</div>
</div>
</div>
<div id="binary-regressors" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Binary Regressors</h2>
<p>SW 5.3</p>
<p>Let’s continue with the same model as above</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span></p>
<p>If <span class="math inline">\(X_1\)</span> is discrete (let’s say binary):
<span class="math display">\[\begin{align*}
  \beta_1 = \mathbb{E}[Y|X_1=1,X_2,X_3] - \mathbb{E}[Y|X_1=0,X_2,X_3]
\end{align*}\]</span>
<span class="math inline">\(\beta_1\)</span> is still the partial effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> and should be interpreted as how much <span class="math inline">\(Y\)</span> increases, on average, when <span class="math inline">\(X_1\)</span> changes from 0 to 1, holding <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> constant.</p>
<p>If <span class="math inline">\(X_1\)</span> can take more than just the values 0 and 1, but is still discrete (an example is a person’s years of education), then</p>
<p><span class="math display">\[
  \beta_1 = \mathbb{E}[Y | X_1=x_1+1, X_2, X_3] - \mathbb{E}[Y|X_1=x_1, X_2, X_3]
\]</span>
which holds for any possible value that <span class="math inline">\(X_1\)</span> could take, so that <span class="math inline">\(\beta_1\)</span> is the effect of a 1 unit increase in <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>, on average, holding constant <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-19" class="example"><strong>Example 4.3  </strong></span>Suppose that you work for an airline and you are interested in predicting the number of passengers for a Saturday morning flight from Atlanta to Memphis. Let <span class="math inline">\(Y\)</span> denote the number of passengers, <span class="math inline">\(X_1\)</span> be equal to 1 for a morning flight and 0 otherwise, and let <span class="math inline">\(X_2\)</span> be equal to 1 for a weekday flight and 0 otherwise. Further suppose that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = 80 + 20 X_1 - 15 X_2\)</span>.</p>
<p>In this case, you would predict,</p>
<p><span class="math display">\[
  80 + 20 (1) - 15 (0) = 100
\]</span>
passengers on the flight.</p>
<p>In addition, the partial effect of being morning flight is equal to 20. This indicates that, on average, morning flights have 20 more passengers than non-morning flights holding whether or not the flight occurs on a weekday constant.</p>
</div>
<div id="computation-2" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Computation</h3>
<p>In order to include a binary or discrete covariate in a regression in <code>R</code> is straightforward. The following regression uses the <code>mtcars</code> data and adds a binary regressor, <code>am</code>, indicating whether or not a car has an automatic transmission.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="linear-regression.html#cb114-1" aria-hidden="true" tabindex="-1"></a>reg2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt <span class="sc">+</span> am, <span class="at">data=</span>mtcars)</span>
<span id="cb114-2"><a href="linear-regression.html#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg2)</span>
<span id="cb114-3"><a href="linear-regression.html#cb114-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-4"><a href="linear-regression.html#cb114-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb114-5"><a href="linear-regression.html#cb114-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt + am, data = mtcars)</span></span>
<span id="cb114-6"><a href="linear-regression.html#cb114-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-7"><a href="linear-regression.html#cb114-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb114-8"><a href="linear-regression.html#cb114-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb114-9"><a href="linear-regression.html#cb114-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -3.4221 -1.7924 -0.3788  1.2249  5.5317 </span></span>
<span id="cb114-10"><a href="linear-regression.html#cb114-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-11"><a href="linear-regression.html#cb114-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb114-12"><a href="linear-regression.html#cb114-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb114-13"><a href="linear-regression.html#cb114-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 34.002875   2.642659  12.867 2.82e-13 ***</span></span>
<span id="cb114-14"><a href="linear-regression.html#cb114-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.037479   0.009605  -3.902 0.000546 ***</span></span>
<span id="cb114-15"><a href="linear-regression.html#cb114-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; wt          -2.878575   0.904971  -3.181 0.003574 ** </span></span>
<span id="cb114-16"><a href="linear-regression.html#cb114-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; am           2.083710   1.376420   1.514 0.141268    </span></span>
<span id="cb114-17"><a href="linear-regression.html#cb114-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb114-18"><a href="linear-regression.html#cb114-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb114-19"><a href="linear-regression.html#cb114-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb114-20"><a href="linear-regression.html#cb114-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.538 on 28 degrees of freedom</span></span>
<span id="cb114-21"><a href="linear-regression.html#cb114-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8399, Adjusted R-squared:  0.8227 </span></span>
<span id="cb114-22"><a href="linear-regression.html#cb114-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 48.96 on 3 and 28 DF,  p-value: 2.908e-11</span></span></code></pre></div>
<p>In this example, cars that had an automatic transmission got about 2 more miles per gallon than cars that had an automatic transmission on average, holding horsepower and weight constant (though the p-value is only 0.14).</p>
</div>
</div>
<div id="nonlinear-regression-functions" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Nonlinear Regression Functions</h2>
<p>SW 8.1, 8.2</p>
<p>Also, please read all of SW Ch. 8</p>
<p>So far, the the partial effects that we have been interested in have corresponded to a particular parameter in the regression, usually <span class="math inline">\(\beta_1\)</span>. I think this can sometimes be a source of confusion as, at least in my view, we are not typically interested in the parameters for their own sake, but rather are interested in partial effects. It just so happens that in some leading cases, they coincide.</p>
<p>In addition, while the <span class="math inline">\(\beta\)</span>’s in the sort of models we have considered so far are easy to interpret, in some cases, it might be <em>restrictive</em> to think that the partial effects are the same across different values of the covariates.</p>
<p>In this section, we’ll see the first of several cases where partial effects do not coincide with a particular parameter.</p>
<p>Suppose that</p>
<p><span class="math display">\[ 
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 + \beta_4 X_3
\]</span></p>
<p>Let’s start with making predictions using this model. If you know the values of <span class="math inline">\(\beta_0,\beta_1,\beta_2,\beta_3,\)</span> and <span class="math inline">\(\beta_4\)</span>, then to get a prediction, you would still just plug in the values of the regressors that you’d like to get a prediction for (including <span class="math inline">\(x_1^2\)</span>).</p>
<p>Next, in this model, the partial effect of <span class="math inline">\(X_1\)</span> is given by</p>
<p><span class="math display">\[
  \frac{\partial \, \mathbb{E}[Y|X_1,X_2,X_3]}{\partial \, X_1} = \beta_1 + 2\beta_2 X_1
\]</span>
In other words, the partial effect of <span class="math inline">\(X_1\)</span> depends on the value that <span class="math inline">\(X_1\)</span> takes.</p>
<p>In this case, it is sometimes useful to report the partial effect for some different values of <span class="math inline">\(X_1\)</span>. In other cases, it is useful to report the <strong>average partial effect</strong> (APE) which is the mean of the partial effects across the distribution of the covariates. In this case, the APE is given by</p>
<p><span class="math display">\[
  APE = \beta_1 + 2 \beta_2 \mathbb{E}[X_1]
\]</span>
and, once you have estimated the regression, you can compute an estimate of <span class="math inline">\(APE\)</span> by</p>
<p><span class="math display">\[
  \widehat{APE} = \hat{\beta}_1 + 2 \hat{\beta}_2 \bar{X}_1
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-20" class="example"><strong>Example 4.4  </strong></span>Let’s continue our example on intergenerational income mobility where <span class="math inline">\(Y\)</span> denotes child’s income, <span class="math inline">\(X_1\)</span> denotes parents’ income, and <span class="math inline">\(X_2\)</span> denotes mother’s education. Now, suppose that</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2] = 15,000 + 0.7 X_1 - 0.000002 X_1^2 + 800 X_2
\]</span>
Then, predicted child’s income when parents’ income is equal to $50,000 is given by</p>
<p><span class="math display">\[
  15,000 + 0.7 (50,000) - 0.000002 (50,000)^2 + 800 (12) = 54,600
\]</span>
In addition, the partial effect of parents’ income is given by</p>
<p><span class="math display">\[
  0.7 - 0.000004 X_1 
\]</span>
Let’s compute a few different partial effects for different values of parents’ income</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(X_1\)</span></th>
<th align="center">PE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">20,000</td>
<td align="center">0.62</td>
</tr>
<tr class="even">
<td align="center">50,000</td>
<td align="center">0.50</td>
</tr>
<tr class="odd">
<td align="center">100,000</td>
<td align="center">0.30</td>
</tr>
</tbody>
</table>
<p>which indicates that the partial effect of parents’ income is decreasing — i.e., the effect of additional parents’ income is largest for children whose parents have the lowest income and gets smaller for those whose parents have high incomes.</p>
<p>Finally, if you wanted to compute the <span class="math inline">\(APE\)</span>, you would just plug in <span class="math inline">\(\mathbb{E}[X_1]\)</span> (or <span class="math inline">\(\bar{X}_1\)</span>) into the expression for the partial effect.</p>
</div>
<div id="computation-3" class="section level3" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Computation</h3>
<p>Including a quadratic (or other higher order term) in <code>R</code> is relatively straightforward. Let’s just do an example.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="linear-regression.html#cb115-1" aria-hidden="true" tabindex="-1"></a>reg3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> <span class="fu">I</span>(hp<span class="sc">^</span><span class="dv">2</span>), <span class="at">data=</span>mtcars)</span>
<span id="cb115-2"><a href="linear-regression.html#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg3)</span>
<span id="cb115-3"><a href="linear-regression.html#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb115-4"><a href="linear-regression.html#cb115-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb115-5"><a href="linear-regression.html#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + I(hp^2), data = mtcars)</span></span>
<span id="cb115-6"><a href="linear-regression.html#cb115-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb115-7"><a href="linear-regression.html#cb115-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb115-8"><a href="linear-regression.html#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb115-9"><a href="linear-regression.html#cb115-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -4.5512 -1.6027 -0.6977  1.5509  8.7213 </span></span>
<span id="cb115-10"><a href="linear-regression.html#cb115-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb115-11"><a href="linear-regression.html#cb115-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb115-12"><a href="linear-regression.html#cb115-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb115-13"><a href="linear-regression.html#cb115-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  4.041e+01  2.741e+00  14.744 5.23e-15 ***</span></span>
<span id="cb115-14"><a href="linear-regression.html#cb115-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -2.133e-01  3.488e-02  -6.115 1.16e-06 ***</span></span>
<span id="cb115-15"><a href="linear-regression.html#cb115-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; I(hp^2)      4.208e-04  9.844e-05   4.275 0.000189 ***</span></span>
<span id="cb115-16"><a href="linear-regression.html#cb115-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb115-17"><a href="linear-regression.html#cb115-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb115-18"><a href="linear-regression.html#cb115-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb115-19"><a href="linear-regression.html#cb115-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.077 on 29 degrees of freedom</span></span>
<span id="cb115-20"><a href="linear-regression.html#cb115-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7561, Adjusted R-squared:  0.7393 </span></span>
<span id="cb115-21"><a href="linear-regression.html#cb115-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 44.95 on 2 and 29 DF,  p-value: 1.301e-09</span></span></code></pre></div>
<p>The only thing that is new here is <code>I(hp^2)</code>. The <code>I</code> function stands for inhibit (you can read the documentation using <code>?I</code>). For us, this is not too important. You can understand it like this: there is no variable names <code>hp^2</code> in the data, but if we put the name of a variable that is in the data (here: <code>hp</code>) then we can apply a function to it (here: squaring it) before including it as a regressor.</p>
<p>Interestingly, here it seems there are nonlinear effects of horsepower on miles per gallon. Let’s just quickly report the estimated partial effects for a few different values of horsepower.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="linear-regression.html#cb116-1" aria-hidden="true" tabindex="-1"></a>hp_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">200</span>,<span class="dv">300</span>)</span>
<span id="cb116-2"><a href="linear-regression.html#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="co"># there might be a native function in r</span></span>
<span id="cb116-3"><a href="linear-regression.html#cb116-3" aria-hidden="true" tabindex="-1"></a><span class="co"># to compute these partial effects; I just</span></span>
<span id="cb116-4"><a href="linear-regression.html#cb116-4" aria-hidden="true" tabindex="-1"></a><span class="co"># don&#39;t know it.</span></span>
<span id="cb116-5"><a href="linear-regression.html#cb116-5" aria-hidden="true" tabindex="-1"></a>pe <span class="ot">&lt;-</span> <span class="cf">function</span>(hp) {</span>
<span id="cb116-6"><a href="linear-regression.html#cb116-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># partial effect is b1 + 2b2*hp</span></span>
<span id="cb116-7"><a href="linear-regression.html#cb116-7" aria-hidden="true" tabindex="-1"></a>  pes <span class="ot">&lt;-</span> <span class="fu">coef</span>(reg3)[<span class="dv">2</span>] <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">coef</span>(reg3)[<span class="dv">3</span>]<span class="sc">*</span>hp</span>
<span id="cb116-8"><a href="linear-regression.html#cb116-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print using a data frame</span></span>
<span id="cb116-9"><a href="linear-regression.html#cb116-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">hp=</span>hp, <span class="at">pe=</span><span class="fu">round</span>(pes,<span class="dv">3</span>))</span>
<span id="cb116-10"><a href="linear-regression.html#cb116-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb116-11"><a href="linear-regression.html#cb116-11" aria-hidden="true" tabindex="-1"></a><span class="fu">pe</span>(hp_vec)</span>
<span id="cb116-12"><a href="linear-regression.html#cb116-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    hp     pe</span></span>
<span id="cb116-13"><a href="linear-regression.html#cb116-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 100 -0.129</span></span>
<span id="cb116-14"><a href="linear-regression.html#cb116-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 200 -0.045</span></span>
<span id="cb116-15"><a href="linear-regression.html#cb116-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 300  0.039</span></span></code></pre></div>
<p>which suggests that the partial effect of horsepower on miles per gallon is large (though negative) at small values of horsepower and decreasing up to essentially no effect at larger values of horsepower.</p>
</div>
</div>
<div id="interpreting-interaction-terms" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Interpreting Interaction Terms</h2>
<p>SW 8.3</p>
<p>Another way to allow for partial effects that vary across different values of the regressors is to include <strong>interaction terms</strong>.</p>
<p>Consider the following regression model</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \beta_4 X_3
\]</span></p>
<p>The term <span class="math inline">\(X_1 X_2\)</span> is called the interaction term. In this model, the partial effect of <span class="math inline">\(X_1\)</span> is given by</p>
<p><span class="math display">\[
  \frac{\partial \, \mathbb{E}[Y|X_1,X_2,X_3]}{\partial \, X_1} = \beta_1 + \beta_3 X_2
\]</span></p>
<p>In this model, the effect of <span class="math inline">\(X_1\)</span> varies with <span class="math inline">\(X_2\)</span>. As in the previous section, you could report the partial effect for different values of <span class="math inline">\(X_2\)</span> or consider <span class="math inline">\(APE = \beta_1 + \beta_3 \mathbb{E}[X_2]\)</span>.</p>
<p>There are a couple of other things worth pointing out for interaction terms</p>
<ul>
<li><p>It is very common for one of the interaction terms, say, <span class="math inline">\(X_2\)</span> to be a binary variable. This gives a way to easily test if the effect of <span class="math inline">\(X_1\)</span> is the same across the two “groups” defined by <span class="math inline">\(X_2\)</span>. For example, suppose you wanted to check if the partial effect of education was the same for men and women. You could run a regression like</p>
<p><span class="math display">\[
    Wage = \beta_0 + \beta_1 Education + \beta_2 Female + \beta_3 Education \cdot Female + U
  \]</span></p>
<p>From the previous discussion, the partial effect of education is given by</p>
<p><span class="math display">\[
    \beta_1 + \beta_3 Female
  \]</span></p>
<p>Thus, the partial effect education for men is given by <span class="math inline">\(\beta_1\)</span>, and the partial effect of education for women is given by <span class="math inline">\(\beta_1 + \beta_3\)</span>. Thus, if you want to test if the partial effect of education differs for men and women, you can just test if <span class="math inline">\(\beta_3=0\)</span>. If <span class="math inline">\(\beta_3&gt;0\)</span>, it suggests a higher partial effect of education for women, and if <span class="math inline">\(\beta_3 &lt; 0\)</span>, it suggests a lower partial effect of education for women.</p></li>
<li><p>Another interesting case is when <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are both binary. In this case, a model that includes an interaction term is called a <strong>saturated model</strong>. It is called this because it is actually nonparametric. In particular, notice that in the model <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>,</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[Y|X_1=0,X_2=0] &amp;= \beta_0 \\
    \mathbb{E}[Y|X_1=1,X_2=0] &amp;= \beta_0 + \beta_1 \\
    \mathbb{E}[Y|X_1=0,X_2=1] &amp;= \beta_0 + \beta_2 \\
    \mathbb{E}[Y|X_1=1,X_2=1] &amp;= \beta_0 + \beta_1 + \beta_2 + \beta_3
  \end{aligned}
  \]</span></p>
<p>This exhausts all possible combinations of the regressors and means that you can recover each possible value of the conditional expectation from the parameters of the model.</p>
<p>It would be possible to write down a saturated model in cases with more than two binary regressors (or even discrete regressors) — you would just need to include more interaction terms. The key thing is that there be no continuous regressors. That said, as you start to add more and more discrete regressors and their interactions, you will effectively start to run into the curse of dimensionality issues that we discussed earlier.</p>
<p>As an example, consider our earlier example of flights from Atlanta to Memphis where <span class="math inline">\(Y\)</span> denoted the number of passengers, <span class="math inline">\(X_1\)</span> was equal to 1 for a a morning flight and 0 otherwise, and <span class="math inline">\(X_2\)</span> was equal to one for a weekday flight and 0 otherwise. Suppose that <span class="math inline">\(\mathbb{E}[Y|X_1,X_2] = 90 - 15 X_1 - 5 X_2 + 25 X_1 X_2\)</span>. Then,</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathbb{E}[Y|X_1=0,X_2=0] &amp;= 90 \quad &amp; \textrm{non-morning, weekend} \\
    \mathbb{E}[Y|X_1=1,X_2=0] &amp;= 90 - 15 = 75 \quad &amp; \textrm{morning, weekend} \\
    \mathbb{E}[Y|X_1=0,X_2=1] &amp;= 90 - 5 = 85 \quad  &amp; \textrm{non-morning, weekday} \\
    \mathbb{E}[Y|X_1=1,X_2=1] &amp;= 90 - 15 - 5 + 25 = 100 \quad &amp; \textrm{morning, weekend}
  \end{aligned}
  \]</span></p></li>
</ul>
<div id="computation-4" class="section level3" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Computation</h3>
<p>Including interaction terms in regressions in <code>R</code> is straightforward. Using the <code>mtcars</code> data, we can do it as follows</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="linear-regression.html#cb117-1" aria-hidden="true" tabindex="-1"></a>reg4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt <span class="sc">+</span> am <span class="sc">+</span> hp<span class="sc">*</span>am, <span class="at">data=</span>mtcars)</span>
<span id="cb117-2"><a href="linear-regression.html#cb117-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg4)</span>
<span id="cb117-3"><a href="linear-regression.html#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-4"><a href="linear-regression.html#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb117-5"><a href="linear-regression.html#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp + wt + am + hp * am, data = mtcars)</span></span>
<span id="cb117-6"><a href="linear-regression.html#cb117-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-7"><a href="linear-regression.html#cb117-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb117-8"><a href="linear-regression.html#cb117-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb117-9"><a href="linear-regression.html#cb117-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -3.435 -1.510 -0.697  1.284  5.245 </span></span>
<span id="cb117-10"><a href="linear-regression.html#cb117-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-11"><a href="linear-regression.html#cb117-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb117-12"><a href="linear-regression.html#cb117-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb117-13"><a href="linear-regression.html#cb117-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 33.34196    2.79711  11.920 2.89e-12 ***</span></span>
<span id="cb117-14"><a href="linear-regression.html#cb117-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.02918    0.01449  -2.014  0.05407 .  </span></span>
<span id="cb117-15"><a href="linear-regression.html#cb117-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; wt          -3.05617    0.94036  -3.250  0.00309 ** </span></span>
<span id="cb117-16"><a href="linear-regression.html#cb117-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; am           3.55141    2.35742   1.506  0.14355    </span></span>
<span id="cb117-17"><a href="linear-regression.html#cb117-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp:am       -0.01129    0.01466  -0.770  0.44809    </span></span>
<span id="cb117-18"><a href="linear-regression.html#cb117-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb117-19"><a href="linear-regression.html#cb117-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb117-20"><a href="linear-regression.html#cb117-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb117-21"><a href="linear-regression.html#cb117-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 2.556 on 27 degrees of freedom</span></span>
<span id="cb117-22"><a href="linear-regression.html#cb117-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8433, Adjusted R-squared:  0.8201 </span></span>
<span id="cb117-23"><a href="linear-regression.html#cb117-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 36.33 on 4 and 27 DF,  p-value: 1.68e-10</span></span></code></pre></div>
<p>The interaction term in the results is in the row that starts with <code>hp:am</code>. These estimates suggest that, while horsepower does seem to decrease miles per gallon controlling for weight and whether or not the car has an automatic transmission, the effect of horsepower does not seem to vary much by whether or not the car has an automatic transmission (at least not in a big enough way that we can detect it with the data that we have).</p>
</div>
</div>
<div id="elasticities" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> Elasticities</h2>
<p>SW 8.2</p>
<p>Economists are often interested in <strong>elasticities</strong>, that is, the percentage change in <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> changes by 1%.</p>
<p>Recall that the definition of percentage change of moving from, say, <span class="math inline">\(x_{old}\)</span> to <span class="math inline">\(x_{new}\)</span> is given by</p>
<p><span class="math display">\[
  \textrm{% change} = \frac{x_{new} - x_{old}}{x_{old}} \times 100
\]</span></p>
<p>Elasticities are closely connected to natural logarithms; following the most common notation in economics, we’ll refer to the natural logarithm using the notation: <span class="math inline">\(\log\)</span>. Further, recall that the derivative of the <span class="math inline">\(\log\)</span> function is given by</p>
<p><span class="math display">\[
  \frac{d \, \log(x)}{d \, x} = \frac{1}{x} \implies d\, \log(x) = \frac{d \, x}{x}
\]</span>
which further implies that</p>
<p><span class="math display">\[
  \Delta \log(x) := \log(x_{new}) - \log(x_{old}) \approx \frac{x_{new} - x_{old}}{x_{old}}
\]</span>
and, thus, that</p>
<p><span class="math display">\[
  100 \cdot \Delta \log(x) \approx \textrm{% change}
\]</span>
where the approximation is better when <span class="math inline">\(x_{new}\)</span> and <span class="math inline">\(x_{old}\)</span> are close to each other.</p>
<p>Now, we’ll use these properties of logarithms in order to interpret several linear models</p>
<ul>
<li><p>For simplicity, I am going to not include an error term or extra covariates, but you should continue to interpret parameter estimates as “on average” and “holding other regressors constant” (if there are other regressors in the model).</p></li>
<li><p><strong>Log-Log</strong> Model</p>
<p><span class="math display">\[
  \log(Y) = \beta_0 + \beta_1 \log(X)
  \]</span></p>
<p>In this case,</p>
<p><span class="math display">\[
  \begin{aligned}
  \beta_1 &amp;= \frac{ d \, \log(Y) }{d \, \log(X)} \\
  &amp;= \frac{ d \, \log(Y) \cdot 100 }{d \, \log(X) \cdot 100} \\
  &amp;\approx \frac{ \% \Delta Y}{ \% \Delta X}
  \end{aligned}
  \]</span></p>
<p>All that to say, in a regression of the log of an outcome on the log of a regressor, you should interpret the corresponding coefficient as the average percentage change in the outcome when the regressor changes by 1%. The log-log model is sometimes called a <strong>constant elasticity</strong> model.</p></li>
<li><p><strong>Log-Level</strong> model</p>
<p><span class="math display">\[
  \log(Y) = \beta_0 + \beta_1 X
  \]</span></p>
<p>In this case,</p>
<p><span class="math display">\[
  \begin{aligned}
  \beta_1 &amp;= \frac{ d \, \log(Y) }{d \, X} \\
  \implies 100 \beta_1 &amp;= \frac{ d \, \log(Y) \cdot 100 }{d \, X} \\
  \implies 100 \beta_1 &amp;\approx \frac{ \% \Delta Y}{ d \, X}
  \end{aligned}
  \]</span></p>
<p>Thus, in a regression of the log of an outcome on the <em>level</em> of a regressor, you should multiply the corresponding coefficient by 100 and interpret it as the average percentage change in the outcome when the regressor changes by 1 unit.</p></li>
<li><p><strong>Level-Log</strong> model</p>
<p><span class="math display">\[
    Y = \beta_0 + \beta_1 \log(X)
  \]</span></p>
<p>In this case,</p>
<p><span class="math display">\[
    \begin{aligned}
    \beta_1 &amp;= \frac{d\, Y}{d \, \log(X)} \\
    \implies \frac{\beta_1}{100} &amp;= \frac{d \, Y}{d \, \log(X) \cdot 100} \\
    \implies \frac{\beta_1}{100} &amp;\approx \frac{d \, Y}{\% \Delta X}
    \end{aligned}
  \]</span></p>
<p>Thus, in a regression of the level of an outcome on the log of a regressor, you should divide the corresponding coefficient by 100 and interpret it as the average change in the outcome when the regressor changes by 1%.</p></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-21" class="example"><strong>Example 4.5  </strong></span>Let’s continue the same example on intergenerational income mobility where <span class="math inline">\(Y\)</span> denotes child’s income, <span class="math inline">\(X_1\)</span> denotes parents’ income and <span class="math inline">\(X_2\)</span> denotes mother’s education. We’ll consider how to interpret several different models.</p>
<p><span class="math display">\[
  \log(Y) = 8.8 + 0.4 \log(X_1) + 0.008 X_2 + U
\]</span>
In this model, we estimate that, on average, when parents’ income increases by 1%, child’s income increases by 0.4% holding mother’s education constant.</p>
<p>Next, consider,
<span class="math display">\[
  \log(Y) = 8.9 + 0.00004 X_1 + 0.007 X_2 + U
\]</span>
In this model, we estimate that, on average, when parents’ income increases by $1, child’s income increases by 0.004% (alternatively, when parents’ income increase by $1000, child’s income increases by 4%) holding mother’s education constant.</p>
<p>Finally, consider</p>
<p><span class="math display">\[
  Y = -1,680,000 + 160,000 \log(X_1) + 900 X_2 + U
\]</span>
In this case, we estimate that, on average, when parents’ income increases by 1%, child’s income increases by $1,600 holding mother’s education constant.</p>
</div>
<div id="computation-5" class="section level3" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> Computation</h3>
<p>Estimating models that include logarithms in <code>R</code> is straightforward.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="linear-regression.html#cb118-1" aria-hidden="true" tabindex="-1"></a>reg5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(mpg) <span class="sc">~</span> <span class="fu">log</span>(hp) <span class="sc">+</span> wt, <span class="at">data=</span>mtcars) </span>
<span id="cb118-2"><a href="linear-regression.html#cb118-2" aria-hidden="true" tabindex="-1"></a>reg6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(mpg) <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data=</span>mtcars) </span>
<span id="cb118-3"><a href="linear-regression.html#cb118-3" aria-hidden="true" tabindex="-1"></a>reg7 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">log</span>(hp) <span class="sc">+</span> wt, <span class="at">data=</span>mtcars)</span></code></pre></div>
<p>Let’s show the results all at once using the <code>modelsummary</code> function from the <code>modelsummary</code> package.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="linear-regression.html#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb119-2"><a href="linear-regression.html#cb119-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-3"><a href="linear-regression.html#cb119-3" aria-hidden="true" tabindex="-1"></a>model_list <span class="ot">&lt;-</span> <span class="fu">list</span>(reg5, reg6, reg7)</span>
<span id="cb119-4"><a href="linear-regression.html#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="fu">modelsummary</span>(model_list)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
Model 1
</th>
<th style="text-align:center;">
Model 2
</th>
<th style="text-align:center;">
Model 3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:center;">
4.832
</td>
<td style="text-align:center;">
3.829
</td>
<td style="text-align:center;">
59.571
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.222)
</td>
<td style="text-align:center;">
(0.069)
</td>
<td style="text-align:center;">
(4.977)
</td>
</tr>
<tr>
<td style="text-align:left;">
log(hp)
</td>
<td style="text-align:center;">
−0.266
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
−5.922
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.056)
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
(1.266)
</td>
</tr>
<tr>
<td style="text-align:left;">
wt
</td>
<td style="text-align:center;">
−0.179
</td>
<td style="text-align:center;">
−0.201
</td>
<td style="text-align:center;">
−3.286
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.027)
</td>
<td style="text-align:center;">
(0.027)
</td>
<td style="text-align:center;">
(0.615)
</td>
</tr>
<tr>
<td style="text-align:left;">
hp
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
−0.002
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;box-shadow: 0px 1px">
</td>
<td style="text-align:center;box-shadow: 0px 1px">
</td>
<td style="text-align:center;box-shadow: 0px 1px">
(0.000)
</td>
<td style="text-align:center;box-shadow: 0px 1px">
</td>
</tr>
<tr>
<td style="text-align:left;">
Num.Obs.
</td>
<td style="text-align:center;">
32
</td>
<td style="text-align:center;">
32
</td>
<td style="text-align:center;">
32
</td>
</tr>
<tr>
<td style="text-align:left;">
R2
</td>
<td style="text-align:center;">
0.885
</td>
<td style="text-align:center;">
0.869
</td>
<td style="text-align:center;">
0.859
</td>
</tr>
<tr>
<td style="text-align:left;">
R2 Adj.
</td>
<td style="text-align:center;">
0.877
</td>
<td style="text-align:center;">
0.860
</td>
<td style="text-align:center;">
0.849
</td>
</tr>
<tr>
<td style="text-align:left;">
AIC
</td>
<td style="text-align:center;">
−49.0
</td>
<td style="text-align:center;">
−44.8
</td>
<td style="text-align:center;">
150.0
</td>
</tr>
<tr>
<td style="text-align:left;">
BIC
</td>
<td style="text-align:center;">
−43.1
</td>
<td style="text-align:center;">
−38.9
</td>
<td style="text-align:center;">
155.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Log.Lik.
</td>
<td style="text-align:center;">
28.501
</td>
<td style="text-align:center;">
26.395
</td>
<td style="text-align:center;">
−71.017
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:center;">
111.812
</td>
<td style="text-align:center;">
96.232
</td>
<td style="text-align:center;">
88.442
</td>
</tr>
</tbody>
</table>
<ul>
<li><p>In the first model, we estimate that, on average, a 1% increase in horsepower decreases miles per gallon by 0.266% holding weight constant.</p></li>
<li><p>In the second model, we estimate that, on average, a 1 unit increase in horsepower decreases miles per gallon by 0.2% holding weight constant.</p></li>
<li><p>In the third model, we estimate that, on average, a 1% increase in horsepower decreases miles per gallon by .059 holding weight constant.</p></li>
</ul>
</div>
</div>
<div id="omitted-variable-bias" class="section level2" number="4.9">
<h2><span class="header-section-number">4.9</span> Omitted Variable Bias</h2>
<p>SW 6.1</p>
<p>Suppose that we are interested in the following regression model</p>
<p><span class="math display">\[
  \mathbb{E}[Y|X_1, X_2, Q] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 Q
\]</span>
and, in particular, we are interested in the the partial effect</p>
<p><span class="math display">\[
  \frac{ \partial \, \mathbb{E}[Y|X_1,X_2,Q]}{\partial \, X_1} = \beta_1
\]</span>
But we are faced with the issue that we do not observe <span class="math inline">\(Q\)</span> (which implies that we cannot control for it in the regression)</p>
<p>Recall that we can equivalently write</p>
<p><span class="math display" id="eq:ovb-y">\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 Q + U \tag{4.1}
\]</span>
where <span class="math inline">\(\mathbb{E}[U|X_1,X_2,Q]=0\)</span>.</p>
<p>Now, for simplicity, suppose that</p>
<p><span class="math display">\[
  \mathbb{E}[Q | X_1, X_2] = \gamma_0 + \gamma_1 X_1 + \gamma_2 X_2
\]</span></p>
<p>Now, let’s consider the idea of just running a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (and just not including <span class="math inline">\(Q\)</span>); in other words, consider the regression
<span class="math display">\[
  \mathbb{E}[Y|X_1,X_2] = \delta_0 + \delta_1 X_1 + \delta_2 X_2
\]</span>
We are interested in the question of whether or not we can recover <span class="math inline">\(\beta_1\)</span> if we do this. If we consider this “feasible” regression, notice if we plug in the expression for <span class="math inline">\(Y\)</span> from Equation <a href="linear-regression.html#eq:ovb-y">(4.1)</a>,</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathbb{E}[Y|X_1,X_2] &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \mathbb{E}[Q|X_1,X_2] \\
  &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (\gamma_0 + \gamma_1 X_1 + \gamma_2 X_2) \\
  &amp;= \underbrace{(\beta_0 + \beta_3 \gamma_0)}_{\delta_0} + \underbrace{(\beta_1 + \beta_3 \gamma_1)}_{\delta_1} X_1 + \underbrace{(\beta_2 + \beta_3 \gamma_2)}_{\delta_2} X_2
  \end{aligned}
\]</span></p>
<p>In other words, if we run the feasible regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\delta_1\)</span> (the coefficient on <span class="math inline">\(X_1\)</span>) is not equal to <span class="math inline">\(\beta_1\)</span>; rather, it is equal to <span class="math inline">\((\beta_1 + \beta_3 \gamma_1)\)</span>.</p>
<p>That you are not generally able to recover <span class="math inline">\(\beta_1\)</span> in this case is called <strong>omitted variable bias</strong></p>
<p>There are two cases where you will recover <span class="math inline">\(\delta_1 = \beta_1\)</span> though which occur when <span class="math inline">\(\beta_3 \gamma_1 = 0\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\beta_3=0\)</span>. This would be the case where <span class="math inline">\(Q\)</span> has no effect on <span class="math inline">\(Y\)</span></p></li>
<li><p><span class="math inline">\(\gamma_1=0\)</span>. This would be the case where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Q\)</span> are uncorrelated after controlling for <span class="math inline">\(X_2\)</span>.</p></li>
</ul>
<p>Interestingly, there may be some case where you can “sign” the bias; i.e., figure out if <span class="math inline">\(\beta_3 \gamma_1\)</span> is positive or negative. For example, you might have theoretical reasons to suspect that <span class="math inline">\(\gamma_1 &gt; 0\)</span> and <span class="math inline">\(\beta_3 &gt; 0\)</span>. In this case,</p>
<p><span class="math display">\[
  \delta_1 = \beta_1 + \textrm{something positive}
\]</span>
which implies that <span class="math inline">\(\delta_1\)</span> (i.e., running a regression that ignores <span class="math inline">\(Q\)</span>) would cause us to tend to over-estimate <span class="math inline">\(\beta_1\)</span>.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span></p>
<ul>
<li><p>The book talks about omitted variable bias in the context of causality (this is probably the leading case), but we have not talked about causality yet. The same issues arise if we just say that we have some <em>regression of interest</em> but are unable to estimate it because some covariates are unobserved.</p></li>
<li><p>The relationship to causality (which is not so important for now), is that under certain conditions, we may have a particular partial effect that we would be willing to interpret as being the “causal effect,” but if we are unable to control for some variables that would lead to this interpretation, then we get to the issues pointed out in the textbook.</p></li>
</ul>
</div>
</div>
<div id="how-to-estimate-the-parameters-in-a-regression-model" class="section level2" number="4.10">
<h2><span class="header-section-number">4.10</span> How to estimate the parameters in a regression model</h2>
<p>SW 4.2, 6.3</p>
<p>Let’s start with the simple linear regression model (i.e., where there is just one regressor):</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X + U
\]</span>
with <span class="math inline">\(\mathbb{E}[U|X]=0\)</span>. This model holds for every observation in the data, so we can write</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + U_i
\]</span>
The question for this section is: How can we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? First, notice that, any choice that we make for an estimate of <span class="math inline">\(\beta_0\)</span> of <span class="math inline">\(\beta_1\)</span> amounts to picking a line. Our strategy will be to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> by choosing values of them that result in the “best fit” of a line to the available data.</p>
<p>This begs the question: How do you choose the line that best fits the data? Let me give you an example</p>
<p><img src="ols_lines.jpg"></p>
<p>It’s clear from the figure that the blue line “fits better” than the green line or the red line. If you take a second and think about the reason why this is the case, you will notice that the reason why you know that it fits better is because the points <em>tend to be closer</em> to the blue line than to the red line or the green line.</p>
<p>In the figure, I drew three additional lines that are labeled <span class="math inline">\(U_i^b\)</span>, <span class="math inline">\(U_i^g\)</span>, and <span class="math inline">\(U_i^r\)</span> which are just the difference between the blue line, the green line, and the red line and the corresponding data point in the figure. That is,</p>
<p><span class="math display">\[
  \begin{aligned}
  U_i^b &amp;= Y_i - b_0^b - b_1^b X_i \\
  U_i^g &amp;= Y_i - b_0^g - b_1^g X_i \\
  U_i^r &amp;= Y_i - b_0^r - b_1^r X_i
  \end{aligned}
\]</span>
where, for example, <span class="math inline">\(b_0^b\)</span> and <span class="math inline">\(b_1^b\)</span> are the intercept and slope of the blue line, <span class="math inline">\(b_0^g\)</span> and <span class="math inline">\(b_1^g\)</span> are the intercept and slope of the green line, etc. Clearly, the blue line fits this data point than the others, but we need to deal with a couple of issues to formalize this thinking. First, <span class="math inline">\(U_i^b\)</span> and <span class="math inline">\(U_i^r\)</span> are both less than 0 (because both of those lines sit above the data point) indicating that we can’t just choose the line where <span class="math inline">\(U_i\)</span> is the smallest — for this point, that strategy would result in us liking the red line the most. Instead, we need a measure of distance that turns negative values of <span class="math inline">\(U_i\)</span> into positive values and is larger for big negative values of <span class="math inline">\(U_i\)</span> too. We’ll use the same quadratic distance that we’ve used before to deal with this issue; that is, we’ll define the distance between a line and the point as <span class="math inline">\({U_i^b}^2\)</span> for the blue line, <span class="math inline">\({U_i^g}^2\)</span> and <span class="math inline">\({U_i^r}^2\)</span> for the green and red lines.</p>
<p>Second, the above discussion computes the distance between the line and the data for a single point. We want to extend this argument to all the data points. And we can do that by computing the average distance between the line and the points across all points. That is given by</p>
<p><span class="math display">\[
  \frac{1}{n}\sum_{i=1}^n {U_i^b}^2, \quad \frac{1}{n}\sum_{i=1}^n {U_i^g}^2, \quad \frac{1}{n}\sum_{i=1}^n {U_i^r}^2
\]</span>
for the blue line, green line, and red line. These are actually numbers that we can compute.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="linear-regression.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># intercept and slope of each line</span></span>
<span id="cb120-2"><a href="linear-regression.html#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (I just picked these)</span></span>
<span id="cb120-3"><a href="linear-regression.html#cb120-3" aria-hidden="true" tabindex="-1"></a>int_blue <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb120-4"><a href="linear-regression.html#cb120-4" aria-hidden="true" tabindex="-1"></a>slope_blue <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb120-5"><a href="linear-regression.html#cb120-5" aria-hidden="true" tabindex="-1"></a>int_green <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb120-6"><a href="linear-regression.html#cb120-6" aria-hidden="true" tabindex="-1"></a>slope_green <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">1.5</span></span>
<span id="cb120-7"><a href="linear-regression.html#cb120-7" aria-hidden="true" tabindex="-1"></a>int_red <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb120-8"><a href="linear-regression.html#cb120-8" aria-hidden="true" tabindex="-1"></a>slope_red <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb120-9"><a href="linear-regression.html#cb120-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-10"><a href="linear-regression.html#cb120-10" aria-hidden="true" tabindex="-1"></a><span class="co"># compute &quot;errors&quot;</span></span>
<span id="cb120-11"><a href="linear-regression.html#cb120-11" aria-hidden="true" tabindex="-1"></a>Ub <span class="ot">&lt;-</span> Y <span class="sc">-</span> int_blue <span class="sc">-</span> slope_blue<span class="sc">*</span>X</span>
<span id="cb120-12"><a href="linear-regression.html#cb120-12" aria-hidden="true" tabindex="-1"></a>Ug <span class="ot">&lt;-</span> Y <span class="sc">-</span> int_green <span class="sc">-</span> slope_green<span class="sc">*</span>X</span>
<span id="cb120-13"><a href="linear-regression.html#cb120-13" aria-hidden="true" tabindex="-1"></a>Ur <span class="ot">&lt;-</span> Y <span class="sc">-</span> int_red <span class="sc">-</span> slope_red<span class="sc">*</span>X</span>
<span id="cb120-14"><a href="linear-regression.html#cb120-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-15"><a href="linear-regression.html#cb120-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compute distances</span></span>
<span id="cb120-16"><a href="linear-regression.html#cb120-16" aria-hidden="true" tabindex="-1"></a>dist_blue <span class="ot">&lt;-</span> <span class="fu">mean</span>(Ub<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb120-17"><a href="linear-regression.html#cb120-17" aria-hidden="true" tabindex="-1"></a>dist_green <span class="ot">&lt;-</span> <span class="fu">mean</span>(Ug<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb120-18"><a href="linear-regression.html#cb120-18" aria-hidden="true" tabindex="-1"></a>dist_red <span class="ot">&lt;-</span> <span class="fu">mean</span>(Ur<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb120-19"><a href="linear-regression.html#cb120-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-20"><a href="linear-regression.html#cb120-20" aria-hidden="true" tabindex="-1"></a><span class="co"># report distances</span></span>
<span id="cb120-21"><a href="linear-regression.html#cb120-21" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(dist_blue, dist_green, dist_red), <span class="dv">3</span>)</span>
<span id="cb120-22"><a href="linear-regression.html#cb120-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   dist_blue dist_green dist_red</span></span>
<span id="cb120-23"><a href="linear-regression.html#cb120-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1     0.255      5.828   14.728</span></span></code></pre></div>
<p>This corroborates our earlier intuition — the blue line appears to fit the data much better than the other two lines.</p>
<p>It turns out that we can use the same sort of idea as above to choose not just among three possible lines to fit the data, but to choose among <em>all possible lines</em>. More generally than we have been doing, let’s write</p>
<p><span class="math display">\[
  Y_i = b_0 + b_1 X_i + U_i(b_0,b_1)
\]</span>
In other words, for any values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that we would like to plug in (say like for the green line or red line in the figure above), we can define <span class="math inline">\(U_i(b_0,b_1)\)</span> to be whatever is leftover between the line and the actual data.</p>
<p>We can choose the line (by choosing values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>) that minimizes the average distance between the line and the points. In other words, we will set</p>
<p><span class="math display">\[
  \begin{aligned}
  (\hat{\beta}_0, \hat{\beta}_1) &amp;= \underset{b_0,b_1}{\textrm{argmin}} \frac{1}{n} \sum_{i=1}^n U_i(b_0,b_1)^2 \\
  &amp;= \underset{b_0,b_1}{\textrm{argmin}} \frac{1}{n} \sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2
  \end{aligned}
\]</span>
This is a complicated looking mathematical expression, but I think the intuition is pretty easy to understand. It says that we want to find the values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that make the distance between the points and the line as small as possible on average. Whatever these values are, that is what we are going to set <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> — our estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> — to be.</p>
<p>How can you do this? One idea is to do it numerically — you could try out a ton of different combinations of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> and pick which one fits the best. Your computer could probably actually do this, but it would become quite a hard problem as we added more and more regressors.</p>
<p>It turns out that we can actually find a solution for the values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that minimize the above expression using calculus.</p>
<p>You probably recall how to minimize a function. You take its derivative, set it equal to 0, and solve that equation. [It’s actually a little more complicated than that because you need to ensure that you’re finding a minimum rather than a maximum, but the above equation is actually quadratic, so it will have a minimum.]</p>
<p>That’s all that we will do here — just the thing we need to take the derivative of looks complicated. Actually, the <span class="math inline">\(\frac{1}{n}\)</span> and <span class="math inline">\(\sum\)</span> terms will just hang around. For the rest though, we need to use the <em>chain rule</em>; that is, we need to take the derivative of the outside and then multiply by the derivative of the inside. We also need to take the derivative both with respect to <span class="math inline">\(b_0\)</span> and with respect to <span class="math inline">\(b_1\)</span>.</p>
<p>Let’s start by taking the derivative with respect to <span class="math inline">\(b_0\)</span> and setting it equal to 0.</p>
<p><span class="math display" id="eq:foc-bet0">\[
  -\frac{2}{n}\sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0 \tag{4.2}
\]</span>
where the <span class="math inline">\(2\)</span> comes from taking the deriviative of the squared part and the negative sign at the beginning comes from taking the derivative of <span class="math inline">\(-b_0\)</span> on the inside. I also replaced <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> here since <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are the values that will solve this equation.</p>
<p>Next, let’s take the derivative with respect to <span class="math inline">\(b_1\)</span>:
<span class="math display" id="eq:foc-bet1">\[
  -\frac{2}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) X_i = 0 \tag{4.3}
\]</span><br />
where the 2 comes from taking the derivative of the squared part, and the negative sign at the beginning and <span class="math inline">\(X_i\)</span> at the end come from taking the derivative of the inside with respect to <span class="math inline">\(b_1\)</span>.</p>
<p>All we have to do now is to solve these Equations <a href="linear-regression.html#eq:foc-bet0">(4.2)</a> and <a href="linear-regression.html#eq:foc-bet1">(4.3)</a> for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Before we do it, let me just mention that we are about to do some fairly challenging algebra, but that is all it is. Conceptually, it is just the same as solving a system of two equations with two unknowns that you probably did at some point in high school.</p>
<p>Starting with the first equation,</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp; \phantom{\implies} -\frac{2}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) &amp;= 0 \\
  &amp; \implies \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) &amp;= 0 \\
  &amp; \implies \bar{Y} - \hat{\beta}_0 - \hat{\beta}_1 \bar{X} &amp;= 0 \\
  &amp; \implies \boxed{\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}} 
  \end{aligned} 
\]</span>
where the second line holds by dividing both sides by <span class="math inline">\(-2\)</span>, the third line holds by pushing the summation through the sums/differences and simplifying terms, and the last line holds by rearranging to solve for <span class="math inline">\(\hat{\beta}_0\)</span>.</p>
<p>Now, let’s use the above result and Equation <a href="linear-regression.html#eq:foc-bet1">(4.3)</a> to solve for <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp; \phantom{\implies} -\frac{2}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1) X_i &amp;= 0 \\
  &amp; \implies  \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1) X_i) X_i &amp;= 0 \\
  &amp; \implies  \frac{1}{n} \sum_{i=1}^n (Y_i - (\bar{Y} - \hat{\beta}_1 \bar{X}) - \hat{\beta}_1 X_i) X_i &amp;= 0 \\
  &amp; \implies  \frac{1}{n} \sum_{i=1}^n X_i Y_i - \bar{Y} \frac{1}{n} \sum_{i=1}^n X_i + \hat{\beta}_1 \bar{X} \frac{1}{n} \sum_{i=1}^n X_i - \hat{\beta}_1 \frac{1}{n} \sum_{i=1}^n X_i^2 &amp;= 0  \\
\end{aligned}
\]</span>
Let me explain each line and then we will keep going. The second line holds because the <span class="math inline">\(-2\)</span> can just be canceled on each side, the third line holds by plugging in the expression we derived for <span class="math inline">\(\hat{\beta}_0\)</span> above, and the last line holds by splitting up the summation and bringing out terms that do not change across <span class="math inline">\(i\)</span>. Let’s keep going</p>
<p><span class="math display">\[
\begin{aligned}
  \implies  \frac{1}{n} \sum_{i=1}^n X_i Y_i - \bar{X}\bar{Y} &amp;= \hat{\beta}_1\left(\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2 \right) \\
  \implies  \hat{\beta}_1 &amp;= \frac{\frac{1}{n} \sum_{i=1}^n X_i Y_i - \bar{X}\bar{Y}}{\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2} \\
  \implies  &amp; \boxed{\hat{\beta}_1 = \frac{\widehat{\mathrm{cov}}(X,Y)}{\widehat{\mathrm{var}}(X)}}
  \end{aligned}
\]</span>
where the first line holds by using the definition of <span class="math inline">\(\bar{X}\)</span> and moving some terms to the other side. The second equality holds by dividing both sides by <span class="math inline">\(\left(\frac{1}{n} \displaystyle \sum_{i=1}^n X_i^2 - \bar{X}^2 \right)\)</span>, and the last equality holds by the definition of sample covariance and variance.</p>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> I’d like to point out one more time that our estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are generally not exactly equal to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. The figure earlier in this section was generated using simulated data where I set <span class="math inline">\(\beta_0=2\)</span> and <span class="math inline">\(\beta_1=1\)</span>. However, we estimate <span class="math inline">\(\hat{\beta}_0 = 1.75\)</span> and <span class="math inline">\(\hat{\beta}_1 = 0.95\)</span> using the simulated data in that figure. These lines are close to each other, but not exactly the same.</p>
<p>Further, recall that we defined the <strong>error term</strong> <span class="math inline">\(U_i\)</span> from</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + U_i
\]</span>
which is the difference between individual <span class="math inline">\(i\)</span>’s outcome and the <em>population</em> regression line.</p>
<p>Similarly, we define the <strong>residual</strong> <span class="math inline">\(\hat{U}_i\)</span> as</p>
<p><span class="math display">\[
  Y_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{U}_i
\]</span>
so that <span class="math inline">\(\hat{U}_i\)</span> is the difference between individual <span class="math inline">\(i\)</span>’s outcome and the estimated regression line. Moreover, since generally <span class="math inline">\(\beta_0 \neq \hat{\beta}_0\)</span> and <span class="math inline">\(\beta_1 \neq \hat{\beta}_1\)</span>, generally <span class="math inline">\(U_i \neq \hat{U}_i\)</span>.</p>
</div>
<div id="computation-6" class="section level3" number="4.10.1">
<h3><span class="header-section-number">4.10.1</span> Computation</h3>
<p>Before moving on, let’s just confirm that the formulas that we derived are actually what <code>R</code> uses in order to estimates the parameters in a regression.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="linear-regression.html#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using lm</span></span>
<span id="cb121-2"><a href="linear-regression.html#cb121-2" aria-hidden="true" tabindex="-1"></a>reg8 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data=</span>mtcars)</span>
<span id="cb121-3"><a href="linear-regression.html#cb121-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg8)</span>
<span id="cb121-4"><a href="linear-regression.html#cb121-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb121-5"><a href="linear-regression.html#cb121-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb121-6"><a href="linear-regression.html#cb121-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp, data = mtcars)</span></span>
<span id="cb121-7"><a href="linear-regression.html#cb121-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb121-8"><a href="linear-regression.html#cb121-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb121-9"><a href="linear-regression.html#cb121-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb121-10"><a href="linear-regression.html#cb121-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -5.7121 -2.1122 -0.8854  1.5819  8.2360 </span></span>
<span id="cb121-11"><a href="linear-regression.html#cb121-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb121-12"><a href="linear-regression.html#cb121-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb121-13"><a href="linear-regression.html#cb121-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb121-14"><a href="linear-regression.html#cb121-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***</span></span>
<span id="cb121-15"><a href="linear-regression.html#cb121-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.06823    0.01012  -6.742 1.79e-07 ***</span></span>
<span id="cb121-16"><a href="linear-regression.html#cb121-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb121-17"><a href="linear-regression.html#cb121-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb121-18"><a href="linear-regression.html#cb121-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb121-19"><a href="linear-regression.html#cb121-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.863 on 30 degrees of freedom</span></span>
<span id="cb121-20"><a href="linear-regression.html#cb121-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 </span></span>
<span id="cb121-21"><a href="linear-regression.html#cb121-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07</span></span>
<span id="cb121-22"><a href="linear-regression.html#cb121-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-23"><a href="linear-regression.html#cb121-23" aria-hidden="true" tabindex="-1"></a><span class="co"># using our formulas</span></span>
<span id="cb121-24"><a href="linear-regression.html#cb121-24" aria-hidden="true" tabindex="-1"></a>bet1 <span class="ot">&lt;-</span> <span class="fu">cov</span>(mtcars<span class="sc">$</span>mpg, mtcars<span class="sc">$</span>hp) <span class="sc">/</span> <span class="fu">var</span>(mtcars<span class="sc">$</span>hp)</span>
<span id="cb121-25"><a href="linear-regression.html#cb121-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-26"><a href="linear-regression.html#cb121-26" aria-hidden="true" tabindex="-1"></a>bet0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(mtcars<span class="sc">$</span>mpg) <span class="sc">-</span> bet1<span class="sc">*</span><span class="fu">mean</span>(mtcars<span class="sc">$</span>hp)</span>
<span id="cb121-27"><a href="linear-regression.html#cb121-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-28"><a href="linear-regression.html#cb121-28" aria-hidden="true" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb121-29"><a href="linear-regression.html#cb121-29" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">bet0=</span>bet0, <span class="at">bet1=</span>bet1)</span>
<span id="cb121-30"><a href="linear-regression.html#cb121-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       bet0        bet1</span></span>
<span id="cb121-31"><a href="linear-regression.html#cb121-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 30.09886 -0.06822828</span></span></code></pre></div>
<p>and they are identical.</p>
</div>
<div id="more-than-one-regressor" class="section level3" number="4.10.2">
<h3><span class="header-section-number">4.10.2</span> More than one regressor</h3>
<p>Now, let’s suppose that you want to estimate a more complicated regressions like</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\]</span></p>
<p>or, even more generally,</p>
<p><span class="math display">\[
  Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k + U
\]</span>
We can still choose the line that best fits the data by solving</p>
<p><span class="math display">\[
  (\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_k) = \underset{b_0,b_1,\ldots,b_k}{\textrm{argmin}} \frac{1}{n} \sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2
\]</span>
To do it, we would need to take the derivative with respect to <span class="math inline">\(b_0, b_1, \ldots, b_k\)</span>. This will give a system of equations with <span class="math inline">\((k+1)\)</span> equations and <span class="math inline">\((k+1)\)</span> unknowns. You can solve this — it actually is quite easy / very similar to what we just did if you know just a little bit of linear algebra, but this is beyond the scope of our course — and it is quite easy for the computer to solve.</p>
</div>
</div>
<div id="inference" class="section level2" number="4.11">
<h2><span class="header-section-number">4.11</span> Inference</h2>
<p>SW 4.5, 5.1, 5.2, 6.6</p>
<p>We discussed in class the practical issues of inference in linear regression models.</p>
<p>These results rely on arguments building on the Central Limit Theorem (this should not surprise you as it is similar to the case for the asymptotic distribution of <span class="math inline">\(\sqrt{n}(\bar{Y} - \mathbb{E}[Y]))\)</span> that we discussed earlier in the semester.</p>
<p>In this section, I sketch these types of arguments for you. This material is advanced, but I suggest that you study this material.</p>
<p>We are going to show that, in the simple linear regression model,
<span class="math display">\[\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) \rightarrow N(0,V) \quad \textrm{as} \ n \rightarrow \infty
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
  V = \frac{\mathbb{E}[(X-\mathbb{E}[X])^2 U^2]}{\mathrm{var}(X)^2}
\end{align*}\]</span>
and discuss how to use this result to conduct inference.</p>
<p>Let’s start by showing why this result holds.</p>
<p>To start with, recall that
<span class="math display" id="eq:b1">\[\begin{align}
  \hat{\beta}_1 = \frac{\widehat{\mathrm{cov}}(X,Y)}{\widehat{\mathrm{var}}(X)} \tag{4.4}
\end{align}\]</span></p>
<p>Before providing a main result, let’s start with noting the following:</p>
<p><em>Helpful Intermediate Result 1</em>
Notice that
<span class="math display">\[\begin{align*}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})\bar{Y}\Big) &amp;= \bar{Y} \frac{1}{n}\sum_{i=1}^n \Big( X_i-\bar{X} \Big) \\
  &amp;= \bar{Y} \left( \frac{1}{n}\sum_{i=1}^n X_i - \frac{1}{n}\sum_{i=1}^n \bar{X} \right) \\
  &amp;= \bar{Y} \Big(\bar{X} - \bar{X} \Big) \\
  &amp;= 0
\end{align*}\]</span>
where the first equality just pulls <span class="math inline">\(\bar{Y}\)</span> out of the summation (it is a constant with respect to the summation), the second equality pushes the summation through the difference, the first part of the third equality holds by the definition of <span class="math inline">\(\bar{X}\)</span> and the second part holds because it is an average of a constant.</p>
<p>This implies that
<span class="math display" id="eq:hr1">\[\begin{align}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})(Y_i - \bar{Y})\Big) = \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})Y_i\Big) \tag{4.5}
\end{align}\]</span>
and very similar arguments (basically the same arguments in reverse) also imply that
<span class="math display" id="eq:hr2">\[\begin{align}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})X_i\Big) = \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})(X_i - \bar{X})\Big) \tag{4.6}
\end{align}\]</span>
We use both <a href="linear-regression.html#eq:hr1">(4.5)</a> and <a href="linear-regression.html#eq:hr2">(4.6)</a> below.</p>
<p>Next, consider the numerator in <a href="linear-regression.html#eq:b1">(4.4)</a>
<span class="math display">\[\begin{align*}
  \widehat{\mathrm{cov}}(X,Y) &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y}) \\
  &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})Y_i \\
  &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(\beta_0 + \beta_1 X_i + U_i) \\
  &amp;= \underbrace{\beta_0 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})}_{(A)} + \underbrace{\beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) X_i}_{(B)} + \underbrace{\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) U_i}_{(C)}) \\
\end{align*}\]</span>
where the first equality holds by the definition of sample covariance, the second equality holds by <a href="linear-regression.html#eq:hr1">(4.5)</a>, the third equality plugs in for <span class="math inline">\(Y_i\)</span>, and the last equality combines terms and passes the summation through the additions/subtractions.</p>
<p>Now, let’s consider each of these in turn.</p>
<p>For (A),
<span class="math display">\[\begin{align*}
  \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} \qquad \textrm{and} \qquad \frac{1}{n} \sum_{i=1}^n \bar{X} = \bar{X}
\end{align*}\]</span>
which implies that this term is equal to 0.</p>
<p>For (B), notice that
<span class="math display">\[\begin{align*}
  \beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) X_i &amp;= \beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) (X_i - \bar{X}) \\
  &amp;= \beta_1 \widehat{\mathrm{var}}(X)
\end{align*}\]</span>
where the first equality holds by <a href="linear-regression.html#eq:hr2">(4.6)</a> and the second equality holds by the definition of sample variance.</p>
<p>For (C), well, we’ll just carry that one around for now.</p>
<p>Plugging in the expressions for (A), (B), and (C) back into Equation <a href="linear-regression.html#eq:b1">(4.4)</a> implies that
<span class="math display">\[\begin{align*}
  \hat{\beta}_1 = \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{(X_i - \bar{X}) U_i}{\widehat{\mathrm{var}}(X)}
\end{align*}\]</span>
Next, re-arranging terms and multiplying both sides by <span class="math inline">\(\sqrt{n}\)</span> implies that
<span class="math display">\[\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) &amp;= \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \frac{(X_i - \bar{X}) U_i}{\widehat{\mathrm{var}}(X)}\right) \\
  &amp; \approx \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \frac{(X_i - \mathbb{E}[X]) U_i}{\mathrm{var}(X)}\right)
\end{align*}\]</span>
The last line (the approximately one) is kind of a weak argument, but basically you can replace <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\widehat{\mathrm{var}}(X)\)</span> and the effect of this replacement will converge to 0 in large samples (this is the reason for the approximately) — if you want a more complete explanation, sign up for my graduate econometrics class next semester.</p>
<p>Is this helpful? It may not be obvious, but the right hand side of the above equation is actually something that we can apply the Central Limit Theorem to. In particular, maybe it is helpful to define <span class="math inline">\(Z_i = \frac{(X_i - \mathbb{E}[X]) U_i}{\mathrm{var}(X)}\)</span>. We know that we could apply a Central Limit Theorem to <span class="math inline">\(\sqrt{n}\left( \frac{1}{n} \sum_{i=1}^n Z_i \right)\)</span> if (i) <span class="math inline">\(Z_i\)</span> had mean 0, and (ii) it is iid. That it is iid holds immediately from the random sampling assumption. For mean 0,
<span class="math display">\[\begin{align*}
  \mathbb{E}[Z] &amp;= \mathbb{E}\left[ \frac{(X - \mathbb{E}[X]) U}{\mathrm{var}(X)}\right] \\
  &amp;= \frac{1}{\mathrm{var}(X)} \mathbb{E}[(X - \mathbb{E}[X]) U] \\
  &amp;= \frac{1}{\mathrm{var}(X)} \mathbb{E}[(X - \mathbb{E}[X]) \underbrace{\mathbb{E}[U|X]}_{=0}] \\
  &amp;= 0
\end{align*}\]</span>
where the only challenging line here is the third one holds from the Law of Iterated Expectations. This means that we can apply the central limit theorem, and in particular,
<span class="math inline">\(\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n Z_i \right) \rightarrow N(0,V)\)</span> where <span class="math inline">\(V=\mathrm{var}(Z) = \mathbb{E}[Z^2]\)</span> (where the 2nd equality here holds because <span class="math inline">\(Z\)</span> has mean 0). Now, just substituting back in for <span class="math inline">\(Z\)</span> implies that
<span class="math display">\[\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) \rightarrow N(0,V)
\end{align*}\]</span>
where
<span class="math display" id="eq:V">\[\begin{align}
  V &amp;= \mathbb{E}\left[ \left( \frac{(X - \mathbb{E}[X]) U}{\mathrm{var}(X)} \right)^2 \right] \nonumber \\
  &amp;= \mathbb{E}\left[ \frac{(X - \mathbb{E}[X])^2 U^2}{\mathrm{var}(X)^2}\right] \tag{4.7}
\end{align}\]</span>
which is what we were aiming for.</p>
<p>Given this result, all our previous work on standard errors, t-statistics, p-values, and confidence intervals applies. First, let me mention the way that you would estimate <span class="math inline">\(V\)</span> (same as always, just replace the population quantities with corresponding sample quantities).</p>
<p><span class="math display">\[
  \hat{V} = \frac{ \frac{1}{n} \displaystyle \sum_{i=1}^n (X_i - \bar{X})^2 \hat{U}_i^2}{\widehat{\mathrm{var}}(X)^2}
\]</span></p>
<p>where <span class="math inline">\(\hat{U}_i\)</span> are the residuals.</p>
<p>Now, standard errors are just the same as before (the only difference is that <span class="math inline">\(\hat{V}\)</span> itself has changed)</p>
<p><span class="math display">\[
\begin{aligned}
  \textrm{s.e.}(\hat{\beta}) &amp;= \frac{\sqrt{\hat{V}}}{\sqrt{n}}
\end{aligned}
\]</span></p>
<p>By far the most common null hypothesis is <span class="math inline">\(H_0: \beta = 0\)</span>, which suggests the following t-statistic:</p>
<p><span class="math display">\[
  t = \frac{\hat{\beta}}{\textrm{s.e.}(\hat{\beta})}
\]</span>
One can continue to calculate a p-value by</p>
<p><span class="math display">\[
  \textrm{p-value} = 2 \Phi(-|t|)
\]</span>
and a 95% confidence interval is given by</p>
<p><span class="math display">\[
  CI = [\hat{\beta} - 1.96 \textrm{s.e.}(\hat{\beta}), \hat{\beta} + 1.96 \textrm{s.e.}(\hat{\beta})]
\]</span></p>
<div id="computation-7" class="section level3" number="4.11.1">
<h3><span class="header-section-number">4.11.1</span> Computation</h3>
<p>Let’s check if what we derived is what we can compute using <code>R</code>.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="linear-regression.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is the same regression as in the previous section</span></span>
<span id="cb122-2"><a href="linear-regression.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg8)</span>
<span id="cb122-3"><a href="linear-regression.html#cb122-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb122-4"><a href="linear-regression.html#cb122-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb122-5"><a href="linear-regression.html#cb122-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ hp, data = mtcars)</span></span>
<span id="cb122-6"><a href="linear-regression.html#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb122-7"><a href="linear-regression.html#cb122-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb122-8"><a href="linear-regression.html#cb122-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb122-9"><a href="linear-regression.html#cb122-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -5.7121 -2.1122 -0.8854  1.5819  8.2360 </span></span>
<span id="cb122-10"><a href="linear-regression.html#cb122-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb122-11"><a href="linear-regression.html#cb122-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb122-12"><a href="linear-regression.html#cb122-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb122-13"><a href="linear-regression.html#cb122-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***</span></span>
<span id="cb122-14"><a href="linear-regression.html#cb122-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.06823    0.01012  -6.742 1.79e-07 ***</span></span>
<span id="cb122-15"><a href="linear-regression.html#cb122-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb122-16"><a href="linear-regression.html#cb122-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb122-17"><a href="linear-regression.html#cb122-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb122-18"><a href="linear-regression.html#cb122-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 3.863 on 30 degrees of freedom</span></span>
<span id="cb122-19"><a href="linear-regression.html#cb122-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 </span></span>
<span id="cb122-20"><a href="linear-regression.html#cb122-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07</span></span>
<span id="cb122-21"><a href="linear-regression.html#cb122-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-22"><a href="linear-regression.html#cb122-22" aria-hidden="true" tabindex="-1"></a><span class="co"># show previous calcuations</span></span>
<span id="cb122-23"><a href="linear-regression.html#cb122-23" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">bet0=</span>bet0, <span class="at">bet1=</span>bet1)</span>
<span id="cb122-24"><a href="linear-regression.html#cb122-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       bet0        bet1</span></span>
<span id="cb122-25"><a href="linear-regression.html#cb122-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 30.09886 -0.06822828</span></span>
<span id="cb122-26"><a href="linear-regression.html#cb122-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-27"><a href="linear-regression.html#cb122-27" aria-hidden="true" tabindex="-1"></a><span class="co"># components of Vhat</span></span>
<span id="cb122-28"><a href="linear-regression.html#cb122-28" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> mtcars<span class="sc">$</span>mpg</span>
<span id="cb122-29"><a href="linear-regression.html#cb122-29" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> mtcars<span class="sc">$</span>hp</span>
<span id="cb122-30"><a href="linear-regression.html#cb122-30" aria-hidden="true" tabindex="-1"></a>Uhat <span class="ot">&lt;-</span> Y <span class="sc">-</span> bet0 <span class="sc">-</span> bet1<span class="sc">*</span>X</span>
<span id="cb122-31"><a href="linear-regression.html#cb122-31" aria-hidden="true" tabindex="-1"></a>Xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb122-32"><a href="linear-regression.html#cb122-32" aria-hidden="true" tabindex="-1"></a>varX <span class="ot">&lt;-</span> <span class="fu">mean</span>( (X<span class="sc">-</span>Xbar)<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb122-33"><a href="linear-regression.html#cb122-33" aria-hidden="true" tabindex="-1"></a>Vhat <span class="ot">&lt;-</span> <span class="fu">mean</span>( (X<span class="sc">-</span>Xbar)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> Uhat<span class="sc">^</span><span class="dv">2</span> ) <span class="sc">/</span> ( varX<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb122-34"><a href="linear-regression.html#cb122-34" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(mtcars)</span>
<span id="cb122-35"><a href="linear-regression.html#cb122-35" aria-hidden="true" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(Vhat)<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb122-36"><a href="linear-regression.html#cb122-36" aria-hidden="true" tabindex="-1"></a>t_stat <span class="ot">&lt;-</span> bet1<span class="sc">/</span>se</span>
<span id="cb122-37"><a href="linear-regression.html#cb122-37" aria-hidden="true" tabindex="-1"></a>p_val <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">abs</span>(t_stat))</span>
<span id="cb122-38"><a href="linear-regression.html#cb122-38" aria-hidden="true" tabindex="-1"></a>ci_L <span class="ot">&lt;-</span> bet1 <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb122-39"><a href="linear-regression.html#cb122-39" aria-hidden="true" tabindex="-1"></a>ci_U <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>se</span>
<span id="cb122-40"><a href="linear-regression.html#cb122-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-41"><a href="linear-regression.html#cb122-41" aria-hidden="true" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb122-42"><a href="linear-regression.html#cb122-42" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(se, t_stat, p_val, ci_L, ci_U),<span class="dv">5</span>)</span>
<span id="cb122-43"><a href="linear-regression.html#cb122-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        se   t_stat p_val     ci_L     ci_U</span></span>
<span id="cb122-44"><a href="linear-regression.html#cb122-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 0.01313 -5.19644     0 -0.09396 -0.04249</span></span></code></pre></div>
<p>Interestingly, these are not <em>exactly</em> the same as what comes from the <code>lm</code> command. Here’s what the difference is: <code>R</code> makes a simplifying assumption called “homoskedasticity” that simplifies the expression for the variance. This can result in slightly different standard errors (and therefore slightly different t-statistics, p-values, and confidence intervals too) than the ones we calculated.</p>
<p>An alternative package that is popular among economists for estimating regressions and getting “heteroskedasticity robust” standard errors is the <code>estimatr</code> package.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="linear-regression.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span>
<span id="cb123-2"><a href="linear-regression.html#cb123-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-3"><a href="linear-regression.html#cb123-3" aria-hidden="true" tabindex="-1"></a>reg9 <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(mpg <span class="sc">~</span> hp, <span class="at">data=</span>mtcars, <span class="at">se_type=</span><span class="st">&quot;HC0&quot;</span>)</span>
<span id="cb123-4"><a href="linear-regression.html#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg9)</span>
<span id="cb123-5"><a href="linear-regression.html#cb123-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb123-6"><a href="linear-regression.html#cb123-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb123-7"><a href="linear-regression.html#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm_robust(formula = mpg ~ hp, data = mtcars, se_type = &quot;HC0&quot;)</span></span>
<span id="cb123-8"><a href="linear-regression.html#cb123-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb123-9"><a href="linear-regression.html#cb123-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Standard error type:  HC0 </span></span>
<span id="cb123-10"><a href="linear-regression.html#cb123-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb123-11"><a href="linear-regression.html#cb123-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb123-12"><a href="linear-regression.html#cb123-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper DF</span></span>
<span id="cb123-13"><a href="linear-regression.html#cb123-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 30.09886    2.01067  14.970 1.851e-15 25.99252 34.20520 30</span></span>
<span id="cb123-14"><a href="linear-regression.html#cb123-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; hp          -0.06823    0.01313  -5.196 1.338e-05 -0.09504 -0.04141 30</span></span>
<span id="cb123-15"><a href="linear-regression.html#cb123-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb123-16"><a href="linear-regression.html#cb123-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.6024 ,    Adjusted R-squared:  0.5892 </span></span>
<span id="cb123-17"><a href="linear-regression.html#cb123-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic:    27 on 1 and 30 DF,  p-value: 1.338e-05</span></span></code></pre></div>
<p>The “HC0” standard errors are “heteroskedasticity consistent” standard errors, and you can see that they match what we calculated above.</p>
</div>
</div>
<div id="lab-3-birthweight-and-smoking" class="section level2" number="4.12">
<h2><span class="header-section-number">4.12</span> Lab 3: Birthweight and Smoking</h2>
<p>For this lab, we’ll use the data <code>Birthweight_Smoking</code> and study the relationship between infant birthweight and mother’s smoking behavior.</p>
<ol style="list-style-type: decimal">
<li><p>Run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>. How do you interpret the results?</p></li>
<li><p>Use the <code>datasummary_balance</code> function from the <code>modelsummary</code> package to provide summary statistics for each variable in the data separately by smoking status of the mother. Do you notice any interesting patterns?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>, <span class="math inline">\(educ\)</span>, <span class="math inline">\(nprevisit\)</span>, <span class="math inline">\(age\)</span>, and <span class="math inline">\(alcohol\)</span>. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span>? How does its magnitude compare to the result from #1? What do you make of this?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>, the interaction of <span class="math inline">\(smoker\)</span> and <span class="math inline">\(age\)</span> and the other covariates (including <span class="math inline">\(age\)</span>) from #3. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span> and the coefficient on the interaction term?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(smoker\)</span>, the interaction of <span class="math inline">\(smoker\)</span> and <span class="math inline">\(alcohol\)</span> and the other covariates from #3. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span> and the coefficient on the interaction term?</p></li>
<li><p>Now run a regression of <span class="math inline">\(birthweight\)</span> on <span class="math inline">\(age\)</span> and <span class="math inline">\(age^2\)</span>. Plot the predicted value of birthweight as a function of age for ages from 18 to 44. What do you make of this?</p></li>
<li><p>Now run a regression of <span class="math inline">\(\log(birthweight)\)</span> on <span class="math inline">\(smoker\)</span> and the other covariates from #3. How do you interpret the coefficient on <span class="math inline">\(smoker\)</span>?</p></li>
</ol>
</div>
<div id="lab-3-solutions" class="section level2" number="4.13">
<h2><span class="header-section-number">4.13</span> Lab 3: Solutions</h2>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="linear-regression.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb124-2"><a href="linear-regression.html#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(haven)</span>
<span id="cb124-3"><a href="linear-regression.html#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb124-4"><a href="linear-regression.html#cb124-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb124-5"><a href="linear-regression.html#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb124-6"><a href="linear-regression.html#cb124-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-7"><a href="linear-regression.html#cb124-7" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb124-8"><a href="linear-regression.html#cb124-8" aria-hidden="true" tabindex="-1"></a>Birthweight_Smoking <span class="ot">&lt;-</span> <span class="fu">read_dta</span>(<span class="st">&quot;data/birthweight_smoking.dta&quot;</span>)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="linear-regression.html#cb125-1" aria-hidden="true" tabindex="-1"></a>reg1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker, <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb125-2"><a href="linear-regression.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg1)</span>
<span id="cb125-3"><a href="linear-regression.html#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-4"><a href="linear-regression.html#cb125-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb125-5"><a href="linear-regression.html#cb125-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker, data = Birthweight_Smoking)</span></span>
<span id="cb125-6"><a href="linear-regression.html#cb125-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-7"><a href="linear-regression.html#cb125-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb125-8"><a href="linear-regression.html#cb125-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb125-9"><a href="linear-regression.html#cb125-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -3007.06  -313.06    26.94   366.94  2322.94 </span></span>
<span id="cb125-10"><a href="linear-regression.html#cb125-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-11"><a href="linear-regression.html#cb125-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb125-12"><a href="linear-regression.html#cb125-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb125-13"><a href="linear-regression.html#cb125-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  3432.06      11.87 289.115   &lt;2e-16 ***</span></span>
<span id="cb125-14"><a href="linear-regression.html#cb125-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; smoker       -253.23      26.95  -9.396   &lt;2e-16 ***</span></span>
<span id="cb125-15"><a href="linear-regression.html#cb125-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb125-16"><a href="linear-regression.html#cb125-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb125-17"><a href="linear-regression.html#cb125-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb125-18"><a href="linear-regression.html#cb125-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 583.7 on 2998 degrees of freedom</span></span>
<span id="cb125-19"><a href="linear-regression.html#cb125-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.0286, Adjusted R-squared:  0.02828 </span></span>
<span id="cb125-20"><a href="linear-regression.html#cb125-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 88.28 on 1 and 2998 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We estimate that, on average, smoking reduces an infant’s birthweight by about 250 grams. The estimated effect is strongly statistically significant, and (I am not an expert but) that seems like a large effect of smoking to me.</p>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="linear-regression.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create smoker factor --- just to make table look nicer</span></span>
<span id="cb126-2"><a href="linear-regression.html#cb126-2" aria-hidden="true" tabindex="-1"></a>Birthweight_Smoking<span class="sc">$</span>smoker_factor <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(Birthweight_Smoking<span class="sc">$</span>smoker<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;smoker&quot;</span>, <span class="st">&quot;non-smoker&quot;</span>))</span>
<span id="cb126-3"><a href="linear-regression.html#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="fu">datasummary_balance</span>(<span class="sc">~</span>smoker_factor, </span>
<span id="cb126-4"><a href="linear-regression.html#cb126-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data=</span>dplyr<span class="sc">::</span><span class="fu">select</span>(Birthweight_Smoking, <span class="sc">-</span>smoker),</span>
<span id="cb126-5"><a href="linear-regression.html#cb126-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">fmt=</span><span class="dv">2</span>)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
non-smoker (N=2418)
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
smoker (N=582)
</div>
</th>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="2">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std. Dev.
</th>
<th style="text-align:right;">
Diff. in Means
</th>
<th style="text-align:right;">
Std. Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
nprevist
</td>
<td style="text-align:right;">
11.19
</td>
<td style="text-align:right;">
3.50
</td>
<td style="text-align:right;">
10.18
</td>
<td style="text-align:right;">
4.23
</td>
<td style="text-align:right;">
-1.01
</td>
<td style="text-align:right;">
0.19
</td>
</tr>
<tr>
<td style="text-align:left;">
alcohol
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre1
</td>
<td style="text-align:right;">
0.83
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
-0.13
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre2
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
0.34
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.41
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre3
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.24
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
tripre0
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
birthweight
</td>
<td style="text-align:right;">
3432.06
</td>
<td style="text-align:right;">
584.62
</td>
<td style="text-align:right;">
3178.83
</td>
<td style="text-align:right;">
580.01
</td>
<td style="text-align:right;">
-253.23
</td>
<td style="text-align:right;">
26.82
</td>
</tr>
<tr>
<td style="text-align:left;">
unmarried
</td>
<td style="text-align:right;">
0.18
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;">
educ
</td>
<td style="text-align:right;">
13.15
</td>
<td style="text-align:right;">
2.21
</td>
<td style="text-align:right;">
11.88
</td>
<td style="text-align:right;">
1.62
</td>
<td style="text-align:right;">
-1.27
</td>
<td style="text-align:right;">
0.08
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
27.27
</td>
<td style="text-align:right;">
5.37
</td>
<td style="text-align:right;">
25.32
</td>
<td style="text-align:right;">
5.06
</td>
<td style="text-align:right;">
-1.95
</td>
<td style="text-align:right;">
0.24
</td>
</tr>
<tr>
<td style="text-align:left;">
drinks
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
0.47
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
1.23
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.05
</td>
</tr>
</tbody>
</table>
<p>The things that stand out to me are:</p>
<ul>
<li><p>Birthweight tends to be notably lower for smokers relative to non-smokers. The difference is about 7.4% lower birthweight for babies whose mothers smoked.</p></li>
<li><p>That said, smoking is also correlated with a number of other things that could be related to lower birthweights. Mothers who smoke went to fewer pre-natal visits on average, were more likely to be unmarried, were more likely to have drink alcohol during their pregnancy, were more likely to be less educated. They also were, on average, somewhat younger than mothers who did not smoke.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="linear-regression.html#cb127-1" aria-hidden="true" tabindex="-1"></a>reg3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb127-2"><a href="linear-regression.html#cb127-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb127-3"><a href="linear-regression.html#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg3)</span>
<span id="cb127-4"><a href="linear-regression.html#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb127-5"><a href="linear-regression.html#cb127-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb127-6"><a href="linear-regression.html#cb127-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker + educ + nprevist + age + alcohol, </span></span>
<span id="cb127-7"><a href="linear-regression.html#cb127-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     data = Birthweight_Smoking)</span></span>
<span id="cb127-8"><a href="linear-regression.html#cb127-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb127-9"><a href="linear-regression.html#cb127-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb127-10"><a href="linear-regression.html#cb127-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb127-11"><a href="linear-regression.html#cb127-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2728.91  -305.26    24.69   359.63  2220.42 </span></span>
<span id="cb127-12"><a href="linear-regression.html#cb127-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb127-13"><a href="linear-regression.html#cb127-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb127-14"><a href="linear-regression.html#cb127-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb127-15"><a href="linear-regression.html#cb127-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 2924.963     74.185  39.428  &lt; 2e-16 ***</span></span>
<span id="cb127-16"><a href="linear-regression.html#cb127-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; smoker      -206.507     27.367  -7.546 5.93e-14 ***</span></span>
<span id="cb127-17"><a href="linear-regression.html#cb127-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; educ           5.644      5.532   1.020    0.308    </span></span>
<span id="cb127-18"><a href="linear-regression.html#cb127-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nprevist      32.979      2.914  11.318  &lt; 2e-16 ***</span></span>
<span id="cb127-19"><a href="linear-regression.html#cb127-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age            2.360      2.178   1.083    0.279    </span></span>
<span id="cb127-20"><a href="linear-regression.html#cb127-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; alcohol      -39.512     76.365  -0.517    0.605    </span></span>
<span id="cb127-21"><a href="linear-regression.html#cb127-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb127-22"><a href="linear-regression.html#cb127-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb127-23"><a href="linear-regression.html#cb127-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb127-24"><a href="linear-regression.html#cb127-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 570.3 on 2994 degrees of freedom</span></span>
<span id="cb127-25"><a href="linear-regression.html#cb127-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07402,    Adjusted R-squared:  0.07247 </span></span>
<span id="cb127-26"><a href="linear-regression.html#cb127-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 47.86 on 5 and 2994 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Here we estimate that smoking reduces an infant’s birthweight by about 200 grams on average holding education, number of pre-natal visits, age, and whether or not the mother consumed alcohol constant. The magnitude of the estimated effect is somewhat smaller than the previous estimate. Due to the discussion in #2 (particularly, that smoking was correlated with a number of other characteristics that are likely associated with lower birthweights), this decrease in the magnitude is not surprising.</p>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="linear-regression.html#cb128-1" aria-hidden="true" tabindex="-1"></a>reg4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker <span class="sc">+</span> <span class="fu">I</span>(smoker<span class="sc">*</span>age) <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb128-2"><a href="linear-regression.html#cb128-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb128-3"><a href="linear-regression.html#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg4)</span>
<span id="cb128-4"><a href="linear-regression.html#cb128-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-5"><a href="linear-regression.html#cb128-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb128-6"><a href="linear-regression.html#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker + I(smoker * age) + educ + </span></span>
<span id="cb128-7"><a href="linear-regression.html#cb128-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     nprevist + age + alcohol, data = Birthweight_Smoking)</span></span>
<span id="cb128-8"><a href="linear-regression.html#cb128-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-9"><a href="linear-regression.html#cb128-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb128-10"><a href="linear-regression.html#cb128-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb128-11"><a href="linear-regression.html#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2722.56  -305.12    23.93   363.43  2244.67 </span></span>
<span id="cb128-12"><a href="linear-regression.html#cb128-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-13"><a href="linear-regression.html#cb128-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb128-14"><a href="linear-regression.html#cb128-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb128-15"><a href="linear-regression.html#cb128-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)     2853.819     77.104  37.013  &lt; 2e-16 ***</span></span>
<span id="cb128-16"><a href="linear-regression.html#cb128-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; smoker           231.578    134.854   1.717 0.086036 .  </span></span>
<span id="cb128-17"><a href="linear-regression.html#cb128-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; I(smoker * age)  -17.145      5.168  -3.317 0.000919 ***</span></span>
<span id="cb128-18"><a href="linear-regression.html#cb128-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; educ               4.895      5.528   0.885 0.375968    </span></span>
<span id="cb128-19"><a href="linear-regression.html#cb128-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nprevist          32.482      2.913  11.151  &lt; 2e-16 ***</span></span>
<span id="cb128-20"><a href="linear-regression.html#cb128-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age                5.528      2.375   2.328 0.019999 *  </span></span>
<span id="cb128-21"><a href="linear-regression.html#cb128-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; alcohol          -22.556     76.409  -0.295 0.767864    </span></span>
<span id="cb128-22"><a href="linear-regression.html#cb128-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb128-23"><a href="linear-regression.html#cb128-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb128-24"><a href="linear-regression.html#cb128-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb128-25"><a href="linear-regression.html#cb128-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 569.4 on 2993 degrees of freedom</span></span>
<span id="cb128-26"><a href="linear-regression.html#cb128-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07741,    Adjusted R-squared:  0.07556 </span></span>
<span id="cb128-27"><a href="linear-regression.html#cb128-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 41.85 on 6 and 2993 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We should be careful about the interpretatio here. We have estimated a model like</p>
<p><span class="math display">\[
  \mathbb{E}[Birthweight|Smoker, Age, X] = \beta_0 + \beta_1 Smoker + \beta_2 Smoker \cdot Age + \cdots
\]</span>
Therefore, the partial effect of smoking is given by</p>
<p><span class="math display">\[
  \mathbb{E}[Birthweight | Smoker=1, Age, X] - \mathbb{E}[Birthweight | Smoker=0, Age, X] = \beta_1 + \beta_2 Age
\]</span>
Therefore, the partial effect of smoking depends on <span class="math inline">\(Age\)</span>. For example, for <span class="math inline">\(Age=18\)</span>, the partial effect is <span class="math inline">\(\beta_1 + \beta_2 (18)\)</span>. For <span class="math inline">\(Age=25\)</span>, the partial effect is <span class="math inline">\(\beta_1 + \beta_2 (25)\)</span>, and for <span class="math inline">\(Age=35\)</span>, the partial effect is <span class="math inline">\(\beta_1 + \beta_2 (35)\)</span>. Let’s calculate the partial effect at each of those ages.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="linear-regression.html#cb129-1" aria-hidden="true" tabindex="-1"></a>bet1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(reg4)[<span class="dv">2</span>]</span>
<span id="cb129-2"><a href="linear-regression.html#cb129-2" aria-hidden="true" tabindex="-1"></a>bet2 <span class="ot">&lt;-</span> <span class="fu">coef</span>(reg4)[<span class="dv">3</span>]</span>
<span id="cb129-3"><a href="linear-regression.html#cb129-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-4"><a href="linear-regression.html#cb129-4" aria-hidden="true" tabindex="-1"></a>pe_18 <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> bet2<span class="sc">*</span><span class="dv">18</span></span>
<span id="cb129-5"><a href="linear-regression.html#cb129-5" aria-hidden="true" tabindex="-1"></a>pe_25 <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> bet2<span class="sc">*</span><span class="dv">25</span></span>
<span id="cb129-6"><a href="linear-regression.html#cb129-6" aria-hidden="true" tabindex="-1"></a>pe_35 <span class="ot">&lt;-</span> bet1 <span class="sc">+</span> bet2<span class="sc">*</span><span class="dv">35</span></span>
<span id="cb129-7"><a href="linear-regression.html#cb129-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-8"><a href="linear-regression.html#cb129-8" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cbind.data.frame</span>(pe_18, pe_25, pe_35),<span class="dv">2</span>)</span>
<span id="cb129-9"><a href="linear-regression.html#cb129-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;         pe_18   pe_25   pe_35</span></span>
<span id="cb129-10"><a href="linear-regression.html#cb129-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; smoker -77.04 -197.05 -368.51</span></span></code></pre></div>
<p>This suggests substantially larger effects of smoking on birthweight for older mothers.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="linear-regression.html#cb130-1" aria-hidden="true" tabindex="-1"></a>reg5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> smoker <span class="sc">+</span> <span class="fu">I</span>(smoker<span class="sc">*</span>alcohol) <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb130-2"><a href="linear-regression.html#cb130-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb130-3"><a href="linear-regression.html#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg5)</span>
<span id="cb130-4"><a href="linear-regression.html#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-5"><a href="linear-regression.html#cb130-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb130-6"><a href="linear-regression.html#cb130-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ smoker + I(smoker * alcohol) + educ + </span></span>
<span id="cb130-7"><a href="linear-regression.html#cb130-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     nprevist + age + alcohol, data = Birthweight_Smoking)</span></span>
<span id="cb130-8"><a href="linear-regression.html#cb130-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-9"><a href="linear-regression.html#cb130-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb130-10"><a href="linear-regression.html#cb130-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb130-11"><a href="linear-regression.html#cb130-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2728.99  -304.16    24.54   359.92  2222.10 </span></span>
<span id="cb130-12"><a href="linear-regression.html#cb130-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-13"><a href="linear-regression.html#cb130-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb130-14"><a href="linear-regression.html#cb130-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb130-15"><a href="linear-regression.html#cb130-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)         2924.844     74.185  39.426  &lt; 2e-16 ***</span></span>
<span id="cb130-16"><a href="linear-regression.html#cb130-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; smoker              -201.852     27.765  -7.270 4.57e-13 ***</span></span>
<span id="cb130-17"><a href="linear-regression.html#cb130-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; I(smoker * alcohol) -151.860    152.717  -0.994    0.320    </span></span>
<span id="cb130-18"><a href="linear-regression.html#cb130-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; educ                   5.612      5.532   1.014    0.310    </span></span>
<span id="cb130-19"><a href="linear-regression.html#cb130-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nprevist              32.844      2.917  11.260  &lt; 2e-16 ***</span></span>
<span id="cb130-20"><a href="linear-regression.html#cb130-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age                    2.403      2.178   1.103    0.270    </span></span>
<span id="cb130-21"><a href="linear-regression.html#cb130-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; alcohol               39.824    110.440   0.361    0.718    </span></span>
<span id="cb130-22"><a href="linear-regression.html#cb130-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb130-23"><a href="linear-regression.html#cb130-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb130-24"><a href="linear-regression.html#cb130-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb130-25"><a href="linear-regression.html#cb130-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 570.3 on 2993 degrees of freedom</span></span>
<span id="cb130-26"><a href="linear-regression.html#cb130-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07432,    Adjusted R-squared:  0.07247 </span></span>
<span id="cb130-27"><a href="linear-regression.html#cb130-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 40.05 on 6 and 2993 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The point estimate suggests that the effect of smoking is larger for women who consume alcohol and smoke than for women who do not drink alcohol. This seems plausible, but our evidence is not very strong here — the estimates are not statistically significant at any conventional significance level (the p-value is equal to 0.32).</p>
<ol start="6" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="linear-regression.html#cb131-1" aria-hidden="true" tabindex="-1"></a>reg6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthweight <span class="sc">~</span> age <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">2</span>), <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb131-2"><a href="linear-regression.html#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg6)</span>
<span id="cb131-3"><a href="linear-regression.html#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-4"><a href="linear-regression.html#cb131-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb131-5"><a href="linear-regression.html#cb131-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = birthweight ~ age + I(age^2), data = Birthweight_Smoking)</span></span>
<span id="cb131-6"><a href="linear-regression.html#cb131-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-7"><a href="linear-regression.html#cb131-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb131-8"><a href="linear-regression.html#cb131-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb131-9"><a href="linear-regression.html#cb131-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2949.81  -312.81    30.43   371.03  2452.72 </span></span>
<span id="cb131-10"><a href="linear-regression.html#cb131-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-11"><a href="linear-regression.html#cb131-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb131-12"><a href="linear-regression.html#cb131-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb131-13"><a href="linear-regression.html#cb131-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 2502.8949   225.6016  11.094  &lt; 2e-16 ***</span></span>
<span id="cb131-14"><a href="linear-regression.html#cb131-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age           58.1670    16.9212   3.438 0.000595 ***</span></span>
<span id="cb131-15"><a href="linear-regression.html#cb131-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; I(age^2)      -0.9099     0.3099  -2.936 0.003353 ** </span></span>
<span id="cb131-16"><a href="linear-regression.html#cb131-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb131-17"><a href="linear-regression.html#cb131-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb131-18"><a href="linear-regression.html#cb131-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb131-19"><a href="linear-regression.html#cb131-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 589.6 on 2997 degrees of freedom</span></span>
<span id="cb131-20"><a href="linear-regression.html#cb131-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.009261,   Adjusted R-squared:  0.0086 </span></span>
<span id="cb131-21"><a href="linear-regression.html#cb131-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 14.01 on 2 and 2997 DF,  p-value: 8.813e-07</span></span></code></pre></div>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="linear-regression.html#cb132-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(reg6, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">age=</span><span class="fu">seq</span>(<span class="dv">18</span>,<span class="dv">40</span>)))</span>
<span id="cb132-2"><a href="linear-regression.html#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">preds=</span>preds, <span class="at">age=</span><span class="fu">seq</span>(<span class="dv">18</span>,<span class="dv">40</span>)), <span class="fu">aes</span>(<span class="at">x=</span>age, <span class="at">y=</span>preds)) <span class="sc">+</span> </span>
<span id="cb132-3"><a href="linear-regression.html#cb132-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb132-4"><a href="linear-regression.html#cb132-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb132-5"><a href="linear-regression.html#cb132-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb132-6"><a href="linear-regression.html#cb132-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;predicted values&quot;</span>)</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-135-1.png" width="672" />
The figure suggests that predicted birthweight is increasing in mother’s age up until about age 34 and then decreasing after that.</p>
<ol start="7" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="linear-regression.html#cb133-1" aria-hidden="true" tabindex="-1"></a>reg7 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">I</span>(<span class="fu">log</span>(birthweight)) <span class="sc">~</span> smoker <span class="sc">+</span> educ <span class="sc">+</span> nprevist <span class="sc">+</span> age <span class="sc">+</span> alcohol,</span>
<span id="cb133-2"><a href="linear-regression.html#cb133-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data=</span>Birthweight_Smoking)</span>
<span id="cb133-3"><a href="linear-regression.html#cb133-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg7)</span>
<span id="cb133-4"><a href="linear-regression.html#cb133-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-5"><a href="linear-regression.html#cb133-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb133-6"><a href="linear-regression.html#cb133-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = I(log(birthweight)) ~ smoker + educ + nprevist + </span></span>
<span id="cb133-7"><a href="linear-regression.html#cb133-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     age + alcohol, data = Birthweight_Smoking)</span></span>
<span id="cb133-8"><a href="linear-regression.html#cb133-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-9"><a href="linear-regression.html#cb133-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb133-10"><a href="linear-regression.html#cb133-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb133-11"><a href="linear-regression.html#cb133-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -1.96324 -0.07696  0.02435  0.12092  0.50070 </span></span>
<span id="cb133-12"><a href="linear-regression.html#cb133-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-13"><a href="linear-regression.html#cb133-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb133-14"><a href="linear-regression.html#cb133-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb133-15"><a href="linear-regression.html#cb133-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  7.9402678  0.0270480 293.562  &lt; 2e-16 ***</span></span>
<span id="cb133-16"><a href="linear-regression.html#cb133-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; smoker      -0.0635764  0.0099782  -6.372 2.16e-10 ***</span></span>
<span id="cb133-17"><a href="linear-regression.html#cb133-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; educ         0.0022169  0.0020171   1.099    0.272    </span></span>
<span id="cb133-18"><a href="linear-regression.html#cb133-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; nprevist     0.0129662  0.0010624  12.205  &lt; 2e-16 ***</span></span>
<span id="cb133-19"><a href="linear-regression.html#cb133-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; age          0.0003059  0.0007941   0.385    0.700    </span></span>
<span id="cb133-20"><a href="linear-regression.html#cb133-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; alcohol     -0.0181053  0.0278428  -0.650    0.516    </span></span>
<span id="cb133-21"><a href="linear-regression.html#cb133-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb133-22"><a href="linear-regression.html#cb133-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb133-23"><a href="linear-regression.html#cb133-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb133-24"><a href="linear-regression.html#cb133-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.2079 on 2994 degrees of freedom</span></span>
<span id="cb133-25"><a href="linear-regression.html#cb133-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.07322,    Adjusted R-squared:  0.07167 </span></span>
<span id="cb133-26"><a href="linear-regression.html#cb133-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 47.31 on 5 and 2994 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The estimated coefficient on <span class="math inline">\(smoker\)</span> says that smoking during pregnancy decreases a baby’s birthweight by 6.3%, on average, holding education, number of pre-natal visits, age of the mother, and whether or not the mother consumed alcohol during the pregnancy constant.</p>
</div>
<div id="coding-questions-1" class="section level2" number="4.14">
<h2><span class="header-section-number">4.14</span> Coding Questions</h2>
<ol style="list-style-type: decimal">
<li><p>For this problem, we will use the data <code>intergenerational_mobility</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of child family income (<span class="math inline">\(child\_fincome\)</span>) on parents’ family income (<span class="math inline">\(parent\_fincome\)</span>). How should you interpret the estimated coefficient on parents’ family income? What is the p-value for the coefficient on parents’ family income?</p></li>
<li><p>Run a regression of <span class="math inline">\(\log(child\_fincome)\)</span> on <span class="math inline">\(parent\_fincome\)</span>. How should you interpret the estimated cofficient on <span class="math inline">\(parent\_fincome\)</span>?</p></li>
<li><p>Run a regression of <span class="math inline">\(child\_fincome\)</span> on <span class="math inline">\(\log(parent\_fincome)\)</span>. How should you interpret the estimated coefficient on <span class="math inline">\(\log(parent\_fincome)\)</span>?</p></li>
<li><p>Run a regression of <span class="math inline">\(\log(child\_fincome)\)</span> on <span class="math inline">\(\log(parent\_fincome)\)</span>. How should you interpret the estimated coefficient on <span class="math inline">\(\log(parent\_fincome)\)</span>?</p></li>
</ol></li>
<li><p>For this question, we’ll use the <code>fertilizer_2000</code> data.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of <span class="math inline">\(\log(avyield)\)</span> on <span class="math inline">\(\log(avfert)\)</span>. How do you interpret the estimated coefficient on <span class="math inline">\(\log(avfert)\)</span>?</p></li>
<li><p>Now suppose that you additionally want to control for precipitation and the region that a country is located in. How would you do this? Estimate the model that you propose here, report the results, and interpret the coefficient on <span class="math inline">\(\log(avfert)\)</span>.</p></li>
<li><p>Now suppose that you are interested in whether the effect of fertilizer varies by region that a country is located in (while still controlling for the same covariates as in part (b)). Propose a model that can be used for this purpose. Estimate the model that you proposed, report the results, and discuss whether the effect of fertilizer appears to vary by region or not.</p></li>
</ol></li>
<li><p>For this question, we will use the data <code>mutual_funds</code>. We’ll be interested in whether mutual funds that have higher expense ratios (these are typically actively managed funds) have higher returns relative to mutual funds that have lower expense ratios (e.g., index funds). For this problem, we will use the variables <code>fund_return_3years</code>, <code>investment_type</code>, <code>risk_rating</code>, <code>size_type</code>, <code>fund_net_annual_expense_ratio</code>, <code>asset_cash</code>, <code>asset_stocks</code>, <code>asset_bonds</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Calculate the median <code>fund_net_annual_expense_ratio</code>.</p></li>
<li><p>Use the <code>datasummary_balance</code> function from the <code>modelsummary</code> package to report summary statistics for <code>fund_return_3year</code>, <code>fund_net_annual_expense_ratio</code>, <code>risk_rating</code>, <code>asset_cash</code>, <code>asset_stocks</code>, <code>asset_bonds</code> based on whether their expense ratio is above or below the median. Do you notice any interesting patterns?</p></li>
<li><p>Run a regression of <code>fund_return_3years</code> on <code>fund_net_annual_expense_ratio</code>. How do you interpret the results?</p></li>
<li><p>Now, additionally control for <code>investment_type</code>, <code>risk_rating</code>, and <code>size_type</code> <strong>Hint:</strong> think carefully about what type of variables each of these are and how they should enter the model. How do these results compare to the ones from part c?</p></li>
<li><p>Now, add the variables <code>assets_cash</code>, <code>assets_stocks</code>, and <code>assets_bonds</code> to the model from part d. How do you interpret these results? Compare and interpret the differences between parts c, d, and e.</p></li>
</ol></li>
<li><p>For this question, we’ll use the data <code>Lead_Mortality</code> to study the effect of lead pipes on infant mortality in 1900.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of infant mortality (<code>infrate</code>) on whether or not a city had lead pipes (<code>lead</code>) and interpret/discuss the results.</p></li>
<li><p>It turns out that the amount of lead in drinking water depends on how acidic the water is, with more acidic water leaching more of the lead (so that there is more exposure to lead with more acidic water). To measure acidity, we’ll use the pH of the water in a particular city (<code>ph</code>); recall that, a lower value of pH indicates higher acidity. Run a regression of infant mortality on whether or not a city has lead pipes, the pH of its water, and the interaction between having lead pipes and pH. Report your results. What is the estimated partial effect of having lead pipes from this model?</p></li>
<li><p>Given the results in part b, calculate an estimate of the average partial effect of having lead pipes on infant mortality.</p></li>
<li><p>Given the results in part b, how much does the partial effect of having lead pipes differ for cities that have a pH of 6.5 relative to a pH of 7.5?</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-1" class="section level2" number="4.15">
<h2><span class="header-section-number">4.15</span> Extra Questions</h2>
<ol style="list-style-type: decimal">
<li><p>Suppose you run the following regression
<span class="math display">\[\begin{align*}
  Earnings = \beta_0 + \beta_1 Education + U
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|Education] = 0\)</span>. How do you interpret <span class="math inline">\(\beta_1\)</span> here?</p></li>
<li><p>Suppose you run the following regression
<span class="math display">\[\begin{align*}
  Earnings = \beta_0 + \beta_1 Education + \beta_2 Experience + \beta_3 Female + U
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|Education, Experience, Female] = 0\)</span>. How do you interpret <span class="math inline">\(\beta_1\)</span> here?</p></li>
<li><p>Suppose you are interested in testing whether an extra year of education increases earnings by the same amount for men and women.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Propose a regression and strategy for this sort of test.</p></li>
<li><p>Suppose you also want to control for experience in conducting this test, how would do it?</p></li>
</ol></li>
<li><p>Suppose you run the following regression
<span class="math display">\[\begin{align*}
  \log(Earnings) = \beta_0 + \beta_1 Education + \beta_2 Experience + \beta_3 Female + U
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|Education, Experience, Female] = 0\)</span>. How do you interpret <span class="math inline">\(\beta_1\)</span> here?</p></li>
<li><p>A common extra condition (though somewhat old-fashioned) is to impose <em>homoskedasticity</em>. Homoskedasticity says that <span class="math inline">\(\mathbb{E}[U^2|X] = \sigma^2\)</span> (i.e., the variance of the error term does not change across different values of <span class="math inline">\(X\)</span>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Under homoskedasticity, the expression for <span class="math inline">\(V\)</span> in <a href="linear-regression.html#eq:V">(4.7)</a> simplifies. Provide a new expression for <span class="math inline">\(V\)</span> under homoskedasticity. <strong>Hint:</strong> you will need to use the law of iterated expectations.</p></li>
<li><p>Using this expression for <span class="math inline">\(V\)</span>, explain how to calculate standard errors for an estimate of <span class="math inline">\(\beta_1\)</span> in a simple linear regression.</p></li>
<li><p>Explain how to construct a t-statistic for testing <span class="math inline">\(H_0: \beta_1=0\)</span> under homoskedasticity.</p></li>
<li><p>Explain how to contruct a p-value for <span class="math inline">\(\beta_1\)</span> under homoskedasticity.</p></li>
<li><p>Explain how to construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> under homoskedasticity.</p></li>
</ol></li>
</ol>
</div>
<div id="answers-to-some-extra-questions" class="section level2" number="4.16">
<h2><span class="header-section-number">4.16</span> Answers to Some Extra Questions</h2>
<p><strong>Answer to Question 2</strong></p>
<p><span class="math inline">\(\beta_1\)</span> is how much <span class="math inline">\(Earnings\)</span> increase on average when <span class="math inline">\(Education\)</span> increases by one year holding <span class="math inline">\(Experience\)</span> and <span class="math inline">\(Female\)</span> constant.</p>
<p><strong>Answer to Question 3</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Run the regression
<span class="math display">\[\begin{align*}
     Earnings &amp;= \beta_0 + \beta_1 Education + \beta_2 Female + \beta_3 Education \times Female + U
 \end{align*}\]</span>
and test (e.g., calculate a t-statistic and check if it is greater than 1.96 in absolute value) if <span class="math inline">\(\beta_3=0\)</span>.</p></li>
<li><p>You can run the following regression:
<span class="math display">\[\begin{align*}
   Earnings &amp;= \beta_0 + \beta_1 Education + \beta_2 Female \\
   &amp; \hspace{25pt} + \beta_3 Education \times Female + \beta_4 Experience + U
\end{align*}\]</span>
Here, you would still be interested in <span class="math inline">\(\beta_3\)</span>. If you thought that the return to experience varied for men and women, you might also include an interaction term involving <span class="math inline">\(Experience\)</span> and <span class="math inline">\(Female\)</span>.</p></li>
</ol>
<p><strong>Partial Answer to Question 5</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Starting from <a href="linear-regression.html#eq:V">(4.7)</a></li>
</ol>
<p><span class="math display">\[\begin{align*}
    V &amp;= \mathbb{E}\left[ \frac{(X - \mathbb{E}[X])^2 U^2}{\mathrm{var}(X)^2} \right] \\
    &amp;= \frac{1}{\mathrm{var}(X)^2} \mathbb{E}[(X-\mathbb{E}[X])^2 U^2] \\
    &amp;= \frac{1}{\mathrm{var}(X)^2} \mathbb{E}\big[(X-\mathbb{E}[X])^2 \mathbb{E}[U^2|X] \big] \\
    &amp;= \frac{1}{\mathrm{var}(X)^2} \mathbb{E}[(X-\mathbb{E}[X])^2 \sigma^2 ] \\
    &amp;= \frac{\sigma^2}{\mathrm{var}(X)^2} \mathbb{E}[(X-\mathbb{E}[X])^2] \\
    &amp;= \frac{\sigma^2}{\mathrm{var}(X)^2} \mathrm{var}(X) \\
    &amp;= \frac{\sigma^2}{\mathrm{var}(X)}
  \end{align*}\]</span></p>
<p>where</p>
<ul>
<li><p>the second equality holds because <span class="math inline">\(\mathrm{var}(X)^2\)</span> is non-random and can come out of the expectation,</p></li>
<li><p>the third equality uses the law of iterated expectations,</p></li>
<li><p>the fourth equality holds by the condition of homoskedasticity,</p></li>
<li><p>the fifth equality holds because <span class="math inline">\(\sigma^2\)</span> is non-random and can come out of the expectation,</p></li>
<li><p>the sixth equality holds by the definition of variance, and</p></li>
<li><p>the last equality holds by canceling <span class="math inline">\(\mathrm{var}(X)\)</span> in the numerator with one of the <span class="math inline">\(\mathrm{var}(X)\)</span>’s in the denominator.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-and-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Detailed Course Notes.pdf", "Detailed Course Notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
