<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 6 Causal Inference | Supplementary Notes and References for ECON 4750</title>
  <meta name="description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 6 Causal Inference | Supplementary Notes and References for ECON 4750" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 6 Causal Inference | Supplementary Notes and References for ECON 4750" />
  
  <meta name="twitter:description" content="This is a set of supplementary notes and additional references for ECON 4750." />
  

<meta name="author" content="Brantly Callaway" />


<meta name="date" content="2021-12-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="prediction.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Outline/Notes for ECON 4750</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-is-this"><i class="fa fa-check"></i><b>1.1</b> What is this?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#what-is-this-not"><i class="fa fa-check"></i><b>1.2</b> What is this not?</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#why-did-i-write-this"><i class="fa fa-check"></i><b>1.3</b> Why did I write this?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#additional-references"><i class="fa fa-check"></i><b>1.4</b> Additional References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#goals-for-the-course"><i class="fa fa-check"></i><b>1.5</b> Goals for the Course</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#studying-for-the-class"><i class="fa fa-check"></i><b>1.6</b> Studying for the Class</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#data-used-in-the-course"><i class="fa fa-check"></i><b>1.7</b> Data Used in the Course</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#first-week-of-class"><i class="fa fa-check"></i><b>1.8</b> First Week of Class</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-programming.html"><a href="statistical-programming.html"><i class="fa fa-check"></i><b>2</b> Statistical Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up R</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-programming.html"><a href="statistical-programming.html#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-programming.html"><a href="statistical-programming.html#downloading-r"><i class="fa fa-check"></i><b>2.1.2</b> Downloading R</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio"><i class="fa fa-check"></i><b>2.1.3</b> RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-programming.html"><a href="statistical-programming.html#rstudio-development-environment"><i class="fa fa-check"></i><b>2.1.4</b> RStudio Development Environment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-programming.html"><a href="statistical-programming.html#installing-r-packages"><i class="fa fa-check"></i><b>2.2</b> Installing R Packages</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-programming.html"><a href="statistical-programming.html#a-list-of-useful-r-packages"><i class="fa fa-check"></i><b>2.2.1</b> A list of useful R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-programming.html"><a href="statistical-programming.html#r-basics"><i class="fa fa-check"></i><b>2.3</b> R Basics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-programming.html"><a href="statistical-programming.html#objects"><i class="fa fa-check"></i><b>2.3.1</b> Objects</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-programming.html"><a href="statistical-programming.html#workspace"><i class="fa fa-check"></i><b>2.3.2</b> Workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-programming.html"><a href="statistical-programming.html#functions-in-r"><i class="fa fa-check"></i><b>2.4</b> Functions in R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-types"><i class="fa fa-check"></i><b>2.5</b> Data types</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-programming.html"><a href="statistical-programming.html#numeric-vectors"><i class="fa fa-check"></i><b>2.5.1</b> Numeric Vectors</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-programming.html"><a href="statistical-programming.html#vector-arithmetic"><i class="fa fa-check"></i><b>2.5.2</b> Vector arithmetic</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-programming.html"><a href="statistical-programming.html#more-helpful-functions-in-r"><i class="fa fa-check"></i><b>2.5.3</b> More helpful functions in R</a></li>
<li class="chapter" data-level="2.5.4" data-path="statistical-programming.html"><a href="statistical-programming.html#other-types-of-vectors"><i class="fa fa-check"></i><b>2.5.4</b> Other types of vectors</a></li>
<li class="chapter" data-level="2.5.5" data-path="statistical-programming.html"><a href="statistical-programming.html#data-frames"><i class="fa fa-check"></i><b>2.5.5</b> Data Frames</a></li>
<li class="chapter" data-level="2.5.6" data-path="statistical-programming.html"><a href="statistical-programming.html#lists"><i class="fa fa-check"></i><b>2.5.6</b> Lists</a></li>
<li class="chapter" data-level="2.5.7" data-path="statistical-programming.html"><a href="statistical-programming.html#matrices"><i class="fa fa-check"></i><b>2.5.7</b> Matrices</a></li>
<li class="chapter" data-level="2.5.8" data-path="statistical-programming.html"><a href="statistical-programming.html#factors"><i class="fa fa-check"></i><b>2.5.8</b> Factors</a></li>
<li class="chapter" data-level="2.5.9" data-path="statistical-programming.html"><a href="statistical-programming.html#understanding-an-object-in-r"><i class="fa fa-check"></i><b>2.5.9</b> Understanding an object in R</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-programming.html"><a href="statistical-programming.html#logicals"><i class="fa fa-check"></i><b>2.6</b> Logicals</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-programming.html"><a href="statistical-programming.html#additional-logical-operators"><i class="fa fa-check"></i><b>2.6.1</b> Additional Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="statistical-programming.html"><a href="statistical-programming.html#programming-basics"><i class="fa fa-check"></i><b>2.7</b> Programming basics</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="statistical-programming.html"><a href="statistical-programming.html#writing-functions"><i class="fa fa-check"></i><b>2.7.1</b> Writing functions</a></li>
<li class="chapter" data-level="2.7.2" data-path="statistical-programming.html"><a href="statistical-programming.html#ifelse"><i class="fa fa-check"></i><b>2.7.2</b> if/else</a></li>
<li class="chapter" data-level="2.7.3" data-path="statistical-programming.html"><a href="statistical-programming.html#for-loops"><i class="fa fa-check"></i><b>2.7.3</b> for loops</a></li>
<li class="chapter" data-level="2.7.4" data-path="statistical-programming.html"><a href="statistical-programming.html#vectorization"><i class="fa fa-check"></i><b>2.7.4</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="statistical-programming.html"><a href="statistical-programming.html#advanced-topics"><i class="fa fa-check"></i><b>2.8</b> Advanced Topics</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="statistical-programming.html"><a href="statistical-programming.html#tidyverse"><i class="fa fa-check"></i><b>2.8.1</b> Tidyverse</a></li>
<li class="chapter" data-level="2.8.2" data-path="statistical-programming.html"><a href="statistical-programming.html#data-visualization"><i class="fa fa-check"></i><b>2.8.2</b> Data Visualization</a></li>
<li class="chapter" data-level="2.8.3" data-path="statistical-programming.html"><a href="statistical-programming.html#reproducible-research"><i class="fa fa-check"></i><b>2.8.3</b> Reproducible Research</a></li>
<li class="chapter" data-level="2.8.4" data-path="statistical-programming.html"><a href="statistical-programming.html#technical-writing-tools"><i class="fa fa-check"></i><b>2.8.4</b> Technical Writing Tools</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-introduction-to-r-programming"><i class="fa fa-check"></i><b>2.9</b> Lab 1: Introduction to R Programming</a></li>
<li class="chapter" data-level="2.10" data-path="statistical-programming.html"><a href="statistical-programming.html#lab-1-solutions"><i class="fa fa-check"></i><b>2.10</b> Lab 1: Solutions</a></li>
<li class="chapter" data-level="2.11" data-path="statistical-programming.html"><a href="statistical-programming.html#coding-exercises"><i class="fa fa-check"></i><b>2.11</b> Coding Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html"><i class="fa fa-check"></i><b>3</b> Probability and Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-probability"><i class="fa fa-check"></i><b>3.1</b> Topics in Probability</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#data-for-this-chapter"><i class="fa fa-check"></i><b>3.1.1</b> Data for this chapter</a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#random-variables"><i class="fa fa-check"></i><b>3.1.2</b> Random Variables</a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#pdfs-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.3</b> pdfs, pmfs, and cdfs</a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#summation-operator"><i class="fa fa-check"></i><b>3.1.4</b> Summation operator</a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-pmfs-and-cdfs"><i class="fa fa-check"></i><b>3.1.5</b> Properties of pmfs and cdfs</a></li>
<li class="chapter" data-level="3.1.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.1.6</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="3.1.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#expected-values"><i class="fa fa-check"></i><b>3.1.7</b> Expected Values</a></li>
<li class="chapter" data-level="3.1.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance"><i class="fa fa-check"></i><b>3.1.8</b> Variance</a></li>
<li class="chapter" data-level="3.1.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-and-variance-of-linear-functions"><i class="fa fa-check"></i><b>3.1.9</b> Mean and Variance of Linear Functions</a></li>
<li class="chapter" data-level="3.1.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.1.10</b> Multiple Random Variables</a></li>
<li class="chapter" data-level="3.1.11" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#conditional-expectations"><i class="fa fa-check"></i><b>3.1.11</b> Conditional Expectations</a></li>
<li class="chapter" data-level="3.1.12" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>3.1.12</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="3.1.13" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#covariance"><i class="fa fa-check"></i><b>3.1.13</b> Covariance</a></li>
<li class="chapter" data-level="3.1.14" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#correlation"><i class="fa fa-check"></i><b>3.1.14</b> Correlation</a></li>
<li class="chapter" data-level="3.1.15" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-expectationsvariances-of-sums-of-rvs"><i class="fa fa-check"></i><b>3.1.15</b> Properties of Expectations/Variances of Sums of RVs</a></li>
<li class="chapter" data-level="3.1.16" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#normal-distribution"><i class="fa fa-check"></i><b>3.1.16</b> Normal Distribution</a></li>
<li class="chapter" data-level="3.1.17" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding"><i class="fa fa-check"></i><b>3.1.17</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#topics-in-statistics"><i class="fa fa-check"></i><b>3.2</b> Topics in Statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#simple-random-sample"><i class="fa fa-check"></i><b>3.2.1</b> Simple Random Sample</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#estimating-mathbbey"><i class="fa fa-check"></i><b>3.2.2</b> Estimating <span class="math inline">\(\mathbb{E}[Y]\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-of-bary"><i class="fa fa-check"></i><b>3.2.3</b> Mean of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#variance-of-bary"><i class="fa fa-check"></i><b>3.2.4</b> Variance of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#properties-of-estimators"><i class="fa fa-check"></i><b>3.2.5</b> Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#relative-efficiency"><i class="fa fa-check"></i><b>3.2.6</b> Relative Efficiency</a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#mean-squared-error"><i class="fa fa-check"></i><b>3.2.7</b> Mean Squared Error</a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#large-sample-properties-of-estimators"><i class="fa fa-check"></i><b>3.2.8</b> Large Sample Properties of Estimators</a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#inference-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.9</b> Inference / Hypothesis Testing</a></li>
<li class="chapter" data-level="3.2.10" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-1"><i class="fa fa-check"></i><b>3.2.10</b> Coding</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-monte-carlo-simulations"><i class="fa fa-check"></i><b>3.3</b> Lab 2: Monte Carlo Simulations</a></li>
<li class="chapter" data-level="3.4" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#lab-2-solutions"><i class="fa fa-check"></i><b>3.4</b> Lab 2 Solutions</a></li>
<li class="chapter" data-level="3.5" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#coding-questions"><i class="fa fa-check"></i><b>3.5</b> Coding Questions</a></li>
<li class="chapter" data-level="3.6" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html#extra-questions"><i class="fa fa-check"></i><b>3.6</b> Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#nonparametric-regression-curse-of-dimensionality"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Regression / Curse of Dimensionality</a></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>4.2</b> Linear Regression Models</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computation"><i class="fa fa-check"></i><b>4.3</b> Computation</a></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#partial-effects"><i class="fa fa-check"></i><b>4.4</b> Partial Effects</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-1"><i class="fa fa-check"></i><b>4.4.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-regression.html"><a href="linear-regression.html#binary-regressors"><i class="fa fa-check"></i><b>4.5</b> Binary Regressors</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-2"><i class="fa fa-check"></i><b>4.5.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-regression.html"><a href="linear-regression.html#nonlinear-regression-functions"><i class="fa fa-check"></i><b>4.6</b> Nonlinear Regression Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-3"><i class="fa fa-check"></i><b>4.6.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-interaction-terms"><i class="fa fa-check"></i><b>4.7</b> Interpreting Interaction Terms</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-4"><i class="fa fa-check"></i><b>4.7.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="linear-regression.html"><a href="linear-regression.html#elasticities"><i class="fa fa-check"></i><b>4.8</b> Elasticities</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-5"><i class="fa fa-check"></i><b>4.8.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="linear-regression.html"><a href="linear-regression.html#omitted-variable-bias"><i class="fa fa-check"></i><b>4.9</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="4.10" data-path="linear-regression.html"><a href="linear-regression.html#how-to-estimate-the-parameters-in-a-regression-model"><i class="fa fa-check"></i><b>4.10</b> How to estimate the parameters in a regression model</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-6"><i class="fa fa-check"></i><b>4.10.1</b> Computation</a></li>
<li class="chapter" data-level="4.10.2" data-path="linear-regression.html"><a href="linear-regression.html#more-than-one-regressor"><i class="fa fa-check"></i><b>4.10.2</b> More than one regressor</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i><b>4.11</b> Inference</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="linear-regression.html"><a href="linear-regression.html#computation-7"><i class="fa fa-check"></i><b>4.11.1</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-birthweight-and-smoking"><i class="fa fa-check"></i><b>4.12</b> Lab 3: Birthweight and Smoking</a></li>
<li class="chapter" data-level="4.13" data-path="linear-regression.html"><a href="linear-regression.html#lab-3-solutions"><i class="fa fa-check"></i><b>4.13</b> Lab 3: Solutions</a></li>
<li class="chapter" data-level="4.14" data-path="linear-regression.html"><a href="linear-regression.html#coding-questions-1"><i class="fa fa-check"></i><b>4.14</b> Coding Questions</a></li>
<li class="chapter" data-level="4.15" data-path="linear-regression.html"><a href="linear-regression.html#extra-questions-1"><i class="fa fa-check"></i><b>4.15</b> Extra Questions</a></li>
<li class="chapter" data-level="4.16" data-path="linear-regression.html"><a href="linear-regression.html#answers-to-some-extra-questions"><i class="fa fa-check"></i><b>4.16</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>5</b> Prediction</a>
<ul>
<li class="chapter" data-level="5.1" data-path="prediction.html"><a href="prediction.html#measures-of-regression-fit"><i class="fa fa-check"></i><b>5.1</b> Measures of Regression Fit</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="prediction.html"><a href="prediction.html#tss-ess-ssr"><i class="fa fa-check"></i><b>5.1.1</b> TSS, ESS, SSR</a></li>
<li class="chapter" data-level="5.1.2" data-path="prediction.html"><a href="prediction.html#r2"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="prediction.html"><a href="prediction.html#model-selection"><i class="fa fa-check"></i><b>5.2</b> Model Selection</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="prediction.html"><a href="prediction.html#limitations-of-r2"><i class="fa fa-check"></i><b>5.2.1</b> Limitations of <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.2" data-path="prediction.html"><a href="prediction.html#adjusted-r2"><i class="fa fa-check"></i><b>5.2.2</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.2.3" data-path="prediction.html"><a href="prediction.html#aic-bic"><i class="fa fa-check"></i><b>5.2.3</b> AIC, BIC</a></li>
<li class="chapter" data-level="5.2.4" data-path="prediction.html"><a href="prediction.html#cross-validation"><i class="fa fa-check"></i><b>5.2.4</b> Cross-Validation</a></li>
<li class="chapter" data-level="5.2.5" data-path="prediction.html"><a href="prediction.html#model-averaging"><i class="fa fa-check"></i><b>5.2.5</b> Model Averaging</a></li>
<li class="chapter" data-level="5.2.6" data-path="prediction.html"><a href="prediction.html#computation-8"><i class="fa fa-check"></i><b>5.2.6</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prediction.html"><a href="prediction.html#machine-learning"><i class="fa fa-check"></i><b>5.3</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="prediction.html"><a href="prediction.html#lasso"><i class="fa fa-check"></i><b>5.3.1</b> Lasso</a></li>
<li class="chapter" data-level="5.3.2" data-path="prediction.html"><a href="prediction.html#ridge-regression"><i class="fa fa-check"></i><b>5.3.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.3.3" data-path="prediction.html"><a href="prediction.html#computation-9"><i class="fa fa-check"></i><b>5.3.3</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="prediction.html"><a href="prediction.html#binary-outcome-models"><i class="fa fa-check"></i><b>5.4</b> Binary Outcome Models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="prediction.html"><a href="prediction.html#linear-probability-model"><i class="fa fa-check"></i><b>5.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="5.4.2" data-path="prediction.html"><a href="prediction.html#probit-and-logit"><i class="fa fa-check"></i><b>5.4.2</b> Probit and Logit</a></li>
<li class="chapter" data-level="5.4.3" data-path="prediction.html"><a href="prediction.html#average-partial-effects"><i class="fa fa-check"></i><b>5.4.3</b> Average Partial Effects</a></li>
<li class="chapter" data-level="5.4.4" data-path="prediction.html"><a href="prediction.html#computation-10"><i class="fa fa-check"></i><b>5.4.4</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prediction.html"><a href="prediction.html#lab-4-predicting-diamond-prices"><i class="fa fa-check"></i><b>5.5</b> Lab 4: Predicting Diamond Prices</a></li>
<li class="chapter" data-level="5.6" data-path="prediction.html"><a href="prediction.html#lab-4-solutions"><i class="fa fa-check"></i><b>5.6</b> Lab 4: Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="prediction.html"><a href="prediction.html#coding-questions-2"><i class="fa fa-check"></i><b>5.7</b> Coding Questions</a></li>
<li class="chapter" data-level="5.8" data-path="prediction.html"><a href="prediction.html#extra-questions-2"><i class="fa fa-check"></i><b>5.8</b> Extra Questions</a></li>
<li class="chapter" data-level="5.9" data-path="prediction.html"><a href="prediction.html#answers-to-some-extra-questions-1"><i class="fa fa-check"></i><b>5.9</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>6</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="causal-inference.html"><a href="causal-inference.html#potential-outcomes"><i class="fa fa-check"></i><b>6.1</b> Potential Outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="causal-inference.html"><a href="causal-inference.html#parameters-of-interest"><i class="fa fa-check"></i><b>6.2</b> Parameters of Interest</a></li>
<li class="chapter" data-level="6.3" data-path="causal-inference.html"><a href="causal-inference.html#experiments"><i class="fa fa-check"></i><b>6.3</b> Experiments</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="causal-inference.html"><a href="causal-inference.html#estimating-att-with-a-regression"><i class="fa fa-check"></i><b>6.3.1</b> Estimating ATT with a Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="causal-inference.html"><a href="causal-inference.html#internal-and-external-validity"><i class="fa fa-check"></i><b>6.3.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="6.3.3" data-path="causal-inference.html"><a href="causal-inference.html#example-project-star"><i class="fa fa-check"></i><b>6.3.3</b> Example: Project STAR</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="causal-inference.html"><a href="causal-inference.html#unconfoundedness"><i class="fa fa-check"></i><b>6.4</b> Unconfoundedness</a></li>
<li class="chapter" data-level="6.5" data-path="causal-inference.html"><a href="causal-inference.html#panel-data-approaches"><i class="fa fa-check"></i><b>6.5</b> Panel Data Approaches</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="causal-inference.html"><a href="causal-inference.html#difference-in-differences"><i class="fa fa-check"></i><b>6.5.1</b> Difference in differences</a></li>
<li class="chapter" data-level="6.5.2" data-path="causal-inference.html"><a href="causal-inference.html#computation-11"><i class="fa fa-check"></i><b>6.5.2</b> Computation</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="causal-inference.html"><a href="causal-inference.html#instrumental-variables"><i class="fa fa-check"></i><b>6.6</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="causal-inference.html"><a href="causal-inference.html#example-return-to-education"><i class="fa fa-check"></i><b>6.6.1</b> Example: Return to Education</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="causal-inference.html"><a href="causal-inference.html#regression-discontinuity"><i class="fa fa-check"></i><b>6.7</b> Regression Discontinuity</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="causal-inference.html"><a href="causal-inference.html#example-causal-effect-of-alcohol-on-driving-deaths"><i class="fa fa-check"></i><b>6.7.1</b> Example: Causal effect of Alcohol on Driving Deaths</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-drunk-driving-laws"><i class="fa fa-check"></i><b>6.8</b> Lab 5: Drunk Driving Laws</a></li>
<li class="chapter" data-level="6.9" data-path="causal-inference.html"><a href="causal-inference.html#lab-5-solutions"><i class="fa fa-check"></i><b>6.9</b> Lab 5: Solutions</a></li>
<li class="chapter" data-level="6.10" data-path="causal-inference.html"><a href="causal-inference.html#coding-questions-3"><i class="fa fa-check"></i><b>6.10</b> Coding Questions</a></li>
<li class="chapter" data-level="6.11" data-path="causal-inference.html"><a href="causal-inference.html#extra-questions-3"><i class="fa fa-check"></i><b>6.11</b> Extra Questions</a></li>
<li class="chapter" data-level="6.12" data-path="causal-inference.html"><a href="causal-inference.html#answers-to-some-extra-questions-2"><i class="fa fa-check"></i><b>6.12</b> Answers to Some Extra Questions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supplementary Notes and References for ECON 4750</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="causal-inference" class="section level1" number="6">
<h1><span class="header-section-number">Topic 6</span> Causal Inference</h1>
<p>For the remainder of the semester, we will talk about methods for understanding the <strong>causal effect</strong> of one variable on another.</p>
<p>Let’s start with an example. Suppose we were interested in understanding the causal effect of attending college on earnings. We know a lot about calculating averages, so let’s consider calculating average earnings of those who went to college and comparing that to the average earnings of those who didn’t go to college. Let’s use the data that we used all the way back in Chapter 2 to make this calculation.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="causal-inference.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;us_data.RData&quot;</span>)</span>
<span id="cb144-2"><a href="causal-inference.html#cb144-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-3"><a href="causal-inference.html#cb144-3" aria-hidden="true" tabindex="-1"></a>col_earnings <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">subset</span>(us_data, educ <span class="sc">&gt;=</span> <span class="dv">16</span>)<span class="sc">$</span>incwage)</span>
<span id="cb144-4"><a href="causal-inference.html#cb144-4" aria-hidden="true" tabindex="-1"></a>non_col_earnings <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">subset</span>(us_data, educ <span class="sc">&lt;</span> <span class="dv">16</span>)<span class="sc">$</span>incwage)</span>
<span id="cb144-5"><a href="causal-inference.html#cb144-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-6"><a href="causal-inference.html#cb144-6" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">data.frame</span>(col_earnings, </span>
<span id="cb144-7"><a href="causal-inference.html#cb144-7" aria-hidden="true" tabindex="-1"></a>                 non_col_earnings, </span>
<span id="cb144-8"><a href="causal-inference.html#cb144-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">diff=</span>(col_earnings<span class="sc">-</span>non_col_earnings)),<span class="dv">3</span>)</span>
<span id="cb144-9"><a href="causal-inference.html#cb144-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   col_earnings non_col_earnings     diff</span></span>
<span id="cb144-10"><a href="causal-inference.html#cb144-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1     87306.33         40557.28 46749.05</span></span></code></pre></div>
<p>This seems to be a huge difference. We are used to calling this difference a partial effect of going to college, but should we call it <em>the causal effect</em> or an average causal effect across individuals? Probably no. The main reason is that earnings of individuals who went to college may have been different from individuals that didn’t go to college even if no one went to college. Another way to say this is that there are a number of other things besides college that affect a person’s earnings (age, race, IQ, “ability,” “hardworking-ness,” luck, etc.). If these are also correlated with whether or not a person goes to college (you would think that at least some of these are), then that would make it hard to interpret our simple difference-in-means as a causal effect.</p>
<p>Before we move on, let me give some more examples of causal questions that may be of interest to economists:</p>
<ul>
<li><p>Causal effects of economic policies</p>
<ul>
<li><p>What was the causal effect of a country raising its interest rate on GDP or employment?</p></li>
<li><p>What was the causal effect of a change in the minimum wage on employment?</p></li>
<li><p>What was the causal effect of changing voter ID laws on the number of ballots cast?</p></li>
</ul></li>
<li><p>Causal effects of individual/firm choices</p>
<ul>
<li><p>What was the causal effect of a price increase on quantity demanded?</p></li>
<li><p>What was the causal effect of changing a product attribute on some outcome of interest (e.g., changing the font type on clicks on Google ads)?</p></li>
<li><p>What was the causal effect of a new cholesterol medication on cholesterol levels?</p></li>
<li><p>What was the causal effect of a job training program on wages?</p></li>
</ul></li>
</ul>
<p>You could easily come up with many others too. A large fraction of the questions that researchers in economics try to address are ultimately about sorting out these sorts of causal effects.</p>
<p>For terminology, we’ll refer to the variable that we are interested in understanding its causal effect (going to college in the earlier example) as the <strong>treatment</strong>. For simplicity, we will mostly focus on the case where the treatment is binary. We will use <span class="math inline">\(D_i\)</span> to denote the treatment, so that <span class="math inline">\(D_i=1\)</span> if individual <span class="math inline">\(i\)</span> participates in the treatment and <span class="math inline">\(D_i=0\)</span> if individual <span class="math inline">\(i\)</span> does not participate in the treatment.</p>
<p>Example: SW 13.3</p>
<div id="potential-outcomes" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Potential Outcomes</h2>
<p>SW 13.1</p>
<p>A powerful tool for thinking about causal effects is <strong>counterfactual reasoning</strong>. For individuals that participate in the treatment, we observe what their outcome is given that they participated in the treatment. But we don’t observe what their outcome would have been if they had not participated in the treatment. For example, among those that went to college, we don’t observe what their earnings would have been if they had not gone to college. For those that don’t participate in the treatment, we face the opposite problem — we don’t observe what their outcome would have been if they had participated in the treatment.</p>
<p>We’ll relate causal inference to the problem of trying to figure out what outcomes individuals that participated in the treatment would have experienced if they had <em>not</em> participated in the treatment (at least on average) and/or figuring out what outcomes individuals that did not participated in the treatment would have experienced if they <em>had</em> participated in the treatment.</p>
<p>To do this, let’s introduce somewhat more formal notation/terminology.</p>
<p><strong>Treated potential outcome</strong>: <span class="math inline">\(Y_i(1)\)</span>, the outcome an individual <em>would experience</em> if they participated in the treatment</p>
<p><strong>Untreated potential outcome</strong>: <span class="math inline">\(Y_i(0)\)</span>, the outcome an individual <em>would experience</em> if they did not participate in the treatment</p>
<p>For individuals that participate in the treatment, we observe <span class="math inline">\(Y_i(1)\)</span> (but not <span class="math inline">\(Y_i(0)\)</span>). For individuals that do not participate in the treatment, we observe <span class="math inline">\(Y_i(0)\)</span> (but not <span class="math inline">\(Y_i(1)\)</span>). Another way to write this is that the observed outcome, <span class="math inline">\(Y_i\)</span> is given by
<span class="math display">\[\begin{align*}
  Y_i = D_i Y_i(1) + (1-D_i) Y_i(0)
\end{align*}\]</span></p>
<p>We can think about the individual-level effect of participating in the treatment:
<span class="math display">\[\begin{align*}
  TE_i = Y_i(1) - Y_i(0)
\end{align*}\]</span></p>
<p>Considering the difference between treated and untreated potential outcomes is a very natural (and, I think, helpful) way to think about causality. The causal effect of the treatment is the difference between the outcome that an individual would experience if they participate in the treatment relative to what they would experience if they did not participate in the treatment.</p>
<p>This notation also makes it clear that we are allowing for <strong>treatment effect heterogenity</strong> — the effect of participating in the treatment can vary across different individuals.</p>
<p>That said, most researchers essentially give up on trying to figure out individual level treatment effects. It is not so much that these are not interesting, more it is just that these are very hard to figure out. Take, for example, going to college, and suppose we are interested in the causal effect of going to college on a person’s earnings. I went to college, so I know what my <span class="math inline">\(Y(1)\)</span> is, but I don’t know what my <span class="math inline">\(Y(0)\)</span> is — and, I’d even have a hard time coming with a good guess as to what it might be.</p>
</div>
<div id="parameters-of-interest" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Parameters of Interest</h2>
<p>Instead of going for individual-level effects of participating in the treatment, most researchers instead go for more aggregated parameters. The two most common ones are the Average Treatment Effect (ATE) and Average Treatment Effect on the Treated (ATT).</p>
<p><span class="math display">\[\begin{align*}
  ATE = \mathbb{E}[Y(1) - Y(0)] \qquad \textrm{and} \qquad ATT = \mathbb{E}[Y(1)-Y(0) | D=1]
\end{align*}\]</span>
<span class="math inline">\(ATE\)</span> is the difference between treated potential outcomes and untreated potential outcomes, on average, and for the entire population. <span class="math inline">\(ATT\)</span> is the difference between treated and untreated potential outcomes, on average, conditional on being in the treated group.</p>
<p>We will mostly focus on <span class="math inline">\(ATT\)</span>.</p>
<p>It is worth considering the challenges for learning about <span class="math inline">\(ATT\)</span>. In particular, notice that we can write
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[Y(1)|D=1] - \mathbb{E}[Y(0)|D=1]
\end{align*}\]</span>
and consider these term separately</p>
<ul>
<li><p><span class="math inline">\(\mathbb{E}[Y(1)|D=1]\)</span> is the average treated potential outcome among the treated group. But we observe treated potential outcomes for the treated group <span class="math inline">\(\implies \mathbb{E}[Y(1)|D=1] = \mathbb{E}[Y|D=1]\)</span>. In other words, if we want to estimate this component of the <span class="math inline">\(ATT\)</span>, we can just look right at the data and compute the average outcome experienced by individuals in the treated group.</p></li>
<li><p><span class="math inline">\(\mathbb{E}[Y(0)|D=1]\)</span> is the average untreated potential outcome among the treated group. This is (potentially much) more challenging than the first term because we do not observe untreated potential outcomes among the treated group. But, in order to learn about the <span class="math inline">\(ATT\)</span>, we will have to <em>somehow</em> deal with this term. I will provide a number of strategies below, but it is important to remember that this is a major challenge, and their may not be a good solution.</p></li>
</ul>
<div class="side-comment">
<p><span class="side-comment">Side-Comment:</span> I think it is also worth clearly pointing out that, while I am a big believer in the power/usefulness of using data to try to answer questions in economics, the above discussion suggests that there are a number of questions that we may just not be able to answer. In economics jargon, this amounts to an <strong>identification problem</strong> — in other words, there may be competing theories of the world which the available data is not able to distinguish among. I probably do not emphasize this issue enough in our class, but it is something that you should remember — there may be a large number of causal questions that we’d be interested in answering, but where it is not possible to answer them (at least given the information that we have available).</p>
</div>
</div>
<div id="experiments" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Experiments</h2>
<p>An experiment is often called the “gold standard” for causal inference. In particular, here, we are thinking about the case where participation in the treatment is randomly assigned — something like: people who show up to possibly participate in the treatment, someone flips a coin, and if the coin comes up heads then the person participates in the treatment or, if tails, they do not participate in the treatment.</p>
<p>Random assignment means that participating in the treatment is independent of potential outcomes, by construction. We can write this in math as
<span class="math display">\[\begin{align*}
  (Y(1), Y(0)) \perp D
\end{align*}\]</span>
For our purposes, this also implies that
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y(0)|D=1] = \mathbb{E}[Y(0)|D=0] = \mathbb{E}[Y|D=0]
\end{align*}\]</span>
In other words, under random assignment, the average untreated potential among the treated group is equal to the average untreated potential outcome among the untreated group (this is the first equality). This is helpful because untreated potential outcomes are observed for those in the untreated group (this is the second equality).</p>
<p>Thus, under random assignment,
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[Y|D=1] - \mathbb{E}[Y|D=0]
\end{align*}\]</span>
In other words, the <span class="math inline">\(ATT\)</span> is just the difference in (population) average outcomes among the treated group relative to average outcomes among the untreated group.</p>
<p>The natural way to estimate the ATT under random assignment is
<span class="math display">\[\begin{align*}
  \widehat{ATT} = \bar{Y}_{D=1} - \bar{Y}_{D=0}
\end{align*}\]</span>
i.e., as we have done many times before, in order to estimate the parameter of interest, we just replace population averages with sample averages.</p>
<div id="estimating-att-with-a-regression" class="section level3" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Estimating ATT with a Regression</h3>
<p>It is also often convenient to introduce a regression based estimator of the ATT. This is primarily convenient as it will allow us to leverage all the things we already know about regressions, and, particularly, we it will immediately provide us with standard errors, t-statistics, etc.</p>
<p>In order to do this, let’s introduce the following assumption:</p>
<p><strong>Treatment Effect Homogeneity</strong>: <span class="math inline">\(Y_i(1) - Y_i(0) = \alpha\)</span> (and <span class="math inline">\(\alpha\)</span> does not vary across individuals).</p>
<p>This is a potentially quite unrealistic assumption; I’ll make some additional comments about it below, but, for now, let’s just go with it.</p>
<p>Notice that we can also write
<span class="math display">\[\begin{align*}
  Y_i(0) = \beta_0 + U_i
\end{align*}\]</span>
where <span class="math inline">\(\mathbb{E}[U|D=0] = \mathbb{E}[U|D=1] = 0\)</span> (this holds under random assignment since random assignment implies that treated and untreated individuals do not have systematically different untreated potential outcomes)</p>
<p>Recalling the definition of the observed outcome, notice that
<span class="math display">\[\begin{align*}
  Y_i &amp;= D_i Y_i(1) + (1-D_i) Y_i(0) \\
  &amp;= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\
  &amp;= \alpha D_i + \beta_0 + U_i
\end{align*}\]</span>
where the second equality just involves re-arranging terms, and the third equality holds under treatment effect homogeneity and using the expression for <span class="math inline">\(Y_i(0)\)</span> in the previous equation.</p>
<p>This suggests running a regression of the observed <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(D_i\)</span> and interpreting the estimated version of <span class="math inline">\(\alpha\)</span> as an estimate of the causal effect of participating in the treatment (and you can pick up standard errors, etc. from the regression output) — this is very convenient.</p>
<p>The previous discussion invoked the extra condition of treatment effect homogeneity. I want to point out some things related to this now. In the above regression model, we can alternatively (and equivalently) write it as
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y|D] = \beta_0 + \alpha D
\end{align*}\]</span>
Now plug in particular values for <span class="math inline">\(D\)</span>:
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y|D=0] = \beta_0 \qquad \textrm{and} \qquad \mathbb{E}[Y|D=1] = \beta_0 + \alpha
\end{align*}\]</span>
Subtracting the second equation from the first implies that
<span class="math display">\[\begin{align*}
  \alpha = \mathbb{E}[Y|D=1] - \mathbb{E}[Y|D=0]
\end{align*}\]</span>
but notice that this is exactly what the <span class="math inline">\(ATT\)</span> is equal to under random assignment. Thus, it is worth pointing out that, although we imposed the assumption of treatment effect homogeneity to arrive at the regression equation, our regression is “robust” to treatment effect heterogeneity.</p>
</div>
<div id="internal-and-external-validity" class="section level3" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Internal and External Validity</h3>
<p>SW 13.2</p>
<p>Although experiments, when they are available, are considered the gold standard of causal inference, there are some ways they can go wrong.</p>
<p>An experiment is said to be <strong>internally valid</strong> if inferences about causal effects are valid for the population being studied.</p>
<p>An experiment is said to be <strong>externally valid</strong> if inferences about causal effects can be generalized from the study setting to other populations/time periods/settings.</p>
<p>Generally, its easier for an experiment to be internally valid than externally valid, but there are a number of “threats” to both internal and external validity.</p>
<p>Threats to internal validity:</p>
<ul>
<li><p>Failure to randomize — individuals weren’t actually randomized into the treatment. This seems trivial, but some famous examples include experiments that assigned treatment participation by the first letter of a person’s last name. It turns out that this can be correlated with a number of other things (violating the random assignment assumption)</p>
<ul>
<li>As a side-comment, random treatment assignment implies that treatment should be uncorrelated with other covariates (e.g., race, sex, etc.) and this can be used as a way to check if the treatment was really randomly assigned.</li>
</ul></li>
<li><p>Failure to follow treatment protocol — Individuals assigned to participate in the treatment don’t participate or vice versa.</p>
<ul>
<li>This is actually a pretty common problem. A leading example would be in the context of an experimental drug where patients who were assigned the placebo somehow to convince the doctor to give them the real drug.</li>
</ul></li>
<li><p>Attrition — subjects non-randomly drop out of the sample</p></li>
<li><p>Placebo effects — Participating in an experiment can cause changes in behavior/outcomes.</p>
<ul>
<li>This is a main reason that medical experiments are often “double-blind”</li>
</ul></li>
<li><p>Small sample sizes — Experiments can be expensive to run and therefore often include only a small number of observations, and, therefore, may be less able to detect small effects of a treatment</p></li>
<li><p>Experiments can sometimes be unethical</p>
<ul>
<li>The 2019 Nobel prize in economics was awarded for experiments in developing countries. Some of these experiments were somewhat controversial (e.g., randomly assigning some schools extra resources and randomly giving glasses to some students but not others).</li>
</ul></li>
</ul>
<p>Threats to External Validity:</p>
<ul>
<li><p>Nonrepresentative samples — The population that is willing to participate in an experiment many be substantially different from the overall population.</p>
<ul>
<li>A closely related problem is that the economic environment may change across time and/or space which means that effects of some treatment might be different in different time periods or locations</li>
</ul></li>
<li><p>Nonrepresentative program/policy — The policy in an experiment may be different from other policies being considered</p>
<ul>
<li>For example, the Perry Preschool Project was an experiment that provided intensive pre-school program that seems to have had large and long-lasting effects on participants. It is not clear if these results should apply to less intensive programs like Head Start.</li>
</ul></li>
<li><p>General equilibrium effects — Expanding policies may alter the “state of the world” in a way that an experiment wouldn’t (e.g., the effects of a local minimum wage change could be different from a country-wide minimum wage change)</p></li>
</ul>
</div>
<div id="example-project-star" class="section level3" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Example: Project STAR</h3>
<p>Project STAR was a an experiment in Tennessee in 1980s on the effects of smaller class sizes on student performance where students and teachers were randomly assigned to be in a small class, a regular class, or a regular class with an aide. In this example, we’ll study the causal effect of being in a small class on reading test scores.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="causal-inference.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Star&quot;</span>, <span class="at">package=</span><span class="st">&quot;Ecdat&quot;</span>)</span>
<span id="cb145-2"><a href="causal-inference.html#cb145-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-3"><a href="causal-inference.html#cb145-3" aria-hidden="true" tabindex="-1"></a><span class="co"># limit data to small class or regular class</span></span>
<span id="cb145-4"><a href="causal-inference.html#cb145-4" aria-hidden="true" tabindex="-1"></a><span class="co"># this drops the other category of regular class size with an aide</span></span>
<span id="cb145-5"><a href="causal-inference.html#cb145-5" aria-hidden="true" tabindex="-1"></a>Star <span class="ot">&lt;-</span> <span class="fu">subset</span>(Star, classk <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;small.class&quot;</span>, <span class="st">&quot;regular&quot;</span>))</span>
<span id="cb145-6"><a href="causal-inference.html#cb145-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-7"><a href="causal-inference.html#cb145-7" aria-hidden="true" tabindex="-1"></a><span class="co"># regression for reading scores</span></span>
<span id="cb145-8"><a href="causal-inference.html#cb145-8" aria-hidden="true" tabindex="-1"></a>reading <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(treadssk) <span class="sc">~</span> classk, <span class="at">data=</span>Star)</span>
<span id="cb145-9"><a href="causal-inference.html#cb145-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reading)</span>
<span id="cb145-10"><a href="causal-inference.html#cb145-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-11"><a href="causal-inference.html#cb145-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb145-12"><a href="causal-inference.html#cb145-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = log(treadssk) ~ classk, data = Star)</span></span>
<span id="cb145-13"><a href="causal-inference.html#cb145-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-14"><a href="causal-inference.html#cb145-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb145-15"><a href="causal-inference.html#cb145-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb145-16"><a href="causal-inference.html#cb145-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -0.31960 -0.04873 -0.00607  0.03929  0.36877 </span></span>
<span id="cb145-17"><a href="causal-inference.html#cb145-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-18"><a href="causal-inference.html#cb145-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb145-19"><a href="causal-inference.html#cb145-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                   Estimate Std. Error  t value Pr(&gt;|t|)    </span></span>
<span id="cb145-20"><a href="causal-inference.html#cb145-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)       6.072176   0.001568 3871.502  &lt; 2e-16 ***</span></span>
<span id="cb145-21"><a href="causal-inference.html#cb145-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; classksmall.class 0.013324   0.002302    5.788  7.7e-09 ***</span></span>
<span id="cb145-22"><a href="causal-inference.html#cb145-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb145-23"><a href="causal-inference.html#cb145-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb145-24"><a href="causal-inference.html#cb145-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb145-25"><a href="causal-inference.html#cb145-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.07014 on 3731 degrees of freedom</span></span>
<span id="cb145-26"><a href="causal-inference.html#cb145-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.0089, Adjusted R-squared:  0.008634 </span></span>
<span id="cb145-27"><a href="causal-inference.html#cb145-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic:  33.5 on 1 and 3731 DF,  p-value: 7.699e-09</span></span></code></pre></div>
<p>You’ll notice that this is a very simple analysis, but that we have access to an experiment means that we do need to do anything complicated — we can just use a regression to compare the differences between test scores of students in small classes relative to large tests. These results say that small class sizes increase reading test scores of students by about 1.3%.</p>
<p>Let me also make a few comments about internal and external validity.</p>
<ul>
<li><p>I don’t have too much to say about internal validity. As far as I know, Project STAR is considered internally valid, but it would be worth reading more about it in order to confirm that.</p></li>
<li><p>Thinking about external validity though is quite interesting. For one thing, this experiment is quite old. Are the results still valid now? It is not clear; one could imagine that better teaching technologies now could possibly make having a small class less important. Another thing that is interesting is that this experiment was for very young students (I think students in kindergarten). Do these results imply that, say, high school or college students would also benefit from smaller class sizes? Again, this is not clear. It is a relevant piece of information, but it also would require a seemingly large amount of extrapolation to say that these results should inform policy decisions about class sizes in the present day and for different age groups.</p></li>
</ul>
</div>
</div>
<div id="unconfoundedness" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Unconfoundedness</h2>
<p>SW 6.8, SW Ch. 9</p>
<p>Unfortunately, we rarely have access to experimental data in applications or the ability to run experiments to evaluate causal effects programs/policies that we’d be interested in studying. For example, it’s hard to imagine convincing a large number of countries to randomly assign their interest rate, minimum wage, or immigration policies though these are all policies that economists would be interested in thinking about the causal effect of.</p>
<p>For the remainder of this chapter, we’ll discuss common approaches to causal inference when the treatment is not randomly assigned. We’ll start with what is probably the most common approach: <strong>unconfoundedness</strong>.</p>
<p><strong>Unconfoundedness Assumption</strong>:
<span class="math display">\[\begin{align*}
  (Y(1),Y(0)) \perp D | X
\end{align*}\]</span>
You can think of this as saying that, among individuals with the same covariates <span class="math inline">\(X\)</span>, they have the same distributions of potential outcomes regardless of whether or not they participate in the treatment. Note that the distribution of <span class="math inline">\(X\)</span> is still allowed to be different between the treated and untreated groups. In other words, after you condition on covariates, there is nothing special (in terms of the distributions of potential outcomes) about the group that participates in the treatment relative to the group that doesn’t participate in the treatment.</p>
<ul>
<li>This is potentially a strong assumption. In order to believe this assumption, you need to believe that untreated individuals with the same characteristics can deliver, on average, the outcome that individuals in the treated group would have experienced if they had not participated in the treatment. In math, you can write this as
<span class="math display">\[\begin{align*}
    \mathbb{E}[Y(0) | X, D=1] = \mathbb{E}[Y(0) | X, D=0]
  \end{align*}\]</span></li>
</ul>
<p>If you are willing to believe this assumption, then you can recover the <span class="math inline">\(ATT\)</span>. There are a few different ways that you could implement this. Probably the most common (and convenient) way is to link this condition to a regression (just like we did in the previous section on experiments).</p>
<p>To do this, let’s continue to make the treatment effect heterogeneity assumption as above. In addition, let’s assume a linear model for untreated potential outcomes
<span class="math display">\[\begin{align*}
  Y(0) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\end{align*}\]</span>
and unconfoundedness implies that <span class="math inline">\(\mathbb{E}[U|X_1,X_2,X_3,D] = 0\)</span> (the conditioning on <span class="math inline">\(D\)</span> is the unconfoundedness part). Now, recalling the definition of the observed outcome, we can write
<span class="math display">\[\begin{align*}
  Y_i &amp;= D_i Y_i(1) + (1-D_i) Y_i(0) \\
  &amp;= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\
  &amp;= D_i \alpha + \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U_i
\end{align*}\]</span>
which suggests running the regression of observed <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1,X_2,X_3,\)</span> and <span class="math inline">\(D\)</span> and interpreting the estimate of <span class="math inline">\(\alpha\)</span> as the causal effect of participating in the treatment. In practice, this will be very similar to what we have done before — so the process would not be hard, but convincing someone (or even yourself) that unconfoundedness holds will be the bigger issue here.</p>
<p>As a final comment, the assumption of treatment effect homogeneity is not quite so innocuous here. It turns out that you can show that, in the presence of treatment effect heterogeneity, <span class="math inline">\(\alpha\)</span> will be equal to a weighted average of individual treatment effects, but the weights can sometimes be “strange.” There are methods that are robust to treatment effect heterogeneity (they are beyond the scope of the current class, but they are not “way” more difficult than what we are doing here). That said, in my experience, the regression estimators (under treatment effect homogeneity) tend to deliver similar estimates to alternative estimators that are robust to treatment effect heterogeneity at least in the setup considered in this section.</p>
</div>
<div id="panel-data-approaches" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Panel Data Approaches</h2>
<p>SW All of Ch. 10 and 13.4</p>
<p>In the previous section, we invoked the assumption of unconfoundedness and were in the setup where <span class="math inline">\(X\)</span> was fully observed. But suppose instead that you thought this alternative version of unconfoundedness held
<span class="math display">\[\begin{align*}
  (Y(1),Y(0)) \perp D | (X,W)
\end{align*}\]</span>
where <span class="math inline">\(X\)</span> were observed random variables, but <span class="math inline">\(W\)</span> were not observed. Following exactly the same argument as in the previous section, this would lead to a regression like
<span class="math display">\[\begin{align*}
  Y_i = \alpha D_i + \beta_0 + \beta_1 X_i + \beta_2 W_i + U_i
\end{align*}\]</span>
(I’m just including one <span class="math inline">\(X\)</span> and one <span class="math inline">\(W\)</span> for simplicity, but you can easily imagine the case where there are more.) If <span class="math inline">\(W\)</span> were observed, then we could just run this regression, but since <span class="math inline">\(W\)</span> is not observed, we run into the problem of omitted variable bias (i.e., if we just ignore <span class="math inline">\(W\)</span>, we won’t be estimating the causal effect <span class="math inline">\(\alpha\)</span>)</p>
<p>In this section, we’ll consider the case where a researcher has access to a different type of data called <strong>panel data</strong>. Panel data is data that follows the same individual (or firm, etc.) over time. In this case, it is often helpful to index variables by time. For example, <span class="math inline">\(Y_{it}\)</span> is the outcome for individual <span class="math inline">\(i\)</span> in time period <span class="math inline">\(t\)</span>. <span class="math inline">\(X_{it}\)</span> is the value of a regressor for individual <span class="math inline">\(i\)</span> in time period <span class="math inline">\(t\)</span> and <span class="math inline">\(D_{it}\)</span> is the value of the treatment for individual <span class="math inline">\(i\)</span> in time period <span class="math inline">\(t\)</span>. If some variable doesn’t vary over time (e.g., a regressor like race), we won’t use a <span class="math inline">\(t\)</span> subscript.</p>
<p>Panel data potentially gives us a way around the problem of not observing some variables that we would like to condition on in the model. This is particularly likely to be the case when <span class="math inline">\(W\)</span> does not vary over time. Let’s start with there are exactly two time periods of panel data. In that case, we can write
<span class="math display">\[\begin{align*}
  Y_{it} = \alpha D_{it} + \beta_0 + \beta_1 X_{it} + \beta_2 W_i + U_{it}
\end{align*}\]</span>
where we consider the case where <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> both change over time. Then, defining <span class="math inline">\(\Delta Y_{it} = Y_{it} - Y_{it-1}\)</span> (and using similar notation for other variables), notice that
<span class="math display">\[\begin{align*}
  \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \Delta U_{it}
\end{align*}\]</span>
which, importantly, no longer involves the unobserved <span class="math inline">\(W_i\)</span> and suggests running the above regression and interpreting the estimated version of <span class="math inline">\(\alpha\)</span> as an estimate of the causal effect of participating in the treatment.</p>
<ul>
<li><p><strong>Time fixed effects</strong> — The previous regression did not include an intercept. It is common in applied work to allow for the intercept to vary over time (i.e., so that <span class="math inline">\(\beta_0 = \beta_{0,t}\)</span>) which allows for “aggregate shocks” such as recessions or common trends in outcomes over time. In practice, this amounts to just including an intercept in the previous regression, for example,</p>
<p><span class="math display">\[
    \Delta Y_{it} = \underbrace{\theta_t} + \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \Delta U_{it} 
  \]</span></p></li>
</ul>
<p>Often, there may be many omitted, time invariant variables. In practice, these are usually just lumped into a single <strong>individual fixed effect</strong> — even if there are many time invariant, unobserved variables, we can difference them all out at the same time
<span class="math display">\[\begin{align*}
  Y_{it} &amp;= \alpha D_{it} + \beta_{0,t} + \beta_1 X_{it} + \underbrace{\beta_2 W_{1i} + \beta_3 W_{2i} + \beta_4 W_{3i}} + U_{it} \\
  &amp;= \alpha D_{it} + \beta_{0,t} + \underbrace{\eta_i} + U_{it} 
\end{align*}\]</span>
and we can follow the same strategies as above.</p>
<p>Another case that is common in practice is when there are more than two time periods. This case is similar to the previous one except there are multiple ways to eliminate the unobserved fixed effect. The two most common are the</p>
<ul>
<li><p><strong>Within estimator</strong></p>
<p>To motivate this approach, notice that, if, for each individual, we average their outcomes over time, the we get
<span class="math display">\[\begin{align*}
    \bar{Y}_i = \alpha \bar{D}_i + \beta_1 \bar{X}_i + (\textrm{time fixed effects}) + \bar{U}_i
  \end{align*}\]</span>
(where I have just written “time fixed effects” to indicate that these are transformed version of original fixed but still show up here.) Subtracting this equation from the expression for <span class="math inline">\(Y_{it}\)</span> gives
<span class="math display">\[\begin{align*}
    Y_{it} - \bar{Y}_i = \alpha (D_{it} - \bar{D}_i) + \beta_1 (X_{it} - \bar{X}_i) + (\textrm{time fixed effects}) + U_{it} - \bar{U}_i
  \end{align*}\]</span></p>
<p>This is a feasible regression to estimate (everything is observed here). This is called a within estimator because the terms <span class="math inline">\(\bar{Y}_i\)</span>, <span class="math inline">\(\bar{D}_i\)</span>, and <span class="math inline">\(\bar{X}_i\)</span> are the within-individual averages-over-time of the corresponding variable.</p></li>
<li><p><strong>First differences</strong></p>
<p>Another approach to eliminating the unobserved fixed effects is to directly consider <span class="math inline">\(\Delta Y_{it}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \Delta U_{it}
  \end{align*}\]</span></p>
<p>This is the same expression as we had before for the two period case. Only here you would include observations from all available time periods on <span class="math inline">\(\Delta Y_{it}, \Delta D_{it}, \Delta X_{it}\)</span> in the regression.</p></li>
</ul>
<p>It’s worth mentioning the cases where a fixed effects strategy can break down:</p>
<ul>
<li><p>Unobserved variables vary over time</p>
<p><span class="math display">\[
    Y_{it} = \alpha D_{it} + \beta_0 + \beta_1 X_{it} + \beta_2 \underbrace{W_{it}} + U_{it}
  \]</span>
In this case,</p>
<p><span class="math display">\[
    \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \beta_2 \underbrace{\Delta W_{it}} + \Delta U_{it}
  \]</span></p>
<p>which still involves the unobserved <span class="math inline">\(W_{it}\)</span>, and implies that the fixed effects regression will contain omitted variable bias.</p></li>
<li><p>The effect of unobserved variables varies over time</p>
<p><span class="math display">\[
    Y_{it} = \alpha D_{it} + \beta_0 + \beta_1 X_{it} + \underbrace{\beta_{2,t}} W_i + U_{it}
  \]</span>
In this case,</p>
<p><span class="math display">\[
    \Delta Y_{it} = \alpha \Delta D_{it} + \beta_1 \Delta X_{it} + \underbrace{(\beta_{2,t} - \beta_{2,t-1})} W_i + \Delta U_{it}
  \]</span></p>
<p>which still involves the unobserved <span class="math inline">\(W_i\)</span> (even though it doesn’t vary over time) and, therefore, the fixed effects regressions we have been considering will contain omitted variable bias.</p></li>
</ul>
<p>Also, the assumption of treatment effect homogeneity can potentially matter a lot in this context. This will particularly be the case when (i) individuals can become treated at different points in time, and (ii) there are treatment effect dynamics (so that the effect of participating in the treatment can vary over time) — both of these are realistic in many applications. This is a main research area of mine and one I am happy to talk way more about.</p>
<div id="difference-in-differences" class="section level3" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Difference in differences</h3>
<p>The panel data approaches that we have been talking about so far are closely related to a <strong>natural-experiment</strong> type of strategy called <strong>difference in differences</strong> (DID).</p>
<p>One important difference relative to the previous approach is that DID is typically implemented when some units (these are often states or particular locations) implement a policy at some time period while others do not; and, in particular, we observe some periods before any units participate in the treatment.</p>
<p>Let’s think about the case with exactly two time periods: <span class="math inline">\(t\)</span> and <span class="math inline">\(t-1\)</span>. In this case, we’ll suppose that the outcomes that we observe are
<span class="math display">\[\begin{align*}
  Y_{it} &amp;= D_i Y_{it}(1) + (1-D_i) Y_{it}(0) \\
  Y_{it-1} &amp;= Y_{it-1}(0)
\end{align*}\]</span>
In other words, in the second period, we observe treated potential outcomes for treated units and untreated potential outcomes for untreated units (this is just like the cross-sectional case above). But in the first period, we observe untreated potential outcomes for all units — because no one is treated yet.</p>
<p>DID is often motivated by an assumption called the parallel trends assumption:</p>
<p><strong>Parallel Trends Assumption</strong>
<span class="math display">\[\begin{align*}
\mathbb{E}[\Delta Y_t(0) | D=1] = \mathbb{E}[\Delta Y_t(0) | D=0]
\end{align*}\]</span>
This says that the <em>path</em> of outcomes that individuals in the treated group would have experienced if they had not been treated is the same as the path of outcomes that individual in the untreated group actually experienced.</p>
<p>As before, we continue to be interested in
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[Y_t(1) - Y_t(0) | D=1]
\end{align*}\]</span>
Recall that the key identification challenge if for <span class="math inline">\(\mathbb{E}[Y_t(0)|D=1]\)</span> here, and notice that
<span class="math display">\[\begin{align*}
  \mathbb{E}[Y_t(0) | D=1] &amp;= \mathbb{E}[\Delta Y_t(0) | D=1] + \mathbb{E}[Y_{t-1}(0) | D=1] \\
  &amp;= \mathbb{E}[\Delta Y_t(0) | D=0] + \mathbb{E}[Y_{t-1}(0)|D=1] \\
  &amp;= \mathbb{E}[\Delta Y_t | D=0] + \mathbb{E}[Y_{t-1}|D=1]
\end{align*}\]</span>
where the first equality adds and subtracts <span class="math inline">\(\mathbb{E}[Y_{t-1}(0)|D=1]\)</span>, the second equality uses the parallel trends assumption, and the last equality holds because all the potential outcomes in the previous line are actually observed outcome. Plugging this expression into the one for <span class="math inline">\(ATT\)</span> yields:
<span class="math display">\[\begin{align*}
  ATT = \mathbb{E}[\Delta Y_t | D=1] - \mathbb{E}[\Delta Y_t | D=0]
\end{align*}\]</span>
In other words, under parallel trends, the <span class="math inline">\(ATT\)</span> can be recovered by comparing the path of outcomes that treated units experienced relative to the path of outcomes that untreated units experienced (the latter of which is the path of outcomes that treated units would have experienced if they had not participated in the treatment).</p>
<p>As above, it is often convenient to estimate <span class="math inline">\(ATT\)</span> using a regression. In fact, you can show that (in the case with two periods), <span class="math inline">\(\alpha\)</span> in the following regression is equal to the <span class="math inline">\(ATT\)</span>:
<span class="math display">\[\begin{align*}
  Y_{it} = \alpha D_{it} + \theta_t + \eta_i + v_{it}
\end{align*}\]</span>
where <span class="math inline">\(\mathbb{E}[v_t | D] = 0\)</span>.</p>
</div>
<div id="computation-11" class="section level3" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Computation</h3>
<p>The lab in this chapter uses panel data to think about causal effects, so I won’t provide an extended discussion here but rather just mention the syntax for a few panel data estimators. Let’s suppose that you had a data frame that, for the first few rows, looked like (this is totally made up data)</p>
<pre><code>#&gt;   id year         Y       X1 X2
#&gt; 1  1 2019  87.92934 495.4021  1
#&gt; 2  1 2020 102.77429 495.6269  1
#&gt; 3  1 2021 110.84441 495.4844  0
#&gt; 4  2 2019  76.54302 492.8797  1
#&gt; 5  2 2020 104.29125 496.1825  1
#&gt; 6  2 2021 105.06056 492.0129  1</code></pre>
<p>This is what panel data typically looks like — here, we are following a single individual (who is distinguished by their <code>id</code> variable) over three years (from 2019-2021), there is an outcome <code>Y</code> and potential regressors <code>X1</code> and <code>X2</code>.</p>
<p>There are several packages in <code>R</code> for estimating the fixed effects models that we have been considering. I mainly use <code>plm</code> (for “panel linear models”), so I’ll show you that one and then mention one more.</p>
<p>For <code>plm</code>, if you want to estimate a fixed effects model in first differences, you would use the <code>plm</code> command with the following sort of syntax</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="causal-inference.html#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb147-2"><a href="causal-inference.html#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> <span class="fu">as.factor</span>(year), </span>
<span id="cb147-3"><a href="causal-inference.html#cb147-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data=</span>name_of_data,</span>
<span id="cb147-4"><a href="causal-inference.html#cb147-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb147-5"><a href="causal-inference.html#cb147-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">model=</span><span class="st">&quot;fd&quot;</span>,</span>
<span id="cb147-6"><a href="causal-inference.html#cb147-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">index=</span><span class="st">&quot;id&quot;</span>)</span></code></pre></div>
<p>We include here <code>as.factor(year)</code> to include time fixed effects, <code>effect="individual"</code> means to include an individual fixed effect, <code>model="fd"</code> says to estimate the model in first differences, and <code>index="id"</code> means that the individual identifier is in the column “id.”</p>
<p>The code for estimating the model using a within transformation is very similar:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="causal-inference.html#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> <span class="fu">as.factor</span>(year), </span>
<span id="cb148-2"><a href="causal-inference.html#cb148-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">data=</span>name_of_data,</span>
<span id="cb148-3"><a href="causal-inference.html#cb148-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb148-4"><a href="causal-inference.html#cb148-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">model=</span><span class="st">&quot;within&quot;</span>,</span>
<span id="cb148-5"><a href="causal-inference.html#cb148-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">index=</span><span class="st">&quot;id&quot;</span>)</span></code></pre></div>
<p>The only difference is that <code>model="fd"</code> has been replaced with <code>model="within"</code>.</p>
<p>Let me also just mention that the <code>estimatr</code> package can estimate a fixed effects model using a within transformation. The code for this case would look like</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="causal-inference.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span>
<span id="cb149-2"><a href="causal-inference.html#cb149-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-3"><a href="causal-inference.html#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lm_robust</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> <span class="fu">as.factor</span>(year), </span>
<span id="cb149-4"><a href="causal-inference.html#cb149-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">data=</span>name_of_data,</span>
<span id="cb149-5"><a href="causal-inference.html#cb149-5" aria-hidden="true" tabindex="-1"></a>          <span class="at">fixed_effects=</span><span class="sc">~</span>id)</span></code></pre></div>
<p>I think the advantage of using this approach is that it seems straightforward to get the heteroskedasticity robust standard errors (or cluster-robust standard errors) that are popular in economics (as we have done before for heteroskedasticity robust standard errors for a regression with just cross sectional data). But I am not sure how (or if it is possible) to use <code>estimatr</code> to estimate the fixed effects model in first differences.</p>
</div>
</div>
<div id="instrumental-variables" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Instrumental Variables</h2>
<p>SW all of chapter 12</p>
<p>In the previous section, I used the word <strong>natural experiment</strong> but didn’t really define it. When an actual experiment is not actually available, a very common strategy used by researchers interested in causal effects is to consider natural experiments — these are not actual experiments, but more like the case where “something weird” happens that makes some individuals more likely to participate in the treatment without otherwise affecting their outcomes. This “something weird” is called an <strong>instrumental variable</strong>.</p>
<p>Let me give you some examples:</p>
<ul>
<li><p>This is not as popular of a topic as it used to be, but many economists used to be interested in the causal effect of military service on earnings. This is challenging because individuals “self-select” into the military (i.e., individuals don’t just randomly choose to join the military, and, while there may be many dimensions of choosing to join the military, probably one dimension is what a person expects the effect to be on their future earnings).</p>
<ul>
<li>A famous example of an instrumental variable in this case is an individual’s Vietname draft lottery number. Here, the idea is that a randomly generated lottery number (by construction) doesn’t have any direct effect on earnings, but it does affect the chances that someone participates in the military. This is therefore a natural experiment and could serve the role of an instrumental variable.</li>
</ul></li>
<li><p>For studying the effect of education on on earnings, researchers have used the day of birth as an instrument for years of education. The idea is that compulsory school laws are set up so that individuals can leave school when they reach a certain age (e.g., 16). But this means that, among students that want to drop out as early as they can, students who have an “early” birthday (usually around October) will have spent less time in school than students who have a “late” birthday (usually around July) at any particular age. This is a kind of natural experiment — comparing earnings of students who drop out at 16 for those who have early birthdays relative to late birthdays.</p></li>
</ul>
<p>Let’s formalize these arguments. Using the same arguments as before, suppose we have a regression that we’d like to run</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \alpha D_i + \underbrace{\beta_1 W_i + U_i}_{V_i}
\]</span>
and interpret our estimate of <span class="math inline">\(\alpha\)</span> as an estimate of the causal effect of participating in the treatment. And where, for simplicity, I am not including any <span class="math inline">\(X\)</span> covariates and where we do not observe <span class="math inline">\(W\)</span>. If <span class="math inline">\(D\)</span> is correlated with <span class="math inline">\(W\)</span>, then just ignoring <span class="math inline">\(W\)</span> and running a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> will result in omitted variable bias so that regression does not recover an estimate of <span class="math inline">\(\alpha\)</span>. To help with the discussion below, we’ll define <span class="math inline">\(V_i\)</span> to be the entire unobservable term, <span class="math inline">\(\beta_1 W_i + U_i\)</span>, in the above equation.</p>
<p>An instrumental variable, which we’ll call <span class="math inline">\(Z\)</span>, needs to satisfy the following two conditions:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathrm{cov}(Z,V) = 0\)</span> — This condition is called the <strong>exclusion restriction</strong>, and it means that the instrument is uncorrelated with the error term in the above equation. In practice, we’d mainly need to make sure that it is uncorrelated with whatever we think is in <span class="math inline">\(W\)</span>.</p></li>
<li><p><span class="math inline">\(\mathrm{cov}(Z,D) \neq 0\)</span> — This condition is called <strong>instrument relevance</strong>, and it means that the instrument needs to actually affect whether or not an individual participates in the treatment. We’ll see why this condition is important momentarily.</p></li>
</ol>
<p>Next, notice that</p>
<p><span class="math display">\[
  \begin{aligned}
  \mathrm{cov}(Z,Y) &amp;= \mathrm{cov}(Z,\beta_0 + \alpha D + V) \\
  &amp;= \alpha \mathrm{cov}(Z,D)
  \end{aligned}
\]</span>
which holds because <span class="math inline">\(\mathrm{cov}(Z,\beta_0) = 0\)</span> (because <span class="math inline">\(\beta_0\)</span> is a constant) and <span class="math inline">\(\mathrm{cov}(Z,V)=0\)</span> by the first condition of <span class="math inline">\(Z\)</span> being a valid instrument. This implies that</p>
<p><span class="math display">\[
  \alpha = \frac{\mathrm{cov}(Z,Y)}{\mathrm{cov}(Z,D)}
\]</span>
That is, if we have a valid instrument, the above formula gives us a path to recovering the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. [Now you can also see why we needed the second condition — otherwise, we could divide by 0 here.]</p>
<p>The intuition for this is the following: changes in the instrument can cause changes in the outcome but only because they can change whether or not an individual participates in the treatment. These changes show up in the numerator. They are scaled by how much changes in the instrument result in changes in the treatment.</p>
<p>If there are other covariates in the model, the formula for <span class="math inline">\(\alpha\)</span> will become more complicated. But you can use the <code>ivreg</code> function in the <code>ivreg</code> package to make these complications for you.</p>
<div id="example-return-to-education" class="section level3" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> Example: Return to Education</h3>
<p>In this example, we’ll estimate the return to education using whether or not an individual lives close to a college as an instrument for attending college. The idea is that (at least after controlling for some other covariates), the distance that a person lives from a college should not directly affect their earnings but it could affect whether or not they attend college due to it being more or less convenient. I think that the papers that use this sort of an idea primarily have in mind that distance-to-college may affect whether or not a student attends a community college rather than a university.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="causal-inference.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ivreg)</span>
<span id="cb150-2"><a href="causal-inference.html#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb150-3"><a href="causal-inference.html#cb150-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-4"><a href="causal-inference.html#cb150-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;SchoolingReturns&quot;</span>, <span class="at">package=</span><span class="st">&quot;ivreg&quot;</span>)</span>
<span id="cb150-5"><a href="causal-inference.html#cb150-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-6"><a href="causal-inference.html#cb150-6" aria-hidden="true" tabindex="-1"></a>lm_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> education <span class="sc">+</span> <span class="fu">poly</span>(experience, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> ethnicity <span class="sc">+</span> smsa <span class="sc">+</span> south,</span>
<span id="cb150-7"><a href="causal-inference.html#cb150-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> SchoolingReturns)</span>
<span id="cb150-8"><a href="causal-inference.html#cb150-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-9"><a href="causal-inference.html#cb150-9" aria-hidden="true" tabindex="-1"></a>iv_reg <span class="ot">&lt;-</span> <span class="fu">ivreg</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> education <span class="sc">+</span> <span class="fu">poly</span>(experience, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> ethnicity <span class="sc">+</span> smsa <span class="sc">+</span> south, </span>
<span id="cb150-10"><a href="causal-inference.html#cb150-10" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span> nearcollege <span class="sc">+</span> <span class="fu">poly</span>(age, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> ethnicity <span class="sc">+</span> smsa <span class="sc">+</span> south,</span>
<span id="cb150-11"><a href="causal-inference.html#cb150-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> SchoolingReturns)</span>
<span id="cb150-12"><a href="causal-inference.html#cb150-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-13"><a href="causal-inference.html#cb150-13" aria-hidden="true" tabindex="-1"></a>reg_list <span class="ot">&lt;-</span> <span class="fu">list</span>(lm_reg, iv_reg)</span>
<span id="cb150-14"><a href="causal-inference.html#cb150-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-15"><a href="causal-inference.html#cb150-15" aria-hidden="true" tabindex="-1"></a><span class="fu">modelsummary</span>(reg_list)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
Model 1
</th>
<th style="text-align:center;">
Model 2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:center;">
4.734
</td>
<td style="text-align:center;">
4.066
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.068)
</td>
<td style="text-align:center;">
(0.608)
</td>
</tr>
<tr>
<td style="text-align:left;">
education
</td>
<td style="text-align:center;">
0.074
</td>
<td style="text-align:center;">
0.133
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.004)
</td>
<td style="text-align:center;">
(0.051)
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(experience, 2, raw = TRUE)1
</td>
<td style="text-align:center;">
0.084
</td>
<td style="text-align:center;">
0.056
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.007)
</td>
<td style="text-align:center;">
(0.026)
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(experience, 2, raw = TRUE)2
</td>
<td style="text-align:center;">
−0.002
</td>
<td style="text-align:center;">
−0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.000)
</td>
<td style="text-align:center;">
(0.001)
</td>
</tr>
<tr>
<td style="text-align:left;">
ethnicityafam
</td>
<td style="text-align:center;">
−0.190
</td>
<td style="text-align:center;">
−0.103
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.018)
</td>
<td style="text-align:center;">
(0.077)
</td>
</tr>
<tr>
<td style="text-align:left;">
smsayes
</td>
<td style="text-align:center;">
0.161
</td>
<td style="text-align:center;">
0.108
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
(0.016)
</td>
<td style="text-align:center;">
(0.050)
</td>
</tr>
<tr>
<td style="text-align:left;">
southyes
</td>
<td style="text-align:center;">
−0.125
</td>
<td style="text-align:center;">
−0.098
</td>
</tr>
<tr>
<td style="text-align:left;box-shadow: 0px 1px">
</td>
<td style="text-align:center;box-shadow: 0px 1px">
(0.015)
</td>
<td style="text-align:center;box-shadow: 0px 1px">
(0.029)
</td>
</tr>
<tr>
<td style="text-align:left;">
Num.Obs.
</td>
<td style="text-align:center;">
3010
</td>
<td style="text-align:center;">
3010
</td>
</tr>
<tr>
<td style="text-align:left;">
R2
</td>
<td style="text-align:center;">
0.291
</td>
<td style="text-align:center;">
0.176
</td>
</tr>
<tr>
<td style="text-align:left;">
R2 Adj.
</td>
<td style="text-align:center;">
0.289
</td>
<td style="text-align:center;">
0.175
</td>
</tr>
<tr>
<td style="text-align:left;">
AIC
</td>
<td style="text-align:center;">
2633.4
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;">
BIC
</td>
<td style="text-align:center;">
2681.5
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Log.Lik.
</td>
<td style="text-align:center;">
−1308.702
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:center;">
204.932
</td>
<td style="text-align:center;">
</td>
</tr>
</tbody>
</table>
<p>The main parameter of interest here is the coefficient on <code>education</code>. The IV estimates are noticeably larger than the OLS estimates (<code>0.133</code> relative to <code>0.074</code>). [This is actually quite surprising as you would think that OLS would tend to <em>over-estimate</em> the return to education. This is a very famous example, and there are actually quite a few “explanations” from labor economists about why this sort of result arises.]</p>
</div>
</div>
<div id="regression-discontinuity" class="section level2" number="6.7">
<h2><span class="header-section-number">6.7</span> Regression Discontinuity</h2>
<p>SW 13.4</p>
<p>The final type of natural experiment that we will talk about is called <strong>regression discontinuity</strong>. The sort of natural experiment is available when there is a <strong>running variable</strong> with a threshold (i.e., cutoff) where individuals above the threshold are treated while individuals below the threshold are not treated. These sorts of thresholds/cutoffs are fairly common.</p>
<p>Here are some examples:</p>
<ul>
<li><p>Cutoffs that make students eligible for a scholarship (e.g., the Hope scholarship)</p></li>
<li><p>Rules about maximum numbers of students allowed in a classroom in a particular school district</p></li>
<li><p>Very close political elections</p></li>
<li><p>Very close union elections</p></li>
<li><p>Thresholds in tax laws</p></li>
</ul>
<p>Then, the idea is to compare outcomes among individuals that “barely” were treated relative to those that “barely” weren’t treated. By construction, this often has properties that are similar to an actual experiment as those that are just above the cutoff should have observed and unobserved characteristics that are the same as those just below the cutoff.</p>
<p>Typically, regression discontinuity designs are implemented using a regression that includes a binary variable for participating in the treatment, the running variable itself, and the interaction between the running variable and the treatment, <em>using only observations that are “close” to the cutoff</em>. [What should be considered “close” to the cutoff is actually a hard choice and there are tons of papers suggesting various approaches to decide what “close” means — we’ll largely avoid this and just pick what we think is close.] The estimated coefficient on the treatment indicator variable is an estimate of the average effect of participating in the treatment among those individuals who are close to the cutoff.</p>
<p>This will become clearer with an example.</p>
<div id="example-causal-effect-of-alcohol-on-driving-deaths" class="section level3" number="6.7.1">
<h3><span class="header-section-number">6.7.1</span> Example: Causal effect of Alcohol on Driving Deaths</h3>
<p>In this section, we’ll be interested in the causal effect of young adult alcohol consumption on the number of deaths in car accidents.</p>
<p>The idea here will be to compare the number of deaths in car accidents that involve someone who is 21 or just over to the number of deaths in car accidents that involve someone who is just under 21. The reason to make this comparison is that alcohol consumption markedly increases when individuals turn 21 (due to that being the legal drinking age in the U.S.). If alcohol consumption increases car accident deaths, then we should also be able to detect a jump in the number of car accident deaths involving those who are just over 21.</p>
<p>The data that we have consists of age groups by age up to a particular month (<code>agecell</code>) and the number of car accident deaths involving that age group (<code>mva</code>).</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="causal-inference.html#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb151-2"><a href="causal-inference.html#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;mlda&quot;</span>, <span class="at">package=</span><span class="st">&quot;masteringmetrics&quot;</span>)</span>
<span id="cb151-3"><a href="causal-inference.html#cb151-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-4"><a href="causal-inference.html#cb151-4" aria-hidden="true" tabindex="-1"></a><span class="co"># drop some data with missing observations</span></span>
<span id="cb151-5"><a href="causal-inference.html#cb151-5" aria-hidden="true" tabindex="-1"></a>mlda <span class="ot">&lt;-</span> mlda[<span class="fu">complete.cases</span>(mlda),]</span>
<span id="cb151-6"><a href="causal-inference.html#cb151-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-7"><a href="causal-inference.html#cb151-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create treated variable</span></span>
<span id="cb151-8"><a href="causal-inference.html#cb151-8" aria-hidden="true" tabindex="-1"></a>mlda<span class="sc">$</span>D <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(mlda<span class="sc">$</span>agecell <span class="sc">&gt;=</span> <span class="dv">21</span>)</span></code></pre></div>
<p>In regression discontinuity designs, it is very common to show a plot of the data. That’s what we’ll do here.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="causal-inference.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mlda, <span class="fu">aes</span>(<span class="at">x=</span>agecell, <span class="at">y=</span>mva)) <span class="sc">+</span> </span>
<span id="cb152-2"><a href="causal-inference.html#cb152-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb152-3"><a href="causal-inference.html#cb152-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">21</span>) <span class="sc">+</span> </span>
<span id="cb152-4"><a href="causal-inference.html#cb152-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;age&quot;</span>) <span class="sc">+</span> </span>
<span id="cb152-5"><a href="causal-inference.html#cb152-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;moving vehicle accident deaths&quot;</span>) <span class="sc">+</span> </span>
<span id="cb152-6"><a href="causal-inference.html#cb152-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-156-1.png" width="672" /></p>
<p>This figure at least suggests that the number of car accident deaths does appear to jump at age 21.</p>
<p>Now, let’s run the regression that we talked about earlier, involving a treatment dummy, age, and age interacted with the treatment.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="causal-inference.html#cb153-1" aria-hidden="true" tabindex="-1"></a>rd_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(mva <span class="sc">~</span> D <span class="sc">+</span> agecell <span class="sc">+</span> agecell<span class="sc">*</span>D, <span class="at">data=</span>mlda)</span>
<span id="cb153-2"><a href="causal-inference.html#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(rd_reg)</span>
<span id="cb153-3"><a href="causal-inference.html#cb153-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb153-4"><a href="causal-inference.html#cb153-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb153-5"><a href="causal-inference.html#cb153-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = mva ~ D + agecell + agecell * D, data = mlda)</span></span>
<span id="cb153-6"><a href="causal-inference.html#cb153-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb153-7"><a href="causal-inference.html#cb153-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb153-8"><a href="causal-inference.html#cb153-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb153-9"><a href="causal-inference.html#cb153-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -2.4124 -0.7774 -0.2913  0.8495  3.2378 </span></span>
<span id="cb153-10"><a href="causal-inference.html#cb153-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb153-11"><a href="causal-inference.html#cb153-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb153-12"><a href="causal-inference.html#cb153-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb153-13"><a href="causal-inference.html#cb153-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  83.8492     9.3328   8.984 1.63e-11 ***</span></span>
<span id="cb153-14"><a href="causal-inference.html#cb153-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; D            28.9450    13.8638   2.088   0.0426 *  </span></span>
<span id="cb153-15"><a href="causal-inference.html#cb153-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; agecell      -2.5676     0.4661  -5.508 1.77e-06 ***</span></span>
<span id="cb153-16"><a href="causal-inference.html#cb153-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; D:agecell    -1.1624     0.6592  -1.763   0.0848 .  </span></span>
<span id="cb153-17"><a href="causal-inference.html#cb153-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb153-18"><a href="causal-inference.html#cb153-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb153-19"><a href="causal-inference.html#cb153-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb153-20"><a href="causal-inference.html#cb153-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 1.299 on 44 degrees of freedom</span></span>
<span id="cb153-21"><a href="causal-inference.html#cb153-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.7222, Adjusted R-squared:  0.7032 </span></span>
<span id="cb153-22"><a href="causal-inference.html#cb153-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 38.13 on 3 and 44 DF,  p-value: 2.671e-12</span></span></code></pre></div>
<p>These results suggest that alcohol consumption increased car accident deaths (you can see this from the estimated coefficient on <code>D</code>). [The setup in this example is somewhat simplified; if you wanted to be careful about how much alcohol consumption increased car accident deaths, then we would probably need to scale up our estimate by how much alcohol consumption increases on average when people turn 21. Nevertheless, what we have presented above does suggest that alcohol consumption increases car accident deaths.]</p>
<p>Finally, let me show one more plot that is common to report in a regression discontinuity design.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="causal-inference.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get predicted values for plotting</span></span>
<span id="cb154-2"><a href="causal-inference.html#cb154-2" aria-hidden="true" tabindex="-1"></a>mlda<span class="sc">$</span>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(rd_reg)</span>
<span id="cb154-3"><a href="causal-inference.html#cb154-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-4"><a href="causal-inference.html#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="co"># make plot</span></span>
<span id="cb154-5"><a href="causal-inference.html#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mlda, <span class="fu">aes</span>(<span class="at">x=</span>agecell, <span class="at">y=</span>mva, <span class="at">color=</span><span class="fu">as.factor</span>(D))) <span class="sc">+</span></span>
<span id="cb154-6"><a href="causal-inference.html#cb154-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb154-7"><a href="causal-inference.html#cb154-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>preds)) <span class="sc">+</span> </span>
<span id="cb154-8"><a href="causal-inference.html#cb154-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">21</span>) <span class="sc">+</span></span>
<span id="cb154-9"><a href="causal-inference.html#cb154-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color=</span><span class="st">&quot;treated&quot;</span>, <span class="at">x=</span><span class="st">&quot;age&quot;</span>, <span class="at">y=</span><span class="st">&quot;moving vehicle accident deaths&quot;</span>) <span class="sc">+</span> </span>
<span id="cb154-10"><a href="causal-inference.html#cb154-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="Detailed-Course-Notes_files/figure-html/unnamed-chunk-158-1.png" width="672" /></p>
<p>This shows the two lines that we effectively fit with the regression that included the binary variable for the treatment, the running variable, and their interaction. The “jump” between the red line and the blue line at <code>age=21</code> is our estimated effect of the treatment.</p>
</div>
</div>
<div id="lab-5-drunk-driving-laws" class="section level2" number="6.8">
<h2><span class="header-section-number">6.8</span> Lab 5: Drunk Driving Laws</h2>
<p>For this lab, we will use the <code>Fatalities</code> data. We will study the causal effect of mandatory jail sentence policies for drunk driving on traffic fatalities. The <code>Fatalities</code> data consists of panel data of traffic fatality death rates, whether or not a state has a mandatory jail sentence policy or not as well as several other variables from 1982-1988. Economic theory suggests that raising the cost of some behavior (in this case, you can think of a mandatory jail sentence as raising the cost of drunk driving) will lead to less of that behavior. That being said, it’s both interesting to test this theory and also consider the magnitude of this effect. That’s what we’ll do in this problem.</p>
<ol style="list-style-type: decimal">
<li><p>This data comes in a somewhat messier format than some of the data that we have used previously. To start with, create a new column in the data called <code>afatal_per_million</code> that is the number of alcohol involved vehicle fatalities per millions people in a state in a particular year. The variable <code>afatal</code> contains the total number of alcohol involved vehicle fatalities, and the variable <code>pop</code> contains the total population in a state.</p></li>
<li><p>Using a subset of the data from 1988, run a regression of <code>afatal_per_million</code> on whether or not a state has a mandatory jail sentence policy <code>jail</code>. How do you interpret the results?</p></li>
<li><p>Using the same subset from part 2, run a regression of <code>afatal_per_million</code> on <code>jail</code>, unemployment rate (<code>unemp</code>), the tax on a case of beer (<code>beertax</code>), the percentage of southern baptists in the state (<code>baptist</code>), the percentage of residents residing in dry counties (<code>dry</code>), the percentage of young drivers in the state, (<code>youngdrivers</code>), and the average miles driven per person in a state (<code>miles</code>). How do you interpret the estimated coefficient on <code>jail</code>? Would you consider this to be a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related fatalities?</p></li>
<li><p>Now, using the full data, let’s estimate a fixed effects model with alcohol related fatalities per million as the outcome and mandatory jail policies as a regressor. Estimate the model using first differences and make sure to include time fixed effects. How do you interpret the results?</p></li>
<li><p>Estimate the same model as in part 4, but using the within estimator instead of first differences. Compare these results to the ones from part 4.</p></li>
<li><p>Using the same within estimator as in part 5, include the same set of covariates from part 3 and interpret the estimated effect of mandatory jail policies. How do these estimates compare to the earlier ones?</p></li>
<li><p>Now, we’ll switch to using a difference in differences approach to estimating the effect of mandatory jail policies. First, we’ll manipulate the data some.</p>
<ol style="list-style-type: lower-alpha">
<li><p>To keep things simple, let’s start by limiting the data to the years 1982 and 1988 and drop the in-between periods.</p></li>
<li><p>Second, let’s calculate the change in alcohol related fatalities per million between 1982 and 1998 and keep the covariates that we have been using from 1982. One way to do this, is to use the <code>pivot_wider</code> function from the <code>tidyr</code>. In the case of panel data, “long format” data means that each row in the data corresponds to a paricular observation <em>and</em> a particular time period. Thus, with long format data, there are <span class="math inline">\(n \times T\)</span> total rows in the data. On the other hand, “wide format” data means that each row holds all the data (across all time periods) for a particular observation. Converting back and forth between long and wide formats is a common data manipulation task. <strong>Hint:</strong> This step is probably unfamiliar, so I’d recommend seeing if you can use <code>?tidyr::pivot_wider</code> to see if you can figure out how to complete this step, but, if not, you can copy this code from the solutions in the next section.</p></li>
<li><p>Finally, drop all states that are already treated in 1982.</p></li>
</ol></li>
<li><p>Using the data that you constructed in part 7, implement the difference in differences regression of the change in alcohol related fatalities per million from 1982 to 1988 on the mandatory jail policy. How do you interpret these results and how do they compare to the previous ones? Now, additionally include the set of covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?</p></li>
<li><p>An alternative to DID, is to include the lagged outcome as a covariate. Using the data constructed in part 7, run a regression of alcohol related fatalities per million in 1988 on the mandatory jail policy and alcohol related fatalities per million in 1982. How do you interpret these results and how do they compare to the previous ones? Now include the additional covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?</p></li>
<li><p>Comment on your results from parts 1-9. Which, if any, of these are you most inclined to interpret as a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related policies?</p></li>
</ol>
</div>
<div id="lab-5-solutions" class="section level2" number="6.9">
<h2><span class="header-section-number">6.9</span> Lab 5: Solutions</h2>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="causal-inference.html#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb155-2"><a href="causal-inference.html#cb155-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb155-3"><a href="causal-inference.html#cb155-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Attaching package: &#39;tidyr&#39;</span></span>
<span id="cb155-4"><a href="causal-inference.html#cb155-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following objects are masked from &#39;package:Matrix&#39;:</span></span>
<span id="cb155-5"><a href="causal-inference.html#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb155-6"><a href="causal-inference.html#cb155-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     expand, pack, unpack</span></span>
<span id="cb155-7"><a href="causal-inference.html#cb155-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb155-8"><a href="causal-inference.html#cb155-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb155-9"><a href="causal-inference.html#cb155-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Attaching package: &#39;plm&#39;</span></span>
<span id="cb155-10"><a href="causal-inference.html#cb155-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following objects are masked from &#39;package:dplyr&#39;:</span></span>
<span id="cb155-11"><a href="causal-inference.html#cb155-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb155-12"><a href="causal-inference.html#cb155-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     between, lag, lead</span></span>
<span id="cb155-13"><a href="causal-inference.html#cb155-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-14"><a href="causal-inference.html#cb155-14" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Fatalities, <span class="at">package=</span><span class="st">&quot;AER&quot;</span>)</span>
<span id="cb155-15"><a href="causal-inference.html#cb155-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-16"><a href="causal-inference.html#cb155-16" aria-hidden="true" tabindex="-1"></a>Fatalities<span class="sc">$</span>afatal_per_million <span class="ot">&lt;-</span> <span class="dv">1000000</span> <span class="sc">*</span> (Fatalities<span class="sc">$</span>afatal <span class="sc">/</span> Fatalities<span class="sc">$</span>pop )</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="causal-inference.html#cb156-1" aria-hidden="true" tabindex="-1"></a>Fatalities88 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Fatalities, year<span class="sc">==</span><span class="dv">1988</span>)</span>
<span id="cb156-2"><a href="causal-inference.html#cb156-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-3"><a href="causal-inference.html#cb156-3" aria-hidden="true" tabindex="-1"></a>reg88 <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million <span class="sc">~</span> jail, <span class="at">data=</span>Fatalities88)</span>
<span id="cb156-4"><a href="causal-inference.html#cb156-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg88)</span>
<span id="cb156-5"><a href="causal-inference.html#cb156-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-6"><a href="causal-inference.html#cb156-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb156-7"><a href="causal-inference.html#cb156-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million ~ jail, data = Fatalities88)</span></span>
<span id="cb156-8"><a href="causal-inference.html#cb156-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-9"><a href="causal-inference.html#cb156-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb156-10"><a href="causal-inference.html#cb156-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb156-11"><a href="causal-inference.html#cb156-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -36.123 -16.622  -1.469   8.642 112.260 </span></span>
<span id="cb156-12"><a href="causal-inference.html#cb156-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-13"><a href="causal-inference.html#cb156-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb156-14"><a href="causal-inference.html#cb156-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb156-15"><a href="causal-inference.html#cb156-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)   59.496      4.273  13.923   &lt;2e-16 ***</span></span>
<span id="cb156-16"><a href="causal-inference.html#cb156-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jailyes        9.155      7.829   1.169    0.248    </span></span>
<span id="cb156-17"><a href="causal-inference.html#cb156-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb156-18"><a href="causal-inference.html#cb156-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb156-19"><a href="causal-inference.html#cb156-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb156-20"><a href="causal-inference.html#cb156-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 24.55 on 45 degrees of freedom</span></span>
<span id="cb156-21"><a href="causal-inference.html#cb156-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   (1 observation deleted due to missingness)</span></span>
<span id="cb156-22"><a href="causal-inference.html#cb156-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.02949,    Adjusted R-squared:  0.007921 </span></span>
<span id="cb156-23"><a href="causal-inference.html#cb156-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 1.367 on 1 and 45 DF,  p-value: 0.2484</span></span></code></pre></div>
<p>The estimated coefficient on mandatory jail laws is 9.155. We should interpret this as just the difference between alcohol related fatalities per million in states that had mandatory jail laws in 1988 relative to states that did not have them. We cannot reject that there is no difference between states where the policy is in place relative to those that do not have the policy.</p>
<ol start="3" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="causal-inference.html#cb157-1" aria-hidden="true" tabindex="-1"></a>reg88_covs <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles, <span class="at">data=</span>Fatalities88)</span>
<span id="cb157-2"><a href="causal-inference.html#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg88_covs)</span>
<span id="cb157-3"><a href="causal-inference.html#cb157-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb157-4"><a href="causal-inference.html#cb157-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb157-5"><a href="causal-inference.html#cb157-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + </span></span>
<span id="cb157-6"><a href="causal-inference.html#cb157-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     dry + youngdrivers + miles, data = Fatalities88)</span></span>
<span id="cb157-7"><a href="causal-inference.html#cb157-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb157-8"><a href="causal-inference.html#cb157-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb157-9"><a href="causal-inference.html#cb157-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb157-10"><a href="causal-inference.html#cb157-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -39.065  -9.907  -1.690   9.673  82.100 </span></span>
<span id="cb157-11"><a href="causal-inference.html#cb157-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb157-12"><a href="causal-inference.html#cb157-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb157-13"><a href="causal-inference.html#cb157-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span id="cb157-14"><a href="causal-inference.html#cb157-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  -29.373536  32.500240  -0.904   0.3717  </span></span>
<span id="cb157-15"><a href="causal-inference.html#cb157-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jailyes        3.120574   6.849271   0.456   0.6512  </span></span>
<span id="cb157-16"><a href="causal-inference.html#cb157-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; unemp          4.815081   1.892369   2.544   0.0150 *</span></span>
<span id="cb157-17"><a href="causal-inference.html#cb157-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; beertax        2.311850   9.521684   0.243   0.8094  </span></span>
<span id="cb157-18"><a href="causal-inference.html#cb157-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; baptist        0.661694   0.527228   1.255   0.2169  </span></span>
<span id="cb157-19"><a href="causal-inference.html#cb157-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dry           -0.026675   0.383956  -0.069   0.9450  </span></span>
<span id="cb157-20"><a href="causal-inference.html#cb157-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; youngdrivers  -0.092100 142.804244  -0.001   0.9995  </span></span>
<span id="cb157-21"><a href="causal-inference.html#cb157-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; miles          0.006802   0.002822   2.411   0.0207 *</span></span>
<span id="cb157-22"><a href="causal-inference.html#cb157-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb157-23"><a href="causal-inference.html#cb157-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb157-24"><a href="causal-inference.html#cb157-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb157-25"><a href="causal-inference.html#cb157-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 19.41 on 39 degrees of freedom</span></span>
<span id="cb157-26"><a href="causal-inference.html#cb157-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   (1 observation deleted due to missingness)</span></span>
<span id="cb157-27"><a href="causal-inference.html#cb157-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.4742, Adjusted R-squared:  0.3798 </span></span>
<span id="cb157-28"><a href="causal-inference.html#cb157-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 5.024 on 7 and 39 DF,  p-value: 0.0003999</span></span></code></pre></div>
<p>The estimated coefficient on jail is 3.12. It is somewhat smaller than the previous estimate, though neither is statistically significant. We should interpret this as the partial effect of the mandatory jail policy, that is, that we estimate that mandatory jail laws increase the number of alcohol related fatalities per million by 3.12 on average controlling for the unemployment rate, beer tax, the fraction of southern baptists in the state, the fraction of residents in dry counties, the fraction of young drivers, and the average miles driven in the state. We cannot reject that the partial effect of mandatory jail policies is equal to 0.</p>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="causal-inference.html#cb158-1" aria-hidden="true" tabindex="-1"></a>fd_reg <span class="ot">&lt;-</span> <span class="fu">plm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> <span class="fu">as.factor</span>(year),</span>
<span id="cb158-2"><a href="causal-inference.html#cb158-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb158-3"><a href="causal-inference.html#cb158-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">index=</span><span class="st">&quot;state&quot;</span>, <span class="at">model=</span><span class="st">&quot;fd&quot;</span>,</span>
<span id="cb158-4"><a href="causal-inference.html#cb158-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>Fatalities)</span>
<span id="cb158-5"><a href="causal-inference.html#cb158-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fd_reg)</span>
<span id="cb158-6"><a href="causal-inference.html#cb158-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Oneway (individual) effect First-Difference Model</span></span>
<span id="cb158-7"><a href="causal-inference.html#cb158-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb158-8"><a href="causal-inference.html#cb158-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb158-9"><a href="causal-inference.html#cb158-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; plm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, </span></span>
<span id="cb158-10"><a href="causal-inference.html#cb158-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     effect = &quot;individual&quot;, model = &quot;fd&quot;, index = &quot;state&quot;)</span></span>
<span id="cb158-11"><a href="causal-inference.html#cb158-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb158-12"><a href="causal-inference.html#cb158-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Unbalanced Panel: n = 48, T = 6-7, N = 335</span></span>
<span id="cb158-13"><a href="causal-inference.html#cb158-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Observations used in estimation: 287</span></span>
<span id="cb158-14"><a href="causal-inference.html#cb158-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb158-15"><a href="causal-inference.html#cb158-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb158-16"><a href="causal-inference.html#cb158-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min.   1st Qu.    Median   3rd Qu.      Max. </span></span>
<span id="cb158-17"><a href="causal-inference.html#cb158-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -51.66677  -5.09887   0.23801   6.28688 119.08976 </span></span>
<span id="cb158-18"><a href="causal-inference.html#cb158-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb158-19"><a href="causal-inference.html#cb158-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients: (1 dropped because of singularities)</span></span>
<span id="cb158-20"><a href="causal-inference.html#cb158-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t-value Pr(&gt;|t|)   </span></span>
<span id="cb158-21"><a href="causal-inference.html#cb158-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)         -2.15376    0.80673 -2.6697 0.008035 **</span></span>
<span id="cb158-22"><a href="causal-inference.html#cb158-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jailyes              2.60763    5.28351  0.4935 0.622016   </span></span>
<span id="cb158-23"><a href="causal-inference.html#cb158-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1983 -5.28423    1.82330 -2.8982 0.004050 **</span></span>
<span id="cb158-24"><a href="causal-inference.html#cb158-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1984 -3.58247    2.29451 -1.5613 0.119577   </span></span>
<span id="cb158-25"><a href="causal-inference.html#cb158-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1985 -5.60800    2.43517 -2.3029 0.022017 * </span></span>
<span id="cb158-26"><a href="causal-inference.html#cb158-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1986 -0.74192    2.28988 -0.3240 0.746180   </span></span>
<span id="cb158-27"><a href="causal-inference.html#cb158-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1987 -2.16244    1.80716 -1.1966 0.232476   </span></span>
<span id="cb158-28"><a href="causal-inference.html#cb158-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb158-29"><a href="causal-inference.html#cb158-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb158-30"><a href="causal-inference.html#cb158-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb158-31"><a href="causal-inference.html#cb158-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Total Sum of Squares:    54692</span></span>
<span id="cb158-32"><a href="causal-inference.html#cb158-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual Sum of Squares: 51620</span></span>
<span id="cb158-33"><a href="causal-inference.html#cb158-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; R-Squared:      0.056171</span></span>
<span id="cb158-34"><a href="causal-inference.html#cb158-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Adj. R-Squared: 0.035946</span></span>
<span id="cb158-35"><a href="causal-inference.html#cb158-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 2.77733 on 6 and 280 DF, p-value: 0.012223</span></span></code></pre></div>
<p>We should interpret the estimated coefficient on <code>jail</code> as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after accounting for time invariant variables whose effects do not change over time. Again, we cannot reject that the effect is equal to 0.</p>
<ol start="5" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="causal-inference.html#cb159-1" aria-hidden="true" tabindex="-1"></a>within_reg <span class="ot">&lt;-</span> <span class="fu">plm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> <span class="fu">as.factor</span>(year),</span>
<span id="cb159-2"><a href="causal-inference.html#cb159-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb159-3"><a href="causal-inference.html#cb159-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">index=</span><span class="st">&quot;state&quot;</span>, <span class="at">model=</span><span class="st">&quot;within&quot;</span>,</span>
<span id="cb159-4"><a href="causal-inference.html#cb159-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">data=</span>Fatalities)</span>
<span id="cb159-5"><a href="causal-inference.html#cb159-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(within_reg)</span>
<span id="cb159-6"><a href="causal-inference.html#cb159-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Oneway (individual) effect Within Model</span></span>
<span id="cb159-7"><a href="causal-inference.html#cb159-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-8"><a href="causal-inference.html#cb159-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb159-9"><a href="causal-inference.html#cb159-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; plm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, </span></span>
<span id="cb159-10"><a href="causal-inference.html#cb159-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     effect = &quot;individual&quot;, model = &quot;within&quot;, index = &quot;state&quot;)</span></span>
<span id="cb159-11"><a href="causal-inference.html#cb159-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-12"><a href="causal-inference.html#cb159-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Unbalanced Panel: n = 48, T = 6-7, N = 335</span></span>
<span id="cb159-13"><a href="causal-inference.html#cb159-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-14"><a href="causal-inference.html#cb159-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb159-15"><a href="causal-inference.html#cb159-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        Min.     1st Qu.      Median     3rd Qu.        Max. </span></span>
<span id="cb159-16"><a href="causal-inference.html#cb159-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -95.1937300  -4.9678238   0.0088078   5.1611249  40.6263546 </span></span>
<span id="cb159-17"><a href="causal-inference.html#cb159-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-18"><a href="causal-inference.html#cb159-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb159-19"><a href="causal-inference.html#cb159-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                     Estimate Std. Error t-value  Pr(&gt;|t|)    </span></span>
<span id="cb159-20"><a href="causal-inference.html#cb159-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jailyes               8.3327     4.9666  1.6777 0.0945164 .  </span></span>
<span id="cb159-21"><a href="causal-inference.html#cb159-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1983  -7.9151     2.6936 -2.9384 0.0035734 ** </span></span>
<span id="cb159-22"><a href="causal-inference.html#cb159-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1984  -8.4863     2.7115 -3.1298 0.0019341 ** </span></span>
<span id="cb159-23"><a href="causal-inference.html#cb159-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1985 -12.7849     2.7331 -4.6778 4.518e-06 ***</span></span>
<span id="cb159-24"><a href="causal-inference.html#cb159-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1986 -10.0726     2.7331 -3.6854 0.0002741 ***</span></span>
<span id="cb159-25"><a href="causal-inference.html#cb159-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1987 -13.5276     2.7115 -4.9890 1.067e-06 ***</span></span>
<span id="cb159-26"><a href="causal-inference.html#cb159-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; as.factor(year)1988 -13.6296     2.7279 -4.9964 1.030e-06 ***</span></span>
<span id="cb159-27"><a href="causal-inference.html#cb159-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb159-28"><a href="causal-inference.html#cb159-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb159-29"><a href="causal-inference.html#cb159-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb159-30"><a href="causal-inference.html#cb159-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Total Sum of Squares:    53854</span></span>
<span id="cb159-31"><a href="causal-inference.html#cb159-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual Sum of Squares: 47607</span></span>
<span id="cb159-32"><a href="causal-inference.html#cb159-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; R-Squared:      0.116</span></span>
<span id="cb159-33"><a href="causal-inference.html#cb159-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Adj. R-Squared: -0.054487</span></span>
<span id="cb159-34"><a href="causal-inference.html#cb159-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 5.24882 on 7 and 280 DF, p-value: 1.2051e-05</span></span></code></pre></div>
<p>The estimated coefficient on <code>jail</code> has the same interpretation as in the previous problem. The estimated effect here is marginally statistically significant.
6.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="causal-inference.html#cb160-1" aria-hidden="true" tabindex="-1"></a>within_reg_covs <span class="ot">&lt;-</span> <span class="fu">plm</span>(afatal_per_million <span class="sc">~</span> jail <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles,</span>
<span id="cb160-2"><a href="causal-inference.html#cb160-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">effect=</span><span class="st">&quot;individual&quot;</span>,</span>
<span id="cb160-3"><a href="causal-inference.html#cb160-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">index=</span><span class="st">&quot;state&quot;</span>, <span class="at">model=</span><span class="st">&quot;within&quot;</span>,</span>
<span id="cb160-4"><a href="causal-inference.html#cb160-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data=</span>Fatalities)</span>
<span id="cb160-5"><a href="causal-inference.html#cb160-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(within_reg_covs)</span>
<span id="cb160-6"><a href="causal-inference.html#cb160-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Oneway (individual) effect Within Model</span></span>
<span id="cb160-7"><a href="causal-inference.html#cb160-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-8"><a href="causal-inference.html#cb160-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb160-9"><a href="causal-inference.html#cb160-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; plm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + </span></span>
<span id="cb160-10"><a href="causal-inference.html#cb160-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     dry + youngdrivers + miles, data = Fatalities, effect = &quot;individual&quot;, </span></span>
<span id="cb160-11"><a href="causal-inference.html#cb160-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     model = &quot;within&quot;, index = &quot;state&quot;)</span></span>
<span id="cb160-12"><a href="causal-inference.html#cb160-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-13"><a href="causal-inference.html#cb160-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Unbalanced Panel: n = 48, T = 6-7, N = 335</span></span>
<span id="cb160-14"><a href="causal-inference.html#cb160-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-15"><a href="causal-inference.html#cb160-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb160-16"><a href="causal-inference.html#cb160-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Min.   1st Qu.    Median   3rd Qu.      Max. </span></span>
<span id="cb160-17"><a href="causal-inference.html#cb160-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -95.62306  -5.69773  -0.56903   4.79219  47.80871 </span></span>
<span id="cb160-18"><a href="causal-inference.html#cb160-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-19"><a href="causal-inference.html#cb160-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb160-20"><a href="causal-inference.html#cb160-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                 Estimate  Std. Error t-value  Pr(&gt;|t|)    </span></span>
<span id="cb160-21"><a href="causal-inference.html#cb160-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jailyes       4.9731e+00  4.9613e+00  1.0024   0.31702    </span></span>
<span id="cb160-22"><a href="causal-inference.html#cb160-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; unemp        -1.1340e+00  5.6592e-01 -2.0038   0.04605 *  </span></span>
<span id="cb160-23"><a href="causal-inference.html#cb160-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; beertax      -2.7456e+01  1.5080e+01 -1.8207   0.06972 .  </span></span>
<span id="cb160-24"><a href="causal-inference.html#cb160-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; baptist       2.5083e+00  4.3324e+00  0.5790   0.56308    </span></span>
<span id="cb160-25"><a href="causal-inference.html#cb160-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dry           4.3092e-01  1.0870e+00  0.3964   0.69208    </span></span>
<span id="cb160-26"><a href="causal-inference.html#cb160-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; youngdrivers  2.6357e+02  5.0169e+01  5.2537 2.957e-07 ***</span></span>
<span id="cb160-27"><a href="causal-inference.html#cb160-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; miles        -6.8899e-04  7.3182e-04 -0.9415   0.34727    </span></span>
<span id="cb160-28"><a href="causal-inference.html#cb160-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb160-29"><a href="causal-inference.html#cb160-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb160-30"><a href="causal-inference.html#cb160-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb160-31"><a href="causal-inference.html#cb160-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Total Sum of Squares:    53854</span></span>
<span id="cb160-32"><a href="causal-inference.html#cb160-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual Sum of Squares: 48083</span></span>
<span id="cb160-33"><a href="causal-inference.html#cb160-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; R-Squared:      0.10717</span></span>
<span id="cb160-34"><a href="causal-inference.html#cb160-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Adj. R-Squared: -0.065023</span></span>
<span id="cb160-35"><a href="causal-inference.html#cb160-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 4.80119 on 7 and 280 DF, p-value: 4.0281e-05</span></span></code></pre></div>
<p>We should interpret the estimated coefficient on <code>jail</code> as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after controlling for the unemployment rate, beer taxes, the fraction of the state that is southern baptist, the fraction of the state that lives in a dry county, the fraction of young drivers in a state, and the average number of miles driven per person in the stata, and accounting for time invariant variables whose effects do not change over time.</p>
<ol start="7" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="causal-inference.html#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="co"># part a: convert data to two period panel data</span></span>
<span id="cb161-2"><a href="causal-inference.html#cb161-2" aria-hidden="true" tabindex="-1"></a>two_period <span class="ot">&lt;-</span> <span class="fu">subset</span>(Fatalities, year<span class="sc">==</span><span class="dv">1982</span> <span class="sc">|</span> year<span class="sc">==</span><span class="dv">1988</span>)</span>
<span id="cb161-3"><a href="causal-inference.html#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and drop some missing</span></span>
<span id="cb161-4"><a href="causal-inference.html#cb161-4" aria-hidden="true" tabindex="-1"></a>two_period <span class="ot">&lt;-</span> <span class="fu">subset</span>(two_period, <span class="sc">!</span><span class="fu">is.na</span>(jail))</span>
<span id="cb161-5"><a href="causal-inference.html#cb161-5" aria-hidden="true" tabindex="-1"></a>two_period <span class="ot">&lt;-</span> BMisc<span class="sc">::</span><span class="fu">makeBalancedPanel</span>(two_period, <span class="st">&quot;state&quot;</span>, <span class="st">&quot;year&quot;</span>)</span>
<span id="cb161-6"><a href="causal-inference.html#cb161-6" aria-hidden="true" tabindex="-1"></a>two_period<span class="sc">$</span>jail <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(two_period<span class="sc">$</span>jail<span class="sc">==</span><span class="st">&quot;yes&quot;</span>)</span>
<span id="cb161-7"><a href="causal-inference.html#cb161-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-8"><a href="causal-inference.html#cb161-8" aria-hidden="true" tabindex="-1"></a><span class="co"># part b: convert into wide format</span></span>
<span id="cb161-9"><a href="causal-inference.html#cb161-9" aria-hidden="true" tabindex="-1"></a>wide_df <span class="ot">&lt;-</span> <span class="fu">pivot_wider</span>(two_period, </span>
<span id="cb161-10"><a href="causal-inference.html#cb161-10" aria-hidden="true" tabindex="-1"></a>                       <span class="at">id_cols=</span><span class="st">&quot;state&quot;</span>, </span>
<span id="cb161-11"><a href="causal-inference.html#cb161-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">names_from=</span><span class="st">&quot;year&quot;</span>,</span>
<span id="cb161-12"><a href="causal-inference.html#cb161-12" aria-hidden="true" tabindex="-1"></a>                       <span class="at">values_from=</span><span class="fu">c</span>(<span class="st">&quot;jail&quot;</span>, <span class="st">&quot;afatal_per_million&quot;</span>))</span>
<span id="cb161-13"><a href="causal-inference.html#cb161-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-14"><a href="causal-inference.html#cb161-14" aria-hidden="true" tabindex="-1"></a><span class="co"># add back other covariates from 1982</span></span>
<span id="cb161-15"><a href="causal-inference.html#cb161-15" aria-hidden="true" tabindex="-1"></a>wide_df <span class="ot">&lt;-</span> <span class="fu">merge</span>(wide_df, <span class="fu">subset</span>(Fatalities, year<span class="sc">==</span><span class="dv">1982</span>)[,<span class="fu">c</span>(<span class="st">&quot;unemp&quot;</span>, <span class="st">&quot;beertax&quot;</span>, <span class="st">&quot;baptist&quot;</span>, <span class="st">&quot;dry&quot;</span>, <span class="st">&quot;youngdrivers&quot;</span>, <span class="st">&quot;miles&quot;</span>,<span class="st">&quot;state&quot;</span>)], <span class="at">by=</span><span class="st">&quot;state&quot;</span>)</span>
<span id="cb161-16"><a href="causal-inference.html#cb161-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-17"><a href="causal-inference.html#cb161-17" aria-hidden="true" tabindex="-1"></a><span class="co"># change in fatal accidents over time</span></span>
<span id="cb161-18"><a href="causal-inference.html#cb161-18" aria-hidden="true" tabindex="-1"></a>wide_df<span class="sc">$</span>Dafatal_per_million <span class="ot">&lt;-</span> wide_df<span class="sc">$</span>afatal_per_million_1988 <span class="sc">-</span> wide_df<span class="sc">$</span>afatal_per_million_1982</span>
<span id="cb161-19"><a href="causal-inference.html#cb161-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-20"><a href="causal-inference.html#cb161-20" aria-hidden="true" tabindex="-1"></a><span class="co"># part c: drop already treated states</span></span>
<span id="cb161-21"><a href="causal-inference.html#cb161-21" aria-hidden="true" tabindex="-1"></a>wide_df <span class="ot">&lt;-</span> <span class="fu">subset</span>(wide_df, jail_1982<span class="sc">==</span><span class="dv">0</span>)</span></code></pre></div>
<ol start="8" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="causal-inference.html#cb162-1" aria-hidden="true" tabindex="-1"></a>did <span class="ot">&lt;-</span> <span class="fu">lm</span>(Dafatal_per_million <span class="sc">~</span> jail_1988, <span class="at">data=</span>wide_df)</span>
<span id="cb162-2"><a href="causal-inference.html#cb162-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(did)</span>
<span id="cb162-3"><a href="causal-inference.html#cb162-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-4"><a href="causal-inference.html#cb162-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb162-5"><a href="causal-inference.html#cb162-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = Dafatal_per_million ~ jail_1988, data = wide_df)</span></span>
<span id="cb162-6"><a href="causal-inference.html#cb162-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-7"><a href="causal-inference.html#cb162-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb162-8"><a href="causal-inference.html#cb162-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb162-9"><a href="causal-inference.html#cb162-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -55.652 -10.993   5.033  10.405  76.822 </span></span>
<span id="cb162-10"><a href="causal-inference.html#cb162-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-11"><a href="causal-inference.html#cb162-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb162-12"><a href="causal-inference.html#cb162-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span id="cb162-13"><a href="causal-inference.html#cb162-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  -12.585      4.242  -2.966  0.00532 **</span></span>
<span id="cb162-14"><a href="causal-inference.html#cb162-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jail_1988      5.102     11.695   0.436  0.66526   </span></span>
<span id="cb162-15"><a href="causal-inference.html#cb162-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb162-16"><a href="causal-inference.html#cb162-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb162-17"><a href="causal-inference.html#cb162-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-18"><a href="causal-inference.html#cb162-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 24.37 on 36 degrees of freedom</span></span>
<span id="cb162-19"><a href="causal-inference.html#cb162-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.005259,   Adjusted R-squared:  -0.02237 </span></span>
<span id="cb162-20"><a href="causal-inference.html#cb162-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 0.1903 on 1 and 36 DF,  p-value: 0.6653</span></span>
<span id="cb162-21"><a href="causal-inference.html#cb162-21" aria-hidden="true" tabindex="-1"></a>did_covs <span class="ot">&lt;-</span> <span class="fu">lm</span>(Dafatal_per_million <span class="sc">~</span> jail_1988 <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles, <span class="at">data=</span>wide_df)</span>
<span id="cb162-22"><a href="causal-inference.html#cb162-22" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(did_covs)</span>
<span id="cb162-23"><a href="causal-inference.html#cb162-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-24"><a href="causal-inference.html#cb162-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb162-25"><a href="causal-inference.html#cb162-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = Dafatal_per_million ~ jail_1988 + unemp + beertax + </span></span>
<span id="cb162-26"><a href="causal-inference.html#cb162-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     baptist + dry + youngdrivers + miles, data = wide_df)</span></span>
<span id="cb162-27"><a href="causal-inference.html#cb162-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-28"><a href="causal-inference.html#cb162-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb162-29"><a href="causal-inference.html#cb162-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb162-30"><a href="causal-inference.html#cb162-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -38.346 -12.383   1.456   9.092  60.585 </span></span>
<span id="cb162-31"><a href="causal-inference.html#cb162-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-32"><a href="causal-inference.html#cb162-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb162-33"><a href="causal-inference.html#cb162-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span id="cb162-34"><a href="causal-inference.html#cb162-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)    6.851636  50.643035   0.135   0.8933  </span></span>
<span id="cb162-35"><a href="causal-inference.html#cb162-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jail_1988     -1.853041  10.834391  -0.171   0.8653  </span></span>
<span id="cb162-36"><a href="causal-inference.html#cb162-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; unemp          3.725007   1.919862   1.940   0.0618 .</span></span>
<span id="cb162-37"><a href="causal-inference.html#cb162-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; beertax        8.300778  10.052007   0.826   0.4154  </span></span>
<span id="cb162-38"><a href="causal-inference.html#cb162-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; baptist        0.527893   0.723263   0.730   0.4711  </span></span>
<span id="cb162-39"><a href="causal-inference.html#cb162-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dry           -0.955636   0.546475  -1.749   0.0906 .</span></span>
<span id="cb162-40"><a href="causal-inference.html#cb162-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; youngdrivers  89.432017 234.379768   0.382   0.7055  </span></span>
<span id="cb162-41"><a href="causal-inference.html#cb162-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; miles         -0.010360   0.005823  -1.779   0.0854 .</span></span>
<span id="cb162-42"><a href="causal-inference.html#cb162-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb162-43"><a href="causal-inference.html#cb162-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb162-44"><a href="causal-inference.html#cb162-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb162-45"><a href="causal-inference.html#cb162-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 21.9 on 30 degrees of freedom</span></span>
<span id="cb162-46"><a href="causal-inference.html#cb162-46" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.3303, Adjusted R-squared:  0.174 </span></span>
<span id="cb162-47"><a href="causal-inference.html#cb162-47" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 2.114 on 7 and 30 DF,  p-value: 0.07276</span></span></code></pre></div>
<p>If we are willing to believe that, in the absence of the policy, that trends in alcohol related fatalities per million people would have followed the same trends over time for treated and untreated states, then we can interpret these as causal effects. These estimates are broadly similar to the previous ones though the second ones (that include additional covariates) are about the only ones where we ever get a negative estimate for the effect of mandatory jail policies. Like the previous estimates, neither of these estimates are statistically different from 0.</p>
<ol start="9" style="list-style-type: decimal">
<li></li>
</ol>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="causal-inference.html#cb163-1" aria-hidden="true" tabindex="-1"></a>lag_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million_1988 <span class="sc">~</span> jail_1988 <span class="sc">+</span> afatal_per_million_1982, <span class="at">data=</span>wide_df)</span>
<span id="cb163-2"><a href="causal-inference.html#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lag_reg)</span>
<span id="cb163-3"><a href="causal-inference.html#cb163-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-4"><a href="causal-inference.html#cb163-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb163-5"><a href="causal-inference.html#cb163-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982, </span></span>
<span id="cb163-6"><a href="causal-inference.html#cb163-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     data = wide_df)</span></span>
<span id="cb163-7"><a href="causal-inference.html#cb163-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-8"><a href="causal-inference.html#cb163-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb163-9"><a href="causal-inference.html#cb163-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb163-10"><a href="causal-inference.html#cb163-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -29.120 -12.663  -0.684   6.873  92.390 </span></span>
<span id="cb163-11"><a href="causal-inference.html#cb163-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-12"><a href="causal-inference.html#cb163-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb163-13"><a href="causal-inference.html#cb163-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb163-14"><a href="causal-inference.html#cb163-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)              19.0810    10.1089   1.888 0.067401 .  </span></span>
<span id="cb163-15"><a href="causal-inference.html#cb163-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jail_1988                 3.4323    10.3171   0.333 0.741363    </span></span>
<span id="cb163-16"><a href="causal-inference.html#cb163-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; afatal_per_million_1982   0.5607     0.1303   4.303 0.000129 ***</span></span>
<span id="cb163-17"><a href="causal-inference.html#cb163-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb163-18"><a href="causal-inference.html#cb163-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb163-19"><a href="causal-inference.html#cb163-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-20"><a href="causal-inference.html#cb163-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 21.47 on 35 degrees of freedom</span></span>
<span id="cb163-21"><a href="causal-inference.html#cb163-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.3462, Adjusted R-squared:  0.3088 </span></span>
<span id="cb163-22"><a href="causal-inference.html#cb163-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 9.266 on 2 and 35 DF,  p-value: 0.0005896</span></span>
<span id="cb163-23"><a href="causal-inference.html#cb163-23" aria-hidden="true" tabindex="-1"></a>lag_reg_covs <span class="ot">&lt;-</span> <span class="fu">lm</span>(afatal_per_million_1988 <span class="sc">~</span> jail_1988 <span class="sc">+</span> afatal_per_million_1982 <span class="sc">+</span> unemp <span class="sc">+</span> beertax <span class="sc">+</span> baptist <span class="sc">+</span> dry <span class="sc">+</span> youngdrivers <span class="sc">+</span> miles, <span class="at">data=</span>wide_df)</span>
<span id="cb163-24"><a href="causal-inference.html#cb163-24" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lag_reg_covs)</span>
<span id="cb163-25"><a href="causal-inference.html#cb163-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-26"><a href="causal-inference.html#cb163-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb163-27"><a href="causal-inference.html#cb163-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982 + </span></span>
<span id="cb163-28"><a href="causal-inference.html#cb163-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     unemp + beertax + baptist + dry + youngdrivers + miles, data = wide_df)</span></span>
<span id="cb163-29"><a href="causal-inference.html#cb163-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-30"><a href="causal-inference.html#cb163-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb163-31"><a href="causal-inference.html#cb163-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb163-32"><a href="causal-inference.html#cb163-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -27.840  -8.793  -1.364   5.146  71.409 </span></span>
<span id="cb163-33"><a href="causal-inference.html#cb163-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-34"><a href="causal-inference.html#cb163-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb163-35"><a href="causal-inference.html#cb163-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                           Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span id="cb163-36"><a href="causal-inference.html#cb163-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)             -17.292595  46.318477  -0.373  0.71161   </span></span>
<span id="cb163-37"><a href="causal-inference.html#cb163-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; jail_1988                 0.817453   9.786782   0.084  0.93401   </span></span>
<span id="cb163-38"><a href="causal-inference.html#cb163-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; afatal_per_million_1982   0.505371   0.173718   2.909  0.00689 **</span></span>
<span id="cb163-39"><a href="causal-inference.html#cb163-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; unemp                     3.189918   1.736443   1.837  0.07647 . </span></span>
<span id="cb163-40"><a href="causal-inference.html#cb163-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; beertax                   2.885796   9.236174   0.312  0.75694   </span></span>
<span id="cb163-41"><a href="causal-inference.html#cb163-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; baptist                   0.965785   0.668259   1.445  0.15911   </span></span>
<span id="cb163-42"><a href="causal-inference.html#cb163-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; dry                      -0.567567   0.509915  -1.113  0.27482   </span></span>
<span id="cb163-43"><a href="causal-inference.html#cb163-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; youngdrivers            120.615853 211.026841   0.572  0.57202   </span></span>
<span id="cb163-44"><a href="causal-inference.html#cb163-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; miles                    -0.002692   0.005888  -0.457  0.65087   </span></span>
<span id="cb163-45"><a href="causal-inference.html#cb163-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb163-46"><a href="causal-inference.html#cb163-46" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb163-47"><a href="causal-inference.html#cb163-47" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb163-48"><a href="causal-inference.html#cb163-48" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 19.7 on 29 degrees of freedom</span></span>
<span id="cb163-49"><a href="causal-inference.html#cb163-49" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.5443, Adjusted R-squared:  0.4186 </span></span>
<span id="cb163-50"><a href="causal-inference.html#cb163-50" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 4.329 on 8 and 29 DF,  p-value: 0.001596</span></span></code></pre></div>
<p>These estimates directly control for alcohol related fatalities per million in the pre-treatment period 1982. These sorts of specifications are less common in economics, but, in my view, it seems like a reasonable approach here. That said, the results are more or less the same as earlier estimates.</p>
<ol start="10" style="list-style-type: decimal">
<li>We don’t have very strong evidence that mandatory jail policies reduced the number traffic fatalities. In my view, probably the best specifications for trying to understand the causal effects are the ones in part 7 (particularly, the ones that include covariates there), but I think that the the results in parts 4-9 are also informative. Broadly, these estimates are more or less similar — none of them are statistically significant and most are positive (which is an unexpected sign).</li>
</ol>
<p>Before we finish, let me mention a few caveats to these results:</p>
<ul>
<li><p>First, I would be very hesitant to interpret these results as definitively saying that mandatory jail policies have no effect on alcohol related traffic fatalities. The main reason to be clear about this is that our standard error are quite large. For example, in the second specification in part 7 (the one I like the most), a 95% confidence interval for our estimate is <span class="math inline">\([-23.1, 19.4]\)</span>. This is a wide confidence interval — the average number of alcohol related traffic fatalities per million across all states and time periods is only 66. So our estimates are basically still compatible with very large reductions in alcohol related traffic fatalities up to large increases in alcohol related traffic fatalities.</p></li>
<li><p>Let me make one more comment about the sign of our results. Many of our point estimates are positive; as we discussed earlier, it is hard to rationalize harsher punishments <em>increasing</em> alcohol related traffic fatalities. I think the main explanation for these results is just that our estimates are pretty noisy and, therefore, more or less “by chance” we are getting estimates that have an unexpected sign. But there are some other possible explanations that are worth mentioning. For one, there are a number of other policies related to drunk driving that occurred in the 1980s (particularly, related to legal drinking age) but perhaps others. It is not clear how these would interact with our estimates, but they could certainly play some role. Besides that, it seems to me that we have a pretty good set of covariates that enter our models, but there could be important covariates that we are missing. For this reason, some expertise in how to model state-level traffic fatalities is actually a very important skill here (actually probably the key skill here!)</p></li>
</ul>
</div>
<div id="coding-questions-3" class="section level2" number="6.10">
<h2><span class="header-section-number">6.10</span> Coding Questions</h2>
<ol style="list-style-type: decimal">
<li><p>For this problem, we will use the data <code>rand_hie</code>. This is data from the RAND health insurance experiment in the 1980s. In the experiment, participants were randomly assigned to get Catastrophic (the least amount of coverage), insurance that came with a Deductible, insurance that came with Cost Sharing (i.e., co-insurance so that an individual pays part of their medical insurance), and Free (so that there is no cost of medical care).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimate the average difference between total medical expenditure (<code>total_med_expenditure</code>) by plan type (<code>plan_type</code>) and report your results. Should you interpret these as average causal effects? Explain.</p></li>
<li><p>Estimate the average difference between face to face doctors visits (<code>face_to_face_visits</code>) by plan type (<code>plan_type</code>) and report your results. Should you interpret these as average causal effects? Explain.</p></li>
<li><p>Estimate the average difference between the overall health index (<code>health_index</code>) by plan type (<code>plan_type</code>) and report your results. Should you interpret these as average causal effects? Explain.</p></li>
<li><p>How do you interpret the results from parts a-c?</p></li>
</ol></li>
<li><p>For this problem, we will study the causal effect of having more children on women’s labor supply using the data <code>Fertility</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Let’s start by running a regression of the number of hours that a woman typically works per week (<code>work</code>) on whether or not she has more than two children (<code>morekids</code>), her age and <span class="math inline">\(age^2\)</span>, and race/ethnicity (<code>afam</code> and <code>hispanic</code>). Report your results. How do you feel about interpreting the estimated coefficient on <code>morekids</code> as the causal effect of having more than two children? Explain.</p></li>
<li><p>One possible instrument in this setup is the sex composition of the first two children (i.e., whether they are both girls, both boys, or a boy and a girl). The thinking here is that, at least in the United States, parents tend to have a preference for having both a girl and a boy and that, therefore, parents whose first two children have the same sex may be more likely to have a third child than they would have been if they have a girl and a boy. Do you think that using a binary variable for whether or not the first two children have the same sex is a reasonable instrument of for <code>morekids</code> from part a?</p></li>
<li><p>Create a new variable called <code>samesex</code> that is equal to one for families whose first two children have the same sex. Using the same specification as in part a, use <code>samesex</code> as an instrument for <code>morekids</code> and report the results. Provide some discussion about your results.</p></li>
</ol></li>
<li><p>For this question, we will use the <code>AJR</code> data. A deep question in development economics is: Why are some countries much richer than other countries? One explanation for this is that richer countries have different institutions (e.g., property rights, democracy, etc.) that are conducive to growth. Its hard to study these questions though because institutions do not arise randomly — there could be reverse causality so that property rights, democracy, etc. are (perhaps partially) caused by being rich rather than the other way around. Alternatively, other factors (say a country’s geography) could cause both of these. We’ll consider one instrumental variables approach to thinking about this question in this problem.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of the log of per capita GDP (the log of per capita GDP is stored in the variable <code>GDP</code>) on a measure of the protection against expropriation risk (this is a measure of how “good” a country’s institutions are (a larger number indicates “better” institutions) and it is in the variable <code>Exprop</code>). How do you interpret these results? Do you think it would be reasonable to interpret the estimated coefficient on <code>Exprop</code> as the causal effect of institutions on GDP.</p></li>
<li><p>One possible instrument for <code>Exprop</code> is settler mortality (we’ll use the log of this which is available in the variable <code>logMort</code>). Settler mortality is a measure of how dangerous it was for early settlers of a particular location. The idea is that places that have high settler mortality may have set up worse (sometimes called “extractive”) institutions than places that had lower settler mortality. But that settler mortality (from a long time ago) does not have any other direct effect on modern GDP. Provide some discussion about whether settler mortality is a valid instrument for institutions.</p></li>
<li><p>Estimate an IV regression of <code>GDP</code> on <code>Exprop</code> using <code>logMort</code> as an instrument for <code>Exprop</code>. How do you interpret the results? How do these results compare to the ones from part a?</p></li>
</ol></li>
<li><p>For this question, we’ll use the data <code>house</code> to study the causal effect of incumbency on the probability that a member of the House of Representatives gets re-elected.</p>
<ol style="list-style-type: lower-alpha">
<li><p>One way to try to estimate the causal effect of incumbency is to just run a regression where the outcome is <code>democratic_vote_share</code> (this is the same outcome we’ll use below) and where the model includes a dummy variable for whether or not the democratic candidate is an incumbent. What are some limitions of this strategy?</p></li>
<li><p>The <code>house</code> data contains data about the margin of victory (is positive if they won the election and negative if they lost) for Democratic candidates in the current election and data about the Democratic margin of victory in the past election. Explain how you could use this data in a regression discontinuity design to estimate the causal effect of incumbency.</p></li>
<li><p>Use the <code>house</code> data to implement the regression discontinuity design that you proposed in part b. What do you estimate as the causal effect of incumbency?</p></li>
</ol></li>
<li><p>For this problem, we will use the data <code>banks</code>. We will study the causal effect of monetary policy on bank closures during the Great Depression. We’ll consider an interesting natural experiment in Mississippi where half the northern half of the state was in St. Louis’s federal reserve district (District 8) and the southern half of the state was in Atlanta’s federal reserve district (District 6). Atlanta had much looser monetary policy (meaning they substantially increased lending) than St. Louis during the early part of the Great Depression and our interest is in whether looser monetary policy made an difference.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Plot the total number of banks separately for District 6 and District 8 across all available time periods in the data.</p></li>
<li><p>An important event in the South early in the Great Depression was the collapse of Caldwell and Company — the largest banking chain in the South at the time. This happened in November 1930. The Atlanta Fed’s lending markedly increased quickly after this event while St. Louis’s did not. Calculate a DID estimate of the effect of looser monetary policy on the number of banks that are still in business. How do you interpret these results? <strong>Hint:</strong> You can calculate this by taking the difference between the number of banks in District 6 relative to the number of banks in District 8 across all time periods relative to the difference between the number of banks in District 6 relative to District 8 in the first period (July 1, 1929).</p></li>
</ol></li>
</ol>
</div>
<div id="extra-questions-3" class="section level2" number="6.11">
<h2><span class="header-section-number">6.11</span> Extra Questions</h2>
<ol style="list-style-type: decimal">
<li><p>What is the difference between treatment effect homogeneity and treatment effect heterogeneity?</p></li>
<li><p>Why do most researchers give up on trying to estimate the individual-level effect of participating in a treatment?</p></li>
<li><p>Explain what unconfoundedness means.</p></li>
<li><p>What is the key condition underlying a difference-in-differences approach to learn about the causal effect of some treatment on some outcome?</p></li>
<li><p>What are two key conditions for a valid instrument?</p></li>
<li><p>Suppose you are interested in the causal effect of participating in a union on a person’s income. Consider the following approaches.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose you run the following regression</p>
<p><span class="math display">\[\begin{align*}
   Earnings_i = \beta_0 + \alpha Union_i + \beta_1 Education_i + U_i
 \end{align*}\]</span></p>
<p>Would it be reasonable to interpret <span class="math inline">\(\hat{\alpha}\)</span> in this regression as an estimate of the causal effect of participating in a union on earnings? Explain.</p></li>
<li><p>Suppose you have access to panel data and run the following fixed effects regression
<span class="math display">\[\begin{align*}
   Earnings_{it} = \beta_{0,t} + \alpha Union_{it} + \beta_1 Education_{it} + \eta_i + U_{it}
 \end{align*}\]</span></p>
<p>where <span class="math inline">\(\eta_i\)</span> is an individual fixed effect. Would it be reasonable to interpert <span class="math inline">\(\hat{\alpha}\)</span> in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?</p></li>
<li><p>Going back to the case with cross-sectional data, consider the regression
<span class="math display">\[\begin{align*}
   Earnings_i = \beta_0 + \alpha Union_i + U_i
 \end{align*}\]</span>
but using the variable <span class="math inline">\(Z_i = 1\)</span> if birthday is between Jan. 1 and Jun. 30 while <span class="math inline">\(Z_i=0\)</span> otherwise. Would it be reasonable to interpert <span class="math inline">\(\hat{\alpha}\)</span> in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?</p></li>
</ol></li>
<li><p>Suppose that you are interested in the effect of lower college costs on the probability of graduating from college. You have access to student-level data from Georgia where students are eligible for the Hope Scholarship if they can keep their GPA above 3.0.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What strategy can use to exploit this institional setting to learn about the causal effect of lower college costs on the probability of going to college?</p></li>
<li><p>What sort of data would you need in order to implement this strategy?</p></li>
<li><p>Can you think of any ways that the approach that you suggested could go wrong?</p></li>
<li><p>Another researcher reads the results from the approach you have implemented and complains that your results are only specific to students who have grades right around the 3.0 cutoff. Is this a fair criticism?</p></li>
</ol></li>
<li><p>Suppose you are willing to believe versions of unconfoundedness, a linear model for untreated potential outcomes, and treatment effect homogeneity so that you could write
<span class="math display">\[\begin{align*}
  Y_i = \beta_0 + \alpha D_i + \beta_1 X_i + \beta_2 W_i + U_i
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[U|D,X,W] = 0\)</span> so that you were willing to interpret <span class="math inline">\(\alpha\)</span> in this regression as the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. However, suppose that <span class="math inline">\(W\)</span> is not observed so that you cannot operationalize the above regression.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Since you do not observe <span class="math inline">\(W\)</span>, you are considering just running a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> and interpreting the estimated coefficient on <span class="math inline">\(D\)</span> as the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>. Does this seem like a good idea?</p></li>
<li><p>In part (a), we can write a version of the model that you are thinking about estimating as
<span class="math display">\[\begin{align*}
   Y_i = \delta_0 + \delta_1 D_i + \delta_2 X_i + \epsilon_i
 \end{align*}\]</span>
Suppose that <span class="math inline">\(\mathbb{E}[\epsilon | D, X] = 0\)</span> and suppose also that
<span class="math display">\[\begin{align*}
W_i = \gamma_0 + \gamma_1 D_i + \gamma_2 X_i + V_i
  \end{align*}\]</span>
with <span class="math inline">\(\mathbb{E}[V|D,X]=0\)</span>. Provide an expression for <span class="math inline">\(\delta_1\)</span> in terms of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\gamma\)</span>’s and <span class="math inline">\(\beta\)</span>’s. Explain what this expression means.</p></li>
</ol></li>
<li><p>Suppose you have access to an experiment where some participants were randomly assigned to participate in a job training program and others were randomly assigned not to participate. However, some individuals that were assigned to participate in the treatment decided not to actually participate. Let’s use the following notation: <span class="math inline">\(D=1\)</span> for individuals who actually participated and <span class="math inline">\(D=0\)</span> for individuals who did not participate. <span class="math inline">\(Z=1\)</span> for individuals who were assigned to the treatment and <span class="math inline">\(Z=0\)</span> for individuals assigned not to participate (here, <span class="math inline">\(D\)</span> and <span class="math inline">\(Z\)</span> are not exactly the same because some individuals who were assigned to the treatment did not actually participate).</p>
<p>You are considering several different approaches to dealing with this issue. Discuss which of the following are good or bad ideas:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estimating <span class="math inline">\(ATT\)</span> by <span class="math inline">\(\bar{Y}_{D=1} - \bar{Y}_{D=0}\)</span>.</p></li>
<li><p>Run the regression <span class="math inline">\(Y_i = \beta_0 + \alpha D_i + U_i\)</span> using <span class="math inline">\(Z_i\)</span> as an instrument.</p></li>
</ol></li>
<li><p>Suppose you and a friend have conducted an experiment (things went well so that everyone complied with the treatment that they were assigned to, etc.). You interpret the difference <span class="math inline">\(\bar{Y}_{D=1} - \bar{Y}_{D=0}\)</span> as an estimate of the <span class="math inline">\(ATT\)</span>, but your friend says that you should interpret it as an estimate of the <span class="math inline">\(ATE\)</span>. In fact, according to your friend, random treatment assignment implies that <span class="math inline">\(\mathbb{E}[Y(1)] = \mathbb{E}[Y(1)|D=1] = \mathbb{E}[Y|D=1]\)</span> and <span class="math inline">\(\mathbb{E}[Y(0)] = \mathbb{E}[Y(0)|D=0] = \mathbb{E}[Y|D=0]\)</span> which implies that <span class="math inline">\(ATE = \mathbb{E}[Y|D=1] - \mathbb{E}[Y|D=0]\)</span>. Who is right?</p></li>
</ol>
</div>
<div id="answers-to-some-extra-questions-2" class="section level2" number="6.12">
<h2><span class="header-section-number">6.12</span> Answers to Some Extra Questions</h2>
<p><strong>Answer to Question 4</strong></p>
<p>The key condition is the parallel trends assumption that says that, in the absence of participating in the treatment, the <em>path</em> of outcomes that individuals in the treated group is the same, on average, as the path of outcomes that individuals in the untreated group actually experienced.</p>
<p><strong>Answer to Question 9</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>When some individuals do not comply with their treatment assignment, this approach is probably not so great. In particular, notice that the comparison in this part of the problem is among individuals who <em>actually</em> participated in the treatment relative to those who didn’t (the latter group includes both those assigned not to participate in the treatment along with those assigned to participate in the treatment, but ultimately didn’t actually participate). This suggests that this approach would generally lead to biased estimates of the <span class="math inline">\(ATT\)</span>. In the particular context of job training, you can see this would not be such a good idea if, for example, the people who were assigned to the job training program but who did not participate tended to do this because they were able to find a job before the job training program started.</p></li>
<li><p>This approach is likely to be better. By construction, <span class="math inline">\(Z\)</span> is not correlated with <span class="math inline">\(U\)</span> (since <span class="math inline">\(Z\)</span> is randomly assigned). <span class="math inline">\(Z\)</span> is also likely to be positively correlated with <span class="math inline">\(Z\)</span> (in particular, this will be the case if being randomly assigned to treatment increases the probability of being treated). This implies that <span class="math inline">\(Z\)</span> is a valid instrument and should be able to deliver a reasonable estimate of the effect of participating in the treatment.</p></li>
</ol>
<p><strong>Answer to Question 10</strong></p>
<p>While your friend’s explanation is not technically wrong, it seems to me that you are more right than your friend. There is an important issue related to external validity here. The group of people that show up to participate in the experiment could be (and likely are) quite different from the general population. Interpreting the results of the experiment as being an <span class="math inline">\(ATE\)</span> (in the sense of across the entire population) is therefore likely to be incorrect — or at least would require extra assumptions and/or justifications. Interpreting them as an <span class="math inline">\(ATT\)</span> (i.e., as the effect among those who participated in the treatment) is still perfectly reasonable though.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prediction.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Detailed Course Notes.pdf", "Detailed Course Notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
