\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\P}{\mathrm{P}}

# Prediction

## Measures of Regression Fit

### TSS, ESS, SSR

SW 6.4

### $R^2$

SW 6.4

## Model Selection

SW 7.5 (note: we cover substantially more details than the textbook about model selection)

### Limitations of $R^2$

SW 6.4

### Adjusted $R^2$

SW 6.4

### AIC, BIC

Alternative criteria functions:

* $AIC = 2k + n \log(SSR)$
* $BIC = k \log(n) + n \log(SSR)$

Choose model that minimizes these criteria functions.

### Cross-Validation

* Intuition: mimic the out-of-sample prediction problem

* Algorithm:

    * Split the data into J ``folds''

    * For the jth fold, do the following:
    
        1. Estimate the model using all observations *not in* the Jth fold ($\implies$ we obtain estimates $\hat{\beta}_0^j, \hat{\beta}_1^j, \ldots, \hat{\beta}_k^j$)
        
        2. Predict outcomes for observations in the Jth fold using the estimated model from part (1):
        \begin{align*}
          \tilde{Y}_{ij} = \hat{\beta}_0^j + \hat{\beta}_1^j X_{1ij} + \cdots + \hat{\beta_k^j} X_{kij}
        \end{align*}
        
        3. Compute the prediction error:
        \begin{align*}
          \tilde{U}_{ij} = Y_{ij} - \tilde{Y}_{ij}
        \end{align*}
        (this is the difference between actual outcomes for individuals in the Jth fold and their predicted outcome based on the model from part (1))
        
    * Do steps 1-3 for all $J$ folds.  This gives a prediction error $\tilde{U}_i$ for each observation in the data
    
    * compute the cross validation criteria (mean squared prediction error):
    \begin{align*}
      CV = \frac{1}{n} \sum_{i=1}^n \tilde{U}_{i}^2
    \end{align*}
    
    * choose the model that produces the smallest value of $CV$.
        
### Model Averaging

In the case where you are considering a large number of possible models, it is pretty common that a number of models will, by any of the above model selection criteria, be expected to perform very similarly when making predictions.

In this case, one strategy that usually does well in terms of making out-of-sample predictions is *model averaging*.

Suppose you have $M$ different models, and that each model can produce a predicted value for $Y_i$ --- let's call the predicted value from model $m$, $\hat{Y}_i^m$.  Model averaging would involve obtaining a new predicted value, call it $\hat{Y}_i$ by computing
\begin{align*}
  \hat{Y}_i = \frac{1}{M} \sum_{m=1}^M \hat{Y}_i^m
\end{align*}

* Usually, you would throw out models that you know predict poorly and only average together ones that perform reasonably well.

## Machine Learning

SW 14.1, 14.2, 14.6

Some extra resources on estimating Lasso and Ridge regressions in R:

* [glmnet tutorial](https://www.statology.org/lasso-regression-in-r/)

* [glmnetUtils vignette](https://cran.r-project.org/web/packages/glmnetUtils/vignettes/intro.html)

"Big" Data typically means one of two things:

1. $n$ --- the number of observations is extremely large

2. $k$ --- the number of regressors is very large

$n$ being large is more of a computer science problem.  That said, there are economists who think about these issues, but I think it is still the case that it is relatively uncommon for an economist to have *so much* data that they have trouble computing their estimators.

We'll focus on the second issue --- where $k$ is large.  A main reason that this may occur in applications is that we may be unsure of the functional form for $\E[Y|X_1,X_2,X_3]$.  Should it include higher order terms like $X^2$ or $X^3$?  Should it include interactions like $X_1 X_2$.  Once you start proceeding this way, even if you only have 5-10 actual covariates, $k$ can become quite large.


As in the case of the mean, perhaps we can improve predictions by introducing some bias while simultaneously decreasing the variance of our predictions.  To do this, we'll estimate the parameters of the model in a similar way to what we have done before except that we'll add a **penalty** term that gets larger as parameter estimates move further away from 0.  In particular, we consider esetimates of the form

$$
  (\hat{\beta}_1, \hat{\beta}_2, \ldots, \hat{\beta}_k) = \underset{b_1,\ldots,b_k}{\textrm{argmin}} \sum_{i=1}^n  (Y_i - b_1 X_{1i} - \cdots - b_k X_{ki})^2 + \textrm{penalty}
$$

Lasso and ridge regression, which are the two approaches to machine learning that we will talk about below, amount to different choices of the penalty term.

::: {.side-comment}

<span class="side-comment">Side-Comment:</span> Generally, you should "standardize" the regressors before implementing Lasso or Ridge regression.  The `glmnet` package does this for you by default.

:::

### Lasso

SW 14.3

For Lasso, the penalty term is given by

$$
  \lambda \sum_{j=1}^k |b_j|
$$
The absolute value on each of the $b_j$ terms means that the penalty function gets larger for larger values of any $b_j$.  Since we are trying to minimize the objective function, this means that there is a "cost" to choosing a larger value of $b_j$ relative to OLS.  Thus, Lasso estimates tend to be "shrunk" towards 0.

$\lambda$ is called a "tuning parameter** --- larger values of $\lambda$ imply that the penalty term is more important.  Notice that, if you send $\lambda \rightarrow \infty$, then the penalty term will be so large that you would set all the parameters to be equal to 0.  On the other hand, if you set $\lambda=0$, then you will get the OLS estimates (because there will be no penalty in this case).  In general, it is hard to "know" what is the right value for $\lambda$, and it is typically chosen using cross validation.


### Ridge Regression

SW 14.4

For Ridge, the penalty term is given by

$$
  \lambda \sum_{j=1}^k b_j^2
$$
Much of the discussion from Lasso applies here.  The only difference is that the form of the penalty is different here: $b_j^2$ instead of $|b_j|$.  Relative to the Lasso penalty, the Ridge penalty "dislikes more" very large values of $b_j$.

**Comparing Lasso and Ridge**

- Both **shrink** the estimated parameters towards 0.  This tends to introduce bias into our predictions but comes with the benefit of reducing the variance of the predictions.

- Interestingly, both Lasso and Ridge can be implemented when $k > n$ (i.e., if you have more regressors than observations).   This is in contrast to to OLS, where the parameters cannot be estimated in this case.

- Both are generally not very computationally intensive.  For ridge regression, we can in fact derive an explicit expression for the estimated $\beta$'s.  We cannot do this with Lasso; a full discussion of how Lasso actually estimates the parameters is beyond the scope of our course, but, suffice it to say, that you can generally compute Lasso estimates quickly.

- Lasso also performs "model selection" --- that is, if you use the Lasso, many of the estimated parameters will be set equal to 0.  This can sometimes be an advantage.  Ridge (like OLS) will generally produce non-zero estimates of all parameters in the model.


## Binary Outcome Models

In addition to referenced material below, please read all of SW Ch. 11

::: {.side-comment}

<span class="side-comment">Side-Comment:</span> We are not necessarily so interested in prediction, but I find this a good spot to teach about binary outcome models before we conclude the course talking about causality

:::

You may or may not have noticed this, but all the outcomes that we have considered so far have involved a continuous outcome.  But lots of economic variables are discrete (we'll mainly focus on binary outcomes).  Examples:

- Labor force participation

- Graduating from college

The question is: Do our linear regression tools still apply to this case?  In other words, does 

$$
  \E[Y | X_1, X_2, X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
$$
still make sense?

- Note: we have already included binary regressors and know how to interpret these, so this section is about binary outcomes rather than binary regressors


### Linear Probability Model

SW 11.1

Let's continue to consider 

$$
  \E[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
$$

when $Y$ is binary.  Of course, you can still run this regression.

One thing that is helpful to notice before we really get started here is that when $Y$ is binary (so that either $Y=0$ or $Y=1$)

$$
  \begin{aligned}
  \E[Y] &= \sum_{y \in \mathcal{Y}} y \P(Y=y) \\
  &= 0 \P(Y=0) + 1 \P(Y=1) \\
  &= \P(Y=1)
  \end{aligned}
$$
And exactly the same sort of argument implies that, when $Y$ is binary, $\E[Y|X] = \P(Y=1|X)$.  Thus, if we believer the model in the first part of this section, this result implies that

$$
  \P(Y=1|X_1,X_2,X_3) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
$$
For this reason, the model in this section is called the **linear probabilty model**.  Moreover, this further implies that we should interpret

$$
  \beta_1 = \frac{\partial \, \P(Y=1|X_1,X_2,X_3)}{\partial \, X_1}
$$
as a partial effect.  That is, $\beta_1$ is how much the probability that $Y=1$ changes when $X_1$ increases by one unit, holding $X_2$ and $X_3$ constant.  This is good (and simple), but there are some drawbacks:

1. It's possible to get non-sensical predictions (predicted probabilities that are less than 0 or greater than 1) with a linear probability model.

2. A related problem is that the linear probability model implies constant partial effects.  That is, the effect of a change in one regressor always changes the probability of $Y=1$ (holding other regressors constant) by the same amount.  It may not be obvious that this is a disadvantage, but it is.  

::: {.example}

Let $Y=1$ if an individual participates in the labor force.  Further let $X_1=1$ if an individual is male and 0 otherwise, $X_2$ denote an individual's age, and $X_3=1$ for college graduates and 0 otherwise.

Additionally, suppose that $\beta_0=0.4, \beta_1=0.2, \beta_2=0.01, \beta_3=0.1$.  

Let's calculate the probability of being in the labor force for a 40 year old woman who is not a college graduate.  This is given by

$$
  \P(Y=1 | X_1=0, X_2=40, X_3=0) = 0.4 + (0.01)(40) = 0.8
$$
In other words, we'd predict that, given these characteristics, the probability of being in the labor force is 0.8.

Now, let's calculate the probability of being in the labor force for a 40 year old man who is a college graduate.  This is given by

$$
  \P(Y=1|X_1=1, X_2=40, X_3=1) = 0.4 + 0.2 + (0.01)(40) + 0.1 = 1.1
$$
We have calculated that the predicted probability of being in the labor force, given these characteristics, is 1.1 --- this makes no sense!  Our maximum predicted probabilty should be 1.

The problem of constant partial effect is closely related.  Here, labor force participation is increasing in age, but with a binary outcome (by construction) the effect has to die off --- for those who are already very likely to participate in the labor force (in this example, older men with a college education, the partial effect of age has to be low because they are already very likely to participate in the labor force).

:::

We can circumvent both of the main problems with the linear probability model by consider **nonlinear models** for binary outcomes.  By far the most common are **probit** and **logit**.  We will discuss these next.

### Probit and Logit

SW 11.2, 11.3

Let's start this section with probit.  A probit model arises from setting

$$
  \P(Y=1|X_1,X_2,X_3) = \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)
$$
where $\Phi$ is the cdf of a standard normal random variable.  This is a nonlinear model due to $\Phi$ making the model nonlinear in parameters.  

Using $\Phi$ (or any cdf) here has a useful property that no matter what value the "index" $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$ takes on, the cdf is always between 0 and 1.  This implies that we cannot get predicted probabilities outside of 0 and 1.

Thus, the circumvents the problems with the linear probability model.  That said, there are some things we have to be careful about.  First, as usual, we are interested in partial effects rather than the parameters themselves.  But partial effects are more complicated here.  Notice that

$$
  \begin{aligned}
  \frac{ \partial \, P(Y=1|X_1,X_2,X_3)}{\partial \, X_1} &= \frac{\partial \, \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}{\partial \, X_1} \\
  &= \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1
  \end{aligned}
$$
where $\phi$ is the pdf of a standard normal random variable.  And the second equality requires using the chain rule --- take the derivative of the "outside" (i.e., $\Phi$) and then the derivative of the "inside" with respect to $X_1$.  Notice that this partial effect is more complicated that in the case of the linear models that we have mainly considered --- it involves $\phi$, but more importantly it also depends on the values of all the covariates.  In other words, the partial effect of $X_1$ can vary across different values of $X_1$, $X_2$, and $X_3$.

**Logit** is conceptually similar to probit, but instead of using $\Phi$, Logit uses the logistic function $\Lambda(z) = \frac{\exp(z)}{1+\exp(z)}$.  The logistic function has the same important properties as $\Phi$: (i) $\Lambda(z)$ is increasing in $z$, (ii) $\Lambda(z) \rightarrow 1$ as $z \rightarrow \infty$, and (iii) $\Lambda(z) \rightarrow 0$ as $z \rightarrow -\infty$.  Thus, in a logit model,

$$
  \begin{aligned}
  \P(Y=1 | X_1, X_2, X_3) &= \Lambda(\beta_0 + \beta_1 X_1 + \beta_2 + \beta_3 X_3) \\
  &= \frac{\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}{1+\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)}
  \end{aligned}
$$


### Average Partial Effects

One of the complications with Probit and Logit is that it is not so simple to interpret the estimated parameters.  

Remember we are generally interested in partial effects, not the parameters themselves.  It just so happens that in many of the linear models that we have considered so far the $\beta$'s correspond to the partial effect --- this means that it is sometimes easy to forget that they are not what we are typically most interested in.

This is helpful framing for thinking about how to interpret the results from a Probit or Logit model.  

Let's focus on the Probit model.  In that case,
\begin{align*}
  \P(Y=1|X_1,X_2,X_3) = \Phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)
\end{align*}
where $\Phi$ is the cdf of standard normal random variable.

*Continuous Case*: When $X_1$ is continuous, the partial effect of $X_1$ is given by
\begin{align*}
  \frac{\partial \P(Y=1|X_1,X_2,X_3)}{\partial X_1} = \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1
\end{align*}
where $\phi$ is the pdf of a standard normal random variable.  This is more complicated than the partial effect in the context of a linear model.  It depends on $\phi$ (which looks complicated, but you can just use `R`'s `dnorm` command to handle that part).  More importantly, the partial effect depends on the values of $X_1,X_2,$ and $X_3$.  [As discussed above, this is likely a good thing in the context of a binary outcome model].  Thus, in order to get a partial effect, we need to put in some values for these.  If you have particular values of the covariates that you are interested in, you can definitely do that, but my general suggestion is to report the *Average Partial Effect*:
\begin{align*}
  APE &= \E\left[ \frac{\partial \P(Y=1|X_1,X_2,X_3)}{\partial X_1} \right] \\
  &= \E\left[ \phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3) \beta_1 \right]
\end{align*}
which you can estimate by
\begin{align*}
  \widehat{APE} &= \frac{1}{n} \sum_{i=1}^n \phi(\hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_3) \hat{\beta}_1
\end{align*}
which amounts to just computing the partial effect at each value of the covariates in your data and then averaging these partial effects together.  This can be a bit cumbersome to do in practice, and it is often convenient to use the `R` package `mfx` to compute these sorts of average partial effects for you.

*Discrete/Binary Case*: When $X_1$ is discrete (let's say binary, but extention to discrete is straightforward), the partial effect of $X_1$ is
\begin{align*}
  & \P(Y=1|X_1=1, X_2, X_3) - \P(Y=1|X_1=0, X_2, X_3) \\
  &\hspace{100pt} = \Phi(\beta_0 + \beta_1 + \beta_2 X_2 + \beta_3 X_3) - \Phi(\beta_0 + \beta_2 X_2 + \beta_3 X_3)
\end{align*}
Notice that $\beta_1$ does not show up in the last term.  As above, the partial effect depends on the values of $X_2$ and $X_3$ which suggests reporting an $APE$ as above (follows the same steps, just replacing the partial effect, as in the continuous case above)

* Extensions to Logit are virtually identical, just replace $\Phi$ with $\Lambda$ and $\phi$ with $\lambda$.

::: {.side-comment}

<span class="side-comment">Side-Comment:</span> The parameters from LPM, Probit, and Logit could be quite different (in fact, they are quite different by construction), but APE's are often very similar.

:::

## Computation

$R^2$ and $\bar{R}^2$ are both reported as output from `R`'s `lm` command.

It is straightforward (and a useful excercise) to compute $TSS, ESS,$ and $SSR$ directly.  It is also straightforward to calculate $AIC$ and $BIC$ --- there are probably packages that will do this for you, but they are so easy that I suggest just calculating on your own.

The same applies to cross validation.  I suspect that there are packages available that will do this for you, but I think these are useful coding exercises and not all that difficult.  Also, if you do this yourself, it removes any kinds of "black box" issues from downloading `R` code where it may not be clear exactly what it is doing.

Computing Lasso and Ridge regressions is substantially more complicated than most other things that we have computed this semester.  The main `R` package for Lasso and Ridge regressions is the `glmnet` package.  For some reason, the syntax of the package is somewhat different from, for example, the `lm` command.  In my view, it is often easier to use the `glmnetUtils` package, which seems to be just a wrapper for `glmnet` but with functions that are analogous to `lm`.

Suppose that you have access to a training dataset called `train` and a testing dataset called `test`, you can use the `glmnetUtils` package in the following way:
```{r eval=FALSE}
library(glmnetUtils)
lasso_model <- cv.glmnet(Y ~ X1 + X2 + X3, data=train, use.model.frame=TRUE) # or whatever formula you want to use
coef(lasso_model) # if you are interested in estimated coefficients
predict(lasso_model, newdata=test) # get predictions
```

The main `R` function for estimating binary outcome models is the `glm` function (this stands for "generalized linear model").  The syntax is very similar to the syntax for the `lm` command.  For example, suppose $Y$ is binary and you are interested in estimating a logit model with regressors $X1$ and $X2$ which are contained in a `data.frame` called `data`.  In this case, you could use the following code
```{r eval=FALSE}
# glm logit
bin_reg <- glm(Y ~ X1 + X2, family=binomial(link="logit"), data=data)
summary(bin_reg)

# average partial effects
library(mfx)
bin_ape <- logitmfx(Y ~ X1 + X2, data=data, atmean=FALSE)
bin_ape

# glm probit
pro_reg <- glm(Y ~ X1 + X2, family=binomial(link="probit"), data=data)
summary(pro_reg)

# probit average partial effects
pro_ape <- probitmfx(Y ~ X1 + X2, data=data, atmean=FALSE)
pro_ape
```

## Lab 4: Predicting Diamond Prices

***use data from Kaggle

## Lab 4: Solutions

```{r}
load("diamond_train.RData")

# formulas
formla1 <- price ~ carat + as.factor(cut) + as.factor(clarity)
formla2 <- price ~ carat + as.factor(cut) + as.factor(clarity) + depth + table + x + y + x
formla3 <- price ~ (carat + as.factor(cut) + as.factor(clarity) + depth + table + x + y + x)^2

# estimate each model
mod1 <- lm(formla1, data=diamond_train)
mod2 <- lm(formla2, data=diamond_train)
mod3 <- lm(formla3, data=diamond_train)

mod_sel <- function(formla) {
  mod <- lm(formla, data=diamond_train)
  r.squared <- summary(mod)$r.squared
  adj.r.squared <- summary(mod)$adj.r.squared

  n <- nrow(diamond_train)
  uhat <- resid(mod)
  ssr <- sum(uhat^2)
  k <- length(coef(mod))
  aic <- 2*k + n*log(ssr)
  bic <- k*log(n) + n*log(ssr)

  # show results
  result <- tidyr::tibble(r.squared, adj.r.squared, aic, bic)
  return(result)
}

res1 <- mod_sel(formla1)
res2 <- mod_sel(formla2)
res3 <- mod_sel(formla3)

round(rbind.data.frame(res1, res2, res3),3)

# k-fold cross validation

# setup data
k <- 10
n <- nrow(diamond_train)
diamond_train$fold <- sample(1:k, size=n, replace=TRUE)
diamond_train$id <- 1:n

cv_mod_sel <- function(formla) {
  u.squared <- rep(NA, n)
  for (i in 1:k) {
    this.train <- subset(diamond_train, fold != i)
    this.test <- subset(diamond_train, fold == i)
    this.id <- this.test$id
    cv_reg <- lm(formla,
              data=this.train)
    pred <- predict(cv_reg, newdata=this.test)
    u <- this.test$price - pred
    u.squared[this.id] <- u^2
  }
  cv <- sqrt(mean(u.squared))
  return(cv)
}

cv_res1 <- cv_mod_sel(formla1)
cv_res2 <- cv_mod_sel(formla2)
cv_res3 <- cv_mod_sel(formla3)

res1 <- cbind.data.frame(res1, cv=cv_res1)
res2 <- cbind.data.frame(res2, cv=cv_res2)
res3 <- cbind.data.frame(res3, cv=cv_res3)

round(rbind.data.frame(res1, res2, res3),3)

# lasso and ridge
library(glmnet)
library(glmnetUtils)

lasso_res <- cv.glmnet(formla3, data=diamond_train, alpha=1)
ridge_res <- cv.glmnet(formla3, data=diamond_train, alpha=0)

# out of sample predictions

load("diamond_test.RData")

# compute prediction errors with test data
u1 <- diamond_test$price - predict(mod1, newdata=diamond_test)
u2 <- diamond_test$price - predict(mod2, newdata=diamond_test)
u3 <- diamond_test$price - predict(mod3, newdata=diamond_test)
u_lasso <- diamond_test$price - predict(lasso_res, newdata=diamond_test)
u_ridge <- diamond_test$price - predict(ridge_res, newdata=diamond_test)

# compute root mean squared prediction errors
rmspe1 <- sqrt(mean(u1^2))
rmspe2 <- sqrt(mean(u2^2))
rmspe3 <- sqrt(mean(u3^2))
rmspe_lasso <- sqrt(mean(u_lasso^2))
rmspe_ridge <- sqrt(mean(u_ridge^2))

# report results
rmspe <- data.frame(mod1=rmspe1, mod2=rmspe2, mod3=rmspe3, lasso=rmspe_lasso, ridge=rmspe_ridge)
round(rmspe,3)
```

## Extra Questions

1. What are some drawbacks of using $R^2$ as a model selection tool?

2. Does AIC or BIC tend to pick "more complicated" models?  What is the reason for this?

3. Suppose you are interested in predicting some outcome $Y$ and have access to covariates $X_1$, $X_2$, and $X_3$.  You estimate the following two models
\begin{align*}
  Y &= 30 + 4 X_1 - 2 X_2 - 10 X_3, \qquad R^2=0.5, AIC=3421 \\
  Y &= 9 + 2 X_1 - 3 X_2 - 2 X_3 + 2 X_1^2 + 1 X_2^2 - 4 X_3^2 + 2 X_1 X_2, \qquad R^2 = 0.75, AIC=4018
\end{align*}

    a) Which model seems to be predicting better?  Explain why.
    
    b) Using the model that is predicting better, what would be your prediction for $Y$ when $X_1=10, X_2=1, X_3=5$?  
    
4. In Lasso and Ridge regressions, it is common to "standardize" the regressors before estimating the model (e.g., the `glmnet` does this automatically for you).  What is the reason for doing this?

5. In Lasso and Ridge regressions, the penalty term lead to "shrinking" the estimated parameters in the model towards 0.  This tends to introduce bias while reducing variance.  Why can introducing bias while reducing variance potentially lead to better predictions?  Does this argument always apply or just apply in some cases?  Explain.

6.  In Lasso and Ridge regressions, the penalty term depends on the tuning parameter $\lambda$.  

    a) How is this tuning parameter often chosen in practice?  Why does it make sense to choose it in this way?
        
    b) What would happen to the estimated coefficients when $\lambda=0$?
    
    c) What would happen to the estimated coefficients as $\lambda \rightarrow \infty$?
    
7. Suppose you try to estimate a linear probability model, probit model, and logit model using the same specifications.  You notice that the estimated coefficients are substantially different from each other.  Does this mean that something has gone wrong?

## Answers to Some Extra Questions

**Answer to Question 3**

a) The first model appears to be predicting better because the AIC is lower. [Also, notice that $R^2$ is higher in the second model, but this is by construction because it includes extra terms relative to the first model which implies that it will fit at least as well in-sample as the first model, but may be suffering from over-fitting.]

b) 
    \begin{align*}
      \hat{Y} &= 30 + 4 (10) - 2 (1) - 10 (5) \\
      &= 18
    \end{align*}

**Answer to Question 6**

a) The tuning parameter is often chosen via cross validation.  It makes sense to choose it this way because this is effectively choosing a value of $\lambda$ that is making good pseudo-out-of-sample predictions.  As we will see below, if you make bad choices of $\lambda$, that could result in very poor predictions.

b) When $\lambda=0$, there would effectively be no penalty term and, therefore, the estimated parameters would coincide with the OLS estimates.

c) When $\lambda \rightarrow \infty$, the penalty term would overwhelm the term corresponding to minimizing SSR.  This would result in setting all the estimated parameters to be equal to 0.  This extreme approach is likely to lead to very poor predictions.