# Linear Regression

In this chapter, our interest will shift to conditional expectations, such as $\E[Y|X_1,X_2,X_3]$ (I'll write $X_1$, $X_2$, and $X_3$ in a lot of examples in this chapter, but you can think of there being an arbitrary number of $X$'s).

I'll refer to $Y$ as the **outcome**.  You might also sometimes here it called the **dependent variable**.

I'll refer to the $X$'s as either **covariates** or **regressors** or **characteristics**.  You might also here them called **independent variables** sometimes.

Before we start to get into the details, let us first discuss why we're interested in conditional expectations.  First, if we are interested in *making predictions*, it will often be the case that the "best" prediction that one can make is the conditional expectation.  This should make sense to you --- if you want to make a reasonable prediction about what the outcome will be for a new observation that has characteristics $x_1$, $x_2$, and $x_3$, a good way to do it would be to predict that their outcome would be the same as the average outcome across all observations that have the same characteristics; that is, $\E[Y|X_1=x_1, X_2=x_2, X_3=x_3]$.

Next, in economics, we are often interested in how much some outcome of interest changes when a particular covariate changes, holding other covariates constants.  To give some examples, we might be interested in the average return of actively managed mutual funds relative to passively managed mutual funds conditional on investing in assets in the same class (e.g., large cap stocks or international bonds).  As another example, we might be interested in the effect of an increase in the amount of fertilizer on average crop yield but while holding constant the temperature and precipitation.

How much the outcome, $Y$, changes on average when one of the covariates, $X_1$, changes by 1 unit and holding other covariates constant is what we'll call the **average partial effect** of $X_1$ on $Y$.  Suppose $X_1$ is binary, then it is given by

$$
  APE(x_2,x_3) = \E[Y | X_1=1, X_2=x_2, X_3=x_3] - \E[Y | X_1=0,X_2=x_2,X_3=x_3]
$$
Notice that the average partial effect can depend on $x_2$ and $x_3$.  For example, it could be that the effect of active management relative to passive management could be different across different asset classes.

Slightly more generally, if $X_1$ is discrete, so that it can take on several different discrete values, then we define the average partial effect as

$$
  APE(x_1,x_2,x_3) = \E[Y | X_1=x_1+1, X_2=x_2, X_3=x_3] - \E[Y | X_1=x_1,X_2=x_2,X_3=x_3]
$$
which now can depend on $x_1$, $x_2$, and $x_3$.  This is the average effect of going from $X_1=x_1$ to $X_1=x_1+1$ holding $x_2$ and $x_3$ constant.  

Finally, consider the case where we are interested in the average partial effect of $X_1$ which is continuous (for example, as in the fertilizer example mentioned above).  In this case the average partial effect is given by the *partial derivative* of $\E[Y|X_1,X_2,X_3]$ with respect to $X_1$.

$$
  APE(x_1,x_2,x_3) = \frac{\partial \, \E[Y|X_1=x_1, X_2=x_2, X_3=x_3]}{\partial \, x_1}
$$


::: {.side-comment}

<span class="side-comment">Side-Comment:</span> This is probably the part of the class where we will jump around in the book the most this semester.  

The pedagogical approach of the textbook is to introduce the notion of causality very early and to emphasize the requirements on linear regression models in order to deliver causality, while increasing the complexity of the models over several chapters.

This is totally reasonable, but I prefer to start by teaching the mechanics of regressions: how to compute them, how to interpret them (even if you are not able to meet the requirements of causality), and how to use them to make predictions.  Then, we'll have a serious discussion about causality over the last few weeks of the semester.  

In practice, this means we'll cover parts Chapters 4-8 in the textbook now, and then we'll circle back to some of the issues covered in these chapters again towards the end of the semester.  

:::

## Nonparametric Regression / Curse of Dimensionality

* If you knew nothing about regressions, it would seem natural to try to estimate $\E[Y|X_1=x_1,X_2=x_2,X_3=x_3]$ by just calculating the average of $Y$ among observations that have values of the regressors equal to $x_1$, $x_2$, and $x_3$ (if these are discrete) or that are, in some sense, close to $x_1$, $x_2$, and $x_3$ (if these are continuous)

* This is actually a pretty attractive idea

* However, you run into the issue that it is practically challenging to do this when the number of regressors starts to get large (i.e., if you have 10 regressors, generally, you would need tons of data to be able to find a suitable number of observations that are ``close'' to any particular value of the regressors).

* This issue is called the "curse of dimensionality"

* We will focus on linear models for $\E[Y|X_1,X_2,X_3]$ largely to get around the curse of dimensionality

## Linear Regression Models

SW 4.1

In order to get around the curse of dimensionality that we discussed in the previous section, we will often an impose a **linear model** for the conditional expectation.  For example,

$$
  \E[Y|X] = \beta_0 + \beta_1 X
$$
or

$$
  \E[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
$$
If we know the values of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$, then it is straighforward for us to make predictions.  In particular, suppose that we want to predict the outcome for a new observation with characteristics $x_1$, $x_2$, and $x_3$.  Our prediction would be

$$
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
$$

::: {.example}

Suppose that you work for an airline and you are interested in predicting the number of passengers for a Saturday morning flight from Atlanta to Memphis.  Let $Y$ denote the number of passengers, $X_1$ be equal to 1 for a morning flight and 0 otherwise, and let $X_2$ be equal to 1 for a weekday flight and 0 otherwise.  Further suppose that $\E[Y|X_1,X_2] = 80 + 20 X_1 - 15 X_2$.  

In this case, you would predict,

$$
  80 + 20 (1) - 15 (0) = 100
$$
passengers on the flight.

:::

## Partial Effects

In the model, 
\begin{align*}
  \E[Y | X_1, X_2, X_3]  &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\end{align*}

* If $X_1$ is continuous,
\begin{align*}
  \beta_1 = \frac{\partial \E[Y|X_1,X_2,X_3]}{\partial X_1}
\end{align*}
In other words, $\beta_1$ should be interpreted as how much $Y$ increases, on average, when $X_1$ increases by one unit holding $X_2$ and $X_3$ constant.  *Make sure to get this interpretation right!*

* If $X_1$ is discrete (let's say binary):
\begin{align*}
  \beta_1 = \E[Y|X_1=1,X_2,X_3] - \E[Y|X_1=0,X_2,X_3]
\end{align*}
In other words, $\beta_1$ should be interpreted as how much $Y$ increases, on average, when $X_1$ changes from 0 to 1, holding $X_2$ and $X_3$ constant.

::: {.side-comment}

Continuing the same example as above,

:::

::: {.side-comment}

<span class="side-comment">Side-Comment:</span>

The above model can be equivalently written as
\begin{align*}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + U
\end{align*}
where $\E[U|X_1,X_2,X_3] = 0$.  There will be a few times where this formulation will be useful for us.

:::


## Interpreting Binary Covariate

SW 5.3

## Nonlinear Regression Functions

SW 8.1, 8.2

Also, please read all of SW Ch. 8

So far, the the partial effects that we have been interested in have corresponded to a particular parameter in the regression, usually $\beta_1$.  I think this can sometimes be a source of confusion as, at least in my view, we are not typically interested in the parameters for their own sake, but rather are interested in partial effects.  It just so happens that in some leading cases, they coincide.

In this section, we'll see the first of several cases where partial effects do not coincide with a particular parameter.

Suppose that

$$ 
  \E[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 + \beta_4 X_3
$$
If you know the values of $\beta_0,\beta_1,\beta_2,\beta_3,$ and $\beta_4$, then to get a prediction, you would still just plug in the values of the regressors that you'd like to get a prediction for (including $x_1^2$).  

In this model, the partial effect of $X_1$ is given by

$$
  \frac{\partial \, \E[Y|X_1,X_2,X_3]}{\partial \, X_1} = \beta_1 + 2\beta_2 X_1
$$
In other words, the partial effect of $X_1$ depends on the value that $X_1$ takes.  



## Interpreting Interaction Terms

SW 8.3


Another interesting model is

$$
  \E[Y|X_1,X_2,X_3] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \beta_4 X_3
$$
The term $X_1 X_2$ is called an **interaction term**.  In this model, the partial effect of $X_1$ is given by

$$
  \frac{\partial \, \E[Y|X_1,X_2,X_3]}{\partial \, X_1} = \beta_1 + \beta_3 X_2
$$
In this model, the effect of $X_1$ varies with $X_2$.


## Elasticities

SW 8.2

Economists are often interested in **elasticities**, that is, the percentage change in $Y$ when $X$ changes by 1%.

Recall that the defintiion of percentage change of moving from, say, $x_{old}$ to $x_{new}$ is given by

$$
  \textrm{% change} = \frac{x_{new} - x_{old}}{x_{old}} \times 100
$$

Elasticities are closely connected to natural logarithms; following the most common notation in economics, we'll refer to the natural logarithm using the notation: $\log$.  Further, recall that the derivative of the $\log$ function is given by

$$
  \frac{d \, \log(x)}{d \, x} = \frac{1}{x} \implies d\, \log(x) = \frac{d \, x}{x}
$$
which further implies that

$$
  \Delta \log(x) := \log(x_{new}) - \log(x_{old}) \approx \frac{x_{new} - x_{old}}{x_{old}}
$$
and, thus, that

$$
  100 \cdot \Delta \log(x) \approx = \textrm{% change}
$$ 
where the approximation is better $x_{new}$ and $x_{old}$ are close to each other.

Now, we'll use these properties of logarithms in order to interpret several linear models (for simplicity, I am going to not include an error term or extra covariates, but you should continue to interpret parameter estimates as "on average" and "holding other regressors constant" (if there are other regressors in the model)).

- **Log-Log** Model

    $$
    \log(Y) = \beta_0 + \beta_1 \log(X)
    $$
    
    In this case, 
    
    $$
    \begin{aligned}
    \beta_1 &= \frac{ d \, \log(Y) }{d \, \log(X)} \\
    &= \frac{ \frac{d \, Y}{ Y}}{ \frac{d \, X}{X}} \\
    &= \frac{ \frac{d \, Y}{ Y} \cdot 100 }{ \frac{d \, X}{X} \cdot 100} \\
    &= \frac{ \% \Delta Y}{ \% \Delta X}
    \end{aligned}
    $$
    
    All that to say, in a regression of the log of an outcome on the log of a regressor, you should interpret the corresponding coefficient as the percentage change in the outcome when the regressor changes by 1%.  The log-log model is sometimes called a **constant elasticity** model.
    
- Log-Level model

    $$
    \log(Y) = \beta_0 + \beta_1 X
    $$
    
    In this case, 
    
    $$
    \begin{aligned}
    \beta_1 &= \frac{ d \, \log(Y) }{d \, X} \\
    &= \frac{ \frac{d \, Y}{ Y}}{ d \, X} \\
    &= \frac{ \frac{d \, Y}{ Y} \cdot 100 }{ \frac{d \, X}{X} \cdot 100} \\
    &= \frac{ \% \Delta Y}{ \% \Delta X}
    \end{aligned}
    $$
    
    All that to say, in a regression of the log of an outcome on the log of a regressor, you should interpret the corresponding coefficient as the percentage change in the outcome when the regressor changes by 1%.  The log-log model is sometimes called a **constant elasticity** model.

## Omitted Variable Bias

SW 6.1

Suppose that we are interested in the following regression model

$$
  \E[Y|X_1, X_2, Q] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 Q
$$
and, in particular, we are interested in the the partial effect

$$
  \frac{ \partial \, \E[Y|X_1,X_2,Q]}{\partial \, X_1} = \beta_1
$$
But we are faced with the issue that we do not observe $Q$ (which implies that we cannot control for it in the regression)

Recall that we can equivalently write

$$
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 Q + U
$$
where $\E[U|X_1,X_2,Q]=0$.  

Now, for simplicity, suppose that

$$
  \E[Q | X_1, X_2] = \gamma_0 + \gamma_1 X_1 + \gamma_2 X_2
$$

Now, let's consider the idea of just running a regression of $Y$ on $X_1$ and $X_2$ (and just not including $Q$).  We are interested in the question of whether or not we can recover $\beta_1$ if we do this.  If we consider this "feasible" regression, notice that

$$
  \begin{aligned}
  \E[Y|X_1,X_2] &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \E[Q|X_1,X_2] \\
  &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (\gamma_0 + \gamma_1 X_1 + \gamma_2 X_2) \\
  &= (\beta_0 + \beta_3 \gamma_0) + (\beta_1 + \beta_3 \gamma_1) X_1 + (\beta_2 + \beta_3 \gamma_2) X_2
  \end{aligned}
$$

In other words, if we run the feasible regression of $Y$ on $X_1$ and $X_2$, the coefficient on $X_1$ is not equal to $\beta_1$; rather, it is equal to $(\beta_1 + \beta_3 \gamma_1)$.  

That you are not generally able to recover $\beta_1$ in this case is called **omitted variable bias**

There are two cases where you will recover $\beta_1$ though:

- $\beta_3=0$.  This would be the case where $Q$ has no effect on $Y$

- $\gamma_1=0$.  This would be the case where $X_1$ and $Q$ are uncorrelated after controlling for $X_2$.

Interestingly, there may be some case where you can "sign" the bias; i.e., figure out if $\beta_3 \gamma_1$ is positive or negative.  

::: {.side-comment}

<span class="side-comment">Side-Comment:</span>

* The book talks about omitted variable bias in the context of causality (this is probably the leading case), but we have not talked about causality yet.  The same issues arise if we just say that we have some *regression of interest* but are unable to estimate it because some covariates are unobserved.  

* In this case, we would just not be able to interpret, say $\beta_1$, as the partial effect of our interest (except under special cases discussed in class).

* The relationship to causality (which is not so important for now), is that under certain conditions, we may have a particular partial effect that we would be willing to interpret as being the "causal effect", but if we are unable to control for some variables that would lead to this interpretation, then we get to the issues pointed out in the textbook.

:::

## How to estimate the parameters in a regression model

SW 4.2, 6.3

## Inference

SW 4.5, 5.1, 5.2, 6.6

**Additional (Optional) Material**

We discussed in class the practical issues of inference in linear regression models.  

These results rely on arguments building on the Central Limit Theorem (this should not surprise you as it is similar to the case for the asymptotic distribution of $\sqrt{n}(\bar{Y} - \E[Y]))$ that we discussed earlier in the semester.  

In this section, I sketch these types of arguments for you.  This material is advanced/optional, but I suggest that you study this material.  

In class, we wrote down that, in the simple linear regression model, 
\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) \rightarrow N(0,V) \quad \textrm{as} \ n \rightarrow \infty
\end{align*}
where 
\begin{align*}
  V = \frac{\E[(X-\E[X])^2 U^2]}{\var(X)^2}
\end{align*}
and discussed how to use this result to conduct inference.

Now, we want to show why this result holds.

To start with, recall that
\begin{align}
  \hat{\beta}_1 = \frac{\widehat{\cov}(X,Y)}{\widehat{\var}(X)} (\#eq:b1)
\end{align}

Before providing a main result, let's start with noting the following:

*Helpful Intermediate Result 1*
Notice that
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})\bar{Y}\Big) &= \bar{Y} \frac{1}{n}\sum_{i=1}^n \Big( X_i-\bar{X} \Big) \\
  &= \bar{Y} \left( \frac{1}{n}\sum_{i=1}^n X_i - \frac{1}{n}\sum_{i=1}^n \bar{X} \right) \\
  &= \bar{Y} \Big(\bar{X} - \bar{X} \Big) \\
  &= 0
\end{align*}
where the first equality just pull $\bar{Y}$ out of the summation (it is a constant with respect to the summation), the second equality pushes the summation through the difference, the first part of the third equality holds by the definition of $\bar{X}$ and the second part holds because it is an average of a constant.

This implies that
\begin{align}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})(Y_i - \bar{Y})\Big) = \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})Y_i\Big) (\#eq:hr1)
\end{align}
and very similar arguments (basically the same arguments in reverse) also imply that
\begin{align}
  \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})X_i\Big) = \frac{1}{n}\sum_{i=1}^n \Big( (X_i - \bar{X})(X_i - \bar{X})\Big) (\#eq:hr2)
\end{align}
We use both \@ref(eq:hr1) and \@ref(eq:hr2) below.

\vspace{100pt}

Next, consider the numerator in \@ref(eq:b1)
\begin{align*}
  \widehat{\cov}(X,Y) &= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y}) \\
  &= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})Y_i \\
  &= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(\beta_0 + \beta_1 X_i + U_i) \\
  &= \underbrace{\beta_0 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})}_{(A)} + \underbrace{\beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) X_i}_{(B)} + \underbrace{\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) U_i}_{(C)}) \\
\end{align*}
Now, let's consider each of these in turn.

For (A),
\begin{align*}
  \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} \qquad \textrm{and} \qquad \frac{1}{n} \sum_{i=1}^n \bar{X} = \bar{X}
\end{align*}
which implies that this term is equal to 0.

For (B), notice that
\begin{align*}
  \beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) X_i &= \beta_1 \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}) (X_i - \bar{X}) \\
  &= \beta_1 \widehat{\var}(X)
\end{align*}

For (C), well, we'll just carry that one around for now.

Plugging in the expressions for (A), (B), and (C) back into Equation \ref{eqn:b1} implies that
\begin{align*}
  \hat{\beta}_1 = \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{(X_i - \bar{X}) U_i}{\widehat{\var}(X)}
\end{align*}
Next, re-arranging terms and multiplying both sides by $\sqrt{n}$ implies that
\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) &= \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \frac{(X_i - \bar{X}) U_i}{\widehat{\var}(X)}\right) \\
  & \approx \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \frac{(X_i - \E[X]) U_i}{\var(X)}\right)
\end{align*}
The last line (the approximately one) is kind of a weak argument, but basically you can replace $\bar{X}$ and $\widehat{\var}{X}$ and the effect of this replacement will converge to 0 in large samples (this is the reason for the approximately) --- if you want a more complete explanation, sign up for my graduate econometrics class next semester.

Is this helpful?  It may not be obvious, but the right hand side of the above equation is actually something that we can apply the Central Limit Theorem to.  In particular, maybe it is helpful to define $Z_i = \frac{(X_i - \E[X]) U_i}{\var(X)}$.  We know that we could apply a Central Limit Theorem to $\sqrt{n}\left( \frac{1}{n} \sum_{i=1}^n Z_i \right)$ if (i) $Z_i$ had mean 0, and (ii) it is iid.  That it is iid holds immediately from the random sampling assumption.  For mean 0,
\begin{align*}
  \E[Z] &= \E\left[ \frac{(X - \E[X]) U}{\var(X)}\right] \\
  &= \frac{1}{\var(X)} \E[(X - \E[X]) U] \\
  &= \frac{1}{\var(X)} \E[(X - \E[X]) \underbrace{\E[U|X]}_{=0}] \\
  &= 0
\end{align*}
where the only challenging line here is the third one holds from the Law of Iterated Expectations.  This means that we can apply the central limit theorem, and in particular,
$\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n Z_i \right) \rightarrow N(0,V)$ where $V=\var(Z) = \E[Z^2]$ (where 2nd equality holds because $Z$ has mean 0).  Now, just substituting back in for $Z$ implies that
\begin{align*}
  \sqrt{n}(\hat{\beta}_1 - \beta_1) \rightarrow N(0,V)
\end{align*}
where 
\begin{align}
  V &= \E\left[ \left( \frac{(X - \E[X]) U}{\var(X)} \right)^2 \right] \nonumber \\
  &= \E\left[ \frac{(X - \E[X])^2 U^2}{\var(X)^2}\right] (\#eq:V)
\end{align}
which is the expression that we used in class.

## Lab 3: Birthweight and Smoking

For this lab, we'll use the data `Birthweight_Smoking` and study the relationship between infant birthweight and mother's smoking behavior.

1. Run a regression of $birthweight$ on $smoker$.  How do you interpret the results?

2. Use the `datasummary_balance` function from the `modelsummary` package to provide summary statistics for each variable in the data separately by smoking status of the mother.  Do you notice any interesting patterns?

3. Now run a regression of $birthweight$ on $smoker$, $educ$, $nprevisit$, $age$, and $alcohol$.  How do you interpret the coefficient on $smoker$?  How does its magnitude compare to the result from #1?  What do you make of this?

4. Now run a regression of $birthweight$ on $smoker$, the interaction of $smoker$ and $age$ and the other covariates (including $age$) from #3.  How do you interpret the coefficient on $smoker$ and the coefficient on the interaction term?

5. Now run a regression of $birthweight$ on $smoker$, the interaction of $smoker$ and $alcohol$ and the other covariates from #3.  How do you interpret the coefficient on $smoker$ and the coefficient on the interaction term?

6. Now run a regression of $birthweight$ on $age$ and $age^2$.  Plot the predicted value of birthweight as a function of age for ages from 18 to 44.  What do you make of this?

7. Now run a regression of $\log(birthweight)$ on $smoker$ and the other covariates from #3.  How do you interpret the coefficient on $smoker$?

## Lab 3: Solutions

```{r, message=FALSE}
# load packages
library(haven)
library(modelsummary)
library(dplyr)
library(ggplot2)

# load data
Birthweight_Smoking <- read_dta("data/birthweight_smoking.dta")
```

1.

```{r}
reg1 <- lm(birthweight ~ smoker, data=Birthweight_Smoking)
summary(reg1)
```
We estimate that, on average, smoking reduces an infant's birthweight by about 250 grams.  The estimated effect is strongly statistically significant, and (I am not an expert but) that seems like a large effect of smoking to me.

2. 
```{r}
# create smoker factor --- just to make table look nicer
Birthweight_Smoking$smoker_factor <- as.factor(ifelse(Birthweight_Smoking$smoker==1, "smoker", "non-smoker"))
datasummary_balance(~smoker_factor, 
                    data=dplyr::select(Birthweight_Smoking, -smoker),
                    fmt=2)

```

The things that stand out to me are:

- Birthweight tends to be notably lower for smokers relative to non-smokers.  The difference is about 7.4% lower birthweight for babies whose mothers smoked.

- That said, smoking is also correlated with a number of other things that could be related to lower birthweights.  Mothers who smoke went to fewer pre-natal visits on average, were more likely to be unmarried, were more likely to have drink alcohol during their pregnancy, were more likely to be less educated.  They also were, on average, somewhat younger than mothers who did not smoke.

3. 

```{r}
reg3 <- lm(birthweight ~ smoker + educ + nprevist + age + alcohol,
           data=Birthweight_Smoking)
summary(reg3)
```
Here we estimate that smoking reduces an infant's birthweight by about 200 grams on average holding education, number of pre-natal visits, age, and whether or not the mother consumed alcohol constant.  The magnitude of the estimated effect is somewhat smaller than the previous estimate.  Due to the discussion in #2 (particularly, that smoking was correlated with a number of other characteristics that are likely associated with lower birthweights), this decrease in the magnitude is not surprising.

4. 

```{r}
reg4 <- lm(birthweight ~ smoker + I(smoker*age) + educ + nprevist + age + alcohol,
           data=Birthweight_Smoking)
summary(reg4)
```
We should be careful about the interpretatio here.  We have estimated a model like 

$$
  \E[Birthweight|Smoker, Age, X] = \beta_0 + \beta_1 Smoker + \beta_2 Smoker \cdot Age + \cdots
$$
Therefore, the partial effect of smoking is given by

$$
  \E[Birthweight | Smoker=1, Age, X] - \E[Birthweight | Smoker=0, Age, X] = \beta_1 + \beta_2 Age
$$
Therefore, the partial effect of smoking depends on $Age$.  For example, for $Age=18$, the partial effect is $\beta_1 + \beta_2 (18)$.  For $Age=25$, the partial effect is $\beta_1 + \beta_2 (25)$, and for $Age=35$, the partial effect is $\beta_1 + \beta_2 (35)$.  Let's calculate the partial effect at each of those ages.

```{r}
bet1 <- coef(reg4)[2]
bet2 <- coef(reg4)[3]

pe_18 <- bet1 + bet2*18
pe_25 <- bet1 + bet2*25
pe_35 <- bet1 + bet2*35

round(cbind.data.frame(pe_18, pe_25, pe_35),2)
```

This suggests substantially larger effects of smoking on birthweight for older mothers.

```{r}
reg5 <- lm(birthweight ~ smoker + I(smoker*alcohol) + educ + nprevist + age + alcohol,
           data=Birthweight_Smoking)
summary(reg5)
```
The point estimate suggests that the effect of smoking is larger for women who consume alcohol and smoke than for women who do not drink alcohol.  This seems plausible, but our evidence is not very strong here --- the estimates are not statistically significant at any conventional significance level (the p-value is equal to 0.32).

6.

```{r}
reg6 <- lm(birthweight ~ age + I(age^2), data=Birthweight_Smoking)
summary(reg6)
```
```{r}
preds <- predict(reg6, newdata=data.frame(age=seq(18,40)))
ggplot(data.frame(preds=preds, age=seq(18,40)), aes(x=age, y=preds)) + 
  geom_line() + 
  geom_point(size=3) + 
  theme_bw() + 
  ylab("predicted values")
```
The figure suggests that predicted birthweight is increasing in mother's age up until about age 34 and then decreasing after that.

7.

```{r}
reg7 <- lm(I(log(birthweight)) ~ smoker + educ + nprevist + age + alcohol,
           data=Birthweight_Smoking)
summary(reg7)
```
The estimated coefficient on $smoker$ says that smoking during pregnancy decreases a baby's birthweight by 6.3%, on average, holding education, number of pre-natal visits, age of the mother, and whether or not the mother consumed alcohol during the pregnancy constant.

## Coding Questions

1. For this problem, we will use the data `mtcars`.  

    a) Run a regression of miles per gallon ($mpg$) on horsepower ($hp$).  How should you interpret the estimated coefficient on horsepower?  What is the p-value for the coefficient on horsepower?  
    
    b) Run a regression of $\log(mpg)$ on $hp$.  How should you interpret the estimated cofficient on $hp$?
    
    c) Run a regression of `mpg` on $\log(hp)$.  How should you interpret the estimated coefficient on $\log(hp)$?
    
    d) Run a regression of $\log(mpg)$ on $\log(hp)$.  How should you interpret the estimated coefficient on $\log(hp)$?

2. For this question, we'll use the `fertilizer_2000` data.  

    a) Run a regression of $\log(avyield)$ on $\log(avfert)$.  How do you interpret the estimated coefficient on $\log(avfert)$?  
    
    b) Now suppose that you additionally want to control for precipitation and the region that a country is located in.  How would you do this?  Estimate the model that you propose here, report the results, and interpret the coefficient on $\log(avfert)$.
    
    c) Now suppose that you are interested in whether the effect of fertilizer varies by region that a country is located in (while still controlling for the same covariates as in part (b)).  Propose a model that can be used for this purpose.  Estimate the model that you proposed, report the results, and discuss whether the effect of fertilizer appears to vary by region or not.
    

## Extra Questions

1. Suppose you run the following regression
\begin{align*}
  Earnings = \beta_0 + \beta_1 Education + U
\end{align*}
with $\E[U|Education] = 0$.  How do you interpret $\beta_1$ here?

2. Suppose you run the following regression
\begin{align*}
  Earnings = \beta_0 + \beta_1 Education + \beta_2 Experience + \beta_3 Female + U
\end{align*}
with $\E[U|Education, Experience, Female] = 0$.  How do you interpret $\beta_1$ here?

3. Suppose you are interested in testing whether an extra year of education increases earnings by the same amount for men and women.  

    a) Propose a regression and strategy for this sort of test.  
    
    b) Suppose you also want to control for experience in conducting this test, how would do it?
    
4. Suppose you run the following regression
\begin{align*}
  \log(Earnings) = \beta_0 + \beta_1 Education + \beta_2 Experience + \beta_3 Female + U
\end{align*}
with $\E[U|Education, Experience, Female] = 0$.  How do you interpret $\beta_1$ here?

5. A common extra condition (though somewhat old-fashioned) is to impose *homoskedasticity*.  Homoskedasticity says that $\E[U^2|X] = \sigma^2$ (i.e., the variance of the error term does not change across different values of $X$).

    a) Under homoskedasticity, the expression for $V$ in \@ref(eq:V) simplifies.  Provide a new expression for $V$ under homoskedasticity.  **Hint:** you will need to use the law of iterated expectations.
    
    b) Using this expression for $V$, explain how to calculate standard errors for an estimate of $\beta_1$ in a simple linear regression.
    
    c) Explain how to construct a t-statistic for testing $H_0: \beta_1=0$ under homoskedasticity.
    
    d) Explain how to contruct a p-value for $\beta_1$ under homoskedasticity.
    
    e) Explain how to construct a 95\% confidence interval for $\beta_1$ under homoskedasticity.
    
## Answers to Some Extra Questions

**Answer to Question 2**

$\beta_1$ is how much $Earnings$ increase on average when $Education$ increases by one year holding $Experience$ and $Female$ constant.

**Answer to Question 3**

a) Run the regression
    \begin{align*}
        Earnings &= \beta_0 + \beta_1 Education + \beta_2 Female + \beta_3 Education \times Female + U
    \end{align*}
    and test (e.g., calculate a t-statistic and check if it is greater than 1.96 in absolute value) if $\beta_3=0$.

b) You can run the following regression:
   \begin{align*}
      Earnings &= \beta_0 + \beta_1 Education + \beta_2 Female \\
      & \hspace{25pt} + \beta_3 Education \times Female + \beta_4 Experience + U
   \end{align*}
   Here, you would still be interested in $\beta_3$.  If you thought that the return to experience varied for men and women, you might also include an interaction term involving $Experience$ and $Female$. 

**Partial Answer to Question 5**

a) Starting from \@ref(eq:V)

  \begin{align*}
    V &= \E\left[ \frac{(X - \E[X])^2 U^2}{\var(X)^2} \right] \\
    &= \frac{1}{\var(X)^2} \E[(X-\E[X])^2 U^2] \\
    &= \frac{1}{\var(X)^2} \E\big[(X-\E[X])^2 \E[U^2|X] \big] \\
    &= \frac{1}{\var(X)^2} \E[(X-\E[X])^2 \sigma^2 ] \\
    &= \frac{\sigma^2}{\var(X)^2} \E[(X-\E[X])^2] \\
    &= \frac{\sigma^2}{\var(X)^2} \var(X) \\
    &= \frac{\sigma^2}{\var(X)}
  \end{align*}

  where 

  * the second equality holds because $\var(X)^2$ is non-random and can come out of the expectation, 
    
  * the third equality uses the law of iterated expectations, 
    
  * the fourth equality holds by the condition of homoskedasticity, 
    
  * the fifth equality holds because $\sigma^2$ is non-random and can come out of the expectation, 
    
  * the sixth equality holds by the definition of variance, and 
    
  * the last equality holds by cancelling $\var(X)$ in the numerator with one of the $\var(X)$'s in the denominator.