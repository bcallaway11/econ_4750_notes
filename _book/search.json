[
  {
    "objectID": "16-natural_experiments.html",
    "href": "16-natural_experiments.html",
    "title": "17  Natural Experiments",
    "section": "",
    "text": "17.1 Instrumental Variables\nThe most common and important natural experiment arises from having an instrumental variable (often abbreviated IV). A instrumental variable often arises when “something weird” happens that makes some individuals more likely to participate in the treatment without otherwise affecting their outcomes. This results in the treatment being effectively randomly assigned for some subgroup.\nLet me give you some examples:",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Natural Experiments</span>"
    ]
  },
  {
    "objectID": "16-natural_experiments.html#instrumental-variables",
    "href": "16-natural_experiments.html#instrumental-variables",
    "title": "17  Natural Experiments",
    "section": "",
    "text": "This is not as popular of a topic as it used to be, but many economists used to be interested in the causal effect of military service on earnings. This is challenging because individuals “self-select” into the military (i.e., individuals don’t just randomly choose to join the military, and, while there may be many dimensions of choosing to join the military, probably one dimension is what a person expects the effect to be on their future earnings).\n\nA famous example of an instrumental variable in this case is an individual’s Vietname draft lottery number. Here, the idea is that a randomly generated lottery number (by construction) doesn’t have any direct effect on earnings, but it does affect the chances that someone participates in the military. This is therefore a natural experiment and could serve the role of an instrumental variable.\n\nFor studying the effect of education on on earnings, researchers have used the day of birth as an instrument for years of education. The idea is that compulsory school laws are set up so that individuals can leave school when they reach a certain age (e.g., 16). But this means that, among students that want to drop out as early as they can, students who have an “early” birthday (usually around October) will have spent less time in school than students who have a “late” birthday (usually around July) at any particular age. This is a kind of natural experiment — comparing earnings of students who drop out at 16 for those who have early birthdays relative to late birthdays.\n\n\nSide Comment:  Instrumental variables has a strong claim for being the main contribution of econometrics to statistics. It dates back to the work of Philip Wright in the 1920s on supply and demand estimation in agricultural markets – it is more difficult than you might at first think to estimate supply and demand curves. And I think that it is fair to say that a lot of approaches to causal inference can be considered as a variation of instrumental variables. Cunningham (2021) has a good and interesting discussion of the history of instrumental variables in economics.\n\n\n17.1.1 Setup\nWe will continue to denote the outcome by \\(Y\\) and consider the case with a binary treatment \\(D\\). We will also consider the case with a binary instrument \\(Z\\). In the military service example, \\(Y\\) is a person’s earnings, \\(D\\) indicates whether they served in the military, and \\(Z\\) indicates whether they were drafted. Our interest is in learning about the causal effect of \\(D\\) on \\(Y\\), exploiting that \\(Z\\) affects the probability of being treated and is effectively randomly assigned.\nPotential Outcomes and Potential Treatments\nWe will continue to use the notation \\(Y(1)\\) and \\(Y(0)\\) to denote the potential outcomes. Each person has potential treatment status depending on the instrument: \\(D_i(0)\\) represents treatment status if not drafted, and \\(D_i(1)\\) represents treatment status if drafted. Similarly, each person has potential outcomes \\(Y_i(0)\\) and \\(Y_i(1)\\) corresponding to earnings if they don’t serve versus if they do serve.\n\n\n17.1.2 Latent Types\nBased on the pair \\((D_i(0), D_i(1))\\), every individual falls into one of four latent types (“latent” just means unobserved, so here we mean that every unit is one of four types, but we do not observe which type they are):\n\nAlways-takers have \\(D_i(0) = 1\\) and \\(D_i(1) = 1\\) — they serve regardless of draft status. Let \\(A_i=1\\) for always-takers.\nNever-takers have \\(D_i(0) = 0\\) and \\(D_i(1) = 0\\) — they never serve. Let \\(N_i=1\\) for never-takers.\nCompliers have \\(D_i(0) = 0\\) and \\(D_i(1) = 1\\) — they serve only if drafted. Let \\(C_i=1\\) for compliers.\nDefiers have \\(D_i(0) = 1\\) and \\(D_i(1) = 0\\) — they do the opposite of what the draft tells them. Let \\(F_i=1\\) for defiers.\n\nThere are a couple of things that are worth pointing out about these types. First, among these types, only compliers and defiers respond to the instrument. Always-takers and never-takers have treatment status that is unaffected by the instrument. Second, defiers are somewhat strange type — for example, it seems very strange that one would serve in the military when not drafted, but avoid service when drafted. To be clear, what I mean by “strange”, is that we would not expect there to be many (or any) defiers in the military service example. That it is strange/uncommon to be a defier will be important below.\n\n\n17.1.3 Four IV Assumptions\nIn order to recover a causal effect using an instrumental variable, we are going to make the following four assumptions. The first two essentially just formalize our discussion above. The second two could be strong in particular applications.\n\nRelevance requires that \\(\\P(D(1)=1) &gt; \\P(D(0)=1)\\), meaning the instrument actually affects whether or not units get treated.\nIndependence requires that \\(Z \\independent (Y(0), Y(1), D(0), D(1))\\), meaning the instrument is as-good-as randomly assigned.\nExclusion Restriction requires that \\(Y_i(D_i(z),z) = Y_i(D_i(z))\\), meaning the instrument affects the outcome only through its effect on treatment.\nMonotonicity requires that \\(D_i(1) \\geq D_i(0)\\) for all \\(i\\), which rules out defiers.\n\nThe relevance condition just says that the instrumental variable affects the probability of being treated. It rules out things like using a random number generator to draw \\(Z_i\\) as this would not affect the treatment status. Usually this is a very mild assumption, and we can check it simply by checking whether the probability of being treated is higher when \\(Z=1\\) than when \\(Z=0\\) using our sample.\nThe independence assumption says that the instrument is as-good-as randomly assigned. The way we have defined a natural experiment (like in the military service example) essentially applies that this assumption holds. That being said, this assumpton does rule out just picking some variable that you happen to observe in the data and using it as an instrument. For example, suppose you were considering using whether or not someone comes from a poor family as an instrument for military service. This would likely satisfy the relevance condition, but it would likely violate independence because family income is likely correlated with unobserved determinants of earnings.\nThe exclusion restriction says that the instrument only affects the outcome through its effect on treatment. For many applications where the instrument is effectively randomly assigned, this is the assumption that is most likely to be violated. In the military service example, this assumption could be violated if the draft lottery affects outcomes through channels other than military service itself. This could happen if there were people who go to college if they are drafted (as a way to avoid military service) but would not go to college if they are not drafted. In this case, the draft lottery would affect earnings through education choices, violating the exclusion restriction.\nFinally, monotonicity rules out defiers. As we discussed above, in many applications are the most strange latent type. Still, this assumption says that there are none at all (not just that it’s rare). To me, this assumption seems plausible in the military service example, but there are some applications where it could have more bite.\n\n\n17.1.4 Identification\nNext, let’s discuss how we can learn about causal effects under the setup that we have been considering. That \\(Z\\) is effectively randomly assigned suggests considering \\[\\begin{align*}\n  \\E[Y | Z=1] - \\E[Y | Z=0]\n\\end{align*}\\] which compares the mean outcome for those with \\(Z=1\\) (e.g., drafted) to those with \\(Z=0\\) (e.g., not drafted). This comparison is often referred to as the reduced form effect of the instrument on the outcome. Our next goal is to relate this equation to potential outcomes. In particular, notice that \\[\\begin{align*}\n  \\E[Y | Z=1] &= \\E[Y(D(1)) | Z=1] \\\\\n  &= \\E[Y(D(1))]\n\\end{align*}\\] where the first equality holds because for those with \\(Z=1\\), the observed outcome is \\(Y(D(1))\\), and the second equality holds by the independence assumption. Using exactly the same sort of argument, it holds that \\[\\begin{align*}\n  \\E[Y | Z=0] &= \\E[Y(D(0))]\n\\end{align*}\\] Thus, we have that \\[\\begin{align*}\n  \\E[Y | Z=1] - \\E[Y | Z=0] &= \\E[Y(D(1)) - Y(D(0))]\n\\end{align*}\\] This is the average causal effect of the instrument on the outcome. It could be of interest in some cases. For example, in the military service example, this is the average effect of being drafted on earnings. However, it is not what we originally set out to learn about: the causal effect of the treatment on the outcome. To proceed, let us use the law of iterated expectations to write: \\[\\begin{align*}\n  \\E[Y(D(1)) - Y(D(0))] &= \\E\\big[\\underbrace{Y(D(1)) - Y(D(0))}_{(A)} | A=1\\big] \\P(A=1) \\\\\n  &+ \\E\\big[\\underbrace{Y(D(1)) - Y(D(0))}_{(B)} | N=1\\big] \\P(N=1) \\\\\n  &+ \\E\\big[Y(D(1)) - Y(D(0)) | C=1\\big] \\P(C=1) \\\\\n  &+ \\E\\big[Y(D(1)) - Y(D(0)) | D=1\\big] \\underbrace{\\P(F=1)}_{(C)}\n\\end{align*}\\] We can simplify this expression significantly:\n\nFor underlined term (A), always-takers have \\(D(1)=D(0)=1\\), so \\(Y(D(1)) = Y(1)\\) and \\(Y(D(0)) = Y(1)\\), i.e., regardless of the value of the instrument that an always-taker experiences, their observed outcome is \\(Y(1)\\). Therefore, the underlined term equals zero.\nFor underlined term (B), never-takers have \\(D(1)=D(0)=0\\), so \\(Y(D(1)) = Y(0)\\) and \\(Y(D(0)) = Y(0)\\), and this term is also equal to zero using a similar argument as for always-takers.\nFor underlined term (C), recall that monotonicity ruled out the existence of defiers, and, therefore, \\(\\P(F=1) = 0\\).\n\nPutting this all together, we have that \\[\\begin{align*}\n  \\E[Y | Z=1] - \\E[Y | Z=0] &= \\E[Y(1) - Y(0) | C=1] \\P(C=1)\n\\end{align*}\\] This is a helpful step as we have now expressed the reduced form effect in terms of potential outcomes of the treatment itself, which is a causal effect. The last thing that we would like to do is to try to get rid of the \\(\\P(C=1)\\) term as it is mainly just “scaling down” the causal effect term. To do this, we will consider \\[\\begin{align*}\n  \\E[D | Z=1] - \\E[D | Z=0]\n\\end{align*}\\] which compares the probability of being treated for those with \\(Z=1\\) to those with \\(Z=0\\). This comparison is often referred to as the first stage effect of the instrument on the treatment. Using a similar argument as above, we can write \\[\\begin{align*}\n  \\E[D | Z=1] - \\E[D | Z=0] &= \\E[D(1) - D(0)] \\\\\n  &= \\E\\big[\\underbrace{D(1) - D(0)}_{=0} | A=1\\big] \\P(A=1) \\\\\n  &+ \\E\\big[\\underbrace{D(1) - D(0)}_{=0} | N=1\\big] \\P(N=1) \\\\\n  &+ \\E\\big[\\underbrace{D(1) - D(0)}_{=1} | C=1\\big] \\P(C=1) \\\\\n  &+ \\E\\big[D(1) - D(0) | D=1\\big] \\underbrace{\\P(F=1)}_{0} \\\\\n  &= \\P(C=1)\n\\end{align*}\\] This argument is extremely similar (and easier) than for the reduced form effect. For compliers, notice that \\(D(1)=1\\) and \\(D(0)=0\\), so \\(D(1) - D(0) = 1\\). Finally, we can combine the reduced form and first stage results: \\[\\begin{align*}\n  \\frac{\\E[Y | Z=1] - \\E[Y | Z=0]}{\\E[D | Z=1] - \\E[D | Z=0]} &= \\frac{\\E[Y(1) - Y(0) | C=1] \\P(C=1)}{\\P(C=1)} \\\\\n  &= \\E[Y(1) - Y(0) | C=1]\n\\end{align*}\\] This is our main result. It says that taking the ratio of the reduced form effect to the first stage effect recovers the average treatment effect for the compliers. The average treatment effect for compliers is often referred to as the local average treatment effect (LATE) as it is the average treatment effect for a specific subpopulation (the compliers). The ratio of the reduced form to the first stage is often referred to as the Wald ratio.\n\n\n17.1.5 Estimation\nEstimating the LATE is fairly straightforward. We can just use the analogy principle. In particular, \\[\\begin{align*}\n  \\widehat{LATE} &= \\frac{\\bar{Y}_{Z=1} - \\bar{Y}_{Z=0}}{\\bar{D}_{Z=1} - \\bar{D}_{Z=0}}\n\\end{align*}\\] where \\(\\bar{Y}_{Z=z}\\) is the sample mean of \\(Y\\) among those with \\(Z=z\\), and \\(\\bar{D}_{Z=z}\\) is the sample mean of \\(D\\) among those with \\(Z=z\\).\nAlthough this is straightforward, in practice, it will be more convenient to use a package to make the calculation for you, as it will also calculate standard errors. You can use code like the following, where your_data is your data frame, Y is the outcome variable, D is the treatment variable, and Z is the instrument variable.\n\nlibrary(estimatr)\niv_model &lt;- iv_robust(Y ~ D | Z, data = your_data)\nsummary(iv_model)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Natural Experiments</span>"
    ]
  },
  {
    "objectID": "16-natural_experiments.html#regression-discontinuity-designs",
    "href": "16-natural_experiments.html#regression-discontinuity-designs",
    "title": "17  Natural Experiments",
    "section": "17.2 Regression Discontinuity Designs",
    "text": "17.2 Regression Discontinuity Designs\nSW 13.4\nThe final type of natural experiment that we will talk about is called regression discontinuity (often abbreviated RD or RDD for “regression discontinuity design”). The sort of natural experiment is available when there is a running variable with a threshold (i.e., cutoff) where individuals above the threshold are treated while individuals below the threshold are not treated. These sorts of thresholds/cutoffs are fairly common.\nHere are some examples:\n\nCutoffs that make students eligible for a scholarship (e.g., the Hope scholarship)\nRules about maximum numbers of students allowed in a classroom in a particular school district\nVery close political elections\nVery close union elections\nThresholds in tax laws\n\nThen, the idea is to compare outcomes among individuals that “barely” were treated relative to those that “barely” weren’t treated. By construction, this often has properties that are similar to an actual experiment as those that are just above the cutoff should have observed and unobserved characteristics that are the same as those just below the cutoff.\nIn some sense, RD is a special case of instrumental variables; however, discontinuities are common enough in economics that it is worth considering these approaches specifically.\nLet’s explain the intuition for RD in the context of a particular example. Suppose that you are interested in how financial aid (a treatment) affects college completion. Policies like the Hope scholarship provide financial aid to students whose high school GPA is above a certain cutoff (e.g., 3.0). Students with GPAs just above 3.0 receive the treatment, while students with GPAs just below 3.0 do not. In the terminology of regression discontinuity, the student’s GPA is called a running variable, which we will denote by \\(R_i\\). The cutoff is denoted by \\(c\\) (in this case, \\(c = 3.0\\)). Treatment status \\(D_i\\) is determined by whether the running variable exceeds the cutoff: \\(D_i = 1\\) if \\(R_i \\geq c\\) and \\(D_i = 0\\) if \\(R_i &lt; c\\). We will end up comparing the average college completion rates for students with GPAs just above 3.0 to those with GPAs just below 3.0, and argue below that this comparison identifies the causal effect of financial aid on college completion for students at the cutoff.\n\n17.2.1 Key Assumption: Continuity\nThe RD design relies on a continuity assumption about potential outcomes. In particular, we assume that \\[\\begin{align*}\n  \\E[Y(1) | R = r] \\text{ and } \\E[Y(0) | R = r] \\text{ are continuous in } r\n\\end{align*}\\] In particular, we need this assumption to hold at the cutoff \\(c\\). Below is a plot that illustrates the continuity assumption for untreated potential outcomes, \\(\\E[Y(0)|R=r]\\). You can see that this function is continuous everywhere.\nIn the context of our financial aid example where college completion is the outcome, this function says that, absent any financial aid, the expected college completion rate is increasing in GPA, but there are not any sudden jumps in college completion rates.\nYou should probably think of continuity as being a mild assumption (with one main possible caveat that we will discuss below). In our example, students with GPAs of 2.99 and 3.01 are likely very similar in terms of their underlying ability and motivation, so we would expect their college completion rates to be similar absent financial aid.\n\n\n\n\n\nIllustration of continuity: E[Y(0)|R=r] smooth at cutoff\n\n\n\n\n\n\n\n\n\nViolation: jump in E[Y(0)|R=r] at cutoff (vertical segment removed)\n\n\n\n\nNext, let’s consider a plot where the continuity assumption is violated. This second plot shows a clear violation: a jump at the cutoff. In many applications, such a jump seems implausible. However, there is one leading case where a jump can occur: when individuals can manipulate the value of their running variable. For example, suppose that some students are highly motivated and strategically select courses to boost their GPA if they have a GPA near the cutoff (presumably this kind of motivation could also increase college completion rates). This could lead to a ``pile-up’’ of highly motivated students just above the cutoff, leading to a jump in the underlying potential outcomes at the cutoff.\nManipulation is a concern in RD designs especially when the running variable is something that individuals can, at least to some extent, control—like GPA in the example above. A different classic RD design involves studying the effect of legal drinking age on mortality, where the running variable is age in years (with a cutoff at age 21). Individuals cannot manipulate their birth date to turn 21 sooner, so manipulation is much less of a concern in this type of application.\n\n\n17.2.2 Identification\nNext, let’s work through what we can learn about the causal effect of the treatment when we have access to a discontinuity. To start with, let us think about what the observed data would look like if the treatment caused outcomes to increase on average.\n\n\n\n\n\nRD Identification: Positive Treatment Effect\n\n\n\n\nRecall that we observe treated potential outcomes for \\(R_i \\geq c\\) and untreated potential outcomes for \\(R_i &lt; c\\). This coresponds to the solid lines in the plots above. The dashed lines correspond to the unobserved counterfactual potential outcomes. Importantly, notice that there is a discontinuity in the observed outcomes at the cutoff.\nNext, let’s consider what the same plot would look like if there were no treatment effect.\n\n\n\n\n\nRD Identification: No Treatment Effect\n\n\n\n\nIn this plot, which may be a bit hard to interpret because the lines are on top of each other, the main thing to notice is that there is no discontinuity at the cutoff.\nThe discussion above suggests that we can learn about the causal effect of the treatment by “zooming in” on the cutoff and checking for a discontinuity (as well as checking the magnitude of the discontinuity).\nIn particular, consider \\[\\begin{align*}\n  \\tau^{RD} := \\lim_{r \\downarrow c} \\E[Y | R = r] &- \\lim_{r \\uparrow c} \\E[Y | R = r]\n\\end{align*}\\] \\(\\tau^{RD}\\) comes from comparing the expected outcomes just above and just below the cutoff. Notice that \\[\\begin{align*}\n  \\tau^{RD} &= \\lim_{r \\downarrow c} \\E[Y | R = r] - \\lim_{r \\uparrow c} \\E[Y | R = r] \\\\\n  &= \\lim_{r \\downarrow c} \\E[Y(1) | R = r] - \\lim_{r \\uparrow c} \\E[Y(0) | R = r] \\\\\n  &= \\E[Y(1) | R = c] - \\E[Y(0) | R = c] \\\\\n  &= \\E[Y(1) - Y(0) | R = c]\n\\end{align*}\\] where the second equality holds by writing the observed outcomes in terms of potential outcomes, and the third equality (which is the key step) holds by the continuity assumption. The result is that \\(\\tau^{RD} = \\E[Y(1) - Y(0) | R = c]\\), which is a local average average treatment effect for individuals at the cutoff.\nIntuitively, you can think of regression discontinuity as delivering a local experiment where individuals just above and just below the cutoff are effectively randomly assigned to treatment and control groups.\n\n\n17.2.3 Estimation\nFrom the discussion above, it may be unclear how to estimate \\(\\tau^{RD}\\) in practice. In particular, it is not clear how to deal with the limits in the definition of \\(\\tau^{RD}\\). In fact, I think RD is the first and only time this semester where we won’t directly use the analogy principle to estimate a parameter of interest (due to the limits). To deal with the limits, instead of using all of the data, we will only use data that is “close” to the cutoff. In particular, we will only use data where \\(R_i\\) is in the interval \\([c - h, c + h]\\) for some bandwidth parameter \\(h &gt; 0\\)—this bandwidth parameter basically determines what we mean by close. Once we have decided on \\(h\\), if you are a little clever, you can estimate \\(\\tau^{RD}\\) from a regression. In particular, you can run the following regression using only data in that interval: \\[\\begin{align*}\n  Y_i = \\beta_0 + \\beta_1 (R_i-c) + \\big(\\beta_2 + \\beta_3 (R_i-c)\\big) D_i + U_i\n\\end{align*}\\] This is a regression where we (1) “center” at c, and (2) we allow for the intercept and slope coefficients to be different on each side of the cutoff. In particular, \\(\\hat{\\beta}_2\\) is our estimate of \\(\\tau^{RD}\\).\n\n\n17.2.4 Example with Simulated Data\nTo illustrate how RD estimation works in practice, let me show you an example with simulated data roughly in line with the financial aid example that we have been discussing.\n\n\n\n\n\n\n\n\n\nNow let’s estimate the treatment effect using a bandwidth of \\(h = 0.5\\) (meaning we only use observations within 0.5 units of the cutoff on either side).\n\n# Choose bandwidth\nh &lt;- 0.5\n\n# Restrict to observations within bandwidth\nrd_data_bw &lt;- rd_data %&gt;% filter(abs(R - c) &lt;= h)\n\n# Estimate RD regression\nrd_reg &lt;- lm(Y ~ D + I(R - c) + D:I(R - c), data = rd_data_bw)\nsummary(rd_reg)\n\n\nCall:\nlm(formula = Y ~ D + I(R - c) + D:I(R - c), data = rd_data_bw)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.92775 -0.18555 -0.02267  0.20556  0.74318 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.46402    0.07083  48.909  &lt; 2e-16 ***\nD            0.75128    0.09851   7.626  2.7e-12 ***\nI(R - c)     0.92051    0.24634   3.737 0.000266 ***\nD:I(R - c)   0.69026    0.34369   2.008 0.046421 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3017 on 148 degrees of freedom\nMultiple R-squared:  0.8522,    Adjusted R-squared:  0.8492 \nF-statistic: 284.4 on 3 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated treatment effect is 0.751, which we can compare to the true treatment effect at the cutoff of \\(0.8 + 0.3 \\times 0 = 0.8\\).\nWhat we’ve done with this regression is, effectively, to have run separate regression for the treated and untreated groups within the bandwidth, allowing for linear trends in \\(R\\). The intercept shift at the cutoff (the coefficient on \\(D\\)) gives us our estimate of the treatment effect.\n\n\n\n\n\n\n\n\n\nThe plot shows the fitted regression lines for treated and untreated observations within the bandwidth. The vertical gap between the two lines at the cutoff (the intercept shift) is our estimate of the treatment effect.\nFinally, notice that we’re only using observations close to the cutoff (those within the bandwidth). We can zoom out to see all the data, and, in particular, notice that we are dropping a lot of data that is far from the cutoff—this typical in RD applications where the focus is local to the cutoff.\n\n\n\n\n\n\n\n\n\nThis visualization shows all the data but highlights the observations within the bandwidth (which appear more opaque) and shows the fitted regression lines. The shaded gray region indicates the bandwidth around the cutoff. This makes it clear that we’re only using local information near the cutoff to estimate the treatment effect.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Natural Experiments</span>"
    ]
  },
  {
    "objectID": "16-natural_experiments.html#additional-material-instrumental-variables-with-covariates",
    "href": "16-natural_experiments.html#additional-material-instrumental-variables-with-covariates",
    "title": "17  Natural Experiments",
    "section": "17.3 Additional Material: Instrumental Variables with Covariates",
    "text": "17.3 Additional Material: Instrumental Variables with Covariates\nIn some applications, the IV assumptions may be more plausible if they are made after conditioning on some covariates \\(X\\). We will keep Assumptions 1, 3, and 4 the same, but modify Assumption 2 to be:\n\nConditional Independence requires that \\(Z \\independent (Y(0), Y(1), D(0), D(1)) | X\\), meaning the instrument is as-good-as randomly assigned after conditioning on \\(X\\).\n\nIn our discussion about military service, especially if we want to consider a more realistic version of this example, an important covariate to condition on is birth year as some years there were substantially more people drafted than in other years.\nTo start with, notice that if we condition on \\(X\\), then we can use exactly the same logic as before to show that \\[\\begin{align*}\n  \\text{LATE}(x) &:= \\E[Y(1) - Y(0) | X=x, C=1] \\\\\n  &= \\frac{\\E[Y | X=x, Z=1] - \\E[Y | X=x, Z=0]}{\\E[D | X=x, Z=1] - \\E[D | X=x, Z=0]}\n\\end{align*}\\] which is the local average treatment effect for compliers with covariate value \\(X=x\\). \nSimilarly, using effectively the same argument as in the conditional case, notice that \\[\\begin{align*}\n  \\P(C=1 | X=x) &= \\E[D| X=x, Z=1] - \\E[D | X=x, Z=0]\n\\end{align*}\\] Another term that shows up below is \\(\\P(C=1)\\). You might think that \\(\\P(C=1) = \\E[D|Z=1] - \\E[D|Z=0]\\) still holds, but this is not the case when we have covariates. Instead, we can use the law of iterated expectations as follows: \\[\\begin{align*}\n  \\P(C=1) &= \\E[\\P(C=1 | X)] \\\\\n  &= \\E[\\E[D| X, Z=1] - \\E[D | X, Z=0]] \\\\\n\\end{align*}\\] i.e., we can recover \\(\\P(C=1)\\) by averaging over the expression for \\(\\P(C=1 | X=x)\\) above.\nNow, let’s move to identifying \\(\\text{LATE}\\). Notice that \\[\\begin{align*}\n  \\text{LATE} &= \\E[Y(1) - Y(0) | C=1] \\\\\n  &= \\sum_x \\E[Y(1) - Y(0) | X=x, C=1] \\P(X=x | C=1) \\\\\n  &= \\sum_x \\frac{\\E[Y | X=x, Z=1] - \\E[Y | X=x, Z=0]}{\\E[D | X=x, Z=1] - \\E[D | X=x, Z=0]} \\P(C=1 | X=x) \\frac{\\P(X=x)}{\\P(C=1)} \\\\\n  &= \\dfrac{\\sum_x \\big(\\E[Y | X=x, Z=1] - \\E[Y | X=x, Z=0]\\big) \\P(X=x)}{\\P(C=1)} \\\\\n  &= \\dfrac{\\sum_x \\big(\\E[Y | X=x, Z=1] - \\E[Y | X=x, Z=0]\\big) \\P(X=x)}{\\sum_x \\big(\\E[D | X=x, Z=1] - \\E[D | X=x, Z=0]\\big) \\P(X=x)} \\\\\n  &= \\dfrac{\\E\\big[\\E[Y | X, Z=1] - \\E[Y | X, Z=0]\\big]}{\\E\\big[\\E[D | X, Z=1] - \\E[D | X, Z=0]\\big]}\n\\end{align*}\\] where the first equality holds by the law of iterated expectations, the second equality holds by substituting in the expression for \\(\\text{LATE}(x)\\) and using Bayes’ rule, the third equality holds by substituting in the expression for \\(\\P(C=1 | X=x)\\), the fourth equality holds by substituting in the expression for \\(\\P(C=1)\\), and the final equality holds by rewriting the sums as expectations.\n\n17.3.1 Estimation\nThe expression for \\(\\text{LATE}\\) above suggests a fairly straightforward estimation strategy based on the analogy principle. In particular, we can estimate the conditional expectations using regression, then plug those estimates into the expression above. Here are the steps:\nStep 1: Estimate the reduced form separately by instrument value. Run two regressions: - Regression 1: Regress \\(Y\\) on \\(X\\) using only observations with \\(Z=1\\). Then, recover the predicted values from that regression for each unit, which we will denote by \\(\\hat{m}_1(X_i)\\). - Regression 2: Regress \\(Y\\) on \\(X\\) using only observations with \\(Z=0\\). Then, recover the predicted values from that regression for each unit, which we will denote by \\(\\hat{m}_0(X_i)\\).\nStep 2: Estimate the first stage separately by instrument value. Run two more regressions: - Regression 3: Regress \\(D\\) on \\(X\\) using only observations with \\(Z=1\\). Then, recover the predicted values from that regression for each unit, which we will denote by \\(\\hat{p}_1(X_i)\\). - Regression 4: Regress \\(D\\) on \\(X\\) using only observations with \\(Z=0\\). Then, recover the predicted values from that regression for each unit, which we will denote by \\(\\hat{p}_0(X_i)\\).\nStep 3: Take differences and the ratio of averages. Then, we can combine all of the predicted values and average to form an estimator of LATE. In particular, : \\[\\begin{align*}\n  \\widehat{LATE} = \\frac{\\displaystyle \\frac{1}{n}\\sum_{i=1}^n \\big(\\hat{m}_1(X_i) - \\hat{m}_0(X_i)\\big)}{\\displaystyle \\frac{1}{n}\\sum_{i=1}^n \\big(\\hat{p}_1(X_i) - \\hat{p}_0(X_i)\\big)}\n\\end{align*}\\]\nBecause we estimate several regressions and average their predicted values to estimate \\(\\text{LATE}\\), the approach discussed above is a regression adjustment estimator.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Natural Experiments</span>"
    ]
  },
  {
    "objectID": "16-natural_experiments.html#additional-material-two-stage-least-squares",
    "href": "16-natural_experiments.html#additional-material-two-stage-least-squares",
    "title": "17  Natural Experiments",
    "section": "17.4 Additional Material: Two Stage Least Squares",
    "text": "17.4 Additional Material: Two Stage Least Squares\nAlthough I like the regression adjustment estimator discussed above in settings where covariates are important, it is much more common to use two stage least squares. This approach typically requires the addtional assumption of treatment effect homogeneity, \\(Y_i(1) - Y_i(0) = \\alpha\\) for all \\(i\\), as we have discussed before. To simplify the discussion, let us also make the linearity assumption that \\(\\E[Y(0)|X] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). Then, \\[\\begin{align*}\n  Y_i &= Y_i(0) + D_i \\big(Y_i(1) - Y_i(0)\\big) \\\\\n      &= \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\alpha D_i + U_i\n\\end{align*}\\] where \\(U_i = Y_i(0) - \\E[Y_i(0)|X_i]\\) is a mean-zero error term. The problem with estimating this regression directly is that \\(D_i\\) is endogenous—that is, \\(D_i\\) is correlated with the error term \\(U_i\\) (e.g., people who choose to get treatment may be different in unobserved ways that also affect outcomes). However, it turns out that, under the IV assumptions that we have discussed, we can run the auxiliary regression \\[\\begin{align*}\n  D_i = \\delta_0 + \\delta_1 X_{1i} + \\delta_2 X_{2i} + \\pi Z_i + V_i\n\\end{align*}\\] Then, we can recover predicted values \\(\\hat{D}_i\\) from this regression, and use those predicted values in place of \\(D_i\\) in the original regression: \\[\\begin{align*}\n  Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\alpha \\hat{D}_i + \\epsilon_i\n\\end{align*}\\] This approach is called two stage least squares (often abbreviated 2SLS) because it involves two regressions (or stages). Under the IV assumptions, the coefficient on \\(\\hat{D}_i\\) in the second regression is a consistent estimator of the treatment effect \\(\\alpha\\). Roughly, the reason why this works is that the predicted values \\(\\hat{D}_i\\) are “purged” of the part of \\(D_i\\) that is correlated with the error term \\(U_i\\) because \\(\\hat{D}_i\\) only uses variation in \\(D_i\\) that is explained by the instrument \\(Z_i\\) (which is independent of \\(U_i\\) under the IV assumptions).\n\n17.4.1 Estimation\nThere are many implementations of two stage least squares. In fact, the iv_robust function from the estimatr package that we discussed above uses two stage least squares under the hood when there are covariates. In particular, you can run the following code, where your_data is your data frame, Y is the outcome variable, D is the treatment variable, Z is the instrument variable, and X1 and X2 are covariates.\n\nlibrary(estimatr)\niv_model &lt;- iv_robust(Y ~ D + X1 + X2 | Z + X1 + X2, data = your_data)\nsummary(iv_model)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Natural Experiments</span>"
    ]
  },
  {
    "objectID": "16-natural_experiments.html#references",
    "href": "16-natural_experiments.html#references",
    "title": "17  Natural Experiments",
    "section": "17.5 References",
    "text": "17.5 References\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Natural Experiments</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html",
    "href": "17-observational_data.html",
    "title": "18  Causal Inference with Observational Data",
    "section": "",
    "text": "18.1 Unconfoundedness\n\\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\textrm{P}}\n\\let\\L\\relax\n\\newcommand{\\L}{\\textrm{L}} %doesn't work in .qmd, place this command at start of qmd file to use it\n\\newcommand{\\F}{\\textrm{F}}\n\\newcommand{\\var}{\\textrm{var}}\n\\newcommand{\\cov}{\\textrm{cov}}\n\\newcommand{\\corr}{\\textrm{corr}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\Corr}{\\mathrm{Corr}}\n\\newcommand{\\sd}{\\mathrm{sd}}\n\\newcommand{\\se}{\\mathrm{s.e.}}\n\\newcommand{\\T}{T}\n\\newcommand{\\indicator}[1]{\\mathbb{1}\\{#1\\}}\n\\newcommand\\independent{\\perp \\!\\!\\! \\perp}\n\\newcommand{\\N}{\\mathcal{N}}\n\\]\nSW 6.8, SW Ch. 9\nThe approach that we will start with is what is probably the most common approach: unconfoundedness.\nUnconfoundedness Assumption: \\[\\begin{align*}\n  (Y(1),Y(0)) \\independent D | X\n\\end{align*}\\] You can think of this as saying that, among individuals with the same covariates \\(X\\), they have the same distributions of potential outcomes regardless of whether or not they participate in the treatment. Note that the distribution of \\(X\\) is still allowed to be different between the treated and untreated groups. In other words, after you condition on covariates, there is nothing special (in terms of the distributions of potential outcomes) about the group that participates in the treatment relative to the group that doesn’t participate in the treatment.\nIf you are willing to believe this assumption, then you can recover the \\(ATT\\). In particular, notice that \\[\\begin{align*}\n  ATT &= \\E[Y(1) - Y(0) \\mid D=1] \\\\\n  &= \\E\\Big[\\E[Y(1) - Y(0) \\mid X, D=1] \\Bigm| D=1\\Big] \\\\\n  &= \\E\\Big[\\E[Y(1) \\mid X, D=1] - \\E[Y(0) \\mid X, D=1] \\Bigm| D=1\\Big] \\\\\n  &= \\E\\Big[\\E[Y(1) \\mid X, D=1] - \\E[Y(0) \\mid X, D=0] \\Bigm| D=1\\Big] \\\\\n  &= \\E\\Big[\\E[Y \\mid X, D=1] - \\E[Y \\mid X, D=0] \\Bigm| D=1\\Big]\n\\end{align*}\\] where the first equality is just the law of iterated expectations, the second equality uses the linearity of expectation, the third equality uses unconfoundedness—this is the key step, and the last equality uses the definition of observed outcomes. This final expression only involves observed data, so we can estimate it from data.\nThe previous expression is quite intuitive. It suggests: (1) finding treated and untreated units that have the same covariates \\(X\\) and comparing their outcomes, (2) then averaging these differences over the distribution of \\(X\\) for treated units.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#unconfoundedness",
    "href": "17-observational_data.html#unconfoundedness",
    "title": "18  Causal Inference with Observational Data",
    "section": "",
    "text": "This is potentially a strong assumption. In order to believe this assumption, you need to believe that untreated individuals with the same characteristics can deliver, on average, the outcome that individuals in the treated group would have experienced if they had not participated in the treatment. In math, you can write this as \\[\\begin{align*}\n    \\E[Y(0) | X, D=1] = \\E[Y(0) | X, D=0]\n  \\end{align*}\\]\n\n\n\n\n18.1.1 Estimation - Regression Adjustment\nNext, let’s discuss how to estimate the \\(ATT\\) given the previous expression. I wrote down what I think is the most intuitive expression for the \\(ATT\\) under unconfoundedness above, but we can simplify it in a way that will make estimation easier. In particular, notice that \\[\\begin{align*}\n  ATT &= \\E\\Big[\\E[Y \\mid X, D=1] - \\E[Y \\mid X, D=0] \\Bigm| D=1\\Big] \\\\\n  &= \\E\\Big[\\E[Y \\mid X, D=1] \\mid D=1\\Big] - \\E\\Big[\\E[Y \\mid X, D=0] \\mid D=1\\Big] \\\\\n  &= \\E[Y \\mid D=1] - \\E\\Big[\\E[Y \\mid X, D=0] \\mid D=1\\Big]\n\\end{align*}\\] so that the only complicated part to estimate is \\(\\E[Y \\mid X, D=0]\\). We will proceed by using the analogy principle—estimate \\(\\E[Y \\mid X, D=0]\\), then average the predicted values.\nStep 1: Regression using Untreated Units\n\nEstimate a regression of \\(Y\\) on \\(X\\) using only untreated units (i.e., those with \\(D=0\\)), and recover predicted values from this regression, which we will denote \\(\\hat{Y}_i^{(0)}\\) for each unit \\(i\\).\n\nStep 2: Average Predicted Values for Treated Units\nOur estimator for \\(ATT\\) will be\n\\[\\begin{align*}\n  \\widehat{ATT} &= \\bar{Y}_{D=1} - \\frac{1}{n_1} \\sum_{i=1}^n D_i \\hat{Y}_i^{(0)}\n\\end{align*}\\] where \\(\\bar{Y}_{D=1}\\) is the average outcome for treated units, \\(n_1\\) is the number of treated units, and the sum is over all units but only includes predicted values for treated units (because of the \\(D_i\\) term).\nBecause we estimate a first-step regression and then average, the approach discussed above is called regression adjustment.\nCode\nMy favorite package for implementing regression adjustment is the DRDID package. This is a package that is built for using panel data, but we can “trick” it into doing cross-sectional regression adjustment. The code below shows how to do this. Suppose that we have a data frame df with outcome variable Y, treatment variable D, and covariates X1, X2, and X3. The\n\nlibrary(DRDID)\n\ny &lt;- df$Y\nD &lt;- df$D\nX &lt;- model.matrix(~ X1 + X2 + X3, data = df)  # create matrix of covariates\nra_att &lt;- DRDID::reg_did_panel(y1 = y, y0 = 0, D = D, covariates = X)\nra_att\n\nDRDID::reg_did_panel expects outcomes for two periods: period 1 and period 0, but our trick is to just set the period 0 outcomes to zero for everyone (i.e., y0 = 0). The function will then estimate the regression adjustment estimator for us. Note that you can include any covariates you want in the regression by changing the formula in model.matrix.\n\n\n18.1.2 Estimation - Regression under Treatment Effect Homogeneity\nAlthough I prefer the regression adjustment approach above, it is more common to try to estimate the causal effect of \\(D\\) using a single regression.\nWe will make two additional assumptions here. First, we will make the treatment effect homogeneity assumption that we have discussed before: \\(Y_i(1) - Y_i(0) = \\alpha\\) for all \\(i\\). Second, we will assume a linear model for untreated potential outcomes: \\[\\begin{align*}\n  Y(0) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + U\n\\end{align*}\\] where, for simplicity, I have assumed that there are only two covariates, \\(X_1\\) and \\(X_2\\). Next, notice that unconfoundedness implies that \\(\\E[U|X_1,X_2,D] = 0\\) (the conditioning on \\(D\\) is the unconfoundedness part). Now, recalling the definition of the observed outcome, we can write \\[\\begin{align*}\n  Y_i &= D_i Y_i(1) + (1-D_i) Y_i(0) \\\\\n  &= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\\\\n  &= \\alpha D_i + \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + U_i\n\\end{align*}\\] which suggests running the regression of observed \\(Y\\) on \\(X_1,X_2,\\) and \\(D\\) and interpreting the estimate of \\(\\alpha\\) as the causal effect of participating in the treatment. In practice, this will be very similar to what we have done before — so the process would not be hard, but convincing someone (or even yourself) that unconfoundedness holds will be the bigger issue here.\nAs a final comment, the assumption of treatment effect homogeneity is not quite so innocuous here. It turns out that you can show that, in the presence of treatment effect heterogeneity, \\(\\alpha\\) will be equal to a weighted average of individual treatment effects, but the weights can sometimes be “strange”. There are methods that are robust to treatment effect heterogeneity (they are beyond the scope of the current class, but they are not “way” more difficult than what we are doing here). That said, in my experience, the regression estimators (under treatment effect homogeneity) tend to deliver similar estimates to alternative estimators that are robust to treatment effect heterogeneity at least in the setup considered in this section.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#panel-data-approaches",
    "href": "17-observational_data.html#panel-data-approaches",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.2 Panel Data Approaches",
    "text": "18.2 Panel Data Approaches\nSW All of Ch. 10 and 13.4\nIn this section, we’ll consider the case where a researcher has access to a different type of data called panel data. Panel data is data that follows the same individual (or firm, etc.) over time. In this case, it is often helpful to index variables by time. For example, \\(Y_{it}\\) is the outcome for individual \\(i\\) in time period \\(t\\). \\(X_{it}\\) is the value of a regressor for individual \\(i\\) in time period \\(t\\) and \\(D_{it}\\) is the value of the treatment for individual \\(i\\) in time period \\(t\\). If some variable doesn’t vary over time (e.g., a regressor like race), we won’t use a \\(t\\) subscript.\n\n18.2.1 Setup\nFor this section, we will consider a setting where a researcher observes exactly two periods of panel data, \\(t=1\\) and \\(t=2\\). We will also suppose that, in the first period, no units are treated (i.e., \\(D_{i1} = 0\\) for all \\(i\\)) while in the second period, some units are treated (i.e., \\(D_{i2}\\) can be either 0 or 1). In this setting, it will be more convenient to work with the “group” variable, \\(G_i\\), which indicates whether or not individual \\(i\\) participates in the treatment in the second period. Slightly updating notation, we will be interested in \\(ATT = \\E[Y_{t=2}(1) - Y_{t=2}(0) | G=1]\\) which is the average treatment effect in period 2 for the treated group. The setup discussed here is sometimes referred to as a “pre-post” design because we have a “pre-treatment” period (period 1) and a “post-treatment” period (period 2).\n\n\n18.2.2 Using Panel Data to Validate Assumptions\nThe first major use of panel data is to try check/validate identification assumptions. Recall that, one of the main challenges that we faced with unconfoundedness was that it is hard to convince yourself or others that unconfoundedness holds. Panel data can help with this as we can effectively check if unconfoundedness holds in the pre-treatment period.\nThe main implication of unconfoundedness that we used above was that we could learn about untreated potential outcomes for the treated group by looking at untreated units with the same covariates. In the pre-treatment period, no one is treated, so we can actually check if this implication holds in the pre-treatment period. That is, \\[\\begin{align*}\n  \\E[Y_{t=1} | G=1] \\stackrel{?}{=} \\E\\Big[ \\E[Y_{t=1} | X, G=0] \\Bigm| G=1 \\Big]\n\\end{align*}\\] If unconfoundedness holds in the first period, then these should be equal, and if they are unequal, that is evidence against unconfoundedness holding. If you squint at this, you can see that \\(\\E[Y_{t=1} | G=1] - \\E\\Big[ \\E[Y_{t=1} | X, G=0] \\Bigm| G=1 \\Big]\\) is exactly the estimand that we would use for the \\(ATT\\) if the treatment had been implemented in period 1. Thus, we can basically implement our estimator in the pre-treatment period and check to see if we get something close to zero (implying that unconfoundedness held in period 1) or not (implying that unconfoundedness is violated in period 1). In the first case, it seems reasonable to hope that unconfoundness might hold in period 2, while in the second case, it provides a strong piece of evidence against unconfoundedness holding in period 2. Because we implement the same estimator in the pre-treatment period as we would in the post-treatment period, this approach is sometimes called a placebo test.\nIn my view, being able to implement this sort of placebo test is the most important feature of using panel data for causal inference. That said, I should mention that this does not guarantee that we will get reliable estimates in post-treatment periods. We really need unconfoundedness to hold in the post-treatment period, not necessarily in the pre-treatment period. Still, it seems like in most applications, this is about as good evidence as you could have about the plausibility of unconfoundedness.\n\n\n18.2.3 Using Panel Data to Make Adjustments\nBesides validating assumptions, the other main use case for panel data is to adjust for certain things that would typically be unobserved if we did not have panel data. We will consider two versions of this.\n\n18.2.3.1 Lagged Outcome Unconfoundedness\nProbably the most straightforward thing that we can do with panel data that is unavailable with cross-sectional data is to assume unconfoundedness holds after conditioning on lagged outcomes. That is, \\[\\begin{align*}\n  \\big(Y_{t=2}(1), Y_{t=2}(0)\\big) \\independent G | \\big(X, Y_{t=1}\\big)\n\\end{align*}\\] Using exactly the same arguments as for unconfoundedness above, it immediately follows that \\[\\begin{align*}\n  ATT &= \\E\\Big[ \\E[Y_{t=2} | X, Y_{t=1}, G=1] - \\E[Y_{t=2} | X, Y_{t=1}, G=0] \\Bigm| G=1 \\Big]\n\\end{align*}\\] In other words, to recover the \\(ATT\\), we simply need to find treated and untreated units that have the same characteristics \\(X\\) and the same pre-treatment outcome \\(Y_{t=1}\\), compare their outcomes in period 2, and then average all of these differences.\nEstimation is exactly the same as for unconfoundedness above; just make sure to include \\(Y_{t=1}\\) as one of the covariates in the regression adjustment or regression approach.\n\n\n18.2.3.2 Difference-in-Differences\nWhile adding lagged outcomes to the unconfoundedness assumption seems very natural, difference-in-differences is an alternative approach to making adjustments that is more popular among economists.\nIn the previous section, we invoked the assumption of unconfoundedness and were in the setup where \\(X\\) was fully observed. But suppose instead that you thought this alternative version of unconfoundedness held \\[\\begin{align*}\n  \\big(Y(1),Y(0)\\big) \\independent D | \\big(X,W\\big)\n\\end{align*}\\] where \\(X\\) were observed random variables, but \\(W\\) were not observed.\nLet us also maintain the assumption of a linear model for untreated potential outcomes: \\[\\begin{align*}\n  Y_t(0) = \\beta_0 + \\beta_1 X + \\beta_2 W + U\n\\end{align*}\\] Unconfoundedness continues to imply that \\(\\E[U|D] = 0\\) (i.e., that the error terms are not systematically different between the groups). Previously, this expression led to a regression adjustment estimator of \\(ATT\\), but now that approach is infeasible because \\(W\\) is not observed. However, notice that, with panel data, we can write this model for both time periods: \\[\\begin{align*}\n  Y_{t=2}(0) = \\beta_0 + \\beta_1 X + \\beta_2 W + U_{t=2} \\\\\n  Y_{t=1}(0) = \\beta_0 + \\beta_1 X + \\beta_2 W + U_{t=1}\n\\end{align*}\\] Subtracting the second equation from the first gives \\[\\begin{align*}\n  \\Delta Y(0) = \\Delta U\n\\end{align*}\\] which implies that \\(\\E[\\Delta Y(0) | D] = 0\\) (i.e., untreated potential outcomes do not systematically change over time).\nNow, let us use this to recover the \\(ATT\\): \\[\\begin{align*}\n  ATT &= \\E[Y_{t=2}(1) - Y_{t=2}(0) | D=1] \\\\\n      &= \\E[Y_{t=2} | D=1] - \\Big(\\E[Y_{t=2}(0) | D=1] - \\E[Y_{t=1}(0) | D=1]\\Big) - \\E[Y_{t=1}(0) | D=1] \\\\\n      &= \\E[Y_{t=2} | D=1] - \\Big(\\underbrace{\\E[\\Delta Y(0) | D=1]}_{=0}\\Big) - \\E[Y_{t=1} | D=1] \\\\\n      &= \\E[Y_{t=2} | D=1] - \\E[Y_{t=1} | D=1]\n\\end{align*}\\] In other words, in the setup above, the \\(ATT\\) is just equal to the average post-treatment outcome for the treated group relative to the average pre-treatment outcome for the treated group. This is called a before-after identification strategy, and it is actually quite intuitive: we are effectively comparing each treated unit’s outcome after treatment to its outcome before it participated in the treatment.\nIn practice, it is common to slightly generalize this approach. In fact, I pulled a little bit of trick on you earlier. Probably a more appropriate linear model for untreated potential outcomes is: \\[\\begin{align*}\n  Y_t(0) = \\beta_{0,t} + \\beta_{1,t} X + \\beta_{2,t} W + U_t\n\\end{align*}\\] where we allow the intercept and the effects of \\(X\\) and \\(W\\) on untreated potential outcomes to vary over time. In this case, we can follow exactly the same steps as above to get \\[\\begin{align*}\n  Y_{t=2}(0) &= \\beta_{0,t=2} + \\beta_{1,t=2} X + \\beta_{2,t=2} W + U_{t=2} \\\\\n  Y_{t=1}(0) &= \\beta_{0,t=1} + \\beta_{1,t=1} X + \\beta_{2,t=1} W + U_{t=1}\n\\end{align*}\\] Subtracting the second equation from the first gives \\[\\begin{align*}\n  \\Delta Y(0) = \\Delta \\beta_0 + \\Delta \\beta_1 X + \\Delta \\beta_2 W + \\Delta U\n\\end{align*}\\] This still presents a bit of a problem as \\(W\\) is still unobserved. In particular, we need one more assumption—the effect of \\(W\\) is constant over time, i.e., \\(\\beta_{2,t=2} = \\beta_{2,t=1}\\) so that \\(\\Delta \\beta_2 = 0\\). In this case, we have that \\[\\begin{align*}\n  \\Delta Y(0) = \\Delta \\beta_0 + \\Delta \\beta_1 X + \\Delta U\n\\end{align*}\\] since \\(\\E[\\Delta U | X, D] = 0\\) (by the version of unconfoundedness we have considered in this section), it follows that \\[\\begin{align*}\n  \\E[\\Delta Y(0) | X, D=1] = \\E[\\Delta Y(0) | X, D=0]\n\\end{align*}\\] This condition is called the parallel trends assumption because it says that, after conditioning on \\(X\\), the untreated potential outcomes for the treated and untreated groups would have followed parallel paths over time. Under this assumption, we can recover the \\(ATT\\) as follows: \\[\\begin{align*}\n  ATT &= \\E[Y_{t=2}(1) - Y_{t=2}(0) | D=1] \\\\\n      &= \\E[Y_{t=2}(1) - Y_{t=1}(0) | D=1] - \\E[Y_{t=2}(0) - Y_{t=1}(0) | D=1] \\\\\n      &= \\E\\Big[ \\E[Y_{t=2} - Y_{t=1} | X, D=1] - \\E[Y_{t=2}(0) - Y_{t=1}(0) | X, D=1] \\Bigm| D=1 \\Big]\n      &= \\E\\Big[ \\E[Y_{t=2} - Y_{t=1} | X, D=1] - \\E[Y_{t=2} - Y_{t=1} | X, D=0] \\Bigm| D=1 \\Big] \\\\\n\\end{align*}\\] which follows essentially the same argument as for unconfoundedness after we take differences in the second equation. This suggests the following estimation approach: regression adjustment but where the outcome variable is \\(\\Delta Y\\) rather than \\(Y\\). This approach is called difference-in-differences because it involves taking differences over time to eliminate unobserved, time invariant variables, and then taking differences between treated and untreated groups to recover the causal effect. It is very popular in empirical work in economics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#lab-7-drunk-driving-laws",
    "href": "17-observational_data.html#lab-7-drunk-driving-laws",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.3 Lab 7: Drunk Driving Laws",
    "text": "18.3 Lab 7: Drunk Driving Laws\nFor this lab, we will use the Fatalities data. We will study the causal effect of mandatory jail sentence policies for drunk driving on traffic fatalities. The Fatalities data consists of panel data of traffic fatality death rates, whether or not a state has a mandatory jail sentence policy or not as well as several other variables from 1982-1988. Economic theory suggests that raising the cost of some behavior (in this case, you can think of a mandatory jail sentence as raising the cost of drunk driving) will lead to less of that behavior. That being said, it’s both interesting to test this theory and also consider the magnitude of this effect. That’s what we’ll do in this problem.\n\nThis data comes in a somewhat messier format than some of the data that we have used previously. To start with, create a new column in the data called afatal_per_million that is the number of alcohol involved vehicle fatalities per millions people in a state in a particular year. The variable afatal contains the total number of alcohol involved vehicle fatalities, and the variable pop contains the total population in a state.\nUsing a subset of the data from 1988, run a regression of afatal_per_million on whether or not a state has a mandatory jail sentence policy jail. How do you interpret the results?\nUsing the same subset from part 2, run a regression of afatal_per_million on jail, unemployment rate (unemp), the tax on a case of beer (beertax), the percentage of southern baptists in the state (baptist), the percentage of residents residing in dry counties (dry), the percentage of young drivers in the state, (youngdrivers), and the average miles driven per person in a state (miles). How do you interpret the estimated coefficient on jail? Would you consider this to be a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related fatalities?\nNow, using the full data, let’s estimate a fixed effects model with alcohol related fatalities per million as the outcome and mandatory jail policies as a regressor. Estimate the model using first differences and make sure to include time fixed effects. How do you interpret the results?\nEstimate the same model as in part 4, but using the within estimator instead of first differences. Compare these results to the ones from part 4.\nUsing the same within estimator as in part 5, include the same set of covariates from part 3 and interpret the estimated effect of mandatory jail policies. How do these estimates compare to the earlier ones?\nNow, we’ll switch to using a difference in differences approach to estimating the effect of mandatory jail policies. First, we’ll manipulate the data some.\n\nTo keep things simple, let’s start by limiting the data to the years 1982 and 1988 and drop the in-between periods.\nSecond, let’s calculate the change in alcohol related fatalities per million between 1982 and 1998 and keep the covariates that we have been using from 1982. One way to do this, is to use the pivot_wider function from the tidyr. In the case of panel data, “long format” data means that each row in the data corresponds to a paricular observation and a particular time period. Thus, with long format data, there are \\(n \\times T\\) total rows in the data. On the other hand, “wide format” data means that each row holds all the data (across all time periods) for a particular observation. Converting back and forth between long and wide formats is a common data manipulation task. Hint: This step is probably unfamiliar, so I’d recommend seeing if you can use ?tidyr::pivot_wider to see if you can figure out how to complete this step, but, if not, you can copy this code from the solutions in the next section.\nFinally, drop all states that are already treated in 1982.\n\nUsing the data that you constructed in part 7, implement the difference in differences regression of the change in alcohol related fatalities per million from 1982 to 1988 on the mandatory jail policy. How do you interpret these results and how do they compare to the previous ones? Now, additionally include the set of covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?\nAn alternative to DID, is to include the lagged outcome as a covariate. Using the data constructed in part 7, run a regression of alcohol related fatalities per million in 1988 on the mandatory jail policy and alcohol related fatalities per million in 1982. How do you interpret these results and how do they compare to the previous ones? Now include the additional covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?\nComment on your results from parts 1-9. Which, if any, of these are you most inclined to interpret as a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related policies?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#lab-7-solutions",
    "href": "17-observational_data.html#lab-7-solutions",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.4 Lab 7: Solutions",
    "text": "18.4 Lab 7: Solutions\n\n\n\n\nlibrary(tidyr)\nlibrary(plm)\n\ndata(Fatalities, package=\"AER\")\n\nFatalities$afatal_per_million &lt;- 1000000 * (Fatalities$afatal / Fatalities$pop )\n\n\n\n\n\nFatalities88 &lt;- subset(Fatalities, year==1988)\n\nreg88 &lt;- lm(afatal_per_million ~ jail, data=Fatalities88)\nsummary(reg88)\n\n\nCall:\nlm(formula = afatal_per_million ~ jail, data = Fatalities88)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.123 -16.622  -1.469   8.642 112.260 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   59.496      4.273  13.923   &lt;2e-16 ***\njailyes        9.155      7.829   1.169    0.248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.55 on 45 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.02949,   Adjusted R-squared:  0.007921 \nF-statistic: 1.367 on 1 and 45 DF,  p-value: 0.2484\n\n\nThe estimated coefficient on mandatory jail laws is 9.155. We should interpret this as just the difference between alcohol related fatalities per million in states that had mandatory jail laws in 1988 relative to states that did not have them. We cannot reject that there is no difference between states where the policy is in place relative to those that do not have the policy.\n\n\n\n\nreg88_covs &lt;- lm(afatal_per_million ~ jail + unemp + beertax + baptist + dry + youngdrivers + miles, data=Fatalities88)\nsummary(reg88_covs)\n\n\nCall:\nlm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + \n    dry + youngdrivers + miles, data = Fatalities88)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.065  -9.907  -1.690   9.673  82.100 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -29.373536  32.500240  -0.904   0.3717  \njailyes        3.120574   6.849271   0.456   0.6512  \nunemp          4.815081   1.892369   2.544   0.0150 *\nbeertax        2.311850   9.521684   0.243   0.8094  \nbaptist        0.661694   0.527228   1.255   0.2169  \ndry           -0.026675   0.383956  -0.069   0.9450  \nyoungdrivers  -0.092100 142.804244  -0.001   0.9995  \nmiles          0.006802   0.002822   2.411   0.0207 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.41 on 39 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4742,    Adjusted R-squared:  0.3798 \nF-statistic: 5.024 on 7 and 39 DF,  p-value: 0.0003999\n\n\nThe estimated coefficient on jail is 3.12. It is somewhat smaller than the previous estimate, though neither is statistically significant. We should interpret this as the partial effect of the mandatory jail policy, that is, that we estimate that mandatory jail laws increase the number of alcohol related fatalities per million by 3.12 on average controlling for the unemployment rate, beer tax, the fraction of southern baptists in the state, the fraction of residents in dry counties, the fraction of young drivers, and the average miles driven in the state. We cannot reject that the partial effect of mandatory jail policies is equal to 0.\n\n\n\n\nfd_reg &lt;- plm(afatal_per_million ~ jail + as.factor(year),\n              effect=\"individual\",\n              index=\"state\", model=\"fd\",\n              data=Fatalities)\nsummary(fd_reg)\n\nOneway (individual) effect First-Difference Model\n\nCall:\nplm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, \n    effect = \"individual\", model = \"fd\", index = \"state\")\n\nUnbalanced Panel: n = 48, T = 6-7, N = 335\nObservations used in estimation: 287\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-51.66677  -5.09887   0.23801   6.28688 119.08976 \n\nCoefficients: (1 dropped because of singularities)\n                    Estimate Std. Error t-value Pr(&gt;|t|)   \n(Intercept)         -2.15376    0.80673 -2.6697 0.008035 **\njailyes              2.60763    5.28351  0.4935 0.622016   \nas.factor(year)1983 -5.28423    1.82330 -2.8982 0.004050 **\nas.factor(year)1984 -3.58247    2.29451 -1.5613 0.119577   \nas.factor(year)1985 -5.60800    2.43517 -2.3029 0.022017 * \nas.factor(year)1986 -0.74192    2.28988 -0.3240 0.746180   \nas.factor(year)1987 -2.16244    1.80716 -1.1966 0.232476   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    54692\nResidual Sum of Squares: 51620\nR-Squared:      0.056171\nAdj. R-Squared: 0.035946\nF-statistic: 2.77733 on 6 and 280 DF, p-value: 0.012223\n\n\nWe should interpret the estimated coefficient on jail as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after accounting for time invariant variables whose effects do not change over time. Again, we cannot reject that the effect is equal to 0.\n\n\n\n\nwithin_reg &lt;- plm(afatal_per_million ~ jail + as.factor(year),\n              effect=\"individual\",\n              index=\"state\", model=\"within\",\n              data=Fatalities)\nsummary(within_reg)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, \n    effect = \"individual\", model = \"within\", index = \"state\")\n\nUnbalanced Panel: n = 48, T = 6-7, N = 335\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-95.1937300  -4.9678238   0.0088078   5.1611249  40.6263546 \n\nCoefficients:\n                    Estimate Std. Error t-value  Pr(&gt;|t|)    \njailyes               8.3327     4.9666  1.6777 0.0945164 .  \nas.factor(year)1983  -7.9151     2.6936 -2.9384 0.0035734 ** \nas.factor(year)1984  -8.4863     2.7115 -3.1298 0.0019341 ** \nas.factor(year)1985 -12.7849     2.7331 -4.6778 4.518e-06 ***\nas.factor(year)1986 -10.0726     2.7331 -3.6854 0.0002741 ***\nas.factor(year)1987 -13.5276     2.7115 -4.9890 1.067e-06 ***\nas.factor(year)1988 -13.6296     2.7279 -4.9964 1.030e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    53854\nResidual Sum of Squares: 47607\nR-Squared:      0.116\nAdj. R-Squared: -0.054487\nF-statistic: 5.24882 on 7 and 280 DF, p-value: 1.2051e-05\n\n\nThe estimated coefficient on jail has the same interpretation as in the previous problem. The estimated effect here is marginally statistically significant. 6.\n\nwithin_reg_covs &lt;- plm(afatal_per_million ~ jail + unemp + beertax + baptist + dry + youngdrivers + miles,\n                       effect=\"individual\",\n                       index=\"state\", model=\"within\",\n                       data=Fatalities)\nsummary(within_reg_covs)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + \n    dry + youngdrivers + miles, data = Fatalities, effect = \"individual\", \n    model = \"within\", index = \"state\")\n\nUnbalanced Panel: n = 48, T = 6-7, N = 335\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-95.62306  -5.69773  -0.56903   4.79219  47.80871 \n\nCoefficients:\n                Estimate  Std. Error t-value  Pr(&gt;|t|)    \njailyes       4.9731e+00  4.9613e+00  1.0024   0.31702    \nunemp        -1.1340e+00  5.6592e-01 -2.0038   0.04605 *  \nbeertax      -2.7456e+01  1.5080e+01 -1.8207   0.06972 .  \nbaptist       2.5083e+00  4.3324e+00  0.5790   0.56308    \ndry           4.3092e-01  1.0870e+00  0.3964   0.69208    \nyoungdrivers  2.6357e+02  5.0169e+01  5.2537 2.957e-07 ***\nmiles        -6.8899e-04  7.3182e-04 -0.9415   0.34727    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    53854\nResidual Sum of Squares: 48083\nR-Squared:      0.10717\nAdj. R-Squared: -0.065023\nF-statistic: 4.80119 on 7 and 280 DF, p-value: 4.0281e-05\n\n\nWe should interpret the estimated coefficient on jail as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after controlling for the unemployment rate, beer taxes, the fraction of the state that is southern baptist, the fraction of the state that lives in a dry county, the fraction of young drivers in a state, and the average number of miles driven per person in the stata, and accounting for time invariant variables whose effects do not change over time.\n\n\n\n\n# part a: convert data to two period panel data\ntwo_period &lt;- subset(Fatalities, year==1982 | year==1988)\n# and drop some missing\ntwo_period &lt;- subset(two_period, !is.na(jail))\ntwo_period &lt;- BMisc::makeBalancedPanel(two_period, \"state\", \"year\")\ntwo_period$jail &lt;- 1*(two_period$jail==\"yes\")\n\n# part b: convert into wide format\nwide_df &lt;- pivot_wider(two_period,\n                       id_cols=\"state\",\n                       names_from=\"year\",\n                       values_from=c(\"jail\", \"afatal_per_million\"))\n\n# add back other covariates from 1982\nwide_df &lt;- merge(wide_df, subset(Fatalities, year==1982)[,c(\"unemp\", \"beertax\", \"baptist\", \"dry\", \"youngdrivers\", \"miles\",\"state\")], by=\"state\")\n\n# change in fatal accidents over time\nwide_df$Dafatal_per_million &lt;- wide_df$afatal_per_million_1988 - wide_df$afatal_per_million_1982\n\n# part c: drop already treated states\nwide_df &lt;- subset(wide_df, jail_1982==0)\n\n\n\n\n\ndid &lt;- lm(Dafatal_per_million ~ jail_1988, data=wide_df)\nsummary(did)\n\n\nCall:\nlm(formula = Dafatal_per_million ~ jail_1988, data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.652 -10.993   5.033  10.405  76.822 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -12.585      4.242  -2.966  0.00532 **\njail_1988      5.102     11.695   0.436  0.66526   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.37 on 36 degrees of freedom\nMultiple R-squared:  0.005259,  Adjusted R-squared:  -0.02237 \nF-statistic: 0.1903 on 1 and 36 DF,  p-value: 0.6653\n\ndid_covs &lt;- lm(Dafatal_per_million ~ jail_1988 + unemp + beertax + baptist + dry + youngdrivers + miles, data=wide_df)\nsummary(did_covs)\n\n\nCall:\nlm(formula = Dafatal_per_million ~ jail_1988 + unemp + beertax + \n    baptist + dry + youngdrivers + miles, data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.346 -12.383   1.456   9.092  60.585 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    6.851636  50.643035   0.135   0.8933  \njail_1988     -1.853041  10.834391  -0.171   0.8653  \nunemp          3.725007   1.919862   1.940   0.0618 .\nbeertax        8.300778  10.052007   0.826   0.4154  \nbaptist        0.527893   0.723263   0.730   0.4711  \ndry           -0.955636   0.546475  -1.749   0.0906 .\nyoungdrivers  89.432017 234.379768   0.382   0.7055  \nmiles         -0.010360   0.005823  -1.779   0.0854 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.9 on 30 degrees of freedom\nMultiple R-squared:  0.3303,    Adjusted R-squared:  0.174 \nF-statistic: 2.114 on 7 and 30 DF,  p-value: 0.07276\n\n\nIf we are willing to believe that, in the absence of the policy, that trends in alcohol related fatalities per million people would have followed the same trends over time for treated and untreated states, then we can interpret these as causal effects. These estimates are broadly similar to the previous ones though the second ones (that include additional covariates) are about the only ones where we ever get a negative estimate for the effect of mandatory jail policies. Like the previous estimates, neither of these estimates are statistically different from 0.\n\n\n\n\nlag_reg &lt;- lm(afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982, data=wide_df)\nsummary(lag_reg)\n\n\nCall:\nlm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982, \n    data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.120 -12.663  -0.684   6.873  92.390 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              19.0810    10.1089   1.888 0.067401 .  \njail_1988                 3.4323    10.3171   0.333 0.741363    \nafatal_per_million_1982   0.5607     0.1303   4.303 0.000129 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.47 on 35 degrees of freedom\nMultiple R-squared:  0.3462,    Adjusted R-squared:  0.3088 \nF-statistic: 9.266 on 2 and 35 DF,  p-value: 0.0005896\n\nlag_reg_covs &lt;- lm(afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982 + unemp + beertax + baptist + dry + youngdrivers + miles, data=wide_df)\nsummary(lag_reg_covs)\n\n\nCall:\nlm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982 + \n    unemp + beertax + baptist + dry + youngdrivers + miles, data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.840  -8.793  -1.364   5.146  71.409 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)             -17.292595  46.318477  -0.373  0.71161   \njail_1988                 0.817453   9.786782   0.084  0.93401   \nafatal_per_million_1982   0.505371   0.173718   2.909  0.00689 **\nunemp                     3.189918   1.736443   1.837  0.07647 . \nbeertax                   2.885796   9.236174   0.312  0.75694   \nbaptist                   0.965785   0.668259   1.445  0.15911   \ndry                      -0.567567   0.509915  -1.113  0.27482   \nyoungdrivers            120.615853 211.026841   0.572  0.57202   \nmiles                    -0.002692   0.005888  -0.457  0.65087   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.7 on 29 degrees of freedom\nMultiple R-squared:  0.5443,    Adjusted R-squared:  0.4186 \nF-statistic: 4.329 on 8 and 29 DF,  p-value: 0.001596\n\n\nThese estimates directly control for alcohol related fatalities per million in the pre-treatment period 1982. These sorts of specifications are less common in economics, but, in my view, it seems like a reasonable approach here. That said, the results are more or less the same as earlier estimates.\n\nWe don’t have very strong evidence that mandatory jail policies reduced the number traffic fatalities. In my view, probably the best specifications for trying to understand the causal effects are the ones in part 7 (particularly, the ones that include covariates there), but I think that the the results in parts 4-9 are also informative. Broadly, these estimates are more or less similar — none of them are statistically significant and most are positive (which is an unexpected sign).\n\nBefore we finish, let me mention a few caveats to these results:\n\nFirst, I would be very hesitant to interpret these results as definitively saying that mandatory jail policies have no effect on alcohol related traffic fatalities. The main reason to be clear about this is that our standard error are quite large. For example, in the second specification in part 7 (the one I like the most), a 95% confidence interval for our estimate is \\([-23.1, 19.4]\\). This is a wide confidence interval — the average number of alcohol related traffic fatalities per million across all states and time periods is only 66. So our estimates are basically still compatible with very large reductions in alcohol related traffic fatalities up to large increases in alcohol related traffic fatalities.\nLet me make one more comment about the sign of our results. Many of our point estimates are positive; as we discussed earlier, it is hard to rationalize harsher punishments increasing alcohol related traffic fatalities. I think the main explanation for these results is just that our estimates are pretty noisy and, therefore, more or less “by chance” we are getting estimates that have an unexpected sign. But there are some other possible explanations that are worth mentioning. For one, there are a number of other policies related to drunk driving that occurred in the 1980s (particularly, related to legal drinking age) but perhaps others. It is not clear how these would interact with our estimates, but they could certainly play some role. Besides that, it seems to me that we have a pretty good set of covariates that enter our models, but there could be important covariates that we are missing. For this reason, some expertise in how to model state-level traffic fatalities is actually a very important skill here (actually probably the key skill here!)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#coding-questions",
    "href": "17-observational_data.html#coding-questions",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.5 Coding Questions",
    "text": "18.5 Coding Questions\n\nFor this problem, we will use the data rand_hie. This is data from the RAND health insurance experiment in the 1980s. In the experiment, participants were randomly assigned to get Catastrophic (the least amount of coverage), insurance that came with a Deductible, insurance that came with Cost Sharing (i.e., co-insurance so that an individual pays part of their medical insurance), and Free (so that there is no cost of medical care).\nFor this problem, we will be interested in whether or not changing the type of health insurance changed the amount of health care utilization and the health status of individuals.\nWe will focus on the difference between the least amount of health insurance (“Catastrophic”) and the most amount of health insurance (“Free”). In particular, you can start this problem by running the following code:\nrand_hie_subset &lt;- subset(rand_hie, plan_type %in% c(\"Catastrophic\", \"Free\")) rand_hie_subset$free &lt;- 1*(rand_hie_subset$plan_type==\"Free\")\nThis code creates a new data frame called rand_hie_subset that only contains individuals who were assigned to either the “Catastrophic” or “Free” plan types. It also creates a new variable called free that is equal to one for individuals who were assigned to the “Free” plan type and zero for individuals who were assigned to the “Catastrophic” plan type. Use this data frame for the rest of this problem.\n\nSuppose you are interested in estimating the average treatment effect on the treated of the “Free” plan relative to the “Catastrophic” plan on total medical expenditure (total_med_expenditure). Can you estimate this effect by running a regression of total_med_expenditure on free? Explain why or why not.\nEstimate the average treatment effect on the treated of a “Free” plan (free) relative to a “Catastrophic” plan on total medical expenditure (total_med_expenditure) Report and interpret your results.\nEstimate the average treatment effect on the treated of a “Free” plan (free) relative to a “Catastrophic” plan on face to face doctor visits (face_to_face_visits) Report and interpret your results.\nEstimate the average treatment effect on the treated of a “Free” plan (free) relative to a “Catastrophic” plan on overall health index (health_index) Report and interpret your results.\nProvide an overall interpration of your results from parts b-d.\n\nFor this problem, we will study the causal effect of having more children on women’s labor supply using the data Fertility.\n\nTo start with, run a regression of the number of hours that a woman typically works per week (work) on whether or not she has more than two children (morekids). Report your results. Should you interpret the estimated coefficient on morekids as the causal effect of having more than two children? Explain.\nOne possible instrument in this setup is the sex composition of the first two children (i.e., whether they are both girls, both boys, or a boy and a girl). The thinking here is that, at least in the United States, parents tend to have a preference for having both a girl and a boy and that, therefore, parents whose first two children have the same sex may be more likely to have a third child than they would have been if they have a girl and a boy. Go through the lise of four assumptions that we discussed in the context of instrumental variables and provide some discussion about whether or not each of these assumptions are likely to hold in this context.\nRegardless of your answer to part (b), create a new variable called samesex that is equal to one for families whose first two children have the same sex. Estimate the effect of morekids on work using samesex as an instrument for morekids and report the results. Provide some discussion about your results.\n\n\n\n\nFor this question, we will use the AJR data. A deep question in development economics is: Why are some countries much richer than other countries? One explanation for this is that richer countries have different institutions (e.g., property rights, democracy, etc.) that are conducive to growth. Its hard to study these questions though because institutions do not arise randomly — there could be reverse causality so that property rights, democracy, etc. are (perhaps partially) caused by being rich rather than the other way around. Alternatively, other factors (say a country’s geography) could cause both of these. We’ll consider one instrumental variables approach to thinking about this question in this problem.\n\nRun a regression of the log of per capita GDP (the log of per capita GDP is stored in the variable GDP) on a measure of the protection against expropriation risk (this is a measure of how “good” a country’s institutions are (a larger number indicates “better” institutions) and it is in the variable Exprop). How do you interpret these results? Do you think it would be reasonable to interpret the estimated coefficient on Exprop as the causal effect of institutions on GDP.\nOne possible instrument for Exprop is settler mortality (we’ll use the log of this which is available in the variable logMort). Settler mortality is a measure of how dangerous it was for early settlers of a particular location. The idea is that places that have high settler mortality may have set up worse (sometimes called “extractive”) institutions than places that had lower settler mortality. But that settler mortality (from a long time ago) does not have any other direct effect on modern GDP. Provide some discussion about whether settler mortality is a valid instrument for institutions.\nEstimate an IV regression of GDP on Exprop using logMort as an instrument for Exprop. How do you interpret the results? How do these results compare to the ones from part a?\n\nFor this question, we’ll use the data house to study the causal effect of incumbency on the probability that a member of the House of Representatives gets re-elected.\n\nOne way to try to estimate the causal effect of incumbency is to just run a regression where the outcome is democratic_vote_share (this is the same outcome we’ll use below) and where the model includes a binary variable for whether or not the democratic candidate is an incumbent. What are some limitions of this strategy?\nThe house data contains data about the margin of victory (is positive if they won the election and negative if they lost) for Democratic candidates in the current election and data about the Democratic margin of victory in the past election. Explain how you could use this data in a regression discontinuity design to estimate the causal effect of incumbency.\nThe main assumption to rationalize a regression discontinuity design is the continuity assumption. Explain what this assumption means in the context of the regression discontinuity design that you proposed in part b. Do you think that this assumption is likely to hold in this context? Why or why not?\nUse the house data to implement the regression discontinuity design that you proposed in part b. What do you estimate as the causal effect of incumbency?\n\nFor this problem, we will use the data banks. We will study the causal effect of monetary policy on bank closures during the Great Depression. We’ll consider an interesting natural experiment in Mississippi where half the northern half of the state was in St. Louis’s federal reserve district (District 8) and the southern half of the state was in Atlanta’s federal reserve district (District 6). Atlanta had much looser monetary policy (meaning they substantially increased lending) than St. Louis during the early part of the Great Depression and our interest is in whether looser monetary policy made an difference.\n\nPlot the total number of banks separately for District 6 and District 8 across all available time periods in the data.\nAn important event in the South early in the Great Depression was the collapse of Caldwell and Company — the largest banking chain in the South at the time. This happened in November 1930. The Atlanta Fed’s lending markedly increased quickly after this event while St. Louis’s did not. Calculate a DID estimate of the effect of looser monetary policy on the number of banks that are still in business. How do you interpret these results? Hint: You can calculate this by taking the difference between the number of banks in District 6 relative to the number of banks in District 8 across all time periods relative to the difference between the number of banks in District 6 relative to District 8 in the first period (July 1, 1929).",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#extra-questions",
    "href": "17-observational_data.html#extra-questions",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.6 Extra Questions",
    "text": "18.6 Extra Questions\n\nWhat is the difference between treatment effect homogeneity and treatment effect heterogeneity?\nWhy do most researchers give up on trying to estimate the individual-level effect of participating in a treatment?\nExplain what unconfoundedness means.\nWhat is the key condition underlying a difference-in-differences approach to learn about the causal effect of some treatment on some outcome?\nWhat are two key conditions for a valid instrument?\nSuppose you are interested in the causal effect of participating in a union on a person’s income. Consider the following approaches.\n\nSuppose you run the following regression\n\\[\\begin{align*}\n   Earnings_i = \\beta_0 + \\alpha Union_i + \\beta_1 Education_i + U_i\n\\end{align*}\\]\nWould it be reasonable to interpret \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain.\nSuppose you have access to panel data and run the following fixed effects regression \\[\\begin{align*}\n   Earnings_{it} = \\beta_{0,t} + \\alpha Union_{it} + \\beta_1 Education_{it} + \\eta_i + U_{it}\n\\end{align*}\\]\nwhere \\(\\eta_i\\) is an individual fixed effect. Would it be reasonable to interpert \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?\nGoing back to the case with cross-sectional data, consider the regression \\[\\begin{align*}\n   Earnings_i = \\beta_0 + \\alpha Union_i + U_i\n\\end{align*}\\] but using the variable \\(Z_i = 1\\) if birthday is between Jan. 1 and Jun. 30 while \\(Z_i=0\\) otherwise. Would it be reasonable to interpert \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?\n\nSuppose that you are interested in the effect of lower college costs on the probability of graduating from college. You have access to student-level data from Georgia where students are eligible for the Hope Scholarship if they can keep their GPA above 3.0.\n\nWhat strategy can use to exploit this institional setting to learn about the causal effect of lower college costs on the probability of going to college?\nWhat sort of data would you need in order to implement this strategy?\nCan you think of any ways that the approach that you suggested could go wrong?\nAnother researcher reads the results from the approach you have implemented and complains that your results are only specific to students who have grades right around the 3.0 cutoff. Is this a fair criticism?\n\nSuppose you are willing to believe versions of unconfoundedness, a linear model for untreated potential outcomes, and treatment effect homogeneity so that you could write \\[\\begin{align*}\n  Y_i = \\beta_0 + \\alpha D_i + \\beta_1 X_i + \\beta_2 W_i + U_i\n\\end{align*}\\] with \\(\\E[U|D,X,W] = 0\\) so that you were willing to interpret \\(\\alpha\\) in this regression as the causal effect of \\(D\\) on \\(Y\\). However, suppose that \\(W\\) is not observed so that you cannot operationalize the above regression.\n\nSince you do not observe \\(W\\), you are considering just running a regression of \\(Y\\) on \\(D\\) and \\(X\\) and interpreting the estimated coefficient on \\(D\\) as the causal effect of \\(D\\) on \\(Y\\). Does this seem like a good idea?\nIn part (a), we can write a version of the model that you are thinking about estimating as \\[\\begin{align*}\n   Y_i = \\delta_0 + \\delta_1 D_i + \\delta_2 X_i + \\epsilon_i\n\\end{align*}\\] Suppose that \\(\\E[\\epsilon | D, X] = 0\\) and suppose also that \\[\\begin{align*}\nW_i = \\gamma_0 + \\gamma_1 D_i + \\gamma_2 X_i + V_i\n  \\end{align*}\\] with \\(\\E[V|D,X]=0\\). Provide an expression for \\(\\delta_1\\) in terms of \\(\\alpha\\), \\(\\gamma\\)’s and \\(\\beta\\)’s. Explain what this expression means.\n\nSuppose you have access to an experiment where some participants were randomly assigned to participate in a job training program and others were randomly assigned not to participate. However, some individuals that were assigned to participate in the treatment decided not to actually participate. Let’s use the following notation: \\(D=1\\) for individuals who actually participated and \\(D=0\\) for individuals who did not participate. \\(Z=1\\) for individuals who were assigned to the treatment and \\(Z=0\\) for individuals assigned not to participate (here, \\(D\\) and \\(Z\\) are not exactly the same because some individuals who were assigned to the treatment did not actually participate).\nYou are considering several different approaches to dealing with this issue. Discuss which of the following are good or bad ideas:\n\nEstimating \\(ATT\\) by \\(\\bar{Y}_{D=1} - \\bar{Y}_{D=0}\\).\nRun the regression \\(Y_i = \\beta_0 + \\alpha D_i + U_i\\) using \\(Z_i\\) as an instrument.\n\nSuppose you and a friend have conducted an experiment (things went well so that everyone complied with the treatment that they were assigned to, etc.). You interpret the difference \\(\\bar{Y}_{D=1} - \\bar{Y}_{D=0}\\) as an estimate of the \\(ATT\\), but your friend says that you should interpret it as an estimate of the \\(ATE\\). In fact, according to your friend, random treatment assignment implies that \\(\\E[Y(1)] = \\E[Y(1)|D=1] = \\E[Y|D=1]\\) and \\(\\E[Y(0)] = \\E[Y(0)|D=0] = \\E[Y|D=0]\\) which implies that \\(ATE = \\E[Y|D=1] - \\E[Y|D=0]\\). Who is right?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "",
    "text": "1 Introduction\nThese are a set of detailed course notes for ECON 4750 at UGA. They largely come from my personal notes that I have used to teach this course in previous years, which, in turn, are largely based on the Stock and Watson textbook serves as the other main reference for the course.\nThese notes have improved drastically over the course of several years. However, they are a work in progress and have not been professionally edited or anything like that…so it is possible that there are some mistakes (if you find any, please let me know). That said, I hope you will find this to be a useful resource this semester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.1 What is this?",
    "text": "1.1 What is this?\nIn previous years, I used to provide these notes as supplementary material for the textbook. Now, I think that there is enough detail here that it can be used as a main reference for the course. And, for example, there are several topics that we will cover in class in substantially more detail than in the textbook.\nThe material provided here can also be useful as a quasi-study guide. In particular, the notes cover pretty much exactly what we will be able to cover in a one semester course. In some ways, covering less material relative to the textbook can (in my view) make things easier for students. The notes also provide cross-references to the corresponding section in the textbook. For some topics here, I provide substantially less detail than the book; this may help you when it comes to studying as it may give you a sense of the material I see as being most important. In addition, there are additional practice questions (some with answers) provided at the end of each section.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-this-not",
    "href": "index.html#what-is-this-not",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.2 What is this not?",
    "text": "1.2 What is this not?\nThere are a couple of things that I want to explicitly say that this material is not. These include:\n\nA substitute for coming to class — I will take attendance anyway, but not attending class and relying on this material is not a good plan. Please do not do this.\nA full substitute for the textbook — In a number of places, the textbook contains substantially more details than I am able to provide here. In my experience, it is also useful to have different “voices” that you can refer to (e.g., you may find my explanation unclear but may find the textbook much clearer). The textbook also contains substantially more material than what I provide in these notes. If there are additional topics in the textbook that we do not cover that you would like to learn about, these should all be understandable for you by the end of this semester.\nSufficient for making good grades on exams — I don’t suspect that it will be a good strategy to rely exclusively on the material provided here in order to do well on the exams. I think this material should be helpful and perhaps even a good starting point when it comes to studying, but it is not sufficient for making a good grade in the class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#why-did-i-write-this",
    "href": "index.html#why-did-i-write-this",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.3 Why did I write this?",
    "text": "1.3 Why did I write this?\nI have a strong opinion about the best order to teach the material in ECON 4750. And, although, there are a number of advanced undergraduate textbooks in Econometrics that I like and reference as I teach the course, none of them go in the same order that I would like to teach. Therefore, I think one way that I can both go in the order that I want to during the semester without confusing you (the student) too much is to provide a detailed set of references to material that we are covering throughout the semester.\nThe book that I strongly suggest for the course is Stock and Watson.  By the end of the semester, we will have covered in much detail the first 14 chapters of the textbook (and some topics we will have covered in substantially more detail than in the textbook). If I had more time, the next topic that I would cover in this course would be Time Series Econometrics. I used to cover this material in the current course, but I have found that I am happier with the tradeoff of understanding slightly fewer topics better relative to covering more topics but at a faster pace. Time series is especially important for students who are interested in macroeconomics or finance. Fortunately, we offer a time series econometrics course, and, if you take it, you should be well prepared for it coming out of this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#additional-references",
    "href": "index.html#additional-references",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.4 Additional References",
    "text": "1.4 Additional References\nThese are all free to download; they are not main textbooks but I sometimes consult them for the class and could potentially be useful for you to consult in the future:\n\nFor R programming: Introduction to Econometrics with R, by Cristoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer\nFor prediction/machine learning: An Introduction to Statistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nFor causal inference: Causal Inference: The Mixtape, by Scott Cunningham\n\nAdditional R References:\nThere are tons of free R resources available online. Here are some that seem particularly useful to me.\n\nManageable Introduction: Introduction to R and RStudio, by Stephanie Spielman\nFull length book: Introduction to Data Science: Data Analysis and Prediction Algorithms with R, by Rafael Irizarry (this is way more than you will need for this course, but I suggest checking out Chapters 1, 2, 3, and 5, and there’s plenty more that you might find interesting).\nFull length book: STAT 545: Data Wrangling, exploration, and analysis with R, by Jenny Bryan",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#goals-for-the-course",
    "href": "index.html#goals-for-the-course",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.5 Goals for the Course",
    "text": "1.5 Goals for the Course\nI have three high level goals — that by the end of the semester, students should be able to\n\nRun regressions and be able to interpret them (this includes even complex regressions) and also be able to think through which regression you ought to run\nUse data in order to be able to predict outcomes of interest\nBe able to think clearly about when statistical results can be interpreted as causal effects.\n\nIn order to make progress towards these three goals, we also will need to learn about two additional topics:\n\nStatistical Programming\nProbability and Statistics\n\nWe’ll start the course off talking about these two topics. Perhaps this will be review material for some of you, but I have found that it is worth it to spend several weeks getting everyone on the same page with respect to these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#studying-for-the-class",
    "href": "index.html#studying-for-the-class",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.6 Studying for the Class",
    "text": "1.6 Studying for the Class\nStudents ask me all the time “How should I study for your class?” My advice (and I think this applies to most classes, not just my class) is for you to start by studying the notes from class. The things that I have discussed in class are the things that I think are most important for you learn in this sort of class and are the material that will be covered on the exam. That said, it may sometimes be the case that you do not fully understand a lecture or the notes that you took from a lecture when you are studying (this certainly applied to me when I was a student). If there are places that you do not understand what the notes mean, then I think that is the time when you should find the relevant portion of the textbook (or supplementary notes provided here) in order to “supplement” what the notes say.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#data-used-in-the-course",
    "href": "index.html#data-used-in-the-course",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.7 Data Used in the Course",
    "text": "1.7 Data Used in the Course\nThe following provides the full list of data that we will use this semester. Some of the links below are to data posted on my website; others point to data hosted externally (if you notice any broken links, please let me know).\nNote: some of the datasets below require installing an R package. For example, to access the Airq data, you need to install the Ecdat package first (i.e., by running install.packages(\"Ecdat\")) and then run data(Airq, package=\"Ecdat\").\n\nacs\n\nData from the 2019 American Community Survey about Education and Earnings\nAccess: Course Website\nDescription: Course Website\n\nAirq\n\nData about air quality in California counties in 1972.\nAccess: data(Airq, package=\"Ecdat\")\nDescription: ?Ecdat::Airq\n\nAJR\n\nData about GDP, institutions, and settler mortality\nAccess: data(AJR, package=\"hdm\")\nDescription: ?hdm::AJR\n\nbanks\n\nData about bank closures in Mississippi during the Great Depression\nAccess: Course Website\nDescription: Course Website\n\nBirthweight_Smoking\n\nData about infant birthweights and mother’s smoking from PA 1989\nAccess: Course Website\nDescription: Mark Watson’s website\n\nCaschool\n\nSchool-level test score data from California in 1998-1999\nAccess: data(Caschool, package=\"Ecdat\")\nDescription: ?Ecdat::Caschool\n\ndiamond_train\n\nData about diamond prices. The full version of this data I got from Kaggle, and then I split it into training and testing data.\nAccess: Course Website\nDescription: A description of each column in the data is available under the Description tab on Kaggle\n\ndiamond_test\n\nOut of sample version of diamond_train data\nAccess: Course Website\nDescription: A description of each column in the data is available under the Description tab on Kaggle\n\nFair\n\nIndividual-level data about affairs in the United States\nAccess: data(Fair, package=\"Ecdat\")\nDescription: ?Ecdat::Fair\n\nfastfood\n\nData about fast food restaurants in New Jersey and Pennsylvania from Card and Krueger (1994)\nAccess: Course Website\nDescription: Course Website\n\nFatalities\n\nState-level panel data about drunk driving laws and traffic fatalities\nAccess: data(Fatality, package=\"AER\")\nDescription: ?AER::Fatality\n\nFertility\n\nData from Angrist and Evans (1998) about fertility and female labor supply\nAccess: Course Website\nDescription: Course Website\n\nfertilizer_2000\n\nCountry-level data about fertilizer and crop yields from the year 2000. See description of fertilizer_panel below for more details\nAccess: Course Website\nDescription: Course Website\n\nfertilizer_panel\n\nCountry-level panel data from 1965-2000 (every 5 years) about fertilizer and crop yields for 68 developing countries. This data is a smaller version of the data used in McArthur, John W., and Gordon C. McCord. “Fertilizing growth: Agricultural inputs and their effects in economic development.” Journal of Development Economics 127 (2017): 133-152. url: https://doi.org/10.1016/j.jdeveco.2017.02.007.\nAccess: Course Website\nDescription: Course Website\n\nhouse\n\nU.S. House of Representatives elections data from 1946-1998\nAccess: Course Website](http://bcallaway11.github.io/Courses/ECON_4750_Fall_2025/)\nDescription: Course Website\n\nintergenerational_mobility\n\nIntergenerational mobility data from PSID\nAccess: Course Website\nDescription: Course Website\n\nLead_Mortality\n\nInfant mortality and lead pipes in 1900\nAccess: Access: Course Website\nDescription: Mark Watson’s website\n\nmlda\n\nCar accident deaths by age group\nAccess: Course Website\nDescription: Course Website\n\nmroz\n\nLabor force particpation of married women\nAccess: data(mroz, package=\"wooldridge\")\nDescription: ?wooldridge::mroz\n\nmutual_funds\n\nMutual fund performance data\nAccess: Course Website\nDescription: Course Website\n\nrand_hie\n\nRAND health insurance experiment\nAccess: Course Website\nDescription: Course Website\n\nStar\n\nData from Project STAR that randomly assigned some students to smaller class sizes\nAccess: data(Star, package=\"Ecdat\")\nDescription: ?Ecdat::Star\n\ntitanic_training\n\nPassenger level data on surviving Titanic. This data is a slightly adapted version of the titanic data on Kaggle\nAccess: Course Website\nDescription: Kaggle\n\ntitanic_testing\n\nOut of sample version of titanic_training data\nAccess: Course Website\nDescription: Kaggle\n\nus_data\n\nData from the 2019 American Community Survey via IPUMS\nAccess: Course Website\nDescription: Course Website",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#first-week-of-class",
    "href": "index.html#first-week-of-class",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.8 First Week of Class",
    "text": "1.8 First Week of Class\nRelated Reading: SW All of Chapter 1\nIn the first few classes, we will talk at very high level about the objectives of Econometrics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#lab-7-minimium-wage-and-employment",
    "href": "17-observational_data.html#lab-7-minimium-wage-and-employment",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.3 Lab 7: Minimium Wage and Employment",
    "text": "18.3 Lab 7: Minimium Wage and Employment\nFor this lab, we will use the njmin data from the causaldata package. This is data that comes from Card and Krueger (1994), which is one of the most well-known empirical papers in all of economics. It is about the causal effect of a minimum wage increase in New Jersey on employment in the fast food industry. It is one of the original difference-in-differences papers and is also, as far as I know, one of the first papers to explicitly use the pre-post research design that we discussed above.\nTo start with, use the following code to load the data, drop rows with missing data, and drop columns that we won’t use in this lab.\n\nload(\"data/fastfood.RData\")\nlibrary(dplyr)\n\n# drop missing data\nfastfood &lt;- subset(fastfood, balanced == 1)\n# drop unused columns\nfastfood &lt;- select(fastfood, id, state, location, chain_name, ownership, fte_pre, hrsopen_pre, wage_st_pre, fte_post, hrsopen_post, wage_st_post)\n\n\nUse modelsummary::datasummary_balance to report summary statistics for all the variables in the data, separately by state (New Jersey vs. Pennsylvania). What do you notice?\nLet us start by assuming unconfoundedness conditional on the restaurant chain (chain_name). Estimate the \\(ATT\\) of the minimum wage on full-time equivalent employees in the post-treatment period using regression adjustment. Report your estimate and interpret it.\nImplement the placebo test for unconfoundedness in the pre-treatment period. What do you find, and what does it suggest about the plausibility of unconfoundedness in this application?\nLet us change the identification strategy to lagged outcome unconfoundedness (let’s continue to also include chain_name as a covariate). Estimate the \\(ATT\\) of the minimum wage on full-time equivalent employees in the post-treatment period using lagged outcome unconfoundedness.\nFinally, estimate the \\(ATT\\) of the minimum wage on full-time equivalent employees in the post-treatment period using difference-in-differences, continuing to include chain_name as a covariate. How do your results compare to the previous two estimates?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  },
  {
    "objectID": "17-observational_data.html#lab-7-solution",
    "href": "17-observational_data.html#lab-7-solution",
    "title": "18  Causal Inference with Observational Data",
    "section": "18.4 Lab 7: Solution",
    "text": "18.4 Lab 7: Solution\n\nSummary statistics by state\n\n\nlibrary(modelsummary)\ndatasummary_balance(\n  ~ state,\n  data = fastfood,\n  fmt=2\n)\n\n\n\n    \n\n    \n    \n      \n        \n\n \n \nPennsylvania (N=75)\nNew Jersey (N=309)\n \n \n\n        \n              \n                 \n                  \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n                Diff. in Means\n                Std. Error\n              \n        \n        \n        \n                \n                  id\n                  \n                  3776.93\n                  1805.12\n                  2176.01\n                  1209.22\n                  -1600.92\n                  219.50\n                \n                \n                  fte_pre\n                  \n                  23.38\n                  12.01\n                  20.43\n                  9.21\n                  -2.95\n                  1.48\n                \n                \n                  hrsopen_pre\n                  \n                  14.51\n                  2.96\n                  14.40\n                  2.82\n                  -0.12\n                  0.38\n                \n                \n                  wage_st_pre\n                  \n                  4.63\n                  0.36\n                  4.61\n                  0.34\n                  -0.02\n                  0.05\n                \n                \n                  fte_post\n                  \n                  21.10\n                  8.38\n                  20.90\n                  9.38\n                  -0.20\n                  1.10\n                \n                \n                  hrsopen_post\n                  \n                  14.64\n                  2.88\n                  14.39\n                  2.76\n                  -0.25\n                  0.37\n                \n                \n                  wage_st_post\n                  \n                  4.62\n                  0.36\n                  5.08\n                  0.10\n                  0.46\n                  0.04\n                \n                \n                  \n                  \n                  N\n                  Pct.\n                  N\n                  Pct.\n                  \n                  \n                \n                \n                  location\n                  NJ_Central\n                  0\n                  0.0\n                  58\n                  18.8\n                  \n                  \n                \n                \n                  \n                  NJ_North\n                  0\n                  0.0\n                  162\n                  52.4\n                  \n                  \n                \n                \n                  \n                  NJ_South\n                  0\n                  0.0\n                  89\n                  28.8\n                  \n                  \n                \n                \n                  \n                  PA_Easton\n                  41\n                  54.7\n                  0\n                  0.0\n                  \n                  \n                \n                \n                  \n                  PA_PhillyNE\n                  34\n                  45.3\n                  0\n                  0.0\n                  \n                  \n                \n                \n                  chain_name\n                  Burger King\n                  33\n                  44.0\n                  126\n                  40.8\n                  \n                  \n                \n                \n                  \n                  KFC\n                  12\n                  16.0\n                  67\n                  21.7\n                  \n                  \n                \n                \n                  \n                  Roy Rogers\n                  17\n                  22.7\n                  77\n                  24.9\n                  \n                  \n                \n                \n                  \n                  Wendys\n                  13\n                  17.3\n                  39\n                  12.6\n                  \n                  \n                \n                \n                  ownership\n                  Franchise\n                  49\n                  65.3\n                  201\n                  65.0\n                  \n                  \n                \n                \n                  \n                  Company\n                  26\n                  34.7\n                  108\n                  35.0\n                  \n                  \n                \n        \n      \n    \n\n\n\n\nRegression adjustment ATT estimate\n\n\n# regression adjustment\nlibrary(DRDID)\n\nfte_post &lt;- fastfood$fte_post\ncovs &lt;- model.matrix(~chain_name, data = fastfood)\nD &lt;- as.numeric(fastfood$state == \"New Jersey\")\n\nra_att_post &lt;- DRDID::reg_did_panel(y1 = fte_post, y0 = 0, D = D, covariates = covs)\nra_att_post\n\n Call:\nDRDID::reg_did_panel(y1 = fte_post, y0 = 0, D = D, covariates = covs)\n------------------------------------------------------------------\n Outcome-Regression DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n  0.5969     0.8762     0.6812     0.4957    -1.1205     2.3143  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: OLS.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.\n\n\n\nPlacebo test in pre-treatment period\n\n\n# check identification strategy in pre-treatment periods\nfte_pre &lt;- fastfood$fte_pre\nra_att_pre &lt;- DRDID::reg_did_panel(y1 = fte_pre, y0 = 0, D = D, covariates = covs)\nra_att_pre\n\n Call:\nDRDID::reg_did_panel(y1 = fte_pre, y0 = 0, D = D, covariates = covs)\n------------------------------------------------------------------\n Outcome-Regression DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n  -1.899     1.1865    -1.6004     0.1095    -4.2246     0.4267  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: OLS.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.\n\n\n\nLagged outcome unconfoundedness ATT estimate\n\n\n# lagged outcome unconfoundedness\nlou_covs &lt;- model.matrix(~ chain_name + fte_pre, data = fastfood)\nlou_att_post &lt;- DRDID::reg_did_panel(y1 = fte_post, y0 = 0, D = D, covariates = lou_covs)\nlou_att_post\n\n Call:\nDRDID::reg_did_panel(y1 = fte_post, y0 = 0, D = D, covariates = lou_covs)\n------------------------------------------------------------------\n Outcome-Regression DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n  0.844      0.8903     0.948      0.3431    -0.9009     2.5889  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: OLS.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.\n\n\n\nDifference-in-differences ATT estimate\n\n\n# difference-in-differences\ndid_att_post &lt;- DRDID::reg_did_panel(y1 = fte_post, y0 = 0, D = D, covariates = covs)\ndid_att_post\n\n Call:\nDRDID::reg_did_panel(y1 = fte_post, y0 = 0, D = D, covariates = covs)\n------------------------------------------------------------------\n Outcome-Regression DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n  0.5969     0.8762     0.6812     0.4957    -1.1205     2.3143  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: OLS.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Causal Inference with Observational Data</span>"
    ]
  }
]