[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "",
    "text": "1 Introduction\nThese are a set of detailed course notes for ECON 4750 at UGA. They largely come from my personal notes that I have used to teach this course in previous years, which, in turn, are largely based on the Stock and Watson textbook serves as the other main reference for the course.\nThese notes have improved drastically over the course of several years. However, they are a work in progress and have not been professionally edited or anything like that…so it is possible that there are some mistakes (if you find any, please let me know). That said, I hope you will find this to be a useful resource this semester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.1 What is this?",
    "text": "1.1 What is this?\nIn previous years, I used to provide these notes as supplementary material for the textbook. Now, I think that there is enough detail here that it can be used as a main reference for the course. And, for example, there are several topics that we will cover in class in substantially more detail than in the textbook.\nThe material provided here can also be useful as a quasi-study guide. In particular, the notes cover pretty much exactly what we will be able to cover in a one semester course. In some ways, covering less material relative to the textbook can (in my view) make things easier for students. The notes also provide cross-references to the corresponding section in the textbook. For some topics here, I provide substantially less detail than the book; this may help you when it comes to studying as it may give you a sense of the material I see as being most important. In addition, there are additional practice questions (some with answers) provided at the end of each section.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-this-not",
    "href": "index.html#what-is-this-not",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.2 What is this not?",
    "text": "1.2 What is this not?\nThere are a couple of things that I want to explicitly say that this material is not. These include:\n\nA substitute for coming to class — I will take attendance anyway, but not attending class and relying on this material is not a good plan. Please do not do this.\nA full substitute for the textbook — In a number of places, the textbook contains substantially more details than I am able to provide here. In my experience, it is also useful to have different “voices” that you can refer to (e.g., you may find my explanation unclear but may find the textbook much clearer). The textbook also contains substantially more material than what I provide in these notes. If there are additional topics in the textbook that we do not cover that you would like to learn about, these should all be understandable for you by the end of this semester.\nSufficient for making good grades on exams — I don’t suspect that it will be a good strategy to rely exclusively on the material provided here in order to do well on the exams. I think this material should be helpful and perhaps even a good starting point when it comes to studying, but it is not sufficient for making a good grade in the class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#why-did-i-write-this",
    "href": "index.html#why-did-i-write-this",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.3 Why did I write this?",
    "text": "1.3 Why did I write this?\nI have a strong opinion about the best order to teach the material in ECON 4750. And, although, there are a number of advanced undergraduate textbooks in Econometrics that I like and reference as I teach the course, none of them go in the same order that I would like to teach. Therefore, I think one way that I can both go in the order that I want to during the semester without confusing you (the student) too much is to provide a detailed set of references to material that we are covering throughout the semester.\nThe book that I strongly suggest for the course is Stock and Watson.  By the end of the semester, we will have covered in much detail the first 14 chapters of the textbook (and some topics we will have covered in substantially more detail than in the textbook). If I had more time, the next topic that I would cover in this course would be Time Series Econometrics. I used to cover this material in the current course, but I have found that I am happier with the tradeoff of understanding slightly fewer topics better relative to covering more topics but at a faster pace. Time series is especially important for students who are interested in macroeconomics or finance. Fortunately, we offer a time series econometrics course, and, if you take it, you should be well prepared for it coming out of this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#additional-references",
    "href": "index.html#additional-references",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.4 Additional References",
    "text": "1.4 Additional References\nThese are all free to download; they are not main textbooks but I sometimes consult them for the class and could potentially be useful for you to consult in the future:\n\nFor R programming: Introduction to Econometrics with R, by Cristoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer\nFor prediction/machine learning: An Introduction to Statistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nFor causal inference: Causal Inference: The Mixtape, by Scott Cunningham\n\nAdditional R References:\nThere are tons of free R resources available online. Here are some that seem particularly useful to me.\n\nManageable Introduction: Introduction to R and RStudio, by Stephanie Spielman\nFull length book: Introduction to Data Science: Data Analysis and Prediction Algorithms with R, by Rafael Irizarry (this is way more than you will need for this course, but I suggest checking out Chapters 1, 2, 3, and 5, and there’s plenty more that you might find interesting).\nFull length book: STAT 545: Data Wrangling, exploration, and analysis with R, by Jenny Bryan",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#goals-for-the-course",
    "href": "index.html#goals-for-the-course",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.5 Goals for the Course",
    "text": "1.5 Goals for the Course\nI have three high level goals — that by the end of the semester, students should be able to\n\nRun regressions and be able to interpret them (this includes even complex regressions) and also be able to think through which regression you ought to run\nUse data in order to be able to predict outcomes of interest\nBe able to think clearly about when statistical results can be interpreted as causal effects.\n\nIn order to make progress towards these three goals, we also will need to learn about two additional topics:\n\nStatistical Programming\nProbability and Statistics\n\nWe’ll start the course off talking about these two topics. Perhaps this will be review material for some of you, but I have found that it is worth it to spend several weeks getting everyone on the same page with respect to these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#studying-for-the-class",
    "href": "index.html#studying-for-the-class",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.6 Studying for the Class",
    "text": "1.6 Studying for the Class\nStudents ask me all the time “How should I study for your class?” My advice (and I think this applies to most classes, not just my class) is for you to start by studying the notes from class. The things that I have discussed in class are the things that I think are most important for you learn in this sort of class and are the material that will be covered on the exam. That said, it may sometimes be the case that you do not fully understand a lecture or the notes that you took from a lecture when you are studying (this certainly applied to me when I was a student). If there are places that you do not understand what the notes mean, then I think that is the time when you should find the relevant portion of the textbook (or supplementary notes provided here) in order to “supplement” what the notes say.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#data-used-in-the-course",
    "href": "index.html#data-used-in-the-course",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.7 Data Used in the Course",
    "text": "1.7 Data Used in the Course\nThe following provides the full list of data that we will use this semester. Some of the links below are to data posted on my website; others point to data hosted externally (if you notice any broken links, please let me know).\nNote: some of the datasets below require installing an R package. For example, to access the Airq data, you need to install the Ecdat package first (i.e., by running install.packages(\"Ecdat\")) and then run data(Airq, package=\"Ecdat\").\n\nacs\n\nData from the 2019 American Community Survey about Education and Earnings\nAccess: Course Website\nDescription: Course Website\n\nAirq\n\nData about air quality in California counties in 1972.\nAccess: data(Airq, package=\"Ecdat\")\nDescription: ?Ecdat::Airq\n\nAJR\n\nData about GDP, institutions, and settler mortality\nAccess: data(AJR, package=\"hdm\")\nDescription: ?hdm::AJR\n\nbanks\n\nData about bank closures in Mississippi during the Great Depression\nAccess: Course Website\nDescription: Course Website\n\nBirthweight_Smoking\n\nData about infant birthweights and mother’s smoking from PA 1989\nAccess: Course Website\nDescription: Mark Watson’s website\n\nCaschool\n\nSchool-level test score data from California in 1998-1999\nAccess: data(Caschool, package=\"Ecdat\")\nDescription: ?Ecdat::Caschool\n\ndiamond_train\n\nData about diamond prices. The full version of this data I got from Kaggle, and then I split it into training and testing data.\nAccess: Course Website\nDescription: A description of each column in the data is available under the Description tab on Kaggle\n\ndiamond_test\n\nOut of sample version of diamond_train data\nAccess: Course Website\nDescription: A description of each column in the data is available under the Description tab on Kaggle\n\nFair\n\nIndividual-level data about affairs in the United States\nAccess: data(Fair, package=\"Ecdat\")\nDescription: ?Ecdat::Fair\n\nFatalities\n\nState-level panel data about drunk driving laws and traffic fatalities\nAccess: data(Fatality, package=\"AER\")\nDescription: ?AER::Fatality\n\nfertilizer_2000\n\nCountry-level data about fertilizer and crop yields from the year 2000. See description of fertilizer_panel below for more details\nAccess: Course Website\nDescription: Course Website\n\nfertilizer_panel\n\nCountry-level panel data from 1965-2000 (every 5 years) about fertilizer and crop yields for 68 developing countries. This data is a smaller version of the data used in McArthur, John W., and Gordon C. McCord. “Fertilizing growth: Agricultural inputs and their effects in economic development.” Journal of Development Economics 127 (2017): 133-152. url: https://doi.org/10.1016/j.jdeveco.2017.02.007.\nAccess: Course Website\nDescription: Course Website\n\nhouse\n\nU.S. House of Representatives elections data from 1946-1998\nAccess: Course Website\nDescription: Course Website\n\nintergenerational_mobility\n\nIntergenerational mobility data from PSID\nAccess: Course Website\nDescription: Course Website\n\nLead_Mortality\n\nInfant mortality and lead pipes in 1900\nAccess: Access: Course Website\nDescription: Mark Watson’s website\n\nmlda\n\nCar accident deaths by age group\nAccess: Course Website\nDescription: Course Website\n\nmroz\n\nLabor force particpation of married women\nAccess: data(mroz, package=\"wooldridge\")\nDescription: ?wooldridge::mroz\n\nmutual_funds\n\nMutual fund performance data\nAccess: Course Website\nDescription: Course Website\n\nrand_hie\n\nRAND health insurance experiment\nAccess: Course Website\nDescription: Course Website\n\nStar\n\nData from Project STAR that randomly assigned some students to smaller class sizes\nAccess: data(Star, package=\"Ecdat\")\nDescription: ?Ecdat::Star\n\ntitanic_training\n\nPassenger level data on surviving Titanic. This data is a slightly adapted version of the titanic data on Kaggle\nAccess: Course Website\nDescription: Kaggle\n\ntitanic_testing\n\nOut of sample version of titanic_training data\nAccess: Course Website\nDescription: Kaggle\n\nus_data\n\nData from the 2019 American Community Survey via IPUMS\nAccess: Course Website\nDescription: Course Website",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#first-week-of-class",
    "href": "index.html#first-week-of-class",
    "title": "Detailed Course Notes for ECON 4750",
    "section": "1.8 First Week of Class",
    "text": "1.8 First Week of Class\nRelated Reading: SW All of Chapter 1\nIn the first few classes, we will talk at very high level about the objectives of Econometrics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction_to_R.html",
    "href": "01-introduction_to_R.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "2.1 Setting up R\nThis section covers how to set up R and RStudio and then what RStudio will look like when you open it up.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction_to_R.html#setting-up-r",
    "href": "01-introduction_to_R.html#setting-up-r",
    "title": "2  Introduction to R",
    "section": "",
    "text": "2.1.1 What is R?\nRelated Reading: IDS 1.1\nR is a statistical programming language. This is important for two reasons\n\nIt looks like a “real” programming language. In my view, this is a big advantage. And many of the programming skills that we will learn in this class will be transferable. What I mean is that, if you one day want to switch to writing code in Stata or Python, I think the switch should be not-too-painful because learning new “syntax” (things like where to put the semi-colons) is usually relatively easy compared to the “way of thinking” about how to write code. Some other statistical programming languages are more “canned” than R. In some sense, this makes them easier to learn, but this also comes with the drawback that whatever skills that you learn are quite specific to that one language.\nEven though R is a real programming language, it is geared towards statistics. Compared to say, Matlab, a lot of common statistical procedures (e.g., running a regression) will be quite easy for you.\n\nR is very popular among statisticians, computer scientists, economists.\nIt is easy to share code across platforms: Linux, Windows, Mac. Besides that, it is easy to write and contribute extensions. I have 10+ R packages that you can easily download and immediately use.\nThere is a large community, and lots of available, helpful resources.\n\nChatGPT\nYour favorite search engine\nStackOverflow\n\n\n\n2.1.2 Downloading R\nWe will use R (https://www.r-project.org/) to analyze data. R is freely available and available across platforms. You should go ahead and download R for your personal computer as soon as possible — this should be relatively straightforward. It is also available at most computer labs on campus.\n\n\n2.1.3 RStudio\nBase R comes with a lightweight development environment (i.e., a place to write and execute code), but most folks prefer RStudio as it has more features. You can download it here: https://www.rstudio.com/products/rstudio/download/#download; choose the free version based on your operating system (Linux, Windows, Mac, etc.).\n\n\n2.1.4 RStudio Development Environment\nRelated Reading: IDS 1.4\nWhen you first open Rstudio, it will look something like this\n\nTypically, we will write scripts, basically just as a way to save the code that we have written. Go to File -&gt; New File -&gt; R Script. This will open up a new pane, and your screen should look something like this\n\nLet’s look around here. The top left pane is called the “Source Pane”. It is where you can write an R script. Try typing\n\n1+1\n\nin that pane. This is a very simple R program. Now, type Ctrl+s to save the script. This will likely prompt you to provide a name for the script. You can call it first_script.R or something like that. The only thing that really matters is that the file name ends in “.R” (although you should at least give the file a reasonably descriptive name).\nNow let’s move to the bottom left pane. This is called the “Console Pane”. It is where the actual computations happen in R (Notice that, although we have already saved our first script, we haven’t actually run any code). Beside the blue arrow in that pane, try typing\n\n2+2\n\nand then press ENTER. This time you should actually see the answer.\nNow, let’s go back to the Source pane. Often, it is convenient to run R programs line by line (mainly in order for it to be easy for you to digest the results). You can do this by pressing Ctrl+ENTER on any line in your script for it to run next. Try this on the first line of your script file where we previously typed 1+1. This code should now run, and you should be able to see the result down in the bottom left Console pane.\nWe will ignore the two panes on the right for now and come back to them once we get a little more experience programming in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction_to_R.html#installing-r-packages",
    "href": "01-introduction_to_R.html#installing-r-packages",
    "title": "2  Introduction to R",
    "section": "2.2 Installing R Packages",
    "text": "2.2 Installing R Packages\nRelated Reading: IDS 1.5\nWhen you download R, you get “base” R. Base R contains “basic” functions that are commonly used by most R users. To give some examples, base R gives you the ability add, subtract, divide, or multiply numbers. Base R gives you the ability to calculate the mean (the function is called mean) or standard deviation (the function is called sd) of a vector of numbers.\nBase R is quite powerful and probably the majority of code you will write in R will only involve Base R.\nThat being said, there are many cases where it is useful to expand the base functionality of R. This is done through packages. Packages expand the functionality of R. R is open source so these packages are contributed by users.\nIt also typically wouldn’t make sense for someone to install all available R packages. For example, a geographer might want to install a much different set of packages relative to an economist. Therefore, we will typically install only the additional functionality that we specifically want.\n\nExample: In this example, we’ll install the dslabs package (which is from the IDS book) and the lubridate package (which is a package for working with dates in R).\n\n# install dslabs package\ninstall.packages(\"dslabs\")\n\n# install lubridate package\ninstall.packages(\"lubridate\")\n\n\nInstalling a package is only the first step to using a package. You can think of installing a package like downloading a package. To actually use a package, you need to load it into memory (i.e., “attach” it) or at least be clear about the package where a function that you are trying to call comes from.\n\nExample:  Dates can be tricky to work with in R (and in programming languages generally). For example, they are not exactly numbers, but they also have more structure than just a character string. The lubridate package contains functions for converting numbers/strings into dates.\n\nbday &lt;- \"07-15-1985\"\nclass(bday) # R doesn't know this is actually a date yet\n\n[1] \"character\"\n\n# load the package\nlibrary(lubridate)\n# mdy stands for \"month, day, year\"\n# if date were in different format, could use ymd, etc.\ndate_bday &lt;- mdy(bday)\ndate_bday\n\n[1] \"1985-07-15\"\n\n# now R knows this is a date\nclass(date_bday)\n\n[1] \"Date\"\n\n\nAnother (and perhaps better) way to call a function from a package is to use the :: syntax. In this case, you do not need the call to library from above. Instead, you can try\n\nlubridate::mdy(bday)\n\n[1] \"1985-07-15\"\n\n\nThis does exactly the same thing as the code before. What is somewhat better about this code is that it is easier to tell that the mdy function came from the lubridate package.\n\n\n2.2.1 A list of useful R packages\n\nAER — package containing data from Applied Econometrics with R\nwooldridge — package containing data from Wooldridge’s text book\nggplot2 — package to produce sophisticated looking plots\ndplyr — package containing tools to manipulate data\nhaven — package for loading different types of data files\nplm — package for working with panel data\nfixest — another package for working with panel data\nivreg — package for IV regressions, diagnostics, etc.\nestimatr — package that runs regressions but with standard errors that economists often like more than the default options in R\nmodelsummary — package for producing nice output of more than one regression and summary statistics\n\nAs of this writing, there are currently 18,004 R packages available on CRAN (R’s main repository for contributed packages).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction_to_R.html#r-basics",
    "href": "01-introduction_to_R.html#r-basics",
    "title": "2  Introduction to R",
    "section": "2.3 R Basics",
    "text": "2.3 R Basics\nRelated Reading: IDS 2.1\nIn this section, we’ll start to work towards writing useful R code.\n\n2.3.1 Objects\nRelated Reading: IDS 2.2\nThe very first step to writing code that can actually do something is to able to store things. In R, we store things in objects (perhaps sometimes I will also use the word variables).\nEarlier, we used R to calculate \\(1+1\\). Let’s go back to the Source pane (top left pane in RStudio) and type\n\nanswer &lt;- 1 + 1\n\nPress Ctrl+ENTER on this line to run it. You should see the same line down in the Console now.\nLet’s think carefully about what is happening here\n\nanswer is the name of the variable (or object) that we are creating here.\nthe &lt;- is the assignment operator. It means that we should assign whatever is on the right hand side of it to the variable that is on the left hand side of it\n1+1 just computes \\(1+1\\) as we did earlier. Soon we will put more complicated expressions here.\n\nYou can think about the above code as computing \\(1+1\\) and then saving it in the variable answer.\n\nSide Comment: The assignment operator, &lt;-, is a “less than sign” followed by a “hyphen”. It’s often convenient though to use the keyboard shortcut Alt+- (i.e., hold down Alt and press the hypen key) to insert it. You can also use an = for assignment, but this is less commonly done in R.\n\n\nPractice: Try creating variable called five_squared that is equal to \\(5 \\times 5\\) (multiplication in R is done using the * symbol).\n\nThere are a number of reasons why you might like to create an object in R. Perhaps the main one is so that you can reuse it. Let’s try multiplying answer by \\(3\\).\n\nanswer*3\n\n[1] 6\n\n\nIf you wanted, you could also save this as its own variable too.\n\n\n2.3.2 Workspace\nRelated Reading: IDS 2.2\nBefore we move on, I just want to show you what my workspace looks like now.\n\nAs we talked about above, you can see the code in my script in the Source pane in the top left. You can also see the code that I actually ran in the Console pane on the bottom left.\nNow, take a look at the top right pane. You will see under the Environment tab that answer shows up there with a value of 2. The Environment tab keeps track of all the variables that you have created in your current session. A couple of other things that might be useful to point out there.\n\nLater on in the class, we will often import data to work with. You will notice the “Import Dataset” button that is located in this top right pane. I will suggest to you a different way of importing data in the next section, but this is also a way to do it.\nOccasionally, you might get into the case where you have saved a bunch of variables and it would be helpful to “start over”. The broom in this pane will “clean” your workspace (this just means delete everything).\n\n\n\n2.3.3 Importing Data\nTo work with actual data in R, we will need to import it. I mentioned the “Import Data” button above, but let me mention a few other possibilities here, including how to import data by writing code.\nOn the course website, I posted three files firm.data.csv, firm_data.RData, and firm_data.dta. All three of these contain exactly the same small, fictitious dataset, but are saved in different formats.\nProbably the easiest way to import data in R is through the Files pane on the bottom right. But, in order to do this, you may need to change your working directory. We will do this using RStudio’s user interface in the following steps:\n\nFirst navigate to Sessions -&gt; Set Working Directory -&gt; Choose Directory. This will open a window that will allow you to choose the directory where you saved the data.\n\n\n\nNext, use the menu to navigate to the place where you saved firm_data.csv. I created a folder ~/Dropbox/Courses/Georgia/Undergrad Econometrics/24 Fall/firm data/ and saved it there.\n\n\n\nNow, we have set the working directory, and this is what RStudio looks like for me. Notice that the working directory is now set to the folder where I saved the data. You can see the difference in the Files pane.\n\n\n\nNext, we will load the data, just by clicking it in the Files pane. I picked firm_data.csv, but any of the three files will work. R is quite good at recognizing different types of data files and importing them, so this same procedure will work for firm_data.RData and firm_data.dta even though they are different types of files. Once you click it, you will get a screen that should look like this\n\n\n\nClick “Import” and the data should be imported. You can see that it is now in the Environment pane.\n\n\nNext, let’s discuss how to import data by writing computer code (by the way, this is actually what is happening behind the scenes when you import data through the user interface as described above). “csv” stands for “Comma Separated Values”. This is basically a plain text file (e.g., try opening it in Notepad or Text Editor) where the columns are separated by commas and the rows are separated by being on different lines. Most any computer program can read this type of file; that is, you could easily import this file into, say, R, Excel, or Stata. You can import a .csv file using R code by\n\nfirm_data &lt;- read.csv(\"firm_data.csv\")\n\nAn RData file is the native format for saving data in R. You can import an RData file using the following command:\n\nfirm_data &lt;- load(\"firm_data.RData\")\n\nSimilarly, a dta file the native format for saving data in Stata. You can import a dta file using the following command:\n\nlibrary(haven) # external package for reading dta file\nfirm_data &lt;- read_dta(\"firm_data.dta\")\n\nIn all three cases above, what we have done is to create a new data.frame (a data.frame is a type of object that we’ll talk about in detail later on in this chapter) called firm_data that contains the data that we were trying to load.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html",
    "href": "02-programming_in_R.html",
    "title": "3  Programming in R",
    "section": "",
    "text": "3.1 Functions in R\nRelated Reading: IDS 2.2\nR has a large number of helpful, built-in functions. Let’s start with a pretty representative example: computing logarithms. This can be done using the R function log.\nlog(5)\n\n[1] 1.609438\nYou can tell this is a function because of the parentheses. The 5 inside of the parentheses is called the argument of the function. As practice, try computing the \\(\\log\\) of 7.\nThe default base in R is \\(e \\approx 2.718\\), so that log(5) actually computes what you might be more used to calling the “natural logarithm”. You can change the default value of the base by adding an extra argument to the function.\nlog(5, base=10)\n\n[1] 0.69897\nIn order to learn about what arguments are available (and what they mean), you can access the help files for a particular function by running either\nhelp(log)\n?log\nand, of course, substituting the name of whatever function you want to learn about in place of log.\nIn RStudio, it can also be helpful to press Tab and RStudio will provide possible completions to the function you are typing as well as what arguments can be provided to that function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#functions-in-r",
    "href": "02-programming_in_R.html#functions-in-r",
    "title": "3  Programming in R",
    "section": "",
    "text": "Side Comment: As a reminder, the logarithm of some number, let’s call it \\(b\\), is is the value of \\(a\\) that solves \\(\\textrm{base}^a = b\\).\n\n\n\n\n\n\n\n\nPractice: R has a function for computing absolute value (you’ll have to find the name of it on your own). Try computing the absolute value of \\(5\\) and \\(-5\\). Try creating a variable called negative_three that is equal to \\(-3\\); then, try to compute the absolute value of negative_three.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#data-types",
    "href": "02-programming_in_R.html#data-types",
    "title": "3  Programming in R",
    "section": "3.2 Data types",
    "text": "3.2 Data types\nRelated Reading: IDS 2.4\n\n3.2.1 Numeric Vectors\nThe most basic data type in R is the vector. In fact, above when we created variables that were just a single number, they are actually stored as a numeric vector.\nTo more explicitly create a vector, you can use the c function in R. For example, let’s create a vector called five that contains the numbers 1 through 5.\n\n  five &lt;- c(1,2,3,4,5)\n\nWe can print the contents of the vector five just by typing its name\n\nfive\n\n[1] 1 2 3 4 5\n\n\nAnother common operation on vectors is to get a particular element of a vector. Let me give an example\n\nfive[3]\n\n[1] 3\n\n\nThis code takes the vector five and returns the third element in the vector. Notice that the above line contains braces, [ and ] rather than parentheses.\nIf you want several different elements from a vector, you can do the following\n\nfive[c(1,4)]\n\n[1] 1 4\n\n\nThis code takes the vector five and returns the first and fourth element in the vector.\nOne more useful function for vectors is the function length. This tells you the number of elements in vector. For example,\n\nlength(five)\n\n[1] 5\n\n\nwhich means that there are five total elements in the vector five.\n\n\n3.2.2 Vector arithmetic\nRelated Reading: IDS 2.8\nThe main operations on numeric vectors are +, -, *, / which correspond to addition, subtraction, multiplication, and division. Often, we would like to carry out these operations on vectors.\nThere are two main cases. The first case is when you try to add a single number (i.e., a scalar) to all the elements in a vector. In this setup, the operation will happen element-wise which means the same number will be added to all numbers in the vector. This will be clear with some examples.\n\nfive &lt;- c(1,2,3,4,5)\n\n# adds one to each element in vector\nfive + 1\n\n[1] 2 3 4 5 6\n\n# also adds one to each element in vector\n1 + five\n\n[1] 2 3 4 5 6\n\n\nSimilar things will happen with the other mathematical operations above. Here are some more examples:\n\nfive * 3\n\n[1]  3  6  9 12 15\n\nfive - 3\n\n[1] -2 -1  0  1  2\n\nfive / 3\n\n[1] 0.3333333 0.6666667 1.0000000 1.3333333 1.6666667\n\n\nThe other interesting case is what happens when you try to apply any of the same mathematical operators to two different vectors.\n\n# just some random numbers\nvec2 &lt;- c(8,-3,4,1,7)\n\nfive + vec2\n\n[1]  9 -1  7  5 12\n\nfive - vec2\n\n[1] -7  5 -1  3 -2\n\nfive * vec2\n\n[1]  8 -6 12  4 35\n\nfive / vec2\n\n[1]  0.1250000 -0.6666667  0.7500000  4.0000000  0.7142857\n\n\nYou can immediately see what happens here. For example, for five + vec2, the first element of five is added to the first element of vec2, the second element of five is added to the second element of vec2 and so on. Similar things happen for each of the other mathematical operations too.\nThere’s one other case that might be interesting to consider too. What happens if you try to apply these mathematical operations to two vectors of different lengths? Let’s find out\n\nvec3 &lt;- c(2,6)\nfive + vec3\n\nWarning in five + vec3: longer object length is not a multiple of shorter\nobject length\n\n\n[1]  3  8  5 10  7\n\n\nYou’ll notice that this computes something but it also issues a warning. What happens here is that the result is equal to the first element of five plus the first element of vec3, the second of five plus the second element of vec3, the third element of five plus the first element of vec3, the fourth element of five plus the second element of vec3, and the fifth element of five plus the first element of vec3. What’s happening here is that, since vec3 contains fewere elements that five, the elements of vec3 are getting recycled. In my experience, this warning often indicates a coding mistake. There are many cases where I want to add the same number to all elements in a vector, and many other cases where I want to add two vectors that have the same length, but I cannot think of any cases where I would want to add two vectors the way that is being carried out here.\nThe same sort of things will happen with subtraction, multiplication, and division (feel free to try it out).\n\n\n3.2.3 More helpful functions in R\nThis is definitely an incomplete list, but I’ll point you here to some more functions in R that are often helpful along with quick examples of them.\n\nseq function — creates a “sequence” of numbers\n\nseq(2,7)\n\n[1] 2 3 4 5 6 7\n\n\nsum function — computes the sum of a vector of numbers\n\nsum(c(1,5,8))\n\n[1] 14\n\n\nsort, order, and rev functions — functions for understanding the order or changing the order of a vector\n\nsort(c(3,1,5))\n\n[1] 1 3 5\n\norder(c(3,1,5))\n\n[1] 2 1 3\n\nrev(c(3,1,5))\n\n[1] 5 1 3\n\n\n%% — modulo function (i.e., returns the remainder from dividing one number by another)\n\n8 %% 3\n\n[1] 2\n\n1 %% 3\n\n[1] 1\n\n\n\n\nPractice: The function seq contains an optional argument length.out. Try running the following code and seeing if you can figure out what length.out does.\n\nseq(1,10,length.out=5)\nseq(1,10,length.out=10)\nseq(1.10,length.out=20)\n\n\n\n\n3.2.4 Other types of vectors\nThere are other types of vectors in R too. Probably the main two other types of vectors are character vectors and logical vectors. We’ll talk about character vectors here and defer logical vectors until later. Character vectors are often referred to as strings.\nWe can create a character vector as follows\n\nstring1 &lt;- \"econometrics\"\nstring2 &lt;- \"class\"\nstring1\n\n[1] \"econometrics\"\n\n\nThe above code creates two character vectors and then prints the first one.\n\nSide Comment c stands for “concatenate”. Concatenate is a computer science word that means to combine two vectors. Probably the most well known version of this is “string concatenation” that combines two vectors of characters. Here is an example of string concatenation.\n\nc(string1, string2)\n\n[1] \"econometrics\" \"class\"       \n\n\nSometimes string concatenation means to put two (or more strings) into the same string. This can be done using the paste command in R.\n\npaste(string1, string2)\n\n[1] \"econometrics class\"\n\n\nNotice that paste puts in a space between string1 and string2. For practice, see if you can find an argument to the paste function that allows you to remove the space between the two strings.\n\n\n\n3.2.5 Data Frames\nAnother very important type of object in R is the data frame. I think it is helpful to think of a data frame as being very similar to an Excel spreadsheet — sort of like a matrix or a two-dimensional array. Each row typically corresponds to a particular observation, and each column typically provides the value of a particular variable for that observation.\nJust to give a simple example, suppose that we had firm-level data about the name of the firm, what industry a firm was in, what county they were located in, and their number of employees. I created a data frame like this (it is totally made up, BTW) and show it to you next\n\nfirm_data\n\n\n\n\nname\nindustry\ncounty\nemployees\n\n\n\n\nABC Manufacturing\nManufacturing\nClarke\n531\n\n\nMartin’s Muffins\nFood Services\nOconee\n6\n\n\nDown Home Appliances\nManufacturing\nClarke\n15\n\n\nClassic City Widgets\nManufacturing\nClarke\n211\n\n\nWatkinsville Diner\nFood Services\nOconee\n25\n\n\n\n\nSide Comment: If you are following along on R, I created this data frame using the following code\n\nfirm_data &lt;- data.frame(name=c(\"ABC Manufacturing\", \"Martin\\'s Muffins\", \"Down Home Appliances\", \"Classic City Widgets\", \"Watkinsville Diner\"),\n                        industry=c(\"Manufacturing\", \"Food Services\", \"Manufacturing\", \"Manufacturing\", \"Food Services\"),\n                        county=c(\"Clarke\", \"Oconee\", \"Clarke\", \"Clarke\", \"Oconee\"),\n                        employees=c(531, 6, 15, 211, 25))\n\nThis is also the same data that we loaded earlier in Section 2.3.\n\nOften, we’ll like to access a particular column in a data frame. For example, you might want to calculate the average number of employees across all the firms in our data.\nTypically, the easiest way to do this, is to use the accessor symbol, which is $ in R. This will make more sense with an example:\n\nfirm_data$employees\n\n[1] 531   6  15 211  25\n\n\nfirm_data$employees just provides the column called “employees” in the data frame called “firm_data”. You can also notice that firm_data$employees is just a numeric vector. This means that you can apply any of the functions that we have been covering on it\n\nmean(firm_data$employees)\n\n[1] 157.6\n\nlog(firm_data$employees)\n\n[1] 6.274762 1.791759 2.708050 5.351858 3.218876\n\n\n\nSide Comment: Notice that the function mean and log behave differently. mean calculates the average over all the elements in the vector firm_data$employees and therefore returns a single number. log calculates the logarithm of each element in the vector firm_data$employees and therefore returns a numeric vector with five elements.\n\n\nSide Comment:\nThe $ is not the only way to access the elements in a data frame. You can also access them by their position. For example, if you want whatever is in the third row and second column of the data frame, you can get it by\n\nfirm_data[3,2]\n\n[1] \"Manufacturing\"\n\n\nSometimes it is also convenient to recover a particular row or column by its position in the data frame. Here is an example of recovering the entire fourth row\n\nfirm_data[4,]\n\n                  name      industry county employees\n4 Classic City Widgets Manufacturing Clarke       211\n\n\nNotice that you just leave the “column index” (which is the second one) blank\n\n\nSide Comment: One other thing that sometimes takes some getting used to is that, for programming in general, you have to be very precise. Suppose you were to make a very small typo. R is not going to understand what you mean. See if you can spot the typo in the next line of code.\n\nfirm_data$employes\n\nNULL\n\n\n\nA few more useful functions for working with data frames are:\n\nnrow and ncol — returns the number of rows or columns in the data frame\ncolnames and rownames — returns the names of the columns or rows\n\n\n\n3.2.6 Lists\nVectors and data frames are the main two types of objects that we’ll use this semester, but let me give you a quick overview of a few other types of objects. Let’s start with lists. Lists are very generic in the sense that they can carry around complicated data. If you are familiar with any object oriented programming language like Java or C++, they have the flavor of an “object”, in the object-oriented sense.\nI’m not sure if we will see any examples this semester where you have to use a list. But here is an example. Suppose that we wanted to put the vector that we created earlier five and the data frame that we created earlier firm_data into the same object. We could do it as follows\n\nunusual_list &lt;- list(numbers=five, df=firm_data)\n\nYou can access the elements of a list in a few different ways. Sometimes it is convenient to access them via the $\n\nunusual_list$numbers\n\n[1] 1 2 3 4 5\n\n\nOther times, it is convenient to access them via their position in the list\n\nunusual_list[[2]] # notice the double brackets\n\n                  name      industry county employees\n1    ABC Manufacturing Manufacturing Clarke       531\n2     Martin's Muffins Food Services Oconee         6\n3 Down Home Appliances Manufacturing Clarke        15\n4 Classic City Widgets Manufacturing Clarke       211\n5   Watkinsville Diner Food Services Oconee        25\n\n\n\n\n3.2.7 Matrices\nMatrices are very similar to data frames, but the data should all be of the same type. Matrices are very useful in some numerical calculations that are beyond the scope of this class. Here is an example of a matrix.\n\nmat &lt;- matrix(c(1,2,3,4), nrow=2, byrow=TRUE)\nmat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nYou can access elements of a matrix by their position in the matrix, just like for the data frame above.\n\n# first row, second column\nmat[1,2]\n\n[1] 2\n\n# all rows in second column\nmat[,2] \n\n[1] 2 4\n\n\n\n\n3.2.8 Factors\nSometimes variables in economics are categorical. This sort of variable is somewhat between a numeric variable and a string. In R, categorical variables are called factors.\nA good example of a categorical variable is firm_data$industry. It tells you the “category” of the industry that a firm is in.\nOftentimes, we may have to tell R that a variable is a “factor” rather than just a string. Let’s create a variable called industry that contains the industry from firm_data but as a factor.\n\nindustry &lt;- as.factor(firm_data$industry)\nindustry\n\n[1] Manufacturing Food Services Manufacturing Manufacturing Food Services\nLevels: Food Services Manufacturing\n\n\nA useful package for working with factor variables is the forcats package.\n\n\n3.2.9 Understanding an object in R\nSometimes you may be in the case where there is a variable where you don’t know what exactly it contains. Some functions that are helpful in this case are\n\nclass — tells you, err, the class of an object (i.e., its “type”)\nhead — shows you the “beginning” of an object; this is especially helpful for large objects (like some data frames)\nstr — stands for “structure” of an object\n\nLet’s try these out\n\nclass(firm_data)\n\n[1] \"data.frame\"\n\n# typically would show the first five rows of a data frame,\n# but that is the whole data frame here\nhead(firm_data) \n\n                  name      industry county employees\n1    ABC Manufacturing Manufacturing Clarke       531\n2     Martin's Muffins Food Services Oconee         6\n3 Down Home Appliances Manufacturing Clarke        15\n4 Classic City Widgets Manufacturing Clarke       211\n5   Watkinsville Diner Food Services Oconee        25\n\nstr(firm_data)\n\n'data.frame':   5 obs. of  4 variables:\n $ name     : chr  \"ABC Manufacturing\" \"Martin's Muffins\" \"Down Home Appliances\" \"Classic City Widgets\" ...\n $ industry : chr  \"Manufacturing\" \"Food Services\" \"Manufacturing\" \"Manufacturing\" ...\n $ county   : chr  \"Clarke\" \"Oconee\" \"Clarke\" \"Clarke\" ...\n $ employees: num  531 6 15 211 25\n\n\n\nPractice: Try running class, head, and str on the vector five that we created earlier.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#logicals",
    "href": "02-programming_in_R.html#logicals",
    "title": "3  Programming in R",
    "section": "3.3 Logicals",
    "text": "3.3 Logicals\nRelated Reading: IDS 2.9\nAll programming languages have ways of tracking whether variables meet certain criteria. These are often called Booleans or Logicals. For us, this will particularly come up in the context of subsetting data (i.e., selecting data based on some condition) and in running particular portions of code based on some condition.\nSome main logical operators are ==, &lt;=, &gt;=, &lt;, &gt; corresponding to whether or not two things are equal, less than or equal to, greater than or equal, strictly less than, and strictly greater than. These can be applied to vectors. And the comparisons result in either TRUE or FALSE. Here are some examples\n\nfive &lt;- c(1,2,3,4,5)\n\n# only 3 is equal to 3\nfive == 3\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n# 1,2,3 are all less than or equal to 3\nfive &lt;= 3\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\n# 3,4,5, are all greater than or equal to 3\nfive &gt;= 3\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n# 1,2 are strictly less than 3\nfive &lt; 3\n\n[1]  TRUE  TRUE FALSE FALSE FALSE\n\n# 4,5 are strictly greater than 3\nfive &gt; 3\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n\n\nExample: Often, we might be interested in learning about a subset of our data. As a simple example, using our firm_data from earlier, you could imagine being interested in average employment for manufacturing firms.\nWe can do this using the subset function along with the logical operations we’ve learned in this section.\n\nmanufacturing_firms &lt;- subset(firm_data, industry==\"Manufacturing\")\nmean(manufacturing_firms$employees)\n\n[1] 252.3333\n\n\nAs practice, try creating a subset of firm_data based on firms having more than 100 employees.\n\n\n3.3.1 Additional Logical Operators\nRelated Reading: IDS 2.9\nThere are a number of additional logical operators that can be useful in practice. Here, we quickly cover several more.\n\n!= — not equal\n\nc(1,2,3) != 3\n\n[1]  TRUE  TRUE FALSE\n\n\nWe can link together multiple logical comparisons. If we want to check whether multiple conditions hold, we can use “logical AND” &; if we want to check whether any of multiple conditions hold, we can use “logical OR” |.\n\n# AND\n( c(1,2,3,4,5) &gt;= 3 ) & ( c(1,2,3,4,5) &lt; 5 )\n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\n# OR\n( c(1,2,3,4,5) &gt;= 4 ) | ( c(1,2,3,4,5) &lt; 2 )\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n\n%in% — checks whether the elements of one vector show up in another vector\n\n# 1 is in the 2nd vector, but 7 is not\nc(1,7) %in% c(1,2,3,4,5)\n\n[1]  TRUE FALSE\n\n\nOften it is useful to check whether any logical conditions are true or all logical conditions are true. This can be done as follows\n\n# this one is TRUE because 1 is in the 2nd vector  \nany(c(1,7) %in% c(1,2,3,4,5))\n\n[1] TRUE\n\n# this one is FALSE because 7 is not in the 2nd vector\nall(c(1,7) %in% c(1,2,3,4,5))\n\n[1] FALSE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#programming-basics",
    "href": "02-programming_in_R.html#programming-basics",
    "title": "3  Programming in R",
    "section": "3.4 Programming basics",
    "text": "3.4 Programming basics\n\n3.4.1 Writing functions\nRelated Reading: IDS 3.2\nIt is often helpful to write your own functions in R. If you ever find yourself repeating the same code over and over, this suggests that you should write this code as a function and repeatedly call the function.\nSuppose we are interesting in solving the quadratic equation \\[\n  ax^2 + bx + c = 0\n\\] If you remember the quadratic formula, the solution to this equation is \\[\n  x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}\n\\] It would be tedious to calculate this by hand (especially if we wanted to calculate it for many different values of \\(a\\), \\(b\\), and \\(c\\)), so let’s write a function to do it.\n\nquadratic_solver &lt;- function(a, b, c) {\n  root1 &lt;- ( -b + sqrt(b^2 - 4*a*c) ) / 2*a\n  root1\n}\n\nBefore we try this out, let’s notice a few things. First, while this particular function is for solving the quadratic equation, this is quite representative of what a function looks like in R.\n\nquadratic_solver — This is the name of the function. It’s good to give your function a descriptive name related to what it does. But you could call it anything you want. If you wanted to call this function uga, it would still work.\nthe part &lt;- function finishes off assigning the function the name quadratic_solver and implies that we are writing down a function rather than a vector or data.frame or something else. This part will show up in all function definitions.\nthe part (a, b, c), a, b, and c are the names of the arguments to the function. In a minute when we call the function, we need to tell the function the particular values of a, b, and c for which to solve the quadratic equation. We could name these whatever we want, but, again, it is good to have descriptive names. When you write a different function, it can have as many arguments as you want it to have.\nthe part { ... } everything that the function does should go between the curly brackets\nthe line root1 &lt;- ( -b + sqrt(b^2 - 4*a*c) ) / 2*a contains the main thing that is calculated by our function. Notice that we only calculate one of the “roots” (i.e., solutions to the quadratic equation) because of the \\(+\\) in this expression.\nthe line root1 R returns whatever variable is on the last line of the function. It might be somewhat more clear to write return(root1). The behavior of the code would be exactly the same, but it is just the more common “style” in R programming to not include the explicit return.\n\nNow let’s try out our function\n\n# solves quadratic equation for a=1, b=4, c=3\nquadratic_solver(1,4,3)\n\n[1] -1\n\n# solves quadratic equation for a=-1, b=5, c=10\nquadratic_solver(-1,5,10)\n\n[1] -1.531129\n\n\nTwo last things that are worth pointing out about functions:\n\nFunctions in R can be set up to take default values for some of their arguments\nBecause the arguments have names, if you are explicit about the name of the argument, then the order of the argument does not matter.\n\nTo give examples, let’s write a slightly modified version of our function to solve quadratic equations.\n\nquadratic_solver2 &lt;- function(a=1, b, c) {\n  root1 &lt;- ( -b + sqrt(b^2 - 4*a*c) ) / 2*a\n  root1\n}\n\nThe only thing different here is that a takes the default value of 1. Now let’s try some different calls to quadratic_solver and quadratic_solver2\n\n# solve again for a=1,b=4,c=3\nquadratic_solver2(b=4,c=3)\n\n[1] -1\n\n# replace default and change order\nquadratic_solver2(c=10,b=5,a=-1)\n\n[1] -1.531129\n\n# no default set for quadratic_solver so it will crash if a not provided\nquadratic_solver(b=4,c=3)\n\nError in quadratic_solver(b = 4, c = 3): argument \"a\" is missing, with no default\n\n\n\n\n3.4.2 if/else\nRelated Reading: IDS 3.1\nOften when writing code, you will want to do different things depending on some condition. Let’s write a function that takes in the number of employees that are in a firm and prints “large” if the firm has more than 100 employees and “small” otherwise.\n\nlarge_or_small &lt;- function(employees) {\n  if (employees &gt; 100) {\n    print(\"large\")\n  } else {\n    print(\"small\")\n  }\n}\n\nI think, at this point, this code should make sense to you. The only new thing is the if/else. The following is not code that will actually run but is just to help understand the logic of if/else.\n\nif (condition) {\n  # do something\n} else {\n  # do something else\n}\n\nAll that happens with if/else is that we check whether condition evaluate to TRUE or FALSE. If it is TRUE, the code will do whatever is inside the first set of brackets; if it is FALSE, the code will do whatever is in the set of brackets following else.\n\n\n3.4.3 for loops\nRelated Reading: IDS 3.4\nOften, we need to run the same code over and over again. A for loop is a main programming tool for this case (for loops show up in pretty much all programming languages).\nWe’ll have more realistic examples later on in the semester, but we’ll do something trivial for now.\n\nout &lt;- c()\nfor (i in 1:10) {\n  out[i] &lt;- i*3\n}\nout\n\n [1]  3  6  9 12 15 18 21 24 27 30\n\n\nThe above code, starts with \\(i=1\\), calculates \\(i*3\\) (which is 3), and then stores that result in the first element of the vector out, then \\(i\\) increases to 2, the code calculates \\(i*3\\) (which is now 6), and stores this result in the second element of out, and so on through \\(i=10\\).\n\n\n3.4.4 Vectorization\nRelated Reading: IDS 3.5\nVectorizing functions is a relatively advanced topic in R programming, but it is an important one, so I am including it here.\nBecause we will often be working with data, we will often be performing the same operation on all of the observations in the data. For example, suppose that you wanted to take the logarithm of the number of employees for all the firms in firm_data. One way to do this is to use a for loop, but this code would be a bit of a mess. Instead, the function log is vectorized — this means that if we apply it to a vector, it will calculate the logarithm of each element in the vector. Besides this, vectorized functions are often faster than for loops.\nNot all functions are vectorized though. Let’s go back to our function earlier called large_or_small. This took in the number of employees at a firm and then printed “large” if the firm had more than 100 employees and “small” otherwise. Let’s see what happens if we call this function on a vector of employees (Ideally, we’d like the function to be applied to each element in the vector).\n\nemployees &lt;- firm_data$employees\nemployees\n\n[1] 531   6  15 211  25\n\nlarge_or_small(employees)\n\nError in if (employees &gt; 100) {: the condition has length &gt; 1\n\n\nThis is not what we wanted to have happen. Instead of determining whether each firm was large or small, we get an error basically said that something may be going wrong here. What’s going on here is that the function large_or_small is not vectorized.\nIn order to vectorize a function, we can use one of a number of “apply” functions in R. I’ll list them here\n\nsapply — this stands for “simplify” apply; it “applies” the function to all the elements in the vector or list that you pass in and then tries to “simplify” the result\nlapply — stands for “list” apply; applies a function to all elements in a vector or list and then returns a list\nvapply — stands for “vector” apply; applies a function to all elements in a vector or list and then returns a vector\napply — applies a function to either the rows or columns of a matrix-like object (i.e., a matrix or a data frame) depending on the value of the argument MARGIN\n\nLet’s use sapply to vectorize large_or_small.\n\nlarge_or_small_vectorized &lt;- function(employees_vec) {\n  sapply(employees_vec, FUN = large_or_small)\n}\n\nAll that this will do is call the function large_or_small for each element in the vector employees. Let’s see it in action\n\nlarge_or_small_vectorized(employees)\n\n[1] \"large\"\n[1] \"small\"\n[1] \"small\"\n[1] \"large\"\n[1] \"small\"\n\n\n[1] \"large\" \"small\" \"small\" \"large\" \"small\"\n\n\nThis is what we were hoping for.\n\nSide Comment: I also typically replace most all for loops with an apply function. In most cases, I don’t think there is much of a performance gain, but the code seems easier to read (or at least more concise).\nEarlier we wrote a function to take a vector of numbers from 1 to 10 and multiply all of them by 3. Here’s how you could do this using sapply\n\nsapply(1:10, function(i) i*3)\n\n [1]  3  6  9 12 15 18 21 24 27 30\n\n\nwhich is considerably shorter.\nOne last thing worth pointing out though is that multiplication is already vectorized, so you don’t actually need to do sapply or the for loop; a better way is just\n\n(1:10)*3\n\n [1]  3  6  9 12 15 18 21 24 27 30\n\n\n\n\nSide Comment: A relatively popular alternative to apply functions are map functions provided in the purrr package.\n\n\nSide Comment: It’s often helpful to have a vectorized version of if/else. In R, this is available in the function ifelse. Here is an alternative way to vectorize the function large_or_small:\n\nlarge_or_small_vectorized2 &lt;- function(employees_vec) {\n  ifelse(employees_vec &gt; 100, \"large\", \"small\")\n}\nlarge_or_small_vectorized2(firm_data$employees)\n\n[1] \"large\" \"small\" \"small\" \"large\" \"small\"\n\n\nHere you can see that ifelse makes every comparison in its first argument, and then returns the second element for every TRUE coming from the first argument, and returns the third element for every FALSE coming from the first argument.\nifelse also works with vectors in the second and third element. For example:\n\n  ifelse(c(1,3,5) &lt; 4, yes=c(1,2,3), no=c(4,5,6))\n\n[1] 1 2 6\n\n\nwhich picks up 1 and 2 from the second (yes) argument and 6 from the third (no) argument.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#reproducible-research",
    "href": "02-programming_in_R.html#reproducible-research",
    "title": "3  Programming in R",
    "section": "3.5 Reproducible Research",
    "text": "3.5 Reproducible Research\nRelated Reading: IDS Ch. 20\nR has very useful tools for mix code and writing to produce reports (or, for our purpose, homeworks and project).\nAs of this writing, the most common way to do this seems to be switching from Rmarkdown to Quarto. I am going to explain how to create a Quarto document below, but, if you are familiar with Rmarkdown, the workflow is going to be very similar.\nTo create a new Quaro document, click File -&gt; New File -&gt; Quarto Document. That will open up a menu that looks like the following\n\nYou will need to set a few options. I set the title to be Quarto Example and author to be Brantly Callaway. I also unchecked the box at the bottom that says “Use visual markdown editor” [I prefer this setting to be unchecked, but you can try it both ways and see what you like.] Now click “Create” and your screen will look something like this\n\nWhen you save a Quarto file, you should save it with a .qmd extension.\nHere is a quick example of how you could use Quarto to write homework solutions. Suppose the first homework of the semester asked you to write a function called sum10 that took in a vector of numbers and calculated the sum of the first 10 numbers in the vector. Then, report sum10(1:100).\n---\ntitle: \"Homework 1\"\nauthor: \"Brantly Callaway\"\nformat: html\n---\n\n## Question 1\n\nTo calculate the sum of the first ten numbers in a vector, \nI wrote the following function:\n\n```{r}\nsum10 &lt;- function(x) {\n  # get first 10 elements of x\n  x10 &lt;- x[1:10] \n  # calculate their sum and return it\n  sum(x10) \n}\n\nsum10(1:100)\n```\nA few last comments for you:\n\nThese notes are written in Quarto, and I write homework solutions in Quarto too.\nIf you are interested, you can view the source for this book at http://github.com/bcallaway11/econ_4750_notes. The source code for this chapter is in the file 02-programming_in_R.qmd.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#advanced-topics",
    "href": "02-programming_in_R.html#advanced-topics",
    "title": "3  Programming in R",
    "section": "3.6 Advanced Topics",
    "text": "3.6 Advanced Topics\nTo conclude this section, I want to briefly point you towards some advanced material. We will probably brush up against some of this material this semester. That being said, R has some very advanced capabilities related to data science, data manipulation, and data visualization. If you have time/interest you might push further in all of these directions. By the end of the semester, we may not have mastered these topics, but they should at least be accessible to you.\n\n3.6.1 Tidyverse\nRelated Reading: IDS Chapter 4 — strongly recommend that you read this\n• R has very good data cleaning / manipulating tools\n\nMany of them are in the “tidyverse”\nMostly this semester, I’ll just give you a data set that is ready to be worked with. But as you move to your own research projects or do work for a company one day, you will realize that a major step in analyzing data is organizing (“cleaning”) the data in a way that you can analyze it\n\n• Main packages\n\nggplot2 – see below\ndplyr — package to manipulate data\ntidyr — more ways to manipulate data\nreadr — read in data\npurrr — alternative versions of apply functions and for loops\ntibble — alternative versions of data.frame\nstringr — tools for working with strings\nforcats — tools for working with factors\n\n• If you see code that uses the pipe operator %&gt;%, it is tidyverse-style code. [You need to load a package to get access to the pipe function. I think this was introduced in the magrittr package, but you can also load it with the dplyr package, which is one of the main tidyverse packages.] This is unusual syntax for most programming languages, but it is (arguably) easier to read. Basically the pipe operator takes the result from one line of code and “pipes” it into the first argument of the next function. Here is an example\n\nlibrary(dplyr) # or library(magrittr)\nfirm_data %&gt;%\n  subset(employees &gt; 100) %&gt;%\n  nrow()\n\n[1] 2\n\n\nWhat the above code does is it takes the data frame firm_data, subsets it to firms that have more than 100 rows, and calculates the number of rows in this subset (i.e., the number of large firms).\nIt is equivalent to the following, more traditional-looking code:\n\nlarge_firms &lt;- subset(firm_data, employees &gt; 100)\nnrow(large_firms)\n\n[1] 2\n\n\n• I won’t emphasize the tidyverse too much as I prefer (at least to some extent) writing code with a more traditional syntax. That said, tidyverse packages are really quite useful for data cleaning / wrangling. And, if you are interested, these are good (and marketable) skills to have.\n\n\n3.6.2 Data Visualization\nRelated Reading: IDS Ch. 6-10 — R has very good data visualization tools. I strongly recommend that you read this.\n\nAnother very strong point of R\nBase R comes with the plot command, but the ggplot2 package provides cutting edge plotting tools. These tools will be somewhat harder to learn, but we’ll use ggplot2 this semester as I think it is worth it.\nYou can produce professional quality plots in R that are publication ready\n\nWe will use ggplot2 this semester, but I will save a longer discussion for later.\n\n\n3.6.3 Version Control\nRelated Reading: IDS Ch. 19\nIf you are interested, GitHub is a very useful version control tool (i.e., keeps track of the version of your project, useful for merging projects, and sharing or co-authoring code) and Dropbox (also useful for sharing code). I use both of these extensively — in general, I use GitHub relatively more for bigger projects and more public projects and Dropbox more for smaller projects and early versions of projects.\n\n\n3.6.4 RStudio Projects\nRelated Reading: IDS 20.1\nYou can create a new project by navigating to File -&gt; New Project. Projects in RStudio give a way to organize your, well… projects. For this course, you don’t necessarily need to use projects, but you could, for example, create separate projects for each of your homeworks. This would give you separate environments for each homework (so you don’t have to worry about accidentally using the same variable name across homeworks leading to any issues) and a separate set of tabs for scripts in each project. Your current project is listed in the very top right corner of the RStudio workspace.\n\n\n3.6.5 Technical Writing Tools\nThis is starting to get beyond the scope of the course, but, especially for students in ECON 6750, I recommend that you look up LaTeX. This is a markup language mainly for technical, academic writing. The big payoff is on writing mathematical equations. The equations in the Course Notes are written in LaTeX. For example, the LaTeX code for the solution to the quadratic equation written above is\n$$\n  x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}\n$$\nwhere the $$ is a delimiter that tells LaTeX to render the text between the delimiters as an equation, \\frac is a command that tells LaTeX to render the text as a fraction, and \\pm is a command that tells LaTeX to render the text as a plus or minus sign.\nAs I mentioned above, the course notes are written in quarto, but it is possible to write entire documents in LaTeX. For example, all of my academic papers are written in pure LaTeX. An easy way to get started is to use the website Overleaf. This is a website that allows you to write LaTeX documents in your web browser. Writing homework solutions fully in LaTex would be overkill for this course, but (especially if you are thinking about doing a Ph.D. in economics), it would be a good thing to poke around with as you have time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#lab-1-introduction-to-r-programming",
    "href": "02-programming_in_R.html#lab-1-introduction-to-r-programming",
    "title": "3  Programming in R",
    "section": "3.7 Lab 1: Introduction to R Programming",
    "text": "3.7 Lab 1: Introduction to R Programming\nFor this lab, we will do several practice problems related to programming in R.\n\nCreate two vectors as follows\nx &lt;- seq(2,10,by=2)\ny &lt;- c(3,5,7,11,13)\nAdd x and y, subtract y from x, multiply x and y, and divide x by y and report your results.\nThe geometric mean of a set of numbers is an alternative measure of central tendency to the more common “arithmetic mean” (this is the mean that we are used to). For a set of \\(J\\) numbers, \\(x_1,x_2,\\ldots,x_J\\), the geometric mean is defined as\n\\[\n   (x_1 \\cdot x_2 \\cdot \\cdots \\cdot x_J)^{1/J}\n\\]\nWrite a function called geometric_mean that takes in a vector of numbers and computes their geometric mean. Compute the geometric mean of c(10,8,13)\nUse the lubridate package to figure out how many days elapsed between Jan. 1, 1981 and Jan. 10, 2022.\nmtcars is one of the data frames that comes packaged with base R.\n\nHow many observations does mtcars have?\nHow many columns does mtcars have?\nWhat are the names of the columns of mtcars?\nPrint only the rows of mtcars for cars that get at least 20 mpg\nPrint only the rows of mtcars that get at least 20 mpg and have at least 100 horsepower (it is in the column called hp)\nPrint only the rows of mtcars that have 6 or more cylinders (it is in the column labeld cyl) or at least 100 horsepower\nRecover the 10th row of mtcars\nSort the rows of mtcars by mpg (from highest to lowest)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#lab-1-solutions",
    "href": "02-programming_in_R.html#lab-1-solutions",
    "title": "3  Programming in R",
    "section": "3.8 Lab 1: Solutions",
    "text": "3.8 Lab 1: Solutions\n1.\n\nx &lt;- seq(2,10,by=2)\ny &lt;- c(3,5,7,11,13)\n\nx+y\n\n[1]  5  9 13 19 23\n\nx-y\n\n[1] -1 -1 -1 -3 -3\n\nx*y\n\n[1]   6  20  42  88 130\n\nx/y\n\n[1] 0.6666667 0.8000000 0.8571429 0.7272727 0.7692308\n\n\n2.\n\ngeometric_mean &lt;- function(x) {\n  J &lt;- length(x)\n  res &lt;- prod(x)^(1/J)\n  res\n}\n\ngeometric_mean(c(10,8,13))\n\n[1] 10.13159\n\n\n3.\n\nfirst_date &lt;- lubridate::mdy(\"01-01-1981\")\nsecond_date &lt;- lubridate::mdy(\"01-10-2022\")\nsecond_date - first_date\n\nTime difference of 14984 days\n\n\n4.\n\n\n\n\nnrow(mtcars)\n\n[1] 32\n\n\n\n\n\n\nncol(mtcars)\n\n[1] 11\n\n\n\n\n\n\ncolnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\n\n\n\nsubset(mtcars, mpg &gt;= 20)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n\n\n\nsubset(mtcars, (mpg &gt;= 20) & (hp &gt;= 100))\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n\n\n\nsubset(mtcars, (cyl &gt;= 6) | (hp &gt;= 100))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n\n\n\nmtcars[10,]\n\n          mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 280 19.2   6 167.6 123 3.92 3.44 18.3  1  0    4    4\n\n\n\n\n\n\n# without reversing the order, we would order from lowest to smallest\nmtcars[rev(order(mtcars$mpg)),]\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "02-programming_in_R.html#coding-exercises",
    "href": "02-programming_in_R.html#coding-exercises",
    "title": "3  Programming in R",
    "section": "3.9 Coding Exercises",
    "text": "3.9 Coding Exercises\n\nThe stringr package contains a number of functions for working with strings. For this problem create the following character vector in R\nx &lt;- c(\"economics\", \"econometrics\", \"ECON 4750\")\nInstall the stringr package and use the str_length function in the package in order to calculate the length (number of characters) in each element of x.\nFor this problem, we are going to write a function to calculate the sum of the numbers from 1 to \\(n\\) where \\(n\\) is some positive integer. There are actually a lot of different ways to do this.\n\nApproach 1: write a function called sum_one_to_n_1 that uses the R functions seq to create a list of numbers from 1 to \\(n\\) and then the function sum to sum over that list.\nApproach 2: The sum of numbers from 1 to \\(n\\) is equal to \\(n(n+1)/2\\). Use this expression to write a function called sum_one_to_n_2 to calculate the sum from 1 to \\(n\\).\n\nApproach 3: A more brute force approach is to create a list of numbers from 1 to \\(n\\) (you can use seq here) and add them up using a for loop — basically, just keep track of what the current total is and add the next number to the total in each iteration of the for loop. Write a function called sum_one_to_n_3 that does this.\n\nHint: All of the functions should look like\nsum_one_to_n &lt;- function(n) {\n  # do something\n}\nTry out all three approaches that you came up with above for \\(n=100\\). What is the answer? Do you get the same answer using all three approaches?\nThe Fibonacci sequence is the sequence of numbers \\(0,1,1,2,3,5,8,13,21,34,55,\\ldots\\) that comes from starting with \\(0\\) and \\(1\\) and where each subsequent number is the sum of the previous two. For example, the 5 in the sequence comes from adding 2 and 3; the 55 in the sequence comes from adding 21 and 34.\n\nWrite a function called fibonacci that takes in a number n and computes the nth element in the Fibonacci sequence. For example fibonacci(5) should return 3 and fibonacci(8) should return 13.\nConsider an alternative sequence where, starting with the third element, each element is computed as the sum of the previous two elements (the same as with the Fibonacci sequence) but where the first two elements can be arbitrary. Write a function alt_seq(a,b,n) where a is the first element in the sequence, b is the second element in the sequence, and n is which element in the sequence to return. For example, if \\(a=3\\) and \\(b=7\\), then the sequence would be \\(3,7,10,17,27,44,71,\\ldots\\) and alt_seq(a=3,b=7,n=4) = 17.\n\nThis problem involves writing functions related to computing prime numbers. Recall that a prime number is a positive integer whose only (integer) factors are 1 and itself (e.g., \\(6\\) is not prime because it factors into \\(2\\times 3\\), but \\(5\\) is a prime number because its only factors are \\(1\\) and \\(5\\)).\nFor this problem, you cannot use any built-in functions in R for computing prime numbers or checking whether or not a number is a prime number. However, a helpful function for this problem is the modulo function, %% discussed earlier in the notes. Hint: Notice that 6 %% 2 = 0 indicates that 2 is a factor of 6; on the other hand, if you divide \\(5\\) by any integer small than itself (except for \\(1\\)), the remainder will always be non-zero.\n\nWrite a function is_prime that takes x as an argument and returns TRUE if x is a prime number and returns FALSE if x is not a prime number.\nWrite a function prime that takes n as an argument and returns a vector of all the prime numbers from \\(1\\) to \\(n\\). If it is helpful, prime can call the function is_prime that you wrote for part (a).\n\nBase R includes a data frame called iris. This is data about iris flowers (you can read the details by running ?iris).\n\nHow many observations are there in the entire data frame?\nCalculate the average Sepal.Length across all observations in iris.\nCalculate the average Sepal.Width among the setosa iris species.\nSort iris by Petal.Length and print the first 10 rows.\n\nOne of the examples that we gave above was about writing a function to solve quadratic equations, but, in the code presented above, we only returned one solution to the quadratic equation. Write a function quadratic_solver that takes in a, b, and c as arguments and returns both solutions to the quadratic equation in a list. For example, quadratic_solver(1,4,3) should return a list with two elements, -1 and -3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming in R</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html",
    "href": "03-random_variables.html",
    "title": "4  Random Variables",
    "section": "",
    "text": "4.1 Data for this chapter\nFor this chapter, we’ll use data from the U.S. Census Bureau from 2019. It is not quite a full census, but we’ll treat it as the population throughout this chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html#random-variables",
    "href": "03-random_variables.html#random-variables",
    "title": "4  Random Variables",
    "section": "4.2 Random Variables",
    "text": "4.2 Random Variables\nSW 2.1\nA random variable is a numerical summary of some random event.\nSome examples:\n\nOutcome of roll of a die\nA person’s height in inches\nA firm’s profits in a particular year\nCreating a random variable sometime involves “coding” non-numeric outcomes, e.g., setting hair=1 if a person’s hair color is black, hair=2 if a person’s hair is blonde, etc.\n\nWe’ll generally classify random variables into one of two categories\n\nDiscrete — A random variable that takes on discrete values such as 0, 1, 2\nContinuous — Takes on a continuum of values\n\nThese are broad categories because a lot of random variables in economics sit in between these two.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html#pdfs-pmfs-and-cdfs",
    "href": "03-random_variables.html#pdfs-pmfs-and-cdfs",
    "title": "4  Random Variables",
    "section": "4.3 pdfs, pmfs, and cdfs",
    "text": "4.3 pdfs, pmfs, and cdfs\nSW 2.1\nThe distribution of a random variable describes how likely it is take on certain values.\nA random variable’s distribution is fully summarized by its:\n\nprobability mass function (pmf) if the random variable is discrete\nprobability density function (pdf) if the random variable is continuous\n\nThe pmf is somewhat easier to explain, so let’s start there. For some discrete random variable \\(X\\), its pmf is given by\n\\[\n  f_X(x) = \\P(X=x)\n\\] That is, the probability that \\(X\\) takes on some particular value \\(x\\).\n\nExample:  Suppose that \\(X\\) denotes the outcome of a roll of a die. Then, \\(f_X(1)\\) is the probability of rolling a one. And, in particular,\n\\[\n  f_X(1) = \\P(X=1) = \\frac{1}{6}\n\\]\n\n\nExample:  Let’s do a bit more realistic example where we look at the pmf of education in the U.S. Suppose that \\(X\\) denotes the years of education that a person has. Then, \\(f_X(x)\\) is the probability that a person has exactly \\(x\\) years of education. We can set \\(x\\) to different values and calculate the probabilities of a person having different amounts of education. That’s what we do in the following figure:\n\n\n\n\n\npmf of U.S. education\n\n\n\n\nThere are some things that are perhaps worth pointing out here. The most common amount of education in the U.S. appears to be exactly 12 years — corresponding to graduating from high school; about 32% of the population has that level of education. The next most common number of years of education is 16 — corresponding to graduating from college; about 24% of individuals have this level of education. Other relatively common values of education are 13 years (14% of individuals) and 18 (13% of individuals). About 1% of individuals report 0 years of education. It’s not clear to me whether or not that is actually true or reflects some individuals mis-reporting their education.\n\nBefore going back to the pdf, let me describe another way to fully summarize the distribution of a random variable.\n\nCumulative distribution function (cdf) - The cdf of some random variable \\(X\\) is defined as\n\n\\[\n  F_X(x) = \\P(X \\leq x)\n\\] In words, this cdf is the probability that the random \\(X\\) takes a value less than or equal to \\(x\\).\n\nExample:  Suppose \\(X\\) is the outcome of a roll of a die. Then, \\(F_X(3) = \\P(X \\leq 3)\\) is the probability of rolling 3 or lower. Thus,\n\\[\n  F_X(3) = \\P(X \\leq 3) = \\frac{1}{2}\n\\]\n\n\nExample:  Let’s go back to our example of years of education in the U.S. In this case, \\(F_X(x)\\) is the fraction of the population that has less than \\(x\\) years of education. We can calculate this for different values of \\(x\\). That’s what we do in the following figure:\n\n\n\n\n\ncdf of U.S. educ\n\n\n\n\nYou can see that the cdf is increasing in the years of education. And there are big “jumps” in the cdf at values of years of education that are common such as 12 and 16.\n\nWe’ll go over some properties of pmfs and cdfs momentarily (perhaps you can already deduce some of them from the above figures), but before we do that, we need to go over some (perhaps new) tools.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html#summation-operator",
    "href": "03-random_variables.html#summation-operator",
    "title": "4  Random Variables",
    "section": "4.4 Summation operator",
    "text": "4.4 Summation operator\nIt will be convenient for us to have a notation that allows us to add up many numbers/variables at the same time. To do this, we’ll introduce the \\(\\sum\\) operation.\nAs a simple example, suppose that we have three variables (it doesn’t matter if they are random or not): \\(x_1,x_2,x_3\\) and we want to add them up. Then, we can write \\[\n  \\sum_{i=1}^3 x_i := x_1 + x_2 + x_3\n\\] Many times, once we have data, there will be n “observations” and we can add them up by: \\[\n  \\sum_{i=1}^n x_i = x_1 + x_2 + \\cdots + x_n\n\\] Properties:\n\nFor any constant \\(c\\),\n\\[\n\\sum_{i=1}^n c = n \\cdot c\n\\]\n[This is just the definition of multiplication]\nFor any constant c,\n\\[\n   \\sum_{i=1}^n c x_i = c \\sum_{i=1}^n x_i\n\\]\nIn words: constants can be moved out of the summation.\nWe will use the property often throughout the semester.\nAs an example,\n\\[\n   \\begin{aligned}\n   \\sum_{i=1}^3 7 x_i &= 7x_1 + 7x_2 + 7x_3 \\\\\n   &= 7(x_1 + x_2 + x_3) \\\\\n   &= 7 \\sum_{i=1}^3 x_i\n   \\end{aligned}\n\\]\nwhere the first line is just the definition of the summation, the second equality factors out the 7, and the last equality writes the part about adding up the \\(x\\)’s using summation notation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html#properties-of-pmfs-and-cdfs",
    "href": "03-random_variables.html#properties-of-pmfs-and-cdfs",
    "title": "4  Random Variables",
    "section": "4.5 Properties of pmfs and cdfs",
    "text": "4.5 Properties of pmfs and cdfs\nLet’s define the support of a random variable \\(X\\) — this is the set of all possible values that \\(X\\) can possibly take. We’ll use the notation \\(\\mathcal{X}\\) to denote the support of \\(X\\).\n\nExample: Suppose \\(X\\) is the outcome from a roll of a die. Then, the support of \\(X\\) is given by \\(\\mathcal{X} = \\{1,2,3,4,5,6\\}\\). In other words, the only possible values for \\(X\\) are from \\(1,\\ldots,6\\).\n\n\nExample: Suppose \\(X\\) is the number of years of education that a person has. The support of \\(X\\) is given by \\(\\mathcal{X} = \\{0, 1, 2, \\ldots, 20\\}\\). Perhaps I should have chosen a larger number than 20 to be the maximum possible value that \\(X\\) could take, but you will get the idea — a person’s years of education can be 0 or 1 or 2 or up to some maximum value.\n\nProperties of pmfs\n\nFor any \\(x\\), \\(0 \\leq f_X(x) \\leq 1\\)\nIn words: the probability of \\(X\\) taking some particular value can’t be less than 0 or greater than 1 (neither of those would make any sense)\n\\(\\sum_{x \\in \\mathcal{X}} f_X(x) = 1\\)\nIn words: if you add up \\(\\P(X=x)\\) across all possible values that \\(X\\) could take, they sum to 1.\n\nProperties of cdfs for discrete random variables\n\nFor any \\(x\\), \\(0 \\leq F_X(x) \\leq 1\\)\nIn words: the probability that \\(X\\) is less than or equal to some particular value \\(x\\) has to be between 0 and 1.\nIf \\(x_1 &lt; x_2\\), then \\(F_X(x_1) \\leq F_X(x_2)\\)\nIn words: the cdf is increasing in \\(x\\) (e.g., it will always be the case that \\(\\P(X \\leq 3) \\leq \\P(X \\leq 4)\\)).\n\\(F_X(-\\infty)=0\\) and \\(F_X(\\infty)=1\\)\nIn words: if you choose small enough values of \\(x\\), the probability that \\(X\\) will be less than that is 0; similar (but opposite) logic applies for big values of \\(x\\).\n\nConnection between pmfs and cdfs\n\n\\(F_X(x) = \\displaystyle \\sum_{z \\in \\mathcal{X} \\\\ z \\leq x} f_X(z)\\)\nIn words: you can “recover” the cdf from the pmf by adding up the pmf across all possible values that the random variable could take that are less than or equal to \\(x\\). This will be clearer with an example:\n\n\nExample: Suppose that \\(X\\) is the outcome of a roll of a die. Earlier we showed that \\(F_X(3) = 1/2\\). We can calculate this by\n\\[\n  \\begin{aligned}\n  F_X(3) &= \\sum_{\\substack{z \\in \\mathcal{X} \\\\ z \\leq 3}} f_X(z) \\\\\n  &= \\sum_{z=1}^3 f_X(z) \\\\\n  &= f_X(1) + f_X(2) + f_X(3) \\\\\n  &= \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} \\\\\n  &= \\frac{1}{2}\n  \\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html#continuous-random-variables",
    "href": "03-random_variables.html#continuous-random-variables",
    "title": "4  Random Variables",
    "section": "4.6 Continuous Random Variables",
    "text": "4.6 Continuous Random Variables\nSW 2.1\nFor continuous random variables, you can define the cdf in exactly the same way as we did for discrete random variables. That is, if \\(X\\) is a continuous random variable,\n\\[\n  F_X(x) = \\P(X \\leq x)\n\\]\n\nExample:  Suppose \\(X\\) denotes an individual’s yearly wage income. The cdf of \\(X\\) looks like\n\n\n\n\n\ncdf of U.S. wage income\n\n\n\n\nFrom the figure, we can see that about 24% of working individuals in the U.S. each $20,000 or less per year, 61% of working individuals earn $50,000 or less, and 88% earn $100,000 or less.\n\nIt’s trickier to define an analogue to the pmf for a continuous random variable (in fact, this is the main reason for our separate treatment of discrete and continuous random variables). For example, suppose \\(X\\) denotes the length of a phone conversation. As long as we can measure time finely enough, the probability that a phone conversation lasts exactly 1189.23975381 seconds (this is about 20 minutes) is 0. Instead, for a continuous random variable, we’ll define its probability density function (pdf) as the derivative of its cdf, that is,\n\\[\n  f_X(x) := \\frac{d \\, F_X(x)}{d \\, x}\n\\] Recall that the slope of the cdf will be larger in places where \\(F_X(x)\\) is “steeper”.\nRegions where the pdf is larger correspond to more likely values of \\(X\\) — in this sense the pdf is very similar to the pmf.\nWe can also write the cdf as an integral over the pdf. That is,\n\\[\n  F_X(x) = \\int_{-\\infty}^x f_X(z) \\, dz\n\\] Integration is roughly the continuous version of a summation — thus, this expression is very similar to the expression above for the cdf in terms of the pmf when \\(X\\) is discrete.\nMore properties of cdfs\n\n\\(\\P(X &gt; x) = 1 - \\P(X \\leq x) = 1-F_X(x)\\)\nIn words, if you want to calculate the probability that \\(X\\) is greater than some particular value \\(x\\), you can do that by calculating \\(1-F_X(x)\\).\n\\(\\P(a \\leq X \\leq b) = F_X(b) - F_X(a)\\)\nIn words: you can also calculate the probability that \\(X\\) falls in some range using the cdf.\n\n\nExample:  Suppose \\(X\\) denotes an individual’s yearly wage income. The pdf of \\(X\\) looks like\n\n\n\n\n\npdf of U.S. wage income\n\n\n\n\nFrom the figure, we can see that the most common values of yearly income are around $25-30,000 per year. Notice that this corresponds to the steepest part of the cdf from the previous figure. The right tail of the distribution is also long. This means that, while incomes of $150,000+ are not common, there are some individuals who have incomes that high.\nMoreover, we can use the properties of pdfs/cdfs above to calculate some specific probabilities. In particular, we can calculating probabilities by calculating integrals (i.e., regions under the curve) / relating the pdf to the cdf. First, the red region above corresponds to the probability of a person’s income being between $50,000 and $100,000. This is given by \\(F(100,000) - F(50000)\\). We can compute this in R using the ecdf function. In particular,\n\nincwage_cdf &lt;- ecdf(us_data$incwage)\nround(incwage_cdf(100000) - incwage_cdf(50000),3)\n\n[1] 0.27\n\n\nThe green region in the figure is the probability of a person’s income being above $150,000. Using the above properties of cdfs, we can calculate it as \\(1-F(150000)\\) which is\n\nround(1-incwage_cdf(150000), 3)\n\n[1] 0.052",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "03-random_variables.html#multiple-random-variables",
    "href": "03-random_variables.html#multiple-random-variables",
    "title": "4  Random Variables",
    "section": "4.7 Multiple Random Variables",
    "text": "4.7 Multiple Random Variables\nSW 2.3\nMost often in economics, we want to consider two (or more) random variables jointly rather than just a single random variable. For example, mean income is interesting, but mean income as a function of education is more interesting.\nWhen there is more than one random variable, you can define joint pmfs, joint pdfs, and joint cdfs.\nLet’s quickly go over these for the case where \\(X\\) and \\(Y\\) are two discrete random variables.\nJoint pmf: \\(f_{X,Y}(x,y) := \\P(X=x, Y=y)\\)\nJoint cdf: \\(F_{X,Y}(x,y) := \\P(X \\leq x, Y \\leq y)\\)\nConditional pmf: \\(f_{Y|X}(y|x) := \\P(Y=y | X=x)\\)\nProperties\nWe use the notation that \\(\\mathcal{X}\\) denotes the support of \\(X\\) and \\(\\mathcal{Y}\\) denotes the support of \\(Y\\).\n\n\\(0 \\leq f_{X,Y}(x,y) \\leq 1\\) for all \\(x,y\\)\nIn words: the probability of \\(X\\) and \\(Y\\) taking any particular values can’t be less than 0 or greater than 1 (because these are probabilities)\n\\(\\displaystyle \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} f_{X,Y}(x,y) = 1\\)\nIn words: If you add up \\(\\P(X=x, Y=y)\\) across all possible values of \\(x\\) and \\(y\\), they sum up to 1 (again, this is just a property of probabilities)\nIf you know the joint pmf, then you can recover the marginal pmf, that is,\n\\[\nf_Y(y) = \\sum_{x \\in \\mathcal{X}} f_{X,Y}(x,y)\n\\]\nThis amounts to just adding up the joint pmf across all values of \\(x\\) while holding \\(y\\) fixed. A main takeaway from this property is the following: if you know the joint pmf of two random variables, then it implies that you know the pmf of each random variable individuals. Thus, if you know the joint pmf, it implies that you know more than if you only knew the marginal pmfs.\n\n\nExample: Suppose that you roll a die, and based on this roll, you create the following random variables.\n\\[\n  X = \\begin{cases} 0 \\quad \\textrm{if roll is 3 or lower} \\\\\n  1 \\quad \\textrm{if roll is greater than 3} \\end{cases} \\qquad Y = \\begin{cases} 0 \\quad \\textrm{if roll is odd} \\\\\n  1 \\quad \\textrm{if roll is even} \\end{cases}\n\\]\nLet’s consider what values \\(X\\) and \\(Y\\) take for different rolls:\n\n\n\nroll\nX\nY\n\n\n\n\n1\n0\n0\n\n\n2\n0\n1\n\n\n3\n0\n0\n\n\n4\n1\n1\n\n\n5\n1\n0\n\n\n6\n1\n1\n\n\n\nThus,\n\\[\n\\begin{aligned}\n  f_{X,Y}(0, 0) = \\frac{2}{6} \\qquad \\qquad f_{X,Y}(0,1) = \\frac{1}{6} \\\\\n  f_{X,Y}(1, 0) = \\frac{1}{6} \\qquad \\qquad f_{X,Y}(1,1) = \\frac{2}{6}\n\\end{aligned}\n\\] and you can immediately see that the first two properties hold here. For the third property, suppose that we want to calculate \\(f_Y(1)\\) (i.e., the probability that we roll an even number). The property says that we can calculate it \\[\n  \\begin{aligned}\n  f_Y(1) &= \\sum_{x=0}^1 f_{X,Y}(x,1) \\\\\n  &= f_{X,Y}(0,1) + f_{X,Y}(1,1) = \\frac{1}{6} + \\frac{2}{6} = \\frac{1}{2}\n  \\end{aligned}\n\\] which, as we know, is the right answer.\n\n\\(X\\) and \\(Y\\) are said to be independent if \\(f_{Y|X}(y|x) = f_Y(y)\\). In other words, if knowing the value of \\(X\\) doesn’t provide any information about the distribution \\(Y\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-expectations.html",
    "href": "04-expectations.html",
    "title": "5  Expectation, Variance, and More",
    "section": "",
    "text": "5.1 Expected Values\nSW 2.2\nThe expected value of some random variable \\(X\\) is its (population) mean and is written as \\(\\E[X]\\). [I tend to write \\(\\E[X]\\) for the expected value, but you might also see notation like \\(\\mu\\) or \\(\\mu_X\\) for the expected value.]\nThe expected value of a random variable is a feature of its distribution. In other words, if you know the distribution of a random variable, then you also know its mean.\nThe expected value is a measure of central tendency (alternative measures of central tendency are the median and mode).\nExpected values are a main concept in the course (and in statistics/econometrics more generally). I think there are two main reasons for this:\nIf \\(X\\) is a discrete random variable, then the expected value is defined as\n\\[\n  \\E[X] = \\sum_{x \\in \\mathcal{X}} x f_X(x)\n\\]\nIf \\(X\\) is a continuous random variable, then the expected value is defined as\n\\[\n  \\E[X] = \\int_{\\mathcal{X}} x f_X(x) \\, dx\n\\] Either way, you can think of these as a weighted average of all possible realizations of the random variable \\(X\\) where the weights are given by the probability of \\(X\\) taking that particular value. This may be more clear with an example…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#expected-values",
    "href": "04-expectations.html#expected-values",
    "title": "5  Expectation, Variance, and More",
    "section": "",
    "text": "Unlike a cdf, pdf, or pmf, the expected value is a single number. This means that it is easy to report. And, if you only knew one feature (at least a feature that that only involves a single number) of the distribution of some random variable, probably the feature that would be most useful to know would be the mean of the random variable.\nBesides that, there are some computational reasons (we will see these later) that the mean can be easier to estimate than, say, the median of a random variable\n\n\n\n\n\n\nExample: Suppose that \\(X\\) is the outcome from a roll of a die. Then, its expected value is given by\n\\[\n  \\begin{aligned}\n  \\E[X] &= \\sum_{x=1}^6 x f_X(x) \\\\\n  &= 1\\left(\\frac{1}{6}\\right) + 2\\left(\\frac{1}{6}\\right) + \\cdots + 6\\left(\\frac{1}{6}\\right) \\\\\n  &= 3.5\n  \\end{aligned}\n\\]\n\n\nSide-Comment: When we start to consider more realistic/interesting applications, we typically won’t know (or be able to easily figure out) \\(\\E[X]\\). Instead, we’ll try to estimate it using available data. We’ll carefully distinguish between population quantities like \\(\\E[X]\\) and sample quantities like an estimate of \\(\\E[X]\\) soon.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#variance",
    "href": "04-expectations.html#variance",
    "title": "5  Expectation, Variance, and More",
    "section": "5.2 Variance",
    "text": "5.2 Variance\nSW 2.2\nThe next most important feature of the distribution of a random variable is its variance. The variance of a random variable \\(X\\) is a measure of its “spread”, and we will denote it \\(\\Var(X)\\) [You might also sometimes see the notation \\(\\sigma^2\\) or \\(\\sigma_X^2\\) for the variance.] The variance is defined as\n\\[\n  \\Var(X) := \\E\\left[ (X - \\E[X])^2 \\right]\n\\] Before we move forward, let’s think about why this is a measure of the spread of a random variable.\n\n\\((X-\\E[X])^2\\) is a common way to measure the “distance” between \\(X\\) and \\(\\E[X]\\). It is always positive (whether \\((X - \\E[X])\\) is positive or negative) which is a good feature for a measure of distance to have. It is also increasing in \\(|X-\\E[X]|\\) which also seems a requirement for a reasonable measure of distance.\nThen, the outer expectation averages the above distance across the distribution of \\(X\\).\n\nAn alternative expression for \\(\\Var(X)\\) that is often useful in calculations is\n\\[\n  \\Var(X) = \\E[X^2] - \\E[X]^2\n\\]\nSometimes, we will also consider the standard deviation of a random variable. The standard deviation is defined as\n\\[\n  \\textrm{sd}(X) := \\sqrt{\\Var(X)}\n\\] You might also see the notation \\(\\sigma\\) or \\(\\sigma_X\\) for the standard deviation.\nThe standard deviation is often easier to interpret than the variance because it has the same “units” as \\(X\\). Variance “units” are squared units of \\(X\\).\nThat said, variances more often show up in formulas/derivations this semester.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#mean-and-variance-of-linear-functions",
    "href": "04-expectations.html#mean-and-variance-of-linear-functions",
    "title": "5  Expectation, Variance, and More",
    "section": "5.3 Mean and Variance of Linear Functions",
    "text": "5.3 Mean and Variance of Linear Functions\nSW 2.2\nFor this part, suppose that \\(Y=a + bX\\) where \\(Y\\) and \\(X\\) are random variables while \\(a\\) and \\(b\\) are fixed constants.\nProperties of Expectations\n\n\\(\\E[a] = a\\) [In words: the expected value of a constant is just the constant. This holds because there is nothing random about \\(a\\) — we just know what it is.]\n\\(\\E[bX] = b\\E[X]\\) [In words: the expected value of a constant times a random variable is equal to the constant times the expected value of the random variable. We will use this property often this semester.]\n\\(\\E[a + bX] = a + b\\E[X]\\) [In words: expected values “pass through” sums. We will use this property often this semester.]\n\nYou’ll also notice the similarity between the properties of summations and expectations. This is not a coincidence — it holds because expectations are defined as summations (or very closely related, as integrals).\nProperties of Variance\n\n\\(\\Var(a) = 0\\) [In words: the variance of a constant is equal to 0.]\n\\(\\Var(bX) = b^2 \\Var(X)\\) [In words: A constant can come out of the variance, but it needs to be squared first.]\n\\(\\Var(a + bX) = \\Var(bX) = b^2 \\Var(X)\\)\n\n\nExample: Later on in the semester, it will sometimes be convenient for us to “standardize” some random variables. We’ll talk more about the reason to do this later, but for now, I’ll just give the typical formula for standardizing a random variable and we’ll see if we can figure out what the mean and variance of the standardized random variable are.\n\\[\n  Y = \\frac{ X - \\E[X]}{\\sqrt{\\Var(X)}}\n\\] Just to be clear here, we are standardizing the random variable \\(X\\) and calling its standardized version \\(Y\\). Let’s calculate its mean\n\\[\n  \\begin{aligned}\n    \\E[Y] &= \\E\\left[ \\frac{X - \\E[X]}{\\sqrt{\\Var(X)}} \\right] \\\\\n    &= \\frac{1}{\\sqrt{\\Var(X)}} \\E\\big[ X - \\E[X] \\big] \\\\\n    &= \\frac{1}{\\sqrt{\\Var(X)}} \\left( \\E[X] - \\E\\big[\\E[X]\\big] \\right) \\\\\n    &= \\frac{1}{\\sqrt{\\Var(X)}} \\left( \\E[X] - \\E[X] \\right) \\\\\n    &= 0\n  \\end{aligned}\n\\] where the first equality just comes from the definition of \\(Y\\), the second equality holds because \\(1/\\sqrt{\\Var(X)}\\) is a constant and can therefore come out of the expectation, the third equality holds because the expectation can pass through the difference, the fourth equality holds because \\(\\E[X]\\) is a constant and therefore \\(\\E\\big[\\E[X]\\big] = \\E[X]\\), and the last equality holds because the term in parentheses is equal to 0. Thus, the mean of \\(Y\\) is equal to 0. Now let’s calculate the variance.\n\\[\n  \\begin{aligned}\n  \\Var(Y) &= \\Var\\left( \\frac{X}{\\sqrt{\\Var(X)}} - \\frac{\\E[X]}{\\sqrt{\\Var(X)}} \\right) \\\\\n  &= \\Var\\left( \\frac{X}{\\sqrt{\\Var(X)}}\\right) \\\\\n  &= \\left( \\frac{1}{\\sqrt{\\Var(X)}} \\right)^2 \\Var(X) \\\\\n  &= \\frac{\\Var(X)}{\\Var(X)} \\\\\n  &= 1\n  \\end{aligned}\n\\] where the first equality holds by the definition of \\(Y\\), the second equality holds because the second term is a constant and by Variance Property 3 above, the third equality holds because \\((1/\\sqrt{\\Var(X)})\\) is a constant and can come out of the variance but needs to be squared, the fourth equality holds by squaring the term on the left, and the last equality holds by cancelling the numerator and denominator.\nTherefore, we have showed that the mean of the standardized random variable is 0 and its variance is 1. This is, in fact, the goal of standardizing a random variable — to transform it so that it has mean 0 and variance 1 and the particular transformation given in this example is one that delivers a new random variable with these properties.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#conditional-expectations",
    "href": "04-expectations.html#conditional-expectations",
    "title": "5  Expectation, Variance, and More",
    "section": "5.4 Conditional Expectations",
    "text": "5.4 Conditional Expectations\nSW 2.3\nAs we discussed earlier, in economics, we often want to learn about the relationship between several different variables. Previously, we had discussed joint pmfs/pdfs/cdfs, and these are useful to know about. However, they are often hard to work with in practice. For example, if you have two random variables, visualizing their joint distribution would involve interpreting a 3D plot which is often challenging in practice. If you had more than two random variables, then fully visualizing their joint distribution would not be possible. Therefore, we will typically look at summaries of the joint distribution. Probably the most useful one is the conditional expectation that we study in this section; in fact, we will spend much of the semester trying to estimate conditional expectations.\nFor two random variables, \\(Y\\) and \\(X\\), the conditional expectation of \\(Y\\) given \\(X=x\\) is the mean value of \\(Y\\) conditional on \\(X\\) taking the particular value \\(x\\). In math, this is written\n\\[\n  \\E[Y|X=x]\n\\]\nOne useful way to think of a conditional expectation is as a function of \\(x\\). For example, suppose that \\(Y\\) is a person’s yearly income and \\(X\\) is a person’s years of education. Clearly, mean income can change for different values of education.\nConditional expectations will be a main focus of ours throughout the semester\nAn extremely useful property of conditional expectations is that they generalize from the case with two variables to the case with multiple variables. For example, suppose that we have four random variables \\(Y\\), \\(X_1\\), \\(X_2\\), and \\(X_3\\). It makes sense to think about \\[\n  \\E[Y|X_1=x_1, X_2=x_2, X_3=x_3]\n\\] which is the expected value of \\(Y\\) conditional on \\(X_1\\) taking the particular value \\(x_1\\), \\(X_2\\) taking the particular value \\(x_2\\), and \\(X_3\\) taking the particular value \\(x_3\\). Just like before, you can think of this as a function, in the sense that changing any of \\(x_1\\), \\(x_2\\), and/or \\(x_3\\) gives a new conditional mean.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#law-of-iterated-expectations",
    "href": "04-expectations.html#law-of-iterated-expectations",
    "title": "5  Expectation, Variance, and More",
    "section": "5.5 Law of Iterated Expectations",
    "text": "5.5 Law of Iterated Expectations\nSW 2.3\nAnother important property of conditional expectations is called the law of iterated expectations. It says that\n\\[\n  \\E[Y] = \\E\\big[ \\E[Y|X] \\big]\n\\] In words: The expected value of \\(Y\\) is equal to the expected value (this expectation is with respect to \\(X\\)) of the conditional expectation of \\(Y\\) given \\(X\\).\nThis may seem like a technical property, but I think the right way to think about the law of iterated expectations is that there is an inherent relationship between unconditional expectations and conditional expectations. In other words, although conditional expectations can vary arbitrarily for different values of \\(X\\), if you know what the conditional expectations are, the overall expected value of \\(Y\\) is fully determined.\nA simple example is one where \\(X\\) takes only two values. Suppose we are interested in mean birthweight (\\(Y\\)) for children of mother’s who either drank alcohol during their pregnancy (\\(X=1\\)) or who didn’t drink alcohol during their pregnancy (\\(X=0\\)). Suppose the following (just to be clear, these are completely made up numbers), \\(\\E[Y|X=1] = 7\\), \\(\\E[Y|X=0]=8\\) \\(\\P(X=1) = 0.1\\) and \\(\\P(X=0)=0.9\\). The law of iterated expectation says that \\[\n  \\begin{aligned}\n  \\E[Y] &= \\E\\big[ \\E[Y|X] \\big] \\\\\n  &= \\sum_{x \\in \\mathcal{X}} \\E[Y|X=x] \\P(X=x) \\\\\n  &= \\E[Y|X=0]\\P(X=0) + \\E[Y|X=1]\\P(X=1) \\\\\n  &= (8)(0.9) + (7)(0.1) \\\\\n  &= 7.9\n  \\end{aligned}\n\\]\nThe law of iterated expectations still applies in more complicated cases (e.g., \\(X\\) takes more than two values, \\(X\\) is continuous, or \\(X_1\\),\\(X_2\\),\\(X_3\\)) but the intuition is still the same.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#covariance",
    "href": "04-expectations.html#covariance",
    "title": "5  Expectation, Variance, and More",
    "section": "5.6 Covariance",
    "text": "5.6 Covariance\nSW 2.3\nThe covariance between two random variables \\(X\\) and \\(Y\\) is a masure of the extent to which they “move together”. It is defined as\n\\[\n  \\Cov(X,Y) := \\E[(X-\\E[X])(Y-\\E[Y])]\n\\] A natural first question to ask is: why does this measure how \\(X\\) and \\(Y\\) move together. Notice that covariance can be positive or negative. It will tend to be negative if big values of \\(X\\) (so that \\(X\\) is above its mean) tend to happen at the same time as big values of \\(Y\\) (so that \\(Y\\) is above its mean) while small values of \\(X\\) (so that \\(X\\) is below its mean) tend to happen at the same time as small values of \\(Y\\) (so that \\(Y\\) is below its mean).\nAn alternative and useful expression for covariance is \\[\n  \\Cov(X,Y) = \\E[XY] - \\E[X]\\E[Y]\n\\] Relative to the first expression, this one is probably less of a natural definition but often more useful in mathematical problems.\nOne more thing to notice, if \\(X\\) and \\(Y\\) are independent, then \\(\\Cov(X,Y) = 0\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#correlation",
    "href": "04-expectations.html#correlation",
    "title": "5  Expectation, Variance, and More",
    "section": "5.7 Correlation",
    "text": "5.7 Correlation\nSW 2.3\nIt’s often hard to interpret covariances directly (the “units” are whatever the units of \\(X\\) are times the units of \\(Y\\)), so it is common to scale the covariance to get the correlation between two random variables:\n\\[\n  \\Corr(X,Y) := \\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)} \\sqrt{\\Var(Y)}}\n\\] The correlation has the property that it is always between \\(-1\\) and \\(1\\).\nIf \\(\\Corr(X,Y) = 0\\), then \\(X\\) and \\(Y\\) are said to be uncorrelated.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#properties-of-expectationsvariances-of-sums-of-rvs",
    "href": "04-expectations.html#properties-of-expectationsvariances-of-sums-of-rvs",
    "title": "5  Expectation, Variance, and More",
    "section": "5.8 Properties of Expectations/Variances of Sums of RVs",
    "text": "5.8 Properties of Expectations/Variances of Sums of RVs\nSW 2.3\nHere are some more properties of expectations and variances when there are multiple random variables. For two random variables \\(X\\) and \\(Y\\)\n\n\\(\\E[X+Y] = \\E[X] + \\E[Y]\\)\n\\(\\Var(X+Y) = \\Var(X) + \\Var(Y) + 2\\Cov(X,Y)\\)\n\nThe first property is probably not surprising — expectations continue to pass through sums. The second property, particularly the covariance term, needs more explanation. To start with, you can just plug \\(X+Y\\) into the definition of variance and (with a few lines of algebra) show that the second property is true. But, for the intuition, let me explain with an example. Suppose that \\(X\\) and \\(Y\\) are rolls of two dice, but somehow these dice are positively correlated with each other — i.e., both rolls coming up with high numbers (and low numbers) are more likely than with regular dice. Now, think about what the sum of two dice rolls can be: the smallest possible sum is 2 and other values are possible up to 12. Moreover, the smallest and largest possible sum of the rolls (2 and 12), which are farthest away from the mean value of 7, are relatively uncommon. You have to roll either \\((1,1)\\) or \\((6,6)\\) to get either of these and the probability of each of those rolls is just \\(1/36\\). However, when the dice are positively correlated, the probability of both rolls being very high or very low becomes more likely — thus, since outcomes far away from the mean become more likely, the variance increases.\nOne last comment here is that, when \\(X\\) and \\(Y\\) are independent (or even just uncorrelated), the formula for the variance does not involve the extra covariance term because it is equal to 0.\nThese properties for sums of random variables generalize to the case with more than two random variables. For example, suppose that \\(Y_1, \\ldots, Y_n\\) are random variables, then\n\n\\(\\E\\left[ \\displaystyle \\sum_{i=1}^n Y_i \\right] = \\displaystyle \\sum_{i=1}^n \\E[Y_i]\\)\nIf \\(Y_i\\) are mutually independent, then \\(\\Var\\left( \\displaystyle \\sum_{i=1}^n Y_i \\right) = \\displaystyle \\sum_{i=1}^n \\Var(Y_i)\\)\n\nNotice that the last line does not involve any covariance terms, but this is only because of the caveat that the \\(Y_i\\) are mutually independent. Otherwise, there would actually be tons of covariance terms that would need to be accounted for.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#normal-distribution",
    "href": "04-expectations.html#normal-distribution",
    "title": "5  Expectation, Variance, and More",
    "section": "5.9 Normal Distribution",
    "text": "5.9 Normal Distribution\nSW 2.4\nYou probably learned about a lot of particular distributions of random variables in your Stats class. There are a number of important distributions:\n\nNormal\nBinomial\nt-distribution\nF-distribution\nChi-squared distribution\nothers\n\nSW discusses a number of these distributions, and I recommend that you read/review those distributions. For us, the most important distribution is the Normal distribution [we’ll see why a few classes from now].\nIf a random variable \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we write\n\\[\n  X \\sim N(\\mu, \\sigma^2)\n\\] where \\(\\mu = \\E[X]\\) and \\(\\sigma^2 = \\Var(X)\\).\nImportantly, if we know that \\(X\\) follows a normal distribution, its entire distribution is fully characterized by its mean and variance. In other words, if \\(X\\) is normally distributed, and we also know its mean and variance, then we know everything about its distribution. [Notice that this is not generally true — if we did not know the distribution of \\(X\\) but knew its mean and variance, we would know two important features of the distribution of \\(X\\), but we would not know everything about its distribution.]\nYou are probably familiar with the pdf of a normal distribution — it is “bell-shaped”.\n\n\n\n\n\n\n\n\n\nFrom the figure, you can see that a normal distribution is unimodal (there is just one “peak”) and symmetric (the pdf is the same if you move the same distance above \\(\\mu\\) as when you move the same distance below \\(\\mu\\)). This means that, for a random variable that follows a normal distribution, its median and mode are also equal to \\(\\mu\\).\nFrom the plot of the pdf, we can also tell that, if you make a draw from \\(X \\sim N(\\mu,\\sigma^2)\\), the most likely values are near the mean. As you move further away from \\(\\mu\\), it becomes less likely (though not impossible) for a draw of \\(X\\) to take that value.\nRecall that we can calculate the probability that \\(X\\) takes on a value in a range by calculating the area under the curve of the pdf. For each shaded region in the figure, there is a 2.5% chance that \\(X\\) falls into that region (so the probability of \\(X\\) falling into either region is 5%). Another way to think about this is that there is a 95% probability that a draw of \\(X\\) will be in the region \\([\\mu-1.96\\sigma, \\mu+1.96\\sigma]\\). Later, we we talk about hypothesis testing, this will be an important property.\nEarlier, we talked about standardizing random variables. If you know that a random variable follows a normal distribution, it is very common to standardize it. In particular notice that, if you create the standardized random variable\n\\[\n  Z := \\frac{X - \\mu}{\\sigma} \\quad \\textrm{then} \\quad Z \\sim N(0,1)\n\\] If you think back to your probability and statistics class, you may have done things like calculating a p-value by looking at a “Z-table” in the back of a textbook (I’m actually not sure if this is still commonly done because it is often easier to just do this on a computer, but, back in “my day” this was a very common exercise in statistics classes). Standardizing allows you to look at just one table for any normally distributed random variable that you could encounter rather than requiring you to have different Z table for each value of \\(\\mu\\) and \\(\\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#coding",
    "href": "04-expectations.html#coding",
    "title": "5  Expectation, Variance, and More",
    "section": "5.10 Coding",
    "text": "5.10 Coding\nTo conclude this section, we’ll use R to compute the features of the joint distribution of income and education that we have discussed above.\n\n# create vectors of income and educ\nincome &lt;- us_data$incwage\neduc &lt;- us_data$educ\n\n# mean of income\nmean(income)\n\n[1] 58605.75\n\n# mean of education\nmean(educ)\n\n[1] 13.96299\n\n# variance\nvar(income)\n\n[1] 4776264026\n\nvar(educ)\n\n[1] 8.345015\n\n# standard deviation\nsd(income)\n\n[1] 69110.52\n\nsd(educ)\n\n[1] 2.888774\n\n# covariance\ncov(income,educ)\n\n[1] 63766.72\n\n# correlation\ncor(income, educ)\n\n[1] 0.3194011",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#lab-2-basic-plots",
    "href": "04-expectations.html#lab-2-basic-plots",
    "title": "5  Expectation, Variance, and More",
    "section": "5.11 Lab 2: Basic Plots",
    "text": "5.11 Lab 2: Basic Plots\nRelated Reading: IDS 9.4 (if you are interested, you can read IDS Chapters 6-10 for much more information about plotting in R)\nIn this lab, I’ll introduce you to some basic plotting. Probably the most common type of plot that I use is a line plot. We’ll go for trying to make a line plot of average income as a function of education.\nTo start with, I’ll introduce you to R’s ggplot2 package. This is one of the most famous plot-producing packages (not just in R, but for any programming language). The syntax may be somewhat challenging to learn, but I think it is worth it to exert some effort here.\n\nSide Comment: Base R has several plotting functions (e.g., plot). Check IDS 2.15 for an introduction to these functions. These are generally easier to learn but less beautiful than plots coming from ggplot2.\n\n\n# load ggplot2 package\n# (if you haven't installed it, you would need to do that first)\nlibrary(ggplot2)\n\n# load dplyr package for \"wrangling\" data\nlibrary(dplyr)\n\n\n# arrange data\nplot_data &lt;- us_data %&gt;%\n    group_by(educ) %&gt;%\n    summarize(income=mean(incwage))\n\n# make the plot\nggplot(data=plot_data,\n       mapping=aes(x=educ,y=income)) +\n    geom_line() +\n    geom_point(size=3) +\n    theme_bw()\n\n\n\n\n\n\n\n\nLet me explain what’s going on here piece-by-piece. Let’s start with this code\n\n# arrange data\nplot_data &lt;- us_data %&gt;%\n    group_by(educ) %&gt;%\n    summarize(income=mean(incwage))\n\nAt a high-level, making plots often involves two steps — first arranging the data in the “appropriate” way (that’s this step) and then actually making the plot.\nThis is “tidyverse-style” code too — in my view, it is a little awkward, but it is also common so I think it is worth explaining here a bit.\nFirst, the pipe operator, %&gt;% takes the thing on the left of it and applies the function to the right of it. So, the line us_data %&gt;% group_by(educ) takes us_data and applies the function group_by to it, and what we group by is educ. That creates a new data frame (you could just run that code and see what you get). The next line takes that new data frame and applies the function summarize to it. In this case, summarize creates a new variable called income that is the mean of the column incwage and it is the mean by educ (since we grouped by education in the previous step).\nTake a second and look through what has actually been created here. plot_data is a new data frame, but it only has 18 observations — corresponding to each distinct value of education in the data. It also has two columns, the first one is educ which is the years of education, and the second one is income which is the average income among individuals that have that amount of education.\nAn alternative way of writing the exact same code (that seems more natural to me) is\n\n# arrange data\ngrouped_data &lt;- group_by(us_data, educ)\nplot_data &lt;- summarize(grouped_data, income=mean(incwage))\n\nIf you’re familiar with other programming languages, the second version of the code probably seems more familiar. Either way is fine with me — tidyverse-style seems to be trendy in R programming these days, but (for me) I think the second version is a little easier to understand. You can find long “debates” about these two styles of writing code if you happen to be interested…\nBefore moving on, let me mention a few other dplyr functions that you might find useful\n\nfilter — this is tidy version of subset\nselect — selects particular columns of interest from your data\nmutate — creates a new variable from existing columns in your data\narrange — useful for sorting your data\n\nNext, let’s consider the second part of the code.\n\n# make the plot\nggplot(data=plot_data,\n       mapping=aes(x=educ,y=income)) +\n    geom_line() +\n    geom_point(size=3) +\n    theme_bw()\n\nThe main function here is ggplot. It takes in two main arguments: data and mapping. Notice that we set data to be equal to plot_data which is the data frame that we just created. The mapping is set equal to aes(x=educ,y=income). aes stands for “aesthetic”, and here you are just telling ggplot the names of the columns in the data frame that should be on the x-axis (here: educ) and on the y-axis (here: income) in the plot. Also, notice the + at the end of the line; you can interpret this as saying “keep going” to the next line before executing.\nIf we just stopped there, we actually wouldn’t plot anything. We still need to tell ggplot what kind of plot we want to make. That’s where the line geom_line comes in. It tells ggplot that we want to plot a line. Try running the code with just those two lines — you will see that you will get a similar (but not exactly the same) plot.\ngeom_point adds the dots in the figure. size=3 controls the size of the points. I didn’t add this argument originally, but the dots were hard to see so I made them bigger.\ntheme_bw changes the color scheme of the plot. It stands for “theme black white”.\nThere is a ton of flexibility with ggplot — way more than I could list here. But let me give you some extras that I tend to use quite frequently.\n\nIn geom_line and geom_point, you can add the extra argument color; for example, you could try geom_line(color=\"blue\") and it would change the color of the line to blue.\nIn geom_line, you can change the “type” of the line by using the argument linetype; for example, geom_line(linetype=\"dashed\") would change the line from being solid to being dashed.\nIn geom_line, the argument size controls the thickness of the line.\nThe functions ylab and xlab control the labels on the y-axis and x-axis\nThe functions ylim and xlim control the “limits” of the y-axis and x-axis. Here’s how you can use these:\n\n# make the plot\nggplot(data=plot_data,\n     mapping=aes(x=educ,y=income)) +\n  geom_line() +\n  geom_point(size=3) +\n  theme_bw() +\n  ylim=c(0,150000) +\n  ylab(\"Income\") +\n  xlab(\"Education\")\n\nwhich will adjust the y-axis and change the labels on each axis.\n\nBesides the line plot (using geom_line) and the scatter plot (using geom_point), probably two other types of plots that I make the most are\n\nHistogram (using geom_histogram) — this is how I made the plot of the pmf of education earlier in this chapter\nAdding a straight line to a plot (using geom_abline which takes in slope and intercept arguments) — we haven’t used this yet, but we will win once we start talking about regressions\nIf you’re interested, here is a to a large number of different types of plots that are available using ggplot: http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#in-case-youre-interested-chebyshevs-inequality",
    "href": "04-expectations.html#in-case-youre-interested-chebyshevs-inequality",
    "title": "5  Expectation, Variance, and More",
    "section": "5.12 In Case You’re Interested: Chebyshev’s Inequality",
    "text": "5.12 In Case You’re Interested: Chebyshev’s Inequality\nAs discussed above, the standard deviation is a measure of the spread of a random variable that is in units that are easy to understand. But what exactly does it tell us about the spread of a random variable? One way to think about the spread of a random variable is to think about the probability that a random variable will take a value within a certain range. For example, when we talked about the normal distribution, we said that there is a 95% chance that a draw of a normally distributed random variable will be within 1.96 standard deviations of the mean. This is a very specific statement about the spread of a random variable.\nBut the claim that there is a 95% chance that a random variable will be within 1.96 standard deviations of its mean only holds for normally distributed random variables. Chebyshev’s Inequality provides a way to relate the standard deviation of a random variable to the probability that it will take a value within a certain range, for any random variable (i.e., the variable can follow essentially any distribution). Chebyshev’s Inequality says that, for any random variable \\(X\\) and any \\(k &gt; 0\\), \\[\\begin{align*}\n  \\P\\Big(\\big|X - \\E[X]\\big| \\geq k \\cdot \\sd(X) \\Big) \\leq \\frac{1}{k^2}\n\\end{align*}\\] This holds for any \\(k\\), so let’s pick an interesting value and be a bit more specific. Suppose that \\(k=2\\). Then, Chebyshev’s Inequality says that the probability that a random variable will be more than 2 standard deviations away from its mean is less than or equal to \\(1/4\\). Let’s compare this to the case where we know that \\(X\\) follows a normal distribution. In that case, there’s a 95% chance that \\(X\\) will be within (essentially) 2 standard deviations of its mean; if we don’t know that \\(X\\) follows a normal distribution, then we can’t make as strong a statement, but we can say that at least 75% of the time, \\(X\\) will be within 2 standard deviations of its mean. You can try other values of \\(k\\) too.\nI am not going to provide a proof of Chebyshev’s Inequality. However, it is not that complicated. It follows from another result called Markov’s Inequality, where the idea is basically that, if you know the mean of a random variable, it limits the probability that a random variable can take an extreme value. Chebyshev’s Inequality extends this idea to the case where you know the mean and standard deviation of a random variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#coding-questions",
    "href": "04-expectations.html#coding-questions",
    "title": "5  Expectation, Variance, and More",
    "section": "5.13 Coding Questions",
    "text": "5.13 Coding Questions\n\nRun the following code to create the data that we will use in the problem\n\nset.seed(1234) # setting the seed means that we will get the same results\nx &lt;- rexp(100) # make 100 draws from an exponential distribution\n\nUse the ggplot2 package to plot a histogram of x.\nFor this question, we’ll use the data fertilizer_2000. A scatter plot is a useful way to visualize 2-dimensional data. Use the ggplot2 package to make a scatter plot with crop yield (avyield) on the y-axis and fertilizer (avfert) on the x-axis. Label the y-axis “Crop Yield” and the x-axis “Fertilizer”. Do you notice any pattern from the scatter plot?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "04-expectations.html#extra-questions",
    "href": "04-expectations.html#extra-questions",
    "title": "5  Expectation, Variance, and More",
    "section": "5.14 Extra Questions",
    "text": "5.14 Extra Questions\n\nSuppose that \\(\\E[X] = 10\\) and \\(\\var(X) = 2\\). Also, suppose that \\(Y=5 + 9 X\\).\n\nWhat is \\(\\E[Y]\\)?\nWhat is \\(\\var(Y)\\)?\n\nUse the definition of variance to show that \\(\\Var(bX) = b^2 \\Var(X)\\) (where \\(b\\) is a constant and \\(X\\) is some random variable).\nSuppose you are interested in average height of students at UGA. Let \\(Y\\) denote a student’s height; also let \\(X\\) denote a binary variable that is equal to 1 if a student is female. Suppose that you know that \\(\\E[Y|X=1] = 5' \\,4''\\) and that \\(\\E[Y|X=0] = 5' \\,9''\\) (and that \\(\\P(X=1) = 0.5\\)).\n\nWhat is \\(\\E[Y]\\)?\nExplain how the answer to part (a) is related to the Law of Iterated Expectations.\n\nConsider a random variable \\(X\\) with support \\(\\mathcal{X} = \\{2,7,13,21\\}\\). Suppose that it has the following pmf:\n\\[\n    \\begin{aligned}\n     f_X(2) &= 0.5 \\\\\n     f_X(7) &= 0.25 \\\\\n     f_X(13) &= 0.15 \\\\\n     f_X(21) &= ??\n   \\end{aligned}\n\\]\n\nWhat is \\(f_X(21)\\)? How do you know?\nWhat is the expected value of \\(X\\)? [Show your calculation.]\nWhat is the variance of \\(X\\)? [Show your calculation.]\nCalculate \\(F_X(x)\\) for \\(x=1\\), \\(x=7\\), \\(x=8\\), and \\(x=25\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Expectation, Variance, and More</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html",
    "href": "05-finite_sample_properties.html",
    "title": "6  Finite Sample Properties",
    "section": "",
    "text": "6.1 Simple Random Sample\nSW 2.5\nLet’s start by talking about how the data that we have access to is collected. There are several possibilities here, but let us start with the most straightforward case (which is also a very common case) called a simple random sample.\nIn math: \\(\\{Y_i\\}_{i=1}^n\\) is called a simple random sample if \\(Y_1, Y_2, \\ldots, Y_n\\) are independent random variables with a common probability distribution \\(f_Y\\). The two key conditions here are (i) independence and (ii) from a common distribution. For this reason, you may sometimes see a random sample called an iid sample which stands for independent and identically distributed.\nIn words: We have access to \\(n\\) observations that are drawn at random from some underlying population and each observation is equally likely to be drawn.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html#estimating-ey",
    "href": "05-finite_sample_properties.html#estimating-ey",
    "title": "6  Finite Sample Properties",
    "section": "6.2 Estimating \\(\\E[Y]\\)",
    "text": "6.2 Estimating \\(\\E[Y]\\)\nSW 2.5, 3.1\nLet’s start with trying to estimate \\(\\E[Y]\\) as this is probably the simplest, non-trivial thing that we can estimate.\nA natural way to estimate population quantities is with their sample analogue. This is called the analogy principle. This is perhaps technical jargon, but it is the way you would immediately think to estimate \\(\\E[Y]\\):\n\\[\n  \\hat{\\E}[Y] = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\] In this course, we will typically put a “hat” on estimated quantities. The expression \\(\\displaystyle \\frac{1}{n}\\sum_{i=1}^n Y_i\\) is just the average value of \\(Y\\) in our sample. Since we will calculate a ton of averages like this one over the course of the rest of the semester, it’s also convenient to give it a shorthand notation, which is what \\(\\bar{Y}\\) means — it is just the sample average of \\(Y\\).\nOne thing that is important to be clear about at this point is that, in general, \\(\\E[Y] \\neq \\bar{Y}\\). \\(\\E[Y]\\) is a population quantity while \\(\\bar{Y}\\) is a sample quantity. We will hope (and provide some related conditions/discussions below) that \\(\\bar{Y}\\) would be close to \\(\\E[Y]\\), but, in general, they will not be exactly the same.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html#mean-of-bary",
    "href": "05-finite_sample_properties.html#mean-of-bary",
    "title": "6  Finite Sample Properties",
    "section": "6.3 Mean of \\(\\bar{Y}\\)",
    "text": "6.3 Mean of \\(\\bar{Y}\\)\nSW 2.5, 3.1\nAnother important thing to notice about \\(\\bar{Y}\\) is that it is a random variable (as it is the average of random variables). This is in sharp contrast to \\(\\E[Y]\\) which is non-random.\nOne related thought experiment is the following: if we could repeatedly collect new samples of size \\(n\\) from the same population and each time were able to estimate \\(\\bar{Y}\\), these estimates would be different from each other.\nIn fact, this means that \\(\\bar{Y}\\) has a distribution. The distribution of a statistic, like \\(\\bar{Y}\\), is called its sampling distribution. We’d like to know about the features of the sampling distribution. Let’s start with its mean. That is, let’s calculate\n\\[\n  \\begin{aligned}\n    \\E[\\bar{Y}] &= \\E\\left[ \\frac{1}{n} \\sum_{i=1}^n Y_i \\right] \\\\\n    &= \\frac{1}{n} \\E\\left[ \\sum_{i=1}^n Y_i \\right] \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n \\E[Y_i] \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n \\E[Y] \\\\\n    &= \\frac{1}{n} n \\E[Y] \\\\\n    &= \\E[Y]\n  \\end{aligned}\n\\] Let’s think carefully about each step here — the arguments rely heavily on the properties of expectations and summations that we have learned earlier. The first equality holds from the definition of \\(\\bar{Y}\\). The second equality holds because \\(1/n\\) is a constant and can therefore come out of the expectation. The third equality holds because the expectation can pass through the sum. The fourth equality holds because \\(Y_i\\) are all from the same distribution which implies that they all of the same mean and that it is equal to \\(\\E[Y]\\). The fifth equality holds because \\(\\E[Y]\\) is a constant and we add it up \\(n\\) times. And the last equality just cancels the \\(n\\) in the numerator with the \\(n\\) in the denominator.\nBefore moving on, let me make an additional comment:\n\nThe fourth equality might be a little confusing. Certainly it is not saying that all the \\(Y_i\\)’s are equal to each other. Rather, they come from the same distribution. For example, if you roll a die \\(n\\) times, you get different outcomes on different rolls, but they are all from the same distribution so that the population expectation of each roll is always 3.5, but you get different realizations on different particular rolls. Another example is if \\(Y\\) is a person’s income. Again, we are not saying that everyone has the same income, but just that we are thinking of income as being a draw from some distribution — sometimes you get a draw of a person with a very high income; other times you get a draw of a person with a low income, but \\(\\E[Y]\\) is a feature of the underlying distribution itself where these draws come from.\n\nHow should interpret the above result? It says that, \\(\\E[\\bar{Y}] = \\E[Y]\\). This doesn’t mean that \\(\\bar{Y}\\) itself is equal to \\(\\E[Y]\\). Rather, it means that, if we could repeatedly obtain (a huge number of times) new samples of size \\(n\\) and compute \\(\\bar{Y}\\) each time, the average of \\(\\bar{Y}\\) across repeated samples would be equal to \\(\\E[Y]\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html#variance-of-bary",
    "href": "05-finite_sample_properties.html#variance-of-bary",
    "title": "6  Finite Sample Properties",
    "section": "6.4 Variance of \\(\\bar{Y}\\)",
    "text": "6.4 Variance of \\(\\bar{Y}\\)\nSW 2.5, 3.1\nNext, let’s calculate the variance of \\(\\bar{Y}\\). As before, we are continuing with the thought experiment of being able to repeatedly draw new samples of size \\(n\\), and, therefore, we call this variance the sampling variance.\n\\[\n  \\begin{aligned}\n    \\Var(\\bar{Y}) &= \\Var\\left(\\frac{1}{n} \\sum_{i=1}^n Y_i\\right) \\\\\n    &= \\frac{1}{n^2} \\Var\\left(\\sum_{i=1}^n Y_i\\right) \\\\\n    &= \\frac{1}{n^2} \\left( \\sum_{i=1}^n \\Var(Y_i) + \\textrm{lots of covariance terms} \\right) \\\\\n    &= \\frac{1}{n^2} \\left( \\sum_{i=1}^n \\Var(Y_i) \\right) \\\\\n    &= \\frac{1}{n^2} \\sum_{i=1}^n \\Var(Y) \\\\\n    &= \\frac{1}{n^2} n \\Var(Y) \\\\\n    &= \\frac{\\Var(Y)}{n}\n  \\end{aligned}\n\\] Let’s go carefully through each step — these arguments rely heavily on the properties of variance that we talked about earlier. The first equality holds by the definition of \\(\\bar{Y}\\). The second equality holds because \\(1/n\\) is a constant and can come out of the variance after squaring it. The third equality holds because the variance of the sum of random variables is equal to the sum of the variances plus all the covariances between the random variables. In the fourth equality, all of the covariance terms go away — this holds because of random sampling which implies that the \\(Y_i\\) are all independent which implies that their covariances are equal to 0. The fifth equality holds because all \\(Y_i\\) are identically distributed so their variances are all the same and equal to \\(\\Var(Y)\\). The sixth equality holds by adding up \\(\\Var(Y)\\) \\(n\\) times. The last equality holds by canceling the \\(n\\) in the numerator with one of the \\(n\\)’s in the denominator.\nInterestingly, the variance of \\(\\bar{Y}\\) depends not just on \\(\\Var(Y)\\) but also on \\(n\\) — the number of observations in the sample. Notice that \\(n\\) is in the denominator, so the variance of \\(\\bar{Y}\\) will be lower for large values of \\(n\\). Here is an example that may be helpful for understanding this. Suppose that you are rolling a die. If \\(n=1\\), then clearly, the variance of \\(\\bar{Y}\\) is just equal to the variance of \\(Y\\) — sometimes you roll extreme values like \\(1\\) or \\(6\\). Now, when you increase \\(n\\), say, to 10, then these extreme values of \\(\\bar{Y}\\) are substantially less common. For \\(\\bar{Y}\\) to be equal to \\(6\\) in this case, you’d need to roll 10 \\(6\\)’s in a row. This illustrates that the sampling variance of \\(\\bar{Y}\\) is decreasing in \\(n\\). If this is not perfectly clear, we will look at some data soon, and I think that should confirm to you that the variance of \\(\\bar{Y}\\) is decreasing in the sample size.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html#properties-of-estimators",
    "href": "05-finite_sample_properties.html#properties-of-estimators",
    "title": "6  Finite Sample Properties",
    "section": "6.5 Properties of Estimators",
    "text": "6.5 Properties of Estimators\nSW 2.5, 3.1\nSuppose we are interested in some population parameter \\(\\theta\\) — we’ll write this pretty generically now, but it could be \\(\\E[Y]\\) or \\(\\E[Y|X]\\) or really any other population quantity that you’d like to estimate.\nAlso, suppose that we have access to a random sample of size \\(n\\) and we have some estimate of \\(\\theta\\) that we’ll call \\(\\hat{\\theta}\\).\nAs before, we are going to consider the repeated sampling thought experiment where we imagine that we could repeatedly obtain new samples of size \\(n\\) and with each new sample calculate a new \\(\\hat{\\theta}\\). Under this thought experiment, \\(\\hat{\\theta}\\) would have a sampling distribution. One possibility for what it could look like is the following\n\n\n\n\n\n\n\n\n\nIn this case, values of \\(\\hat{\\theta}\\) are more common around 3 and 4, but it is not highly unusual to get a value of \\(\\hat{\\theta}\\) that is around 1 or 2 or 5 or 6 either.\nThe first property of an estimator that we will take about is called unbiasedness. An estimator \\(\\hat{\\theta}\\) is said to be unbiased if \\(\\E[\\hat{\\theta}] = \\theta\\). Alternatively, we can define the bias of an estimator as\n\\[\n  \\textrm{Bias}(\\hat{\\theta}) = \\E[\\hat{\\theta}] - \\theta\n\\] For example, if \\(\\textrm{Bias}(\\hat{\\theta}) &gt; 0\\), it means that, on average (in the repeated sampling thought experiment), our estimates of \\(\\theta\\) would be greater than the actual value of \\(\\theta\\).\nIn general, unbiasedness is a good property for an estimator to have. That being said, we can come up with examples of not-very-good unbiased estimators and good biased estimators, but all-else-equal, it is better for an estimator to be unbiased.\nThe next property of estimators that we will talk about is their sampling variance. This is just \\(\\Var(\\hat{\\theta})\\). In general, we would like estimators with low (or 0) bias and low sampling variance. Let me give an example\n\n\n\n\n\n\n\n\n\nThis is a helpful figure for thinking about the properties of estimators. In this case, \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) are both unbiased (because their means are \\(\\theta\\)) while \\(\\hat{\\theta}_3\\) is biased — it’s mean is greater than \\(\\theta\\). On the other hand the sampling variance of \\(\\hat{\\theta}_2\\) and \\(\\hat{\\theta}_3\\) are about the same and both substantially smaller than for \\(\\hat{\\theta}_1\\). Clearly, \\(\\hat{\\theta}_2\\) is the best estimator of \\(\\theta\\) out of the three. But which is the second best? It is not clear. \\(\\hat{\\theta}_3\\) systematically over-estimates \\(\\theta\\), but since the variance is relatively small, the misses are systematic but tend to be relatively small. On the other hand, \\(\\hat{\\theta}_1\\) is, on average, equal to \\(\\theta\\), but sometimes the estimate of \\(\\theta\\) could be quite poor due to the large sampling variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html#relative-efficiency",
    "href": "05-finite_sample_properties.html#relative-efficiency",
    "title": "6  Finite Sample Properties",
    "section": "6.6 Relative Efficiency",
    "text": "6.6 Relative Efficiency\nSW 3.1\nIf \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) are two unbiased estimators of \\(\\theta\\), then \\(\\hat{\\theta}_1\\) is more efficient than \\(\\hat{\\theta}_2\\) if \\(\\Var(\\hat{\\theta}_1) &lt; \\Var(\\hat{\\theta}_2)\\).\nRelative efficiency gives us a way to rank unbiased estimators.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "05-finite_sample_properties.html#mean-squared-error",
    "href": "05-finite_sample_properties.html#mean-squared-error",
    "title": "6  Finite Sample Properties",
    "section": "6.7 Mean Squared Error",
    "text": "6.7 Mean Squared Error\nMore generally, two estimators can be compared by their mean squared error which is defined as\n\\[\n  \\textrm{MSE}(\\hat{\\theta}) := \\E\\left[ (\\hat{\\theta} - \\theta)^2\\right]\n\\]\nThe mean squared error of \\(\\hat{\\theta}\\) is the average “distance” between \\(\\hat{\\theta}\\) and \\(\\theta\\) in the thought experiment of having repeated samples of size \\(n\\).\nAnother equivalent expression for the mean squared error is\n\\[\n  \\textrm{MSE}(\\hat{\\theta}) = \\textrm{Bias}(\\hat{\\theta})^2 + \\Var(\\hat{\\theta})\n\\] In other words, if we can figure out the bias and variance of \\(\\hat{\\theta}\\), then we can recover mean squared error.\n\nSide-Comment: I think it is worth quickly explaining where the second expression for \\(\\textrm{MSE}(\\hat{\\theta})\\) comes from. Starting from the definition of \\(\\textrm{MSE}(\\hat{\\theta})\\),\n\\[\n  \\begin{aligned}\n    \\textrm{MSE}(\\hat{\\theta}) &= \\E\\left[ (\\hat{\\theta} - \\theta)^2\\right] \\\\\n    &= \\E\\left[ \\left( (\\hat{\\theta} - \\E[\\hat{\\theta}]) + (\\E[\\hat{\\theta}] - \\theta)\\right)^2 \\right] \\\\\n    &= \\E\\left[ (\\hat{\\theta} - \\E[\\hat{\\theta}])^2 \\right] + \\E\\left[ (\\E[\\hat{\\theta}] - \\theta)^2\\right] + 2 \\E\\left[ (\\hat{\\theta} - \\E[\\hat{\\theta}])(\\E[\\hat{\\theta}] - \\theta) \\right] \\\\\n    &= \\Var(\\hat{\\theta}) + \\textrm{Bias}(\\hat{\\theta})^2\n  \\end{aligned}\n\\] where the first equality is just the definition of \\(\\textrm{MSE}(\\hat{\\theta})\\), the second equality adds and subtracts \\(\\E[\\hat{\\theta}]\\), the third equality squares everything in parentheses from the previous line and pushes the expectation through the sum. For the last equality, the first term in the previous line corresponds to the definition of \\(\\Var(\\hat{\\theta})\\); for the second term, recall that \\(\\textrm{Bias}(\\hat{\\theta}) = \\E[\\hat{\\theta}-\\theta]\\) (and this is non-random so the outside expectation just goes away); the last term is equal to 0 which just holds by the properties of expectations after noticing that \\((\\E[\\hat{\\theta}] - \\theta)\\) is non-random and can therefore come out of the expectation.\n\nGenerally, we would like to choose estimators that have low mean squared error (this essentially means that they have low bias and variance). Moreover, mean squared error gives us a way to compare estimators that are potentially biased. [Also, notice that for unbiased estimators, comparing mean squared errors of different estimators just compares their variance (because the bias term is equal to 0), so this is a generalization of relative efficiency from the previous section.]\n\nExample: Let’s compare three estimators of \\(\\E[Y]\\) based on their mean squared error. Let’s consider the three following estimators\n\\[\n  \\begin{aligned}\n    \\hat{\\mu} &:= \\frac{1}{n} \\sum_{i=1}^n Y_i \\\\\n    \\hat{\\mu}_1 &:= Y_1 \\\\\n    \\hat{\\mu}_\\lambda &:= \\lambda \\bar{Y} \\quad \\textrm{for some } \\lambda &gt; 0\n  \\end{aligned}\n\\] \\(\\hat{\\mu}\\) is just the sample average of \\(Y\\)’s that we have already discussed. \\(\\hat{\\mu}_1\\) is the (somewhat strange) estimator of \\(\\E[Y]\\) that just uses the first observation in the data (regardless of the sample size). \\(\\hat{\\mu}_\\lambda\\) is an estimator of \\(\\E[Y]\\) that multiplies \\(\\bar{Y}\\) by some positive constant \\(\\lambda\\).\nTo calculate the mean squared error of each of these estimators, let’s calculate their means and their variances.\n\\[\n  \\begin{aligned}\n    \\E[\\hat{\\mu}] &= \\E[Y] \\\\\n    \\E[\\hat{\\mu}_1] &= \\E[Y_1] = \\E[Y] \\\\\n    \\E[\\hat{\\mu}_\\lambda] &= \\lambda \\E[\\bar{Y}] = \\lambda \\E[Y]\n  \\end{aligned}\n\\] This means that \\(\\hat{\\mu}\\) and \\(\\hat{\\mu}_1\\) are both unbiased. \\(\\hat{\\mu}_\\lambda\\) is biased (unless \\(\\lambda=1\\) though this is a relatively uninteresting case as it would mean that \\(\\hat{\\mu}_\\lambda\\) is exactly the same as \\(\\hat{\\mu}\\)) with \\(\\textrm{Bias}(\\hat{\\mu}_\\lambda) = (\\lambda - 1) \\E[Y]\\).\nNext, let’s calculate the variance for each estimator\n\\[\n  \\begin{aligned}\n  \\Var(\\hat{\\mu}) &= \\frac{\\Var(Y)}{n} \\\\\n  \\Var(\\hat{\\mu}_1) &= \\Var(Y_1) = \\Var(Y) \\\\\n  \\Var(\\hat{\\mu}_\\lambda) &= \\lambda^2 \\Var(\\bar{Y}) = \\lambda^2 \\frac{\\Var(Y)}{n}\n  \\end{aligned}\n\\] This means that we can now calculate mean squared error for each estimator.\n\\[\n  \\begin{aligned}\n    \\textrm{MSE}(\\hat{\\mu}) &= \\frac{\\Var{Y}}{n} \\\\\n    \\textrm{MSE}(\\hat{\\mu}_1) &= \\Var(Y) \\\\\n    \\textrm{MSE}(\\hat{\\mu}_\\lambda) &= (\\lambda-1)^2\\E[Y]^2 + \\lambda^2 \\frac{\\Var(Y)}{n}\n  \\end{aligned}\n\\] The first thing to notice is that \\(\\hat{\\mu}\\) dominates \\(\\hat{\\mu}_1\\) (where dominates means that there isn’t any scenario where you could make a reasonable case that \\(\\hat{\\mu}_1\\) is a better estimator) because its MSE is strictly lower (they tie only if \\(n=1\\) when they become the same estimator). This is probably not surprising — \\(\\hat{\\mu}_1\\) just throws away a lot of potentially useful information.\nThe more interesting case is \\(\\hat{\\mu}_\\lambda\\). The first term is the bias term — it is greater than the bias from \\(\\hat{\\mu}\\) or \\(\\hat{\\mu}_1\\) because the bias of both of these is equal to 0. However, relative to \\(\\hat{\\mu}\\), the variance of \\(\\hat{\\mu}_\\lambda\\) can be smaller when \\(\\lambda\\) is less than 1. In fact, you can show that there are estimators that have smaller mean squared error than \\(\\hat{\\mu}\\) by choosing a \\(\\lambda\\) that is smaller than (usually just slightly smaller than) 1. This sort of estimator would be biased, but are able to compensate introducing some bias by having smaller variance. For now, we won’t talk much about this sort of estimator (and stick to \\(\\bar{Y}\\)), but this sort of estimator has the “flavor” of modern machine learning estimators that typically introduce some bias while reducing variance. One last comment: if you were to make a “bad” choice of \\(\\lambda\\), \\(\\hat{\\mu}_\\lambda\\) could have higher mean squared error than even \\(\\hat{\\mu}_1\\), so if you wanted to proceed this way, you’d have to choose \\(\\lambda\\) with some care.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finite Sample Properties</span>"
    ]
  },
  {
    "objectID": "06-asymptotic_properties.html",
    "href": "06-asymptotic_properties.html",
    "title": "7  Asymptotic Properties",
    "section": "",
    "text": "7.1 Large Sample Properties of Estimators\nSW 2.6\nStatistics/Econometrics often relies on “large sample” (meaning: the number of observations, \\(n\\), is large) properties of estimators.\nIntuition: We generally expect that estimators that use a large number of observations will perform better than in the case with only a few observations.\nThe second goal of this section will be to introduce an approach to conduct hypothesis testing. In particular, we may have some theory and want a way to test whether or not the data that we have “is consistent with” the theory or not. These arguments typically involve either making strong assumptions or having a large sample — we’ll mainly study the large sample case as I think this is more useful.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Asymptotic Properties</span>"
    ]
  },
  {
    "objectID": "06-asymptotic_properties.html#consistency",
    "href": "06-asymptotic_properties.html#consistency",
    "title": "7  Asymptotic Properties",
    "section": "7.2 Consistency",
    "text": "7.2 Consistency\nAn estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is said to be consistent if \\(\\hat{\\theta}\\) gets close to \\(\\theta\\) for large values of \\(n\\).\nThe main tool for studying consistency is the law of large numbers. The law of large numbers says that sample averages converge to population averages as the sample size gets large. In math, this is\n\\[\n  \\frac{1}{n} \\sum_{i=1}^n Y_i \\rightarrow \\E[Y] \\quad \\textrm{as } n \\rightarrow \\infty\n\\] In my view, the law of large numbers is very intuitive. If you have a large sample and calculate a sample average, it should be close to the population average.\n\nExample: Let’s consider the same three estimators as before and whether or not they are consistent. First, the LLN implies that\n\\[\n  \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n Y_i \\rightarrow \\E[Y]\n\\] This implies that \\(\\hat{\\mu}\\) is consistent. Next,\n\\[\n  \\hat{\\mu}_1 = Y_1\n\\] doesn’t change depending on the size of the sample (you just use the first observation), so this is not consistent. This is an example of an unbiased estimator that is not consistent. Next,\n\\[\n  \\hat{\\mu}_\\lambda = \\lambda \\bar{Y} \\rightarrow \\lambda \\E[Y] \\neq \\E[Y]\n\\] which implies that (as long as \\(\\lambda\\) doesn’t change with \\(n\\)), \\(\\hat{\\mu}_{\\lambda}\\) is not consistent. Let’s give one more example. Consider the estimator\n\\[\n  \\hat{\\mu}_c := \\bar{Y} + \\frac{c}{n}\n\\] where \\(c\\) is some constant (this is a strange estimate of \\(\\E[Y]\\) where we take \\(\\bar{Y}\\) and add a constant divided by the sample size). In this case,\n\\[\n  \\hat{\\mu}_c \\rightarrow \\E[Y] + 0 = \\E[Y]\n\\]\nwhich implies that it is consistent. It is interesting to note that\n\\[\n  \\E[\\hat{\\mu}_c] = \\E[Y] + \\frac{c}{n}\n\\] which implies that it is biased. This is an example of a biased estimator that is consistent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Asymptotic Properties</span>"
    ]
  },
  {
    "objectID": "06-asymptotic_properties.html#asymptotic-normality",
    "href": "06-asymptotic_properties.html#asymptotic-normality",
    "title": "7  Asymptotic Properties",
    "section": "7.3 Asymptotic Normality",
    "text": "7.3 Asymptotic Normality\nThe next large sample property that we’ll talk about is asymptotic normality. This is a hard one to wrap your mind around, but I’ll try to explain as clearly as possible. We’ll start by talking about what it is, and then we’ll move to why it’s useful.\nMost of the estimators that we will talk about this semester have the following property\n\\[\n  \\sqrt{n}\\left( \\hat{\\theta} - \\theta \\right) \\rightarrow N(0,V) \\quad \\textrm{as } n \\rightarrow \\infty\n\\] In words, what this says is that we can learn something about the sampling distribution of \\(\\hat{\\theta}\\) as long as we have a large enough sample. More specifically, if \\(\\hat{\\theta}\\) is asymptotically normal, it means that if we take \\(\\hat{\\theta}\\) subtract the true value of the parameter \\(\\theta\\) (this is often referred to as “centering”) and multiply by \\(\\sqrt{n}\\), then that object (as long as the sample size is large enough) will seem like a draw from a normal distribution with mean 0 and variance \\(V\\). Since we know lots about normal distributions, we’ll be able to exploit this in very useful ways in the next section.\nAn equivalent, alternative expression that is sometimes useful is\n\\[\n  \\frac{\\sqrt{n}\\left( \\hat{\\theta} - \\theta\\right)}{\\sqrt{V}} \\rightarrow N(0,1) \\quad \\textrm{as } n \\rightarrow \\infty\n\\]\nTo establish asymptotic normality of a particular estimator, the main tool is the central limit theorem. The central limit theorem (sometimes abbreviated CLT) says that\n\\[\n  \\sqrt{n}\\left( \\frac{1}{n} \\sum_{i=1}^n Y_i - \\E[Y]\\right) \\rightarrow N(0,V) \\quad \\textrm{as } n \\rightarrow \\infty\n\\] where \\(V = \\Var(Y)\\).\nIn words, the CLT says that if you take the difference between \\(\\bar{Y}\\) and \\(\\E[Y]\\) (which, by the LLN converges to 0 as \\(n \\rightarrow \\infty\\)) and “scale it up” by \\(\\sqrt{n}\\) (which goes to \\(\\infty\\) as \\(n \\rightarrow \\infty\\)), then \\(\\sqrt{n}(\\bar{Y} - \\E[Y])\\) will act like a draw from a normal distribution with variance \\(\\Var(Y)\\).\nThere are a few things to point out:\n\nJust to start with, this is not nearly as “natural” a result as the LLN. The LLN basically makes perfect sense. For me, I know how to prove the CLT (though we are not going to do it in class), but I don’t think that I would have ever been able to come up with this on my own.\nNotice that the CLT does not rely on any distributional assumptions. We do not need to assume that \\(Y\\) follows a normal distribution and it will apply when \\(Y\\) follows any distribution (up to some relatively minor technical conditions that we will not worry about).\nIt is also quite remarkable. We usually have the sense that as the sample size gets large that things will converge to something (e.g., LLN saying that sample averages converge to population averages) or that they will diverge (i.e., go off to positive or negative infinity themselves). The CLT provides an intermediate case — \\(\\sqrt{n}(\\bar{Y} - \\E[Y])\\) is neither converging to a particular value or diverging to infinity. Instead, it is converging in distribution — meaning: it is settling down to something that looks like a draw from some distribution rather than converging to a particular number.\nIn some sense, you can think of this “convergence in distribution” as a “tie” between the part \\((\\bar{Y}-\\E[Y])\\) which, by itself, is converging to 0, and \\(\\sqrt{n}\\) which, by itself, is diverging to infinity. In particular, notice that \\[\\begin{align*}\n\\Var\\Big( \\sqrt{n} (\\bar{Y} - \\E[Y]) \\Big) &= n \\times \\Var(\\bar{Y}) \\\\\n&= n \\times \\frac{\\Var(Y)}{n} \\\\\n&= \\Var(Y)\n\\end{align*}\\] where this argument just holds by the properties of variance that we have used many times before. This means that the variance of \\(\\sqrt{n}(\\bar{Y}-\\E[Y])\\) does not go to 0 (which would suggest that the whole term converges to 0) nor does it go to \\(\\infty\\) (which would suggest that the term diverges). Moreover, if you multiplied \\((\\bar{Y}-E[Y])\\) instead by something somewhat smaller, say, \\(n^{1/3}\\), then the term \\((\\bar{Y}-\\E[Y])\\) would “win” and the whole expression would converge to 0 (to see this, try calculating \\(\\Var\\Big(n^{1/3}(\\bar{Y} - \\E[Y]\\Big)\\)). On the other hand, if you multiplied by something somewhat larger, say, \\(n\\), then the \\(n\\) part would “win” and the whole thing would diverge (to see this, try calculating \\(\\Var\\Big(n(\\bar{Y}-\\E[Y])\\Big)\\)). \\(\\sqrt{n}\\) turns out to be “just right” so that there is essentially a “tie” and this term neither converges to a particular number nor diverges.\nA very common question for students is: “how large does \\(n\\) need to be for the central limit theorem to apply?” Unfortunately, there is a not a great answer to this (though some textbooks have sometimes given explicit numbers here). Here is a basic explanation for why it is hard to give a definite number. Suppose \\(Y\\) follows a normal distribution, then it will not take many observations for the normal approximation to hold. On the other hand, if \\(Y\\) were to come from a discrete distribution or just a generally complicated distribution, then it might take many more observations for the normal approximation to hold.\n\nAll that to say, I know that the CLT is hard to understand, but the flip-side of that is that it really is a fascinating result. We’ll see how its useful next.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Asymptotic Properties</span>"
    ]
  },
  {
    "objectID": "07-inference.html",
    "href": "07-inference.html",
    "title": "8  Inference",
    "section": "",
    "text": "8.1 Inference / Hypothesis Testing\nSW 3.2, 3.3\nOften in statistics/econometrics, we have some theory that we would like to test. Pretty soon, we will be interested in testing a theory like: some economic policy had no effect on some outcome of interest.\nIn this section, we’ll focus on the relatively simple case of conducting inference on \\(\\E[Y]\\), but very similar arguments will apply when we try to start estimating more complicated things soon. Because we’re just focusing on \\(\\E[Y]\\), the examples in this section may be a somewhat trivial/uninteresting, but I want us to learn some mechanics, and then we’ll be able to apply these in more complicated situations.\nLet’s start with defining some terms.\nNull Hypothesis This is the hypothesis (or theory) that we want to test. We’ll often write it in the following way\n\\[\n  H_0 : \\E[Y] = \\mu_0\n\\] where \\(\\mu_0\\) is some actual number (e.g., 0 or 10 or just whatever coincides with the theory you want to test).\nAlternative Hypothesis This is what is true if \\(H_0\\) is not. There are other possibilities, but I think the only alternative hypothesis that we will consider this semester is\n\\[\n  H_1 : \\E[Y] \\neq \\mu_0\n\\] i.e., that \\(\\E[Y]\\) is not equal to the particular value \\(\\mu_0\\).\nThe key conceptual issue is that, even if the null hypothesis is true, because we estimate \\(\\E[Y]\\) with a sample, it will generally be the case that \\(\\bar{Y} \\neq \\mu_0\\). This is just the nature of trying to estimate things with a sample.\nWhat we are going to go for is essentially trying to tell the difference (or at least be able to weigh the evidence) regarding whether the difference between \\(\\bar{Y}\\) and \\(\\mu_0\\) can be fully explained by sampling variation or that the difference is “too big” to be explained by sampling variation. Things will start to get “mathy” in this section, but I think it is helpful to just hold this high-level idea in your head as we go along.\nNext, let’s define the standard error of an estimator. Suppose that we know that our estimator is asymptotically normal so that\n\\[\n  \\sqrt{n}(\\hat{\\theta} - \\theta) \\rightarrow N(0,V) \\quad \\textrm{as } n \\rightarrow \\infty\n\\] Then, we define the standard error of \\(\\hat{\\theta}\\) as\n\\[\n  \\textrm{s.e.}(\\hat{\\theta}) := \\frac{\\sqrt{\\hat{V}}}{\\sqrt{n}}\n\\] which is just the square root of the estimate of the asymptotic variance \\(V\\) divided by the square root of the sample size. For example, in the case where we are trying to estimate \\(\\E[Y]\\), recall that, by the CLT, \\(\\sqrt{n}(\\bar{Y} - \\E[Y]) \\rightarrow N(0,V)\\) where \\(V=\\Var(Y)\\), so that\n\\[\n  \\textrm{s.e.}(\\bar{Y}) = \\frac{\\sqrt{\\widehat{\\Var}(Y)}}{\\sqrt{n}}\n\\] where \\(\\widehat{\\Var}(Y)\\) is just an estimate of the variance of \\(Y\\), i.e., just run var(Y) in R.\nOver the next few sections, we are going to consider several different way to conduct inference (i.e., weigh the evidence) about some theory (i.e., the null hypothesis) using the data that we have. For all of the approaches that we consider below, the key ingredients are going to an estimate of the parameter of interest (e.g., \\(\\bar{Y}\\)), the value of \\(\\mu_0\\) coming from the null hypothesis, and the standard error of the estimator.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#t-statistics",
    "href": "07-inference.html#t-statistics",
    "title": "8  Inference",
    "section": "8.2 t-statistics",
    "text": "8.2 t-statistics\nA t-statistic is given by\n\\[\n  t = \\frac{\\sqrt{n} (\\bar{Y} - \\mu_0)}{\\sqrt{\\hat{V}}}\n\\] Alternatively (from the definition of standard error), we can write\n\\[\n  t = \\frac{(\\bar{Y} - \\mu_0)}{\\textrm{s.e.}(\\bar{Y})}\n\\] though I’ll tend to use the first expression, just because I think it makes the arguments below slightly more clear.\nNotice that \\(t\\) is something that we can calculate with our available data. \\(\\sqrt{n}\\) is the square root of the sample size, \\(\\bar{Y}\\) is the sample average of \\(Y\\), \\(\\mu_0\\) is a number (that we have picked) coming from the null hypothesis, and \\(\\hat{V}\\) is the sample variance of \\(Y\\) (e.g., computed with var(Y) in R).\nNow, here is the interesting thing about t-statistics. If the null hypothesis is true, then\n\\[\n  t = \\frac{\\sqrt{n} (\\bar{Y} - \\E[Y])}{\\sqrt{\\hat{V}}} \\approx \\frac{\\sqrt{n} (\\bar{Y} - \\E[Y])}{\\sqrt{V}}\n\\]\nwhere we have substituted in \\(\\E[Y]\\) for \\(\\mu_0\\) (due to \\(H_0\\) being true) and then replaced \\(\\hat{V}\\) with \\(V\\) (which holds under the law of large numbers). This is something that we can apply the CLT to, and, in particular, if \\(H_0\\) holds, then \\[\n  t \\rightarrow N(0,1)\n\\] That is, if \\(H_0\\) is true, then \\(t\\) should look like a draw from a normal distribution.\nNow, let’s think about what happens when the null hypothesis isn’t true. Then, we can write\n\\[\n  t = \\frac{\\sqrt{n} (\\bar{Y} - \\mu_0)}{\\sqrt{\\hat{V}}}\n\\] which is just the definition of \\(t\\), but something different will happen here. In order for \\(t\\) to follow a normal distribution, we need \\((\\bar{Y} - \\mu_0)\\) to converge to 0. But \\(\\bar{Y}\\) converges to \\(\\E[Y]\\), and if the null hypothesis does not hold, then \\(\\E[Y] \\neq \\mu_0\\) which implies that \\((\\bar{Y} - \\mu_0) \\rightarrow (\\E[Y] - \\mu_0) \\neq 0\\) as \\(n \\rightarrow \\infty\\). It’s still the case that \\(\\sqrt{n} \\rightarrow \\infty\\). Thus, if \\(H_0\\) is not true, then \\(t\\) will diverge (recall: this means that it will either go to positive infinity or negative infinity depending on the sign of \\((\\E[Y] - \\mu_0)\\)).\nThis gives us a very good way to start to think about whether or not the data is compatible with our theory. For example, suppose that you calculate \\(t\\) (using your data and under your null hypothesis) and that it is equal to 1. 1 is not an “unusual” looking draw from a standard normal distribution — this suggests that you at least do not have strong evidence from data against your theory. Alternatively, suppose that you calculate that \\(t=-24\\). While its technically possible that you could draw \\(-24\\) from a standard normal distribution — it is exceedingly unlikely. We would interpret this as strong evidence against the null hypothesis, and it should probably lead you to “reject” the null hypothesis.\nWe have talked about some clear cases, but what about the “close calls”? Suppose you calculate that \\(t=2\\). Under the null hypothesis, there is about a 4.6% chance of getting a t-statistic at least this large (in absolute value). So…if \\(H_0\\) is true, this is a fairly unusual t-statistic, but it is not extremely unusual. What should you do?\nBefore we decide what to do, let’s introduce a little more terminology regarding what could go wrong with hypothesis testing. There are two ways that we could go wrong:\nType I Error — This would be to reject \\(H_0\\) when \\(H_0\\) is true\nType II Error — This would be to fail to reject \\(H_0\\) when \\(H_0\\) is false\nClearly, there is a tradeoff here. If you are really concerned with type I errors, you can be very cautious about rejecting \\(H_0\\). If you are very concerned about type II errors, you could aggressively reject \\(H_0\\). The traditional approach to trading these off in statistics is to pre-specify a significance level indicating what percentage of the time you are willing to commit a type I error. Usually the significance level is denoted by \\(\\alpha\\) and the most common choice of \\(\\alpha\\) is 0.05 and other common choices are \\(\\alpha=0.1\\) or \\(\\alpha=0.01\\). Then, good statistical tests try to make as few type II errors as possible subject to the constraint on the rate of type I errors.\nOften, once you have specified a significance level, it comes with a critical value. The critical value is the value of a test statistic for which the test just rejects \\(H_0\\).\nIn practice, this leads to the following decision rule:\n\nReject \\(H_0\\) if \\(|t| &gt; c_{1-\\alpha}\\) where \\(c_{1-\\alpha}\\) is the critical value corresponding to the significance level \\(\\alpha\\).\nFail to reject \\(H_0\\) if \\(|t| &lt; c_{1-\\alpha}\\)\n\nIn our case, since \\(t\\) follows a normal distribution under \\(H_0\\), the corresponding critical value (when \\(\\alpha=0.05\\)) is 1.96. In particular, recall what the pdf of a standard normal random variable looks like\n\n\n\n\n\n\n\n\n\nThe sum of the two blue, shaded areas is 0.05. In other words, under \\(H_0\\), there is a 5% chance that, by chance, \\(t\\) would fall in the shaded areas. If you want to change the significance level, it would result in a corresponding change in the critical value so that the area in the new shaded region would adjust too. For example, if you set the significance level to be \\(\\alpha=0.1\\), then you would need to adjust the critical value to be 1.64, and if you set \\(\\alpha=0.01\\), then you would need to adjust the critical value to be 2.58.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#p-values",
    "href": "07-inference.html#p-values",
    "title": "8  Inference",
    "section": "8.3 P-values",
    "text": "8.3 P-values\nChoosing a significance level is somewhat arbitrary. What did we choose 5%?\nPerhaps more importantly, we are essentially throwing away a lot of information if we are to reduce the information from standard errors/t-statistics to a binary “reject” or “fail to reject”.\nOne alternative is to report a p-value. A p-value is the probability of observing a t-statistic as “extreme” as we did if \\(H_0\\) were true.\nHere is an example of how to calculate a p-value. Suppose we calculate \\(t=1.85\\). Then,\n\n\n\n\n\n\n\n\n\nThen, under \\(H_0\\), the probability of getting a t-statistic “as extreme” as 1.85 corresponds to the area of the two shaded regions above. In other words, we need to to compute\n\\[\n  \\textrm{p-value} = \\P(Z \\leq -1.85) + \\P(Z \\geq 1.85)\n\\] where \\(Z \\sim N(0,1)\\). One thing that is helpful to notice here is that, a standard normal random variable is symmetric. This means that \\(\\P(Z \\leq -1.85) = \\P(Z \\geq 1.85)\\). We also typically denote the cdf of a standard normal random variable with the symbol \\(\\Phi\\). Thus,\n\\[\n  \\textrm{p-value} = 2 \\Phi(-1.85)\n\\] I don’t know what this is off the top of my head, but it is easy to compute from a table or using R. In R, you can use the function pnorm — here, the p-value is given by 2*pnorm(-1.85) which is equal to 0.064.\nMore generally, if you calculate a t-statistic, \\(t\\), using your data and under \\(H_0\\), then,\n\\[\n  \\textrm{p-value} = 2 \\Phi(-|t|)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#confidence-interval",
    "href": "07-inference.html#confidence-interval",
    "title": "8  Inference",
    "section": "8.4 Confidence Interval",
    "text": "8.4 Confidence Interval\nAnother idea is to report a \\((1-\\alpha)\\%\\) (e.g., 95%) confidence interval.\nThe interpretation of a confidence interval is a bit subtle. It is this: if we collected a large number of samples, and computed a confidence interval each time, 95% of these would contain the true value. This is subtly different than: there is a 95% probability that \\(\\theta\\) (the population parameter of interest) falls within the confidence interval — this second interpretation doesn’t make sense because \\(\\theta\\) is non-random.\nA 95% confidence interval is given by\n\\[\n  CI_{95\\%} = \\left[\\hat{\\theta} - 1.96 \\ \\textrm{s.e.}(\\hat{\\theta}), \\hat{\\theta} + 1.96 \\  \\textrm{s.e.}(\\hat{\\theta})\\right]\n\\]\nFor the particular case where we are interested in \\(\\E[Y]\\), this becomes\n\\[\n  CI_{95\\%} = \\left[ \\bar{Y} - 1.96 \\ \\textrm{s.e.}(\\bar{Y}), \\bar{Y} + 1.96 \\ \\textrm{s.e.}(\\bar{Y}) \\right]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#inference-in-practice",
    "href": "07-inference.html#inference-in-practice",
    "title": "8  Inference",
    "section": "8.5 Inference in Practice",
    "text": "8.5 Inference in Practice\nI have covered the main approaches to inference in this section. I’d like to make a couple of concluding comments. First, all of the approaches discussed here (standard errors, t-statistics, p-values, and confidence intervals) are very closely related (in some sense, they are just alternative ways to report the same information). They all rely heavily on establishing asymptotic normality of the estimate of the parameter of interest — in fact, this is why we were interested in asymptotic normality in the first place. My sense is that the most common thing to report (at least in economics) is an estimate of the parameter of interest (e.g., \\(\\hat{\\theta}\\) or \\(\\bar{Y}\\)) along with its standard error. If you know this information, you (or your reader) can easily compute any of the other expressions that we’ve considered in this section.\nAnother important thing to mention is that there is often a distinction between statistical significance and economic significance.\nIn the next chapter, we’ll start to think about the effect of one variable on another (e.g., the effect of some economic policy on some outcome of interest). By far the most common null hypothesis in this case is that “the effect” is equal to 0. However, in economics/social sciences/business applications, there probably aren’t too many cases where (i) it would be interesting enough to consider the effect of one variable on another (ii) while simultaneously the effect is literally equal to 0. Since, all else equal, standard errors get smaller with more observations, as datasets in economics tend to get larger over time, we tend to find more statistically significant effects. This doesn’t mean that effects are getting bigger or more important — just that we are able to detect smaller and smaller effects if we have enough data. And most questions in economics involve more than just answering the binary question: does variable \\(X\\) have any effect at all on variable \\(Y\\)? For example, if you are trying to evaluate the effect of some economic policy, it is usually more helpful to think in terms of a cost-benefit analysis — what are the benefits or the policy relative to the costs and these sorts of comparisons inherently involve thinking about magnitudes of effects.\nA more succinct way to say all this is: the effect of one variable on another can be both “statistically significant” and “economically” small at the same time. Alternatively, if you do not have much data or the data is very “noisy”, it may be possible that there are relatively large effects, but that the estimates are not statistically significant (i.e., you are not able to detect them very well with the data that you have). Therefore, it is important to not become too fixated on statistical significance and to additionally think carefully about the magnitudes of estimates.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#coding",
    "href": "07-inference.html#coding",
    "title": "8  Inference",
    "section": "8.6 Coding",
    "text": "8.6 Coding\nIn this section, we’ll use the acs data to calculate an estimate of average wage/salary income among employed individuals in the United States. We’ll test the null hypothesis that the mean income in the United States is $50,000 as well as report the standard error of our estimate of mean income, as well as corresponding p-values, t-statistics, and 95% confidence interval. Finally, we’ll report a table of summary statistics using the modelsummary package separately by college graduates relative to non-college graduates.\n\nload(\"data/acs.RData\")\n\n# estimate of mean income\nybar &lt;- mean(acs$incwage)\nybar\n\n[1] 59263.46\n\n# calculate standard error\nV &lt;- var(acs$incwage)\nn &lt;- nrow(acs)\nse &lt;- sqrt(V) / sqrt(n)\nse\n\n[1] 713.8138\n\n# calculate t-statistic\nt_stat &lt;- (ybar - 50000) / se\nt_stat\n\n[1] 12.97742\n\n\nThis clearly exceeds 1.96 (or any common critical value) which implies that we would reject the null hypothesis that mean income is equal to $50,000.\n\n# calculate p-value\np_val &lt;- 2*pnorm(-abs(t_stat))\n\nThe p-value is essentially equal to 0. This is expected given the value of the t-statistic that we calculated earlier.\n\n# 95% confidence interval\nci_L &lt;- ybar - 1.96*se\nci_U &lt;- ybar + 1.96*se\npaste0(\"[\",round(ci_L,1),\",\",round(ci_U,1),\"]\")\n\n[1] \"[57864.4,60662.5]\"\n\n\n\nlibrary(modelsummary)\n\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n  options(modelsummary_factory_latex = 'kableExtra')\n  options(modelsummary_factory_html = 'kableExtra')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# create a factor variable for going to college\nacs$col &lt;- ifelse(acs$educ &gt;= 16, \"college\", \"non-college\")\nacs$col &lt;- as.factor(acs$col)\nacs$female &lt;- 1*(acs$sex==2)\nacs$incwage &lt;- acs$incwage/1000\ndatasummary_balance(~ col, data=dplyr::select(acs, incwage, female, age, col),\n                    fmt=2)\n\n\n\n    \n\n    \n    \n      \n        \n\n \ncollege (N=3871)\nnon-college (N=6129)\n \n \n\n        \n              \n                 \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n                Diff. in Means\n                Std. Error\n              \n        \n        \n        \n                \n                  incwage\n                  89.69\n                  96.15\n                  40.05\n                  39.01\n                  -49.65\n                  1.62\n                \n                \n                  female \n                  0.51 \n                  0.50 \n                  0.46 \n                  0.50 \n                  -0.04 \n                  0.01\n                \n                \n                  age    \n                  44.38\n                  13.43\n                  42.80\n                  15.71\n                  -1.58 \n                  0.29",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#lab-3-monte-carlo-simulations",
    "href": "07-inference.html#lab-3-monte-carlo-simulations",
    "title": "8  Inference",
    "section": "8.7 Lab 3: Monte Carlo Simulations",
    "text": "8.7 Lab 3: Monte Carlo Simulations\nIn this lab, we will study the theoretical properties of the estimators that we have been discussing in this chapter.\nMonte Carlo simulations are a useful way to study/understand the properties of an estimation procedure. The basic idea is that, instead of using real data, we are going to use simulated data where we control the data generating process. This will be useful for two reasons. First, we will know what the truth is and compare results coming from our estimation procedure to the truth. Second, because we are simulating data, we can actually carry out our thought experiment of repeatedly drawing a sample of some particular size.\nFor this lab, we are going to make simulated coin flips.\n\nWrite a function called flip that takes in an argument p where p stands for the probability of flipping a heads (you can code this as a 1 and 0 for tails) and outputs either 1 or 0. Run the code\nflip(0.5)\nHint: It may be helpful to use the R function sample.\nWrite a function called generate_sample that takes in the arguments n and p and generates a sample of n coin flips where the probability of flipping heads is p. Run the code\ngenerate_sample(10,0.5)\nNext, over 1000 Monte Carlo simulations (i.e., do the following 1000 times),\n\ngenerate a new sample with 10 observations\ncalculate an estimate of \\(p\\)\n\n(Hint: you can estimate \\(p\\) by just calculating the average number of heads flipped in a particular simulation)\n\na t-statistic for the null hypothesis that \\(p=0.5\\)\nand record whether or not you reject the null hypothesis that \\(p=0.5\\) in that simulation\n\n\nThen, using all 1000 Monte Carlo simulations, report (i) an estimate of the bias of your estimator, (ii) an estimate of the variance of your estimator, (iii) an estimate of the mean squared error of your estimator, (iv) plot a histogram of the t-statistics across iterations, and (v) report the fraction of times that you reject \\(H_0\\).\n\nSame as #3, but with 50 observations in each simulation. What differences do you notice?\nSame as #3, but with 50 observations and test \\(H_0:p=0.6\\). What differences do you notice?\nSame as #3, but with 50 observations and test \\(H_0:p=0.9\\). What differences do you notice?\nSame as #3, but with 1000 observations and test \\(H_0:p=0.6\\). What differences do you notice?\nSame as #3, but now set \\(p=0.95\\) (so that this is an unfair coin that flips heads 95% of the time) and with 10 observations and test \\(H_0:p=0.95\\). What differences do you notice?\nSame as #8, but with 50 observations. What differences do you notice?\nSame as #8, but with 1000 observations. What differences do you notice?\n\nHint: Since problems 3-10 ask you to do roughly the same thing over and over, it is probably useful to try to write a function to do all of these but with arguments that allow you to change the number of observations per simulation, the true value of \\(p\\), and the null hypothesis that you are testing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#lab-3-solutions",
    "href": "07-inference.html#lab-3-solutions",
    "title": "8  Inference",
    "section": "8.8 Lab 3 Solutions",
    "text": "8.8 Lab 3 Solutions\n\n\n\n\n# function to flip a coin with probability p\nflip &lt;- function(p) {\n  sample(c(0,1), size=1, prob=(c(1-p,p)))\n}\n\n# test out flip function\nflip(0.5)\n\n[1] 1\n\n\n\n\n\n\n# function to generate a sample of size n\ngenerate_sample &lt;- function(n,p) {\n  Y &lt;- c()\n  for (i in 1:n) {\n    Y[i] &lt;- flip(p)\n  }\n  Y\n}\n\n# test out generate_sample function\ngenerate_sample(10,0.5)\n\n [1] 0 0 0 0 1 1 0 0 1 0\n\n\n\n\n\n\n# carry out monte carlo simulations\nn &lt;- 10\np &lt;- 0.5\nnsims &lt;- 1000  # need to pick large number of monte carlo simulations\nmc_est &lt;- c()  # vector to hold estimation results\nmc_var &lt;- c()  # vector to hold estimated variance\n\nfor (i in 1:nsims) {\n  Y &lt;- generate_sample(n,p)\n  mc_est[i] &lt;- mean(Y)\n  mc_var[i] &lt;- var(Y)\n}\n\n# compute bias\nbias &lt;- mean(mc_est) - p\nbias\n\n[1] 0.0069\n\n# compute sampling variance\nvar &lt;- var(mc_est)\nvar\n\n[1] 0.02278518\n\n# compute mean squared error\nmse &lt;- bias^2 + var\nmse\n\n[1] 0.02283279\n\nH0 &lt;- p\nt &lt;- sqrt(n)*(mc_est - H0) / sqrt(mc_var)\nggplot(data.frame(t=t), aes(x=t)) +\n  geom_histogram(bins=30) +\n  theme_bw()\n\n\n\n\n\n\n\nrej &lt;- mean(1*(abs(t) &gt;= 1.96))\nrej\n\n[1] 0.094\n\n\n\n\n\n\n# since we are going to do this over and over, let's write a function to do it\nmc_sim &lt;- function(n, p, H0) {\n  mc_est &lt;- c()  # vector to hold estimation results\n  mc_var &lt;- c()  # vector to hold estimated variance\n\n  for (i in 1:nsims) {\n    Y &lt;- generate_sample(n,p)\n    mc_est[i] &lt;- mean(Y)\n    mc_var[i] &lt;- var(Y)\n  }\n\n  # compute bias\n  bias &lt;- mean(mc_est) - p\n\n  # compute sampling variance\n  var &lt;- var(mc_est)\n\n  # compute mean squared error\n  mse &lt;- bias^2 + var\n  \n  t &lt;- sqrt(n)*(mc_est - H0) / sqrt(mc_var)\n  hist_plot &lt;- ggplot(data.frame(t=t), aes(x=t)) +\n    geom_histogram(bins=30) + \n    theme_bw()\n\n  rej &lt;- mean(1*(abs(t) &gt;= 1.96))\n  \n  # print results\n  print(paste0(\"bias: \", round(bias,4)))\n  print(paste0(\"var : \", round(var,4)))\n  print(paste0(\"mse : \", round(mse,4)))\n  print(hist_plot)\n  print(paste0(\"rej : \", round(rej,4)))\n}\nmc_sim(50, 0.5, 0.5)\n\n[1] \"bias: 0.0032\"\n[1] \"var : 0.0048\"\n[1] \"mse : 0.0049\"\n\n\n\n\n\n\n\n\n\n[1] \"rej : 0.061\"\n\n\n\n\n\n\nmc_sim(50, 0.5, 0.6)\n\n[1] \"bias: 0.0026\"\n[1] \"var : 0.0052\"\n[1] \"mse : 0.0052\"\n\n\n\n\n\n\n\n\n\n[1] \"rej : 0.327\"\n\n\n\n\n\n\nmc_sim(50, 0.5, 0.9)\n\n[1] \"bias: 0.0025\"\n[1] \"var : 0.0049\"\n[1] \"mse : 0.0049\"\n\n\n\n\n\n\n\n\n\n[1] \"rej : 1\"\n\n\n\n\n\n\nmc_sim(1000, 0.5, 0.6)\n\n[1] \"bias: 2e-04\"\n[1] \"var : 3e-04\"\n[1] \"mse : 3e-04\"\n\n\n\n\n\n\n\n\n\n[1] \"rej : 1\"\n\n\n\n\n\n\nmc_sim(10, 0.95, 0.95)\n\n[1] \"bias: 0.0012\"\n[1] \"var : 0.0044\"\n[1] \"mse : 0.0044\"\n\n\nWarning: Removed 601 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n[1] \"rej : 0.601\"\n\n\n\n\n\n\nmc_sim(50, 0.95, 0.95)\n\n[1] \"bias: -0.0011\"\n[1] \"var : 9e-04\"\n[1] \"mse : 9e-04\"\n\n\nWarning: Removed 63 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n[1] \"rej : 0.065\"\n\n\n\n\n\n\nmc_sim(1000, 0.95, 0.95)\n\n[1] \"bias: -4e-04\"\n[1] \"var : 0\"\n[1] \"mse : 0\"\n\n\n\n\n\n\n\n\n\n[1] \"rej : 0.058\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#coding-questions",
    "href": "07-inference.html#coding-questions",
    "title": "8  Inference",
    "section": "8.9 Coding Questions",
    "text": "8.9 Coding Questions\n\nFor this question, we’ll use the data Airq. The variable rain contains the amount of rainfall in the county in a year (in inches). For this question, we’ll be interested in testing whether or not the mean rainfall across counties in California is 25 inches.\n\nEstimate the mean rainfall across counties.\nCalculate the standard error of your estimate of rainfall.\nCalculate a t-statistic for \\(H_0 : \\E[Y] = 25\\) where \\(Y\\) denotes rainfall. Do you reject \\(H_0\\) at a 5% significance level? Explain.\nCalculate a p-value for \\(H_0: \\E[Y] = 25\\). How should you interpret this?\nCalculate a 95% confidence interval for average rainfall.\nUse the datasummary_balance function from the modelsummary package to report average air quality, value added, rain, population density, and average income, separately by whether or not the county is located in a coastal area.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "07-inference.html#extra-questions",
    "href": "07-inference.html#extra-questions",
    "title": "8  Inference",
    "section": "8.10 Extra Questions",
    "text": "8.10 Extra Questions\n\nWhat is the difference between consistency and unbiasedness?\nSuppose you have an estimator that is unbiased. Will it necessarily be consistent? If not, provide an example of an unbiased estimator that is not consistent.\nSuppose you have an estimator that is consistent. Will it necessarily be unbiased? If not, provide an example of a consistent estimator that is not unbiased.\nThe Central Limit Theorem says that, \\(\\sqrt{n}\\left(\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\E[Y])\\right) \\rightarrow N(0,V)\\) as \\(n \\rightarrow \\infty\\) where \\(V = \\var(Y)\\).\n\nWhat happens to \\(n \\left(\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\E[Y])\\right)\\) as \\(n \\rightarrow \\infty\\)? Explain.\nWhat happens to \\(n^{1/3} \\left(\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\E[Y])\\right)\\) as \\(n \\rightarrow \\infty\\)? Explain.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html",
    "href": "08-interpreting_regressions.html",
    "title": "9  Interpreting Regressions",
    "section": "",
    "text": "9.1 Nonparametric Regression / Curse of Dimensionality\nIf you knew nothing about regressions, it would seem natural to try to estimate \\(\\E[Y|X_1=x_1,X_2=x_2,X_3=x_3]\\) by just calculating the average of \\(Y\\) among observations that have values of the regressors equal to \\(x_1\\), \\(x_2\\), and \\(x_3\\) (if these are discrete) or that are, in some sense, close to \\(x_1\\), \\(x_2\\), and \\(x_3\\) (if these are continuous).\nThis is actually a pretty attractive idea.\nHowever, you run into the issue that it is practically challenging to do this when the number of regressors starts to get large (i.e., if you have 10 regressors, generally, you would need tons of data to be able to find a suitable number of observations that are “close” to any particular value of the regressors).\nLet me give a more concrete example. Suppose that you were trying to estimate mean house price as a function of a house’s characteristics. If the only characteristic of the house that you knew was the number of bedrooms, then it would be pretty easy to just calculate the average house price among houses with 2, 3, 4, etc. bedrooms. Now suppose that you knew both the number of bedrooms and the number of square feet. In this case, if we wanted to estimate mean house prices as a function of these characteristics, we would need to find houses that have the same number of bedrooms and (at least) a similar number of square feet. This starts to “slice” the data that you have more thinly. If you continue with this idea (suppose that you want to estimate mean house price as a function of number of bedrooms, number of bathrooms, number of square feet, what year the house was built in, whether or not it has a basement, what zip code it is located in, etc.) then you will start to stretch your data extremely thin to the point that you may have very few relevant observations (or perhaps no relevant observations) for particular values of the characteristics.\nThis issue is called the “curse of dimensionality”.\nWe will focus on linear models for \\(\\E[Y|X_1,X_2,X_3]\\) largely to get around the curse of dimensionality.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#nonparametric-regression-curse-of-dimensionality",
    "href": "08-interpreting_regressions.html#nonparametric-regression-curse-of-dimensionality",
    "title": "9  Interpreting Regressions",
    "section": "",
    "text": "This idea of using observations that are very close in terms of characteristics in order to estimate a conditional expectation is called nonparametric econometrics/statistics. You can take entire courses (typically graduate-level) on this topic if you were interested. The reason that it is is called nonparametric is that it doesn’t involve making any functional form assumptions (like linearity) but the cost is that it would typically require many more observations (due to the curse of dimensionality).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#linear-regression-models",
    "href": "08-interpreting_regressions.html#linear-regression-models",
    "title": "9  Interpreting Regressions",
    "section": "9.2 Linear Regression Models",
    "text": "9.2 Linear Regression Models\nSW 4.1\nIn order to get around the curse of dimensionality that we discussed in the previous section, we will often an impose a linear model for the conditional expectation. For example,\n\\[\n  \\E[Y|X] = \\beta_0 + \\beta_1 X\n\\] or\n\\[\n  \\E[Y|X_1,X_2,X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n\\] If we know the values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\), then it is straighforward for us to make predictions. In particular, suppose that we want to predict the outcome for a new observation with characteristics \\(x_1\\), \\(x_2\\), and \\(x_3\\). Our prediction would be\n\\[\n  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\n\\]\n\nExample: Suppose that you are studying intergenerational income mobility and that you are interested in predicting a child’s income whose parents’ income was $50,000 and whose mother had 12 years of education. Let \\(Y\\) denote child’s income, \\(X_1\\) denote parents’ income, and \\(X_2\\) denote mother’s education. Further, suppose that \\(\\E[Y|X_1,X_2] = 20,000 + 0.5 X_1 + 1000 X_2\\).\nIn this case, you would predict child’s income to be\n\\[\n  20,000 + 0.5 (50,000) + 1000(12) = 57,000\n\\]\n\n\nSide-Comment:\nThe above model can be equivalently written as \\[\\begin{align*}\n  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U\n\\end{align*}\\] where \\(U\\) is called the error term and satisfies \\(\\E[U|X_1,X_2,X_3] = 0\\). There will be a few times where this formulation will be useful for us.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#computation",
    "href": "08-interpreting_regressions.html#computation",
    "title": "9  Interpreting Regressions",
    "section": "9.3 Computation",
    "text": "9.3 Computation\nEven if we know that \\(\\E[Y|X_1,X_2,X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\), in general, we do not know the values of the population parameters (the \\(\\beta\\)’s). This is analogous to the framework in the previous chapter where we were interested in the population parameter \\(\\E[Y]\\) and estimated it by \\(\\bar{Y}\\).\nIn this section, we’ll discuss how to estimate \\((\\beta_0,\\beta_1,\\beta_2,\\beta_3)\\) using R. We’ll refer to the estimated values of the parameters as \\((\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3)\\). As in the previous section, it will not be the case that the estimated \\(\\hat{\\beta}\\)’s are exactly equal to the population \\(\\beta\\)’s. Later on in this chapter, we will establish properties like consistency (so that, as long as we have a large sample, the estimated \\(\\hat{\\beta}\\)’s should be “close” to the population \\(\\beta\\)’s) and asymptotic normality (so that we can conduct inference).\nAlso later on in this chapter, we’ll talk about how R itself actually makes these computations.\nThe main function in R for estimating linear regressions is the lm function (lm stands for linear model). The key things to specify for running a regression in R are a formula argument which tells lm which variables are the outcome and which variables are the regressors and a data argument which tells the lm command what data we are using to estimate the regression. Let’s give an example using the mtcars data.\n\nreg &lt;- lm(mpg ~ hp + wt, data=mtcars)\n\nWhat this line of code does is to run a regression. The formula is mpg ~ hp + wt. In other words mpg (standing for miles per gallon) is the outcome, and we are running a regression on hp (horse power) and wt (weight). The ~ symbol is a “tilde”. In order to add regressors, we separate them with a +. The second argument data=mtcars says to use the mtcars data. All of the variables in the formula need to correspond to column names in the data. We saved the results of the regression in a variable called reg. It’s most common to report the results of the regression using the summary command.\n\nsummary(reg)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe main thing that this reports is the estimated parameters. Our estimate of the “Intercept” (i.e., this is \\(\\hat{\\beta}_0\\)) is in the first row of the table; our estimate is 37.227. The estimated coefficient on hp is -0.0318, and the estimated coefficient on wt is -3.878.\nYou can also see standard errors for each estimated parameter, a t-statistic, and a p-value in the other columns. We will talk about these in more detail in the next section.\nFor now, we’ll also ignore the information provided at the bottom of the summary.\nNow that we have estimated the parameters, we can use these to predict \\(mpg\\) given a value of \\(hp\\) and \\(wt\\). For example, suppose that you wanted to predict the \\(mpg\\) of a 2500 pound car (note: weight in \\(mtcars\\) is in 1000s of pounds) and 120 horsepower car, you could compute\n\\[\n  37.227 - 0.0318(120) - 3.878(2.5) = 23.716\n\\] Alternatively, there is a built-in function in R called predict that can be used to generate predicted values. We just need to specify the values that we would like to get predicted values for by passing in a data frame with the relevant columns though the newdata argument. For example,\n\npred &lt;- predict(reg, newdata=data.frame(hp=120,wt=2.5))\nround(pred,3)\n\n    1 \n23.72 \n\n\n\nA popular alternative to R’s lm function is the lm_robust function from the estimatr package. This provides different standard errors from the default standard errors provided by lm that are, at least in most applications in economics, typically a better choice — we’ll have a further discussion on this topic when we talk about inference later on in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#partial-effects",
    "href": "08-interpreting_regressions.html#partial-effects",
    "title": "9  Interpreting Regressions",
    "section": "9.4 Partial Effects",
    "text": "9.4 Partial Effects\nAs we discussed in the beginning of this chapter, besides predicting outcomes, a second main goal for us is to think about partial effects of a regressor on the outcome. We’ll consider partial effects over the next few sections.\nIn the model, \\[\\begin{align*}\n  \\E[Y | X_1, X_2, X_3]  &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n\\end{align*}\\]\nIf \\(X_1\\) is continuous, then \\[\\begin{align*}\n  \\beta_1 = \\frac{\\partial \\E[Y|X_1,X_2,X_3]}{\\partial X_1}\n\\end{align*}\\]\nThus, \\(\\beta_1\\) is the partial effect of \\(X_1\\) on \\(Y\\). In other words, \\(\\beta_1\\) should be interpreted as how much \\(Y\\) increases, on average, when \\(X_1\\) increases by one unit holding \\(X_2\\) and \\(X_3\\) constant. Make sure to get this interpretation right!\n\nExample: Continuing the same example as above about intergenerational income mobility and where \\(Y\\) denotes child’s income, \\(X_1\\) denotes parents’ income, \\(X_2\\) denotes mother’s education, and\n\\[\n  \\E[Y|X_1,X_2] = 20,000 + 0.5 X_1 + 1000 X_2\n\\] The partial effect of parents’ income on child’s income is 0.5. This means that, for every one dollar increase in parents’ income, child’s income is 0.5 dollars higher on average holding mother’s education constant.\n\n\n9.4.1 Computation\nLet’s run the same regression as in the previous section, but think about partial effects in this case.\n\nreg1 &lt;- lm(mpg ~ hp + wt, data=mtcars)\nsummary(reg1)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe partial effect of horsepower on miles per gallon is -0.032. In other words, we estimate that if horsepower increases by one then, on average, miles per gallon decreases by 0.032 holding weight constant.\nThe t-statistic and p-value are computed for the null hypothesis that the corresponding coefficient is equal to 0. For example, for hp the t-statistic is equal to -3.519 which is greater than 1.96 and indicates that the partial effect of hp is statistically significant at a 5% significance level. The corresponding p-value for hp is 0.0145 indicating that there is only about a 1.5% chance of getting a t-statistic this extreme if the partial effect of hp were actually 0 (i.e., under \\(H_0 : \\beta_1=0\\)).\n\nPractice: What is the partial effect of wt in the previous example? Provide a careful interpretation. Is the partial effect of wt statistically significant? Explain. What is the p-value for wt? How do you interpret the p-value?\n\n\nSide-Comment: One horsepower is a very small increase in horsepower, so it might be a good idea to multiply the coefficient by some larger number, say 50. In this case, we could say that we estimate that if horsepower increases by 50 then, on average, miles per gallon decreases by 1.59 (\\(=50 \\times 0.03177\\)) holding weight constant. From the above discussion, we know that this effect is statistically different from 0. That said, it is not clear to me if we should interpret this as a large partial effect; I do not know too much about cars, but a 50 horsepower increase seems rather large while a 1.59 decrease in miles per gallon seems relatively small (at least to me).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#binary-regressors",
    "href": "08-interpreting_regressions.html#binary-regressors",
    "title": "9  Interpreting Regressions",
    "section": "9.5 Binary Regressors",
    "text": "9.5 Binary Regressors\nSW 5.3\nLet’s continue with the same model as above\n\\[\n  \\E[Y|X_1,X_2,X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n\\]\nIf \\(X_1\\) is discrete (let’s say binary): \\[\\begin{align*}\n  \\beta_1 = \\E[Y|X_1=1,X_2,X_3] - \\E[Y|X_1=0,X_2,X_3]\n\\end{align*}\\] \\(\\beta_1\\) is still the partial effect of \\(X_1\\) on \\(Y\\) and should be interpreted as how much \\(Y\\) increases, on average, when \\(X_1\\) changes from 0 to 1, holding \\(X_2\\) and \\(X_3\\) constant.\nIf \\(X_1\\) can take more than just the values 0 and 1, but is still discrete (an example is a person’s years of education), then\n\\[\n  \\beta_1 = \\E[Y | X_1=x_1+1, X_2, X_3] - \\E[Y|X_1=x_1, X_2, X_3]\n\\] which holds for any possible value that \\(X_1\\) could take, so that \\(\\beta_1\\) is the effect of a 1 unit increase in \\(X_1\\) on \\(Y\\), on average, holding constant \\(X_2\\) and \\(X_3\\).\n\nExample: Suppose that you work for an airline and you are interested in predicting the number of passengers for a Saturday morning flight from Atlanta to Memphis. Let \\(Y\\) denote the number of passengers, \\(X_1\\) be equal to 1 for a morning flight and 0 otherwise, and let \\(X_2\\) be equal to 1 for a weekday flight and 0 otherwise. Further suppose that \\(\\E[Y|X_1,X_2] = 80 + 20 X_1 - 15 X_2\\).\nIn this case, you would predict,\n\\[\n  80 + 20 (1) - 15 (0) = 100\n\\] passengers on the flight.\nIn addition, the partial effect of being morning flight is equal to 20. This indicates that, on average, morning flights have 20 more passengers than non-morning flights holding whether or not the flight occurs on a weekday constant.\n\n\n9.5.1 Computation\nIn order to include a binary or discrete covariate in a regression in R is straightforward. The following regression uses the mtcars data and adds a binary regressor, am, indicating whether or not a car has an automatic transmission.\n\nreg2 &lt;- lm(mpg ~ hp + wt + am, data=mtcars)\nsummary(reg2)\n\n\nCall:\nlm(formula = mpg ~ hp + wt + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4221 -1.7924 -0.3788  1.2249  5.5317 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.002875   2.642659  12.867 2.82e-13 ***\nhp          -0.037479   0.009605  -3.902 0.000546 ***\nwt          -2.878575   0.904971  -3.181 0.003574 ** \nam           2.083710   1.376420   1.514 0.141268    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.538 on 28 degrees of freedom\nMultiple R-squared:  0.8399,    Adjusted R-squared:  0.8227 \nF-statistic: 48.96 on 3 and 28 DF,  p-value: 2.908e-11\n\n\nIn this example, cars that had an automatic transmission got about 2 more miles per gallon than cars that had an automatic transmission on average, holding horsepower and weight constant (though the p-value is only 0.14).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#nonlinear-regression-functions",
    "href": "08-interpreting_regressions.html#nonlinear-regression-functions",
    "title": "9  Interpreting Regressions",
    "section": "9.6 Nonlinear Regression Functions",
    "text": "9.6 Nonlinear Regression Functions\nSW 8.1, 8.2\nAlso, please read all of SW Ch. 8\nSo far, the the partial effects that we have been interested in have corresponded to a particular parameter in the regression, usually \\(\\beta_1\\). I think this can sometimes be a source of confusion as, at least in my view, we are not typically interested in the parameters for their own sake, but rather are interested in partial effects. It just so happens that in some leading cases, they coincide.\nIn addition, while the \\(\\beta\\)’s in the sort of models we have considered so far are easy to interpret, in some cases, it might be restrictive to think that the partial effects are the same across different values of the covariates.\nIn this section, we’ll see the first of several cases where partial effects do not coincide with a particular parameter.\nSuppose that\n\\[\n  \\E[Y|X_1,X_2,X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_2 + \\beta_4 X_3\n\\]\nLet’s start with making predictions using this model. If you know the values of \\(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\) and \\(\\beta_4\\), then to get a prediction, you would still just plug in the values of the regressors that you’d like to get a prediction for (including \\(x_1^2\\)).\nNext, in this model, the partial effect of \\(X_1\\) is given by\n\\[\n  \\frac{\\partial \\, \\E[Y|X_1,X_2,X_3]}{\\partial \\, X_1} = \\beta_1 + 2\\beta_2 X_1\n\\] In other words, the partial effect of \\(X_1\\) depends on the value that \\(X_1\\) takes.\nIn this case, it is sometimes useful to report the partial effect for some different values of \\(X_1\\). In other cases, it is useful to report the average partial effect (APE) which is the mean of the partial effects across the distribution of the covariates. In this case, the APE is given by\n\\[\n  APE = \\beta_1 + 2 \\beta_2 \\E[X_1]\n\\] and, once you have estimated the regression, you can compute an estimate of \\(APE\\) by\n\\[\n  \\widehat{APE} = \\hat{\\beta}_1 + 2 \\hat{\\beta}_2 \\bar{X}_1\n\\]\n\nExample: Let’s continue our example on intergenerational income mobility where \\(Y\\) denotes child’s income, \\(X_1\\) denotes parents’ income, and \\(X_2\\) denotes mother’s education. Now, suppose that\n\\[\n  \\E[Y|X_1,X_2] = 15,000 + 0.7 X_1 - 0.000002 X_1^2 + 800 X_2\n\\] Then, predicted child’s income when parents’ income is equal to $50,000 is given by\n\\[\n  15,000 + 0.7 (50,000) - 0.000002 (50,000)^2 + 800 (12) = 54,600\n\\] In addition, the partial effect of parents’ income is given by\n\\[\n  0.7 - 0.000004 X_1\n\\] Let’s compute a few different partial effects for different values of parents’ income\n\n\n\n\\(X_1\\)\nPE\n\n\n\n\n20,000\n0.62\n\n\n50,000\n0.50\n\n\n100,000\n0.30\n\n\n\nwhich indicates that the partial effect of parents’ income is decreasing — i.e., the effect of additional parents’ income is largest for children whose parents have the lowest income and gets smaller for those whose parents have high incomes.\nFinally, if you wanted to compute the \\(APE\\), you would just plug in \\(\\E[X_1]\\) (or \\(\\bar{X}_1\\)) into the expression for the partial effect.\n\n\n9.6.1 Computation\nIncluding a quadratic (or other higher order term) in R is relatively straightforward. Let’s just do an example.\n\nreg3 &lt;- lm(mpg ~ hp + I(hp^2), data=mtcars)\nsummary(reg3)\n\n\nCall:\nlm(formula = mpg ~ hp + I(hp^2), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5512 -1.6027 -0.6977  1.5509  8.7213 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.041e+01  2.741e+00  14.744 5.23e-15 ***\nhp          -2.133e-01  3.488e-02  -6.115 1.16e-06 ***\nI(hp^2)      4.208e-04  9.844e-05   4.275 0.000189 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.077 on 29 degrees of freedom\nMultiple R-squared:  0.7561,    Adjusted R-squared:  0.7393 \nF-statistic: 44.95 on 2 and 29 DF,  p-value: 1.301e-09\n\n\nThe only thing that is new here is I(hp^2). The I function stands for inhibit (you can read the documentation using ?I). For us, this is not too important. You can understand it like this: there is no variable names hp^2 in the data, but if we put the name of a variable that is in the data (here: hp) then we can apply a function to it (here: squaring it) before including it as a regressor.\nInterestingly, here it seems there are nonlinear effects of horsepower on miles per gallon. Let’s just quickly report the estimated partial effects for a few different values of horsepower.\n\nhp_vec &lt;- c(100,200,300)\n# there might be a native function in r\n# to compute these partial effects; I just\n# don't know it.\npe &lt;- function(hp) {\n  # partial effect is b1 + 2b2*hp\n  pes &lt;- coef(reg3)[2] + 2*coef(reg3)[3]*hp\n  # print using a data frame\n  data.frame(hp=hp, pe=round(pes,3))\n}\npe(hp_vec)\n\n   hp     pe\n1 100 -0.129\n2 200 -0.045\n3 300  0.039\n\n\nwhich suggests that the partial effect of horsepower on miles per gallon is large (though negative) at small values of horsepower and decreasing up to essentially no effect at larger values of horsepower.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#interpreting-interaction-terms",
    "href": "08-interpreting_regressions.html#interpreting-interaction-terms",
    "title": "9  Interpreting Regressions",
    "section": "9.7 Interpreting Interaction Terms",
    "text": "9.7 Interpreting Interaction Terms\nSW 8.3\nAnother way to allow for partial effects that vary across different values of the regressors is to include interaction terms.\nConsider the following regression model\n\\[\n  \\E[Y|X_1,X_2,X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\beta_4 X_3\n\\]\nThe term \\(X_1 X_2\\) is called the interaction term. In this model, the partial effect of \\(X_1\\) is given by\n\\[\n  \\frac{\\partial \\, \\E[Y|X_1,X_2,X_3]}{\\partial \\, X_1} = \\beta_1 + \\beta_3 X_2\n\\]\nIn this model, the effect of \\(X_1\\) varies with \\(X_2\\). As in the previous section, you could report the partial effect for different values of \\(X_2\\) or consider \\(APE = \\beta_1 + \\beta_3 \\E[X_2]\\).\nThere are a couple of other things worth pointing out for interaction terms\n\nIt is very common for one of the interaction terms, say, \\(X_2\\) to be a binary variable. This gives a way to easily test if the effect of \\(X_1\\) is the same across the two “groups” defined by \\(X_2\\). For example, suppose you wanted to check if the partial effect of education was the same for men and women. You could run a regression like\n\\[\n    Wage = \\beta_0 + \\beta_1 Education + \\beta_2 Female + \\beta_3 Education \\cdot Female + U\n  \\]\nFrom the previous discussion, the partial effect of education is given by\n\\[\n    \\beta_1 + \\beta_3 Female\n  \\]\nThus, the partial effect education for men is given by \\(\\beta_1\\), and the partial effect of education for women is given by \\(\\beta_1 + \\beta_3\\). Thus, if you want to test if the partial effect of education differs for men and women, you can just test if \\(\\beta_3=0\\). If \\(\\beta_3&gt;0\\), it suggests a higher partial effect of education for women, and if \\(\\beta_3 &lt; 0\\), it suggests a lower partial effect of education for women.\nAnother interesting case is when \\(X_1\\) and \\(X_2\\) are both binary. In this case, a model that includes an interaction term is called a saturated model. It is called this because it is actually nonparametric. In particular, notice that in the model \\(\\E[Y|X_1,X_2] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2\\),\n\\[\n  \\begin{aligned}\n    \\E[Y|X_1=0,X_2=0] &= \\beta_0 \\\\\n    \\E[Y|X_1=1,X_2=0] &= \\beta_0 + \\beta_1 \\\\\n    \\E[Y|X_1=0,X_2=1] &= \\beta_0 + \\beta_2 \\\\\n    \\E[Y|X_1=1,X_2=1] &= \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\n  \\end{aligned}\n  \\]\nThis exhausts all possible combinations of the regressors and means that you can recover each possible value of the conditional expectation from the parameters of the model.\nIt would be possible to write down a saturated model in cases with more than two binary regressors (or even discrete regressors) — you would just need to include more interaction terms. The key thing is that there be no continuous regressors. That said, as you start to add more and more discrete regressors and their interactions, you will effectively start to run into the curse of dimensionality issues that we discussed earlier.\nAs an example, consider our earlier example of flights from Atlanta to Memphis where \\(Y\\) denoted the number of passengers, \\(X_1\\) was equal to 1 for a a morning flight and 0 otherwise, and \\(X_2\\) was equal to one for a weekday flight and 0 otherwise. Suppose that \\(\\E[Y|X_1,X_2] = 90 - 15 X_1 - 5 X_2 + 25 X_1 X_2\\). Then,\n\\[\n  \\begin{aligned}\n    \\E[Y|X_1=0,X_2=0] &= 90 \\quad & \\textrm{non-morning, weekend} \\\\\n    \\E[Y|X_1=1,X_2=0] &= 90 - 15 = 75 \\quad & \\textrm{morning, weekend} \\\\\n    \\E[Y|X_1=0,X_2=1] &= 90 - 5 = 85 \\quad  & \\textrm{non-morning, weekday} \\\\\n    \\E[Y|X_1=1,X_2=1] &= 90 - 15 - 5 + 25 = 100 \\quad & \\textrm{morning, weekend}\n  \\end{aligned}\n  \\]\n\n\n9.7.1 Computation\nIncluding interaction terms in regressions in R is straightforward. Using the mtcars data, we can do it as follows\n\nreg4 &lt;- lm(mpg ~ hp + wt + am + hp*am, data=mtcars)\nsummary(reg4)\n\n\nCall:\nlm(formula = mpg ~ hp + wt + am + hp * am, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.435 -1.510 -0.697  1.284  5.245 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.34196    2.79711  11.920 2.89e-12 ***\nhp          -0.02918    0.01449  -2.014  0.05407 .  \nwt          -3.05617    0.94036  -3.250  0.00309 ** \nam           3.55141    2.35742   1.506  0.14355    \nhp:am       -0.01129    0.01466  -0.770  0.44809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.556 on 27 degrees of freedom\nMultiple R-squared:  0.8433,    Adjusted R-squared:  0.8201 \nF-statistic: 36.33 on 4 and 27 DF,  p-value: 1.68e-10\n\n\nThe interaction term in the results is in the row that starts with hp:am. These estimates suggest that, while horsepower does seem to decrease miles per gallon controlling for weight and whether or not the car has an automatic transmission, the effect of horsepower does not seem to vary much by whether or not the car has an automatic transmission (at least not in a big enough way that we can detect it with the data that we have).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#elasticities",
    "href": "08-interpreting_regressions.html#elasticities",
    "title": "9  Interpreting Regressions",
    "section": "9.8 Elasticities",
    "text": "9.8 Elasticities\nSW 8.2\nEconomists are often interested in elasticities, that is, the percentage change in \\(Y\\) when \\(X\\) changes by 1%.\nRecall that the definition of percentage change of moving from, say, \\(x_{old}\\) to \\(x_{new}\\) is given by\n\\[\n  \\textrm{\\% change} = \\frac{x_{new} - x_{old}}{x_{old}} \\times 100\n\\]\nElasticities are closely connected to natural logarithms; following the most common notation in economics, we’ll refer to the natural logarithm using the notation: \\(\\log\\). Further, recall that the derivative of the \\(\\log\\) function is given by\n\\[\n  \\frac{d \\, \\log(x)}{d \\, x} = \\frac{1}{x} \\implies d\\, \\log(x) = \\frac{d \\, x}{x}\n\\] which further implies that\n\\[\n  \\Delta \\log(x) := \\log(x_{new}) - \\log(x_{old}) \\approx \\frac{x_{new} - x_{old}}{x_{old}}\n\\] and, thus, that\n\\[\n  100 \\cdot \\Delta \\log(x) \\approx \\textrm{\\% change}\n\\] where the approximation is better when \\(x_{new}\\) and \\(x_{old}\\) are close to each other.\nNow, we’ll use these properties of logarithms in order to interpret several linear models\n\nFor simplicity, I am going to not include an error term or extra covariates, but you should continue to interpret parameter estimates as “on average” and “holding other regressors constant” (if there are other regressors in the model).\nLog-Log Model\n\\[\n  \\log(Y) = \\beta_0 + \\beta_1 \\log(X)\n  \\]\nIn this case,\n\\[\n  \\begin{aligned}\n  \\beta_1 &= \\frac{ d \\, \\log(Y) }{d \\, \\log(X)} \\\\\n  &= \\frac{ d \\, \\log(Y) \\cdot 100 }{d \\, \\log(X) \\cdot 100} \\\\\n  &\\approx \\frac{ \\% \\Delta Y}{ \\% \\Delta X}\n  \\end{aligned}\n  \\]\nAll that to say, in a regression of the log of an outcome on the log of a regressor, you should interpret the corresponding coefficient as the average percentage change in the outcome when the regressor changes by 1%. The log-log model is sometimes called a constant elasticity model.\nLog-Level model\n\\[\n  \\log(Y) = \\beta_0 + \\beta_1 X\n  \\]\nIn this case,\n\\[\n  \\begin{aligned}\n  \\beta_1 &= \\frac{ d \\, \\log(Y) }{d \\, X} \\\\\n  \\implies 100 \\beta_1 &= \\frac{ d \\, \\log(Y) \\cdot 100 }{d \\, X} \\\\\n  \\implies 100 \\beta_1 &\\approx \\frac{ \\% \\Delta Y}{ d \\, X}\n  \\end{aligned}\n  \\]\nThus, in a regression of the log of an outcome on the level of a regressor, you should multiply the corresponding coefficient by 100 and interpret it as the average percentage change in the outcome when the regressor changes by 1 unit.\nLevel-Log model\n\\[\n    Y = \\beta_0 + \\beta_1 \\log(X)\n  \\]\nIn this case,\n\\[\n    \\begin{aligned}\n    \\beta_1 &= \\frac{d\\, Y}{d \\, \\log(X)} \\\\\n    \\implies \\frac{\\beta_1}{100} &= \\frac{d \\, Y}{d \\, \\log(X) \\cdot 100} \\\\\n    \\implies \\frac{\\beta_1}{100} &\\approx \\frac{d \\, Y}{\\% \\Delta X}\n    \\end{aligned}\n  \\]\nThus, in a regression of the level of an outcome on the log of a regressor, you should divide the corresponding coefficient by 100 and interpret it as the average change in the outcome when the regressor changes by 1%.\n\n\nExample: Let’s continue the same example on intergenerational income mobility where \\(Y\\) denotes child’s income, \\(X_1\\) denotes parents’ income and \\(X_2\\) denotes mother’s education. We’ll consider how to interpret several different models.\n\\[\n  \\log(Y) = 8.8 + 0.4 \\log(X_1) + 0.008 X_2 + U\n\\] In this model, we estimate that, on average, when parents’ income increases by 1%, child’s income increases by 0.4% holding mother’s education constant.\nNext, consider, \\[\n  \\log(Y) = 8.9 + 0.00004 X_1 + 0.007 X_2 + U\n\\] In this model, we estimate that, on average, when parents’ income increases by $1, child’s income increases by 0.004% (alternatively, when parents’ income increase by $1000, child’s income increases by 4%) holding mother’s education constant.\nFinally, consider\n\\[\n  Y = -1,680,000 + 160,000 \\log(X_1) + 900 X_2 + U\n\\] In this case, we estimate that, on average, when parents’ income increases by 1%, child’s income increases by $1,600 holding mother’s education constant.\n\n\n9.8.1 Computation\nEstimating models that include logarithms in R is straightforward.\n\nreg5 &lt;- lm(log(mpg) ~ log(hp) + wt, data=mtcars) \nreg6 &lt;- lm(log(mpg) ~ hp + wt, data=mtcars) \nreg7 &lt;- lm(mpg ~ log(hp) + wt, data=mtcars)\n\nLet’s show the results all at once using the modelsummary function from the modelsummary package.\n\nlibrary(modelsummary)\n\nmodel_list &lt;- list(reg5, reg6, reg7)\nmodelsummary(model_list)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  4.832  \n                  3.829  \n                  59.571 \n                \n                \n                             \n                  (0.222)\n                  (0.069)\n                  (4.977)\n                \n                \n                  log(hp)    \n                  -0.266 \n                         \n                  -5.922 \n                \n                \n                             \n                  (0.056)\n                         \n                  (1.266)\n                \n                \n                  wt         \n                  -0.179 \n                  -0.201 \n                  -3.286 \n                \n                \n                             \n                  (0.027)\n                  (0.027)\n                  (0.615)\n                \n                \n                  hp         \n                         \n                  -0.002 \n                         \n                \n                \n                             \n                         \n                  (0.000)\n                         \n                \n                \n                  Num.Obs.   \n                  32     \n                  32     \n                  32     \n                \n                \n                  R2         \n                  0.885  \n                  0.869  \n                  0.859  \n                \n                \n                  R2 Adj.    \n                  0.877  \n                  0.860  \n                  0.849  \n                \n                \n                  AIC        \n                  140.3  \n                  144.5  \n                  150.0  \n                \n                \n                  BIC        \n                  146.1  \n                  150.4  \n                  155.9  \n                \n                \n                  Log.Lik.   \n                  28.501 \n                  26.395 \n                  -71.017\n                \n                \n                  F          \n                  111.812\n                  96.232 \n                  88.442 \n                \n                \n                  RMSE       \n                  0.10   \n                  0.11   \n                  2.23   \n                \n        \n      \n    \n\n\n\n\nIn the first model, we estimate that, on average, a 1% increase in horsepower decreases miles per gallon by 0.266% holding weight constant.\nIn the second model, we estimate that, on average, a 1 unit increase in horsepower decreases miles per gallon by 0.2% holding weight constant.\nIn the third model, we estimate that, on average, a 1% increase in horsepower decreases miles per gallon by .059 holding weight constant.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "08-interpreting_regressions.html#omitted-variable-bias",
    "href": "08-interpreting_regressions.html#omitted-variable-bias",
    "title": "9  Interpreting Regressions",
    "section": "9.9 Omitted Variable Bias",
    "text": "9.9 Omitted Variable Bias\nSW 6.1\nSuppose that we are interested in the following regression model\n\\[\n  \\E[Y|X_1, X_2, Q] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 Q\n\\] and, in particular, we are interested in the the partial effect\n\\[\n  \\frac{ \\partial \\, \\E[Y|X_1,X_2,Q]}{\\partial \\, X_1} = \\beta_1\n\\] But we are faced with the issue that we do not observe \\(Q\\) (which implies that we cannot control for it in the regression)\nRecall that we can equivalently write\n\\[\n  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 Q + U\n\\tag{9.1}\\] where \\(\\E[U|X_1,X_2,Q]=0\\).\nNow, for simplicity, suppose that\n\\[\n  \\E[Q | X_1, X_2] = \\gamma_0 + \\gamma_1 X_1 + \\gamma_2 X_2\n\\]\nNow, let’s consider the idea of just running a regression of \\(Y\\) on \\(X_1\\) and \\(X_2\\) (and just not including \\(Q\\)); in other words, consider the regression \\[\n  \\E[Y|X_1,X_2] = \\delta_0 + \\delta_1 X_1 + \\delta_2 X_2\n\\] We are interested in the question of whether or not we can recover \\(\\beta_1\\) if we do this. If we consider this “feasible” regression, notice if we plug in the expression for \\(Y\\) from Equation 9.1,\n\\[\n  \\begin{aligned}\n  \\E[Y|X_1,X_2] &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 \\E[Q|X_1,X_2] \\\\\n  &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (\\gamma_0 + \\gamma_1 X_1 + \\gamma_2 X_2) \\\\\n  &= \\underbrace{(\\beta_0 + \\beta_3 \\gamma_0)}_{\\delta_0} + \\underbrace{(\\beta_1 + \\beta_3 \\gamma_1)}_{\\delta_1} X_1 + \\underbrace{(\\beta_2 + \\beta_3 \\gamma_2)}_{\\delta_2} X_2\n  \\end{aligned}\n\\]\nIn other words, if we run the feasible regression of \\(Y\\) on \\(X_1\\) and \\(X_2\\), \\(\\delta_1\\) (the coefficient on \\(X_1\\)) is not equal to \\(\\beta_1\\); rather, it is equal to \\((\\beta_1 + \\beta_3 \\gamma_1)\\).\nThat you are not generally able to recover \\(\\beta_1\\) in this case is called omitted variable bias\nThere are two cases where you will recover \\(\\delta_1 = \\beta_1\\) though which occur when \\(\\beta_3 \\gamma_1 = 0\\):\n\n\\(\\beta_3=0\\). This would be the case where \\(Q\\) has no effect on \\(Y\\)\n\\(\\gamma_1=0\\). This would be the case where \\(X_1\\) and \\(Q\\) are uncorrelated after controlling for \\(X_2\\).\n\nInterestingly, there may be some case where you can “sign” the bias; i.e., figure out if \\(\\beta_3 \\gamma_1\\) is positive or negative. For example, you might have theoretical reasons to suspect that \\(\\gamma_1 &gt; 0\\) and \\(\\beta_3 &gt; 0\\). In this case,\n\\[\n  \\delta_1 = \\beta_1 + \\textrm{something positive}\n\\] which implies that \\(\\delta_1\\) (i.e., running a regression that ignores \\(Q\\)) would cause us to tend to over-estimate \\(\\beta_1\\).\n\nSide-Comment:\n\nThe book talks about omitted variable bias in the context of causality (this is probably the leading case), but we have not talked about causality yet. The same issues arise if we just say that we have some regression of interest but are unable to estimate it because some covariates are unobserved.\nThe relationship to causality (which is not so important for now), is that under certain conditions, we may have a particular partial effect that we would be willing to interpret as being the “causal effect”, but if we are unable to control for some variables that would lead to this interpretation, then we get to the issues pointed out in the textbook.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpreting Regressions</span>"
    ]
  },
  {
    "objectID": "09-regression_computation.html",
    "href": "09-regression_computation.html",
    "title": "10  Computation",
    "section": "",
    "text": "10.1 How to estimate the parameters in a regression model\nSW 4.2, 6.3\nLet’s start with the simple linear regression model (i.e., where there is just one regressor):\n\\[\n  Y = \\beta_0 + \\beta_1 X + U\n\\] with \\(\\E[U|X]=0\\). This model holds for every observation in the data, so we can write\n\\[\n  Y_i = \\beta_0 + \\beta_1 X_i + U_i\n\\] The question for this section is: How can we estimate \\(\\beta_0\\) and \\(\\beta_1\\)? First, notice that, any choice that we make for an estimate of \\(\\beta_0\\) of \\(\\beta_1\\) amounts to picking a line. Our strategy will be to estimate \\(\\beta_0\\) and \\(\\beta_1\\) by choosing values of them that result in the “best fit” of a line to the available data.\nThis begs the question: How do you choose the line that best fits the data? Let me give you an example\nIt’s clear from the figure that the blue line “fits better” than the green line or the red line. If you take a second and think about the reason why this is the case, you will notice that the reason why you know that it fits better is because the points tend to be closer to the blue line than to the red line or the green line.\nIn the figure, I drew three additional lines that are labeled \\(U_i^b\\), \\(U_i^g\\), and \\(U_i^r\\) which are just the difference between the blue line, the green line, and the red line and the corresponding data point in the figure. That is,\n\\[\n  \\begin{aligned}\n  U_i^b &= Y_i - b_0^b - b_1^b X_i \\\\\n  U_i^g &= Y_i - b_0^g - b_1^g X_i \\\\\n  U_i^r &= Y_i - b_0^r - b_1^r X_i\n  \\end{aligned}\n\\] where, for example, \\(b_0^b\\) and \\(b_1^b\\) are the intercept and slope of the blue line, \\(b_0^g\\) and \\(b_1^g\\) are the intercept and slope of the green line, etc. Clearly, the blue line fits this data point than the others, but we need to deal with a couple of issues to formalize this thinking. First, \\(U_i^b\\) and \\(U_i^r\\) are both less than 0 (because both of those lines sit above the data point) indicating that we can’t just choose the line where \\(U_i\\) is the smallest — for this point, that strategy would result in us liking the red line the most. Instead, we need a measure of distance that turns negative values of \\(U_i\\) into positive values and is larger for big negative values of \\(U_i\\) too. We’ll use the same quadratic distance that we’ve used before to deal with this issue; that is, we’ll define the distance between a line and the point as \\({U_i^b}^2\\) for the blue line, \\({U_i^g}^2\\) and \\({U_i^r}^2\\) for the green and red lines.\nSecond, the above discussion computes the distance between the line and the data for a single point. We want to extend this argument to all the data points. And we can do that by computing the average distance between the line and the points across all points. That is given by\n\\[\n  \\frac{1}{n}\\sum_{i=1}^n {U_i^b}^2, \\quad \\frac{1}{n}\\sum_{i=1}^n {U_i^g}^2, \\quad \\frac{1}{n}\\sum_{i=1}^n {U_i^r}^2\n\\] for the blue line, green line, and red line. These are actually numbers that we can compute.\n# intercept and slope of each line\n# (I just picked these)\nint_blue &lt;- 2\nslope_blue &lt;- 1\nint_green &lt;- 3\nslope_green &lt;- -1.5\nint_red &lt;- 6\nslope_red &lt;- 0.5\n\n# compute \"errors\"\nUb &lt;- Y - int_blue - slope_blue*X\nUg &lt;- Y - int_green - slope_green*X\nUr &lt;- Y - int_red - slope_red*X\n\n# compute distances\ndist_blue &lt;- mean(Ub^2)\ndist_green &lt;- mean(Ug^2)\ndist_red &lt;- mean(Ur^2)\n\n# report distances\nround(data.frame(dist_blue, dist_green, dist_red), 3)\n\n  dist_blue dist_green dist_red\n1     0.255      5.828   14.728\nThis corroborates our earlier intuition — the blue line appears to fit the data much better than the other two lines.\nIt turns out that we can use the same sort of idea as above to choose not just among three possible lines to fit the data, but to choose among all possible lines. More generally than we have been doing, let’s write\n\\[\n  Y_i = b_0 + b_1 X_i + U_i(b_0,b_1)\n\\] In other words, for any values of \\(b_0\\) and \\(b_1\\) that we would like to plug in (say like for the green line or red line in the figure above), we can define \\(U_i(b_0,b_1)\\) to be whatever is leftover between the line and the actual data.\nWe can choose the line (by choosing values of \\(b_0\\) and \\(b_1\\)) that minimizes the average distance between the line and the points. In other words, we will set\n\\[\n  \\begin{aligned}\n  (\\hat{\\beta}_0, \\hat{\\beta}_1) &= \\underset{b_0,b_1}{\\textrm{argmin}} \\frac{1}{n} \\sum_{i=1}^n U_i(b_0,b_1)^2 \\\\\n  &= \\underset{b_0,b_1}{\\textrm{argmin}} \\frac{1}{n} \\sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2\n  \\end{aligned}\n\\] This is a complicated looking mathematical expression, but I think the intuition is pretty easy to understand. It says that we want to find the values of \\(b_0\\) and \\(b_1\\) that make the distance between the points and the line as small as possible on average. Whatever these values are, that is what we are going to set \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) — our estimates of \\(\\beta_0\\) and \\(\\beta_1\\) — to be.\nHow can you do this? One idea is to do it numerically — you could try out a ton of different combinations of \\(b_0\\) and \\(b_1\\) and pick which one fits the best. Your computer could probably actually do this, but it would become quite a hard problem as we added more and more regressors.\nIt turns out that we can actually find a solution for the values of \\(b_0\\) and \\(b_1\\) that minimize the above expression using calculus.\nYou probably recall how to minimize a function. You take its derivative, set it equal to 0, and solve that equation. [It’s actually a little more complicated than that because you need to ensure that you’re finding a minimum rather than a maximum, but the above equation is actually quadratic, so it will have a minimum.]\nThat’s all that we will do here — just the thing we need to take the derivative of looks complicated. Actually, the \\(\\frac{1}{n}\\) and \\(\\sum\\) terms will just hang around. For the rest though, we need to use the chain rule; that is, we need to take the derivative of the outside and then multiply by the derivative of the inside. We also need to take the derivative both with respect to \\(b_0\\) and with respect to \\(b_1\\).\nLet’s start by taking the derivative with respect to \\(b_0\\) and setting it equal to 0.\n\\[\n  -\\frac{2}{n}\\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0\n\\tag{10.1}\\] where the \\(2\\) comes from taking the deriviative of the squared part and the negative sign at the beginning comes from taking the derivative of \\(-b_0\\) on the inside. I also replaced \\(b_0\\) and \\(b_1\\) here since \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the values that will solve this equation.\nNext, let’s take the derivative with respect to \\(b_1\\): \\[\n  -\\frac{2}{n} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) X_i = 0\n\\tag{10.2}\\] where the 2 comes from taking the derivative of the squared part, and the negative sign at the beginning and \\(X_i\\) at the end come from taking the derivative of the inside with respect to \\(b_1\\).\nAll we have to do now is to solve Equation 10.1 and Equation 10.2 for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Before we do it, let me just mention that we are about to do some fairly challenging algebra, but that is all it is. Conceptually, it is just the same as solving a system of two equations with two unknowns that you probably did at some point in high school.\nStarting with the first equation,\n\\[\n  \\begin{aligned}\n  & \\phantom{\\implies} -\\frac{2}{n} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) &= 0 \\\\\n  & \\implies \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) &= 0 \\\\\n  & \\implies \\bar{Y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{X} &= 0 \\\\\n  & \\implies \\boxed{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}}\n  \\end{aligned}\n\\] where the second line holds by dividing both sides by \\(-2\\), the third line holds by pushing the summation through the sums/differences and simplifying terms, and the last line holds by rearranging to solve for \\(\\hat{\\beta}_0\\).\nNow, let’s use the above result and Equation 10.2 to solve for \\(\\hat{\\beta}_1\\).\n\\[\n  \\begin{aligned}\n  & \\phantom{\\implies} -\\frac{2}{n} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1) X_i &= 0 \\\\\n  & \\implies  \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1) X_i) X_i &= 0 \\\\\n  & \\implies  \\frac{1}{n} \\sum_{i=1}^n (Y_i - (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) - \\hat{\\beta}_1 X_i) X_i &= 0 \\\\\n  & \\implies  \\frac{1}{n} \\sum_{i=1}^n X_i Y_i - \\bar{Y} \\frac{1}{n} \\sum_{i=1}^n X_i + \\hat{\\beta}_1 \\bar{X} \\frac{1}{n} \\sum_{i=1}^n X_i - \\hat{\\beta}_1 \\frac{1}{n} \\sum_{i=1}^n X_i^2 &= 0  \\\\\n\\end{aligned}\n\\] Let me explain each line and then we will keep going. The second line holds because the \\(-2\\) can just be canceled on each side, the third line holds by plugging in the expression we derived for \\(\\hat{\\beta}_0\\) above, and the last line holds by splitting up the summation and bringing out terms that do not change across \\(i\\). Let’s keep going\n\\[\n\\begin{aligned}\n  \\implies  \\frac{1}{n} \\sum_{i=1}^n X_i Y_i - \\bar{X}\\bar{Y} &= \\hat{\\beta}_1\\left(\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar{X}^2 \\right) \\\\\n  \\implies  \\hat{\\beta}_1 &= \\frac{\\frac{1}{n} \\sum_{i=1}^n X_i Y_i - \\bar{X}\\bar{Y}}{\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar{X}^2} \\\\\n  \\implies  & \\boxed{\\hat{\\beta}_1 = \\frac{\\widehat{\\Cov}(X,Y)}{\\widehat{\\Var}(X)}}\n  \\end{aligned}\n\\] where the first line holds by using the definition of \\(\\bar{X}\\) and moving some terms to the other side. The second equality holds by dividing both sides by \\(\\left(\\frac{1}{n} \\displaystyle \\sum_{i=1}^n X_i^2 - \\bar{X}^2 \\right)\\), and the last equality holds by the definition of sample covariance and variance.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computation</span>"
    ]
  },
  {
    "objectID": "09-regression_computation.html#how-to-estimate-the-parameters-in-a-regression-model",
    "href": "09-regression_computation.html#how-to-estimate-the-parameters-in-a-regression-model",
    "title": "10  Computation",
    "section": "",
    "text": "Side-Comment: I’d like to point out one more time that our estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are generally not exactly equal to \\(\\beta_0\\) and \\(\\beta_1\\). The figure earlier in this section was generated using simulated data where I set \\(\\beta_0=2\\) and \\(\\beta_1=1\\). However, we estimate \\(\\hat{\\beta}_0 = 1.75\\) and \\(\\hat{\\beta}_1 = 0.95\\) using the simulated data in that figure. These lines are close to each other, but not exactly the same.\nFurther, recall that we defined the error term \\(U_i\\) from\n\\[\n  Y_i = \\beta_0 + \\beta_1 X_i + U_i\n\\] which is the difference between individual \\(i\\)’s outcome and the population regression line.\nSimilarly, we define the residual \\(\\hat{U}_i\\) as\n\\[\n  Y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\hat{U}_i\n\\] so that \\(\\hat{U}_i\\) is the difference between individual \\(i\\)’s outcome and the estimated regression line. Moreover, since generally \\(\\beta_0 \\neq \\hat{\\beta}_0\\) and \\(\\beta_1 \\neq \\hat{\\beta}_1\\), generally \\(U_i \\neq \\hat{U}_i\\).\n\n\n10.1.1 Computation\nBefore moving on, let’s just confirm that the formulas that we derived are actually what R uses in order to estimates the parameters in a regression.\n\n# using lm\nreg8 &lt;- lm(mpg ~ hp, data=mtcars)\nsummary(reg8)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# using our formulas\nbet1 &lt;- cov(mtcars$mpg, mtcars$hp) / var(mtcars$hp)\n\nbet0 &lt;- mean(mtcars$mpg) - bet1*mean(mtcars$hp)\n\n# print results\ndata.frame(bet0=bet0, bet1=bet1)\n\n      bet0        bet1\n1 30.09886 -0.06822828\n\n\nand they are identical.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computation</span>"
    ]
  },
  {
    "objectID": "09-regression_computation.html#more-than-one-regressor",
    "href": "09-regression_computation.html#more-than-one-regressor",
    "title": "10  Computation",
    "section": "10.2 More than one regressor",
    "text": "10.2 More than one regressor\nNow, let’s suppose that you want to estimate a more complicated regressions like\n\\[\n  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U\n\\]\nor, even more generally,\n\\[\n  Y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k + U\n\\] We can still choose the line that best fits the data by solving\n\\[\n  (\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k) = \\underset{b_0,b_1,\\ldots,b_k}{\\textrm{argmin}} \\frac{1}{n} \\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \\cdots - b_k X_{ki})^2\n\\] To do it, we would need to take the derivative with respect to \\(b_0, b_1, \\ldots, b_k\\). This will give a system of equations with \\((k+1)\\) equations and \\((k+1)\\) unknowns. You can solve this — it actually is quite easy / very similar to what we just did if you know just a little bit of linear algebra, but this is beyond the scope of our course — and it is quite easy for the computer to solve.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computation</span>"
    ]
  },
  {
    "objectID": "10-regression_inference.html",
    "href": "10-regression_inference.html",
    "title": "11  Inference",
    "section": "",
    "text": "11.1 Inference\nSW 4.5, 5.1, 5.2, 6.6\nWe discussed in class the practical issues of inference in linear regression models.\nThese results rely on arguments building on the Central Limit Theorem (this should not surprise you as it is similar to the case for the asymptotic distribution of \\(\\sqrt{n}(\\bar{Y} - \\E[Y]))\\) that we discussed earlier in the semester.\nIn this section, I sketch these types of arguments for you. This material is advanced, but I suggest that you study this material.\nWe are going to show that, in the simple linear regression model, \\[\\begin{align*}\n  \\sqrt{n}(\\hat{\\beta}_1 - \\beta_1) \\rightarrow N(0,V) \\quad \\textrm{as} \\ n \\rightarrow \\infty\n\\end{align*}\\] where \\[\\begin{align*}\n  V = \\frac{\\E[(X-\\E[X])^2 U^2]}{\\var(X)^2}\n\\end{align*}\\] and discuss how to use this result to conduct inference.\nLet’s start by showing why this result holds.\nTo start with, recall that \\[\n\\hat{\\beta}_1 = \\frac{\\widehat{\\cov}(X,Y)}{\\widehat{\\var}(X)}\n\\tag{11.1}\\]\nBefore providing a main result, let’s start with noting the following:\nHelpful Intermediate Result 1 Notice that \\[\\begin{align*}\n  \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})\\bar{Y}\\Big) &= \\bar{Y} \\frac{1}{n}\\sum_{i=1}^n \\Big( X_i-\\bar{X} \\Big) \\\\\n  &= \\bar{Y} \\left( \\frac{1}{n}\\sum_{i=1}^n X_i - \\frac{1}{n}\\sum_{i=1}^n \\bar{X} \\right) \\\\\n  &= \\bar{Y} \\Big(\\bar{X} - \\bar{X} \\Big) \\\\\n  &= 0\n\\end{align*}\\] where the first equality just pulls \\(\\bar{Y}\\) out of the summation (it is a constant with respect to the summation), the second equality pushes the summation through the difference, the first part of the third equality holds by the definition of \\(\\bar{X}\\) and the second part holds because it is an average of a constant.\nThis implies that \\[\n  \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})(Y_i - \\bar{Y})\\Big) = \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})Y_i\\Big)\n\\tag{11.2}\\] and very similar arguments (basically the same arguments in reverse) also imply that \\[\n  \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})X_i\\Big) = \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})(X_i - \\bar{X})\\Big)\n\\tag{11.3}\\] We use both Equation 11.2 and Equation 11.3 below.\nNext, consider the numerator in Equation 11.1 \\[\\begin{align*}\n  \\widehat{\\cov}(X,Y) &= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}) \\\\\n  &= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})Y_i \\\\\n  &= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(\\beta_0 + \\beta_1 X_i + U_i) \\\\\n  &= \\underbrace{\\beta_0 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})}_{(A)} + \\underbrace{\\beta_1 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) X_i}_{(B)} + \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) U_i}_{(C)}) \\\\\n\\end{align*}\\] where the first equality holds by the definition of sample covariance, the second equality holds by Equation 11.2, the third equality plugs in for \\(Y_i\\), and the last equality combines terms and passes the summation through the additions/subtractions.\nNow, let’s consider each of these in turn.\nFor (A), \\[\\begin{align*}\n  \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} \\qquad \\textrm{and} \\qquad \\frac{1}{n} \\sum_{i=1}^n \\bar{X} = \\bar{X}\n\\end{align*}\\] which implies that this term is equal to 0.\nFor (B), notice that \\[\\begin{align*}\n  \\beta_1 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) X_i &= \\beta_1 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) (X_i - \\bar{X}) \\\\\n  &= \\beta_1 \\widehat{\\var}(X)\n\\end{align*}\\] where the first equality holds by Equation 11.3 and the second equality holds by the definition of sample variance.\nFor (C), well, we’ll just carry that one around for now.\nPlugging in the expressions for (A), (B), and (C) back into Equation Equation 11.1 implies that \\[\\begin{align*}\n  \\hat{\\beta}_1 = \\beta_1 + \\frac{1}{n} \\sum_{i=1}^n \\frac{(X_i - \\bar{X}) U_i}{\\widehat{\\var}(X)}\n\\end{align*}\\] Next, re-arranging terms and multiplying both sides by \\(\\sqrt{n}\\) implies that \\[\\begin{align*}\n  \\sqrt{n}(\\hat{\\beta}_1 - \\beta_1) &= \\sqrt{n} \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{(X_i - \\bar{X}) U_i}{\\widehat{\\var}(X)}\\right) \\\\\n  & \\approx \\sqrt{n} \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{(X_i - \\E[X]) U_i}{\\var(X)}\\right)\n\\end{align*}\\] The last line (the approximately one) is kind of a weak argument, but basically you can replace \\(\\bar{X}\\) and \\(\\widehat{\\var}(X)\\) and the effect of this replacement will converge to 0 in large samples (this is the reason for the approximately) — if you want a more complete explanation, sign up for my graduate econometrics class next semester.\nIs this helpful? It may not be obvious, but the right hand side of the above equation is actually something that we can apply the Central Limit Theorem to. In particular, maybe it is helpful to define \\(Z_i = \\frac{(X_i - \\E[X]) U_i}{\\var(X)}\\). We know that we could apply a Central Limit Theorem to \\(\\sqrt{n}\\left( \\frac{1}{n} \\sum_{i=1}^n Z_i \\right)\\) if (i) \\(Z_i\\) had mean 0, and (ii) it is iid. That it is iid holds immediately from the random sampling assumption. For mean 0, \\[\\begin{align*}\n  \\E[Z] &= \\E\\left[ \\frac{(X - \\E[X]) U}{\\var(X)}\\right] \\\\\n  &= \\frac{1}{\\var(X)} \\E[(X - \\E[X]) U] \\\\\n  &= \\frac{1}{\\var(X)} \\E[(X - \\E[X]) \\underbrace{\\E[U|X]}_{=0}] \\\\\n  &= 0\n\\end{align*}\\] where the only challenging line here is the third one holds from the Law of Iterated Expectations. This means that we can apply the central limit theorem, and in particular, \\(\\sqrt{n} \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i \\right) \\rightarrow N(0,V)\\) where \\(V=\\var(Z) = \\E[Z^2]\\) (where the 2nd equality here holds because \\(Z\\) has mean 0). Now, just substituting back in for \\(Z\\) implies that \\[\\begin{align*}\n  \\sqrt{n}(\\hat{\\beta}_1 - \\beta_1) \\rightarrow N(0,V)\n\\end{align*}\\] where \\[\n\\begin{aligned}\n  V &= \\E\\left[ \\left( \\frac{(X - \\E[X]) U}{\\var(X)} \\right)^2 \\right] \\nonumber \\\\\n  &= \\E\\left[ \\frac{(X - \\E[X])^2 U^2}{\\var(X)^2}\\right]\n\\end{aligned}\n\\tag{11.4}\\] which is what we were aiming for.\nGiven this result, all our previous work on standard errors, t-statistics, p-values, and confidence intervals applies. First, let me mention the way that you would estimate \\(V\\) (same as always, just replace the population quantities with corresponding sample quantities).\n\\[\n  \\hat{V} = \\frac{ \\displaystyle \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\hat{U}_i^2}{\\widehat{\\Var}(X)^2}\n\\]\nwhere \\(\\hat{U}_i\\) are the residuals.\nNow, standard errors are just the same as before (the only difference is that \\(\\hat{V}\\) itself has changed)\n\\[\n\\begin{aligned}\n  \\textrm{s.e.}(\\hat{\\beta}) &= \\frac{\\sqrt{\\hat{V}}}{\\sqrt{n}}\n\\end{aligned}\n\\]\nBy far the most common null hypothesis is \\(H_0: \\beta = 0\\), which suggests the following t-statistic:\n\\[\n  t = \\frac{\\hat{\\beta}}{\\textrm{s.e.}(\\hat{\\beta})}\n\\] One can continue to calculate a p-value by\n\\[\n  \\textrm{p-value} = 2 \\Phi(-|t|)\n\\] and a 95% confidence interval is given by\n\\[\n  CI = [\\hat{\\beta} - 1.96 \\textrm{s.e.}(\\hat{\\beta}), \\hat{\\beta} + 1.96 \\textrm{s.e.}(\\hat{\\beta})]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10-regression_inference.html#inference",
    "href": "10-regression_inference.html#inference",
    "title": "11  Inference",
    "section": "",
    "text": "11.1.1 Computation\nLet’s check if what we derived is what we can compute using R.\n\n# this is the same regression as in the previous section\nsummary(reg8)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# show previous calcuations\ndata.frame(bet0=bet0, bet1=bet1)\n\n      bet0        bet1\n1 30.09886 -0.06822828\n\n# components of Vhat\nY &lt;- mtcars$mpg\nX &lt;- mtcars$hp\nUhat &lt;- Y - bet0 - bet1*X\nXbar &lt;- mean(X)\nvarX &lt;- mean( (X-Xbar)^2 )\nVhat &lt;- mean( (X-Xbar)^2 * Uhat^2 ) / ( varX^2 )\nn &lt;- nrow(mtcars)\nse &lt;- sqrt(Vhat)/sqrt(n)\nt_stat &lt;- bet1/se\np_val &lt;- 2*pnorm(-abs(t_stat))\nci_L &lt;- bet1 - 1.96*se\nci_U &lt;- bet1 + 1.96*se\n\n# print results\nround(data.frame(se, t_stat, p_val, ci_L, ci_U),5)\n\n       se   t_stat p_val     ci_L     ci_U\n1 0.01313 -5.19644     0 -0.09396 -0.04249\n\n\nInterestingly, these are not exactly the same as what comes from the lm command. Here’s what the difference is: R makes a simplifying assumption called “homoskedasticity” that simplifies the expression for the variance. This can result in slightly different standard errors (and therefore slightly different t-statistics, p-values, and confidence intervals too) than the ones we calculated.\nAn alternative package that is popular among economists for estimating regressions and getting “heteroskedasticity robust” standard errors is the estimatr package.\n\nlibrary(estimatr)\n\nreg9 &lt;- lm_robust(mpg ~ hp, data=mtcars, se_type=\"HC0\")\nsummary(reg9)\n\n\nCall:\nlm_robust(formula = mpg ~ hp, data = mtcars, se_type = \"HC0\")\n\nStandard error type:  HC0 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper DF\n(Intercept) 30.09886    2.01067  14.970 1.851e-15 25.99252 34.20520 30\nhp          -0.06823    0.01313  -5.196 1.338e-05 -0.09504 -0.04141 30\n\nMultiple R-squared:  0.6024 ,   Adjusted R-squared:  0.5892 \nF-statistic:    27 on 1 and 30 DF,  p-value: 1.338e-05\n\n\nThe “HC0” standard errors are “heteroskedasticity consistent” standard errors, and you can see that they match what we calculated above.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10-regression_inference.html#lab-4-birthweight-and-smoking",
    "href": "10-regression_inference.html#lab-4-birthweight-and-smoking",
    "title": "11  Inference",
    "section": "11.2 Lab 4: Birthweight and Smoking",
    "text": "11.2 Lab 4: Birthweight and Smoking\nFor this lab, we’ll use the data Birthweight_Smoking and study the relationship between infant birthweight and mother’s smoking behavior.\n\nRun a regression of \\(birthweight\\) on \\(smoker\\). How do you interpret the results?\nUse the datasummary_balance function from the modelsummary package to provide summary statistics for each variable in the data separately by smoking status of the mother. Do you notice any interesting patterns?\nNow run a regression of \\(birthweight\\) on \\(smoker\\), \\(educ\\), \\(nprevisit\\), \\(age\\), and \\(alcohol\\). How do you interpret the coefficient on \\(smoker\\)? How does its magnitude compare to the result from #1? What do you make of this?\nNow run a regression of \\(birthweight\\) on \\(smoker\\), the interaction of \\(smoker\\) and \\(age\\) and the other covariates (including \\(age\\)) from #3. How do you interpret the coefficient on \\(smoker\\) and the coefficient on the interaction term?\nNow run a regression of \\(birthweight\\) on \\(smoker\\), the interaction of \\(smoker\\) and \\(alcohol\\) and the other covariates from #3. How do you interpret the coefficient on \\(smoker\\) and the coefficient on the interaction term?\nNow run a regression of \\(birthweight\\) on \\(age\\) and \\(age^2\\). Plot the predicted value of birthweight as a function of age for ages from 18 to 44. What do you make of this?\nNow run a regression of \\(\\log(birthweight)\\) on \\(smoker\\) and the other covariates from #3. How do you interpret the coefficient on \\(smoker\\)?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10-regression_inference.html#lab-4-solutions",
    "href": "10-regression_inference.html#lab-4-solutions",
    "title": "11  Inference",
    "section": "11.3 Lab 4: Solutions",
    "text": "11.3 Lab 4: Solutions\n\n# load packages\nlibrary(haven)\nlibrary(modelsummary)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# load data\nBirthweight_Smoking &lt;- read_dta(\"data/birthweight_smoking.dta\")\n\n\n\n\n\nreg1 &lt;- lm(birthweight ~ smoker, data=Birthweight_Smoking)\nsummary(reg1)\n\n\nCall:\nlm(formula = birthweight ~ smoker, data = Birthweight_Smoking)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3007.06  -313.06    26.94   366.94  2322.94 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3432.06      11.87 289.115   &lt;2e-16 ***\nsmoker       -253.23      26.95  -9.396   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 583.7 on 2998 degrees of freedom\nMultiple R-squared:  0.0286,    Adjusted R-squared:  0.02828 \nF-statistic: 88.28 on 1 and 2998 DF,  p-value: &lt; 2.2e-16\n\n\nWe estimate that, on average, smoking reduces an infant’s birthweight by about 250 grams. The estimated effect is strongly statistically significant, and (I am not an expert but) that seems like a large effect of smoking to me.\n\n\n\n\n# create smoker factor --- just to make table look nicer\nBirthweight_Smoking$smoker_factor &lt;- as.factor(ifelse(Birthweight_Smoking$smoker==1, \"smoker\", \"non-smoker\"))\ndatasummary_balance(~smoker_factor, \n                    data=dplyr::select(Birthweight_Smoking, -smoker),\n                    fmt=2)\n\n\n\n    \n\n    \n    \n      \n        \n\n \nnon-smoker (N=2418)\nsmoker (N=582)\n \n \n\n        \n              \n                 \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n                Diff. in Means\n                Std. Error\n              \n        \n        \n        \n                \n                  nprevist   \n                  11.19  \n                  3.50  \n                  10.18  \n                  4.23  \n                  -1.01  \n                  0.19 \n                \n                \n                  alcohol    \n                  0.01   \n                  0.11  \n                  0.05   \n                  0.22  \n                  0.04   \n                  0.01 \n                \n                \n                  tripre1    \n                  0.83   \n                  0.38  \n                  0.70   \n                  0.46  \n                  -0.13  \n                  0.02 \n                \n                \n                  tripre2    \n                  0.14   \n                  0.34  \n                  0.22   \n                  0.41  \n                  0.08   \n                  0.02 \n                \n                \n                  tripre3    \n                  0.03   \n                  0.16  \n                  0.06   \n                  0.24  \n                  0.04   \n                  0.01 \n                \n                \n                  tripre0    \n                  0.01   \n                  0.08  \n                  0.02   \n                  0.15  \n                  0.02   \n                  0.01 \n                \n                \n                  birthweight\n                  3432.06\n                  584.62\n                  3178.83\n                  580.01\n                  -253.23\n                  26.82\n                \n                \n                  unmarried  \n                  0.18   \n                  0.38  \n                  0.43   \n                  0.50  \n                  0.25   \n                  0.02 \n                \n                \n                  educ       \n                  13.15  \n                  2.21  \n                  11.88  \n                  1.62  \n                  -1.27  \n                  0.08 \n                \n                \n                  age        \n                  27.27  \n                  5.37  \n                  25.32  \n                  5.06  \n                  -1.95  \n                  0.24 \n                \n                \n                  drinks     \n                  0.03   \n                  0.47  \n                  0.19   \n                  1.23  \n                  0.16   \n                  0.05 \n                \n        \n      \n    \n\n\n\nThe things that stand out to me are:\n\nBirthweight tends to be notably lower for smokers relative to non-smokers. The difference is about 7.4% lower birthweight for babies whose mothers smoked.\nThat said, smoking is also correlated with a number of other things that could be related to lower birthweights. Mothers who smoke went to fewer pre-natal visits on average, were more likely to be unmarried, were more likely to have drink alcohol during their pregnancy, were more likely to be less educated. They also were, on average, somewhat younger than mothers who did not smoke.\n\n\n\n\n\nreg3 &lt;- lm(birthweight ~ smoker + educ + nprevist + age + alcohol,\n           data=Birthweight_Smoking)\nsummary(reg3)\n\n\nCall:\nlm(formula = birthweight ~ smoker + educ + nprevist + age + alcohol, \n    data = Birthweight_Smoking)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2728.91  -305.26    24.69   359.63  2220.42 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2924.963     74.185  39.428  &lt; 2e-16 ***\nsmoker      -206.507     27.367  -7.546 5.93e-14 ***\neduc           5.644      5.532   1.020    0.308    \nnprevist      32.979      2.914  11.318  &lt; 2e-16 ***\nage            2.360      2.178   1.083    0.279    \nalcohol      -39.512     76.365  -0.517    0.605    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 570.3 on 2994 degrees of freedom\nMultiple R-squared:  0.07402,   Adjusted R-squared:  0.07247 \nF-statistic: 47.86 on 5 and 2994 DF,  p-value: &lt; 2.2e-16\n\n\nHere we estimate that smoking reduces an infant’s birthweight by about 200 grams on average holding education, number of pre-natal visits, age, and whether or not the mother consumed alcohol constant. The magnitude of the estimated effect is somewhat smaller than the previous estimate. Due to the discussion in #2 (particularly, that smoking was correlated with a number of other characteristics that are likely associated with lower birthweights), this decrease in the magnitude is not surprising.\n\n\n\n\nreg4 &lt;- lm(birthweight ~ smoker + I(smoker*age) + educ + nprevist + age + alcohol,\n           data=Birthweight_Smoking)\nsummary(reg4)\n\n\nCall:\nlm(formula = birthweight ~ smoker + I(smoker * age) + educ + \n    nprevist + age + alcohol, data = Birthweight_Smoking)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2722.56  -305.12    23.93   363.43  2244.67 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     2853.819     77.104  37.013  &lt; 2e-16 ***\nsmoker           231.578    134.854   1.717 0.086036 .  \nI(smoker * age)  -17.145      5.168  -3.317 0.000919 ***\neduc               4.895      5.528   0.885 0.375968    \nnprevist          32.482      2.913  11.151  &lt; 2e-16 ***\nage                5.528      2.375   2.328 0.019999 *  \nalcohol          -22.556     76.409  -0.295 0.767864    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 569.4 on 2993 degrees of freedom\nMultiple R-squared:  0.07741,   Adjusted R-squared:  0.07556 \nF-statistic: 41.85 on 6 and 2993 DF,  p-value: &lt; 2.2e-16\n\n\nWe should be careful about the interpretatio here. We have estimated a model like\n\\[\n  \\E[Birthweight|Smoker, Age, X] = \\beta_0 + \\beta_1 Smoker + \\beta_2 Smoker \\cdot Age + \\cdots\n\\] Therefore, the partial effect of smoking is given by\n\\[\n  \\E[Birthweight | Smoker=1, Age, X] - \\E[Birthweight | Smoker=0, Age, X] = \\beta_1 + \\beta_2 Age\n\\] Therefore, the partial effect of smoking depends on \\(Age\\). For example, for \\(Age=18\\), the partial effect is \\(\\beta_1 + \\beta_2 (18)\\). For \\(Age=25\\), the partial effect is \\(\\beta_1 + \\beta_2 (25)\\), and for \\(Age=35\\), the partial effect is \\(\\beta_1 + \\beta_2 (35)\\). Let’s calculate the partial effect at each of those ages.\n\nbet1 &lt;- coef(reg4)[2]\nbet2 &lt;- coef(reg4)[3]\n\npe_18 &lt;- bet1 + bet2*18\npe_25 &lt;- bet1 + bet2*25\npe_35 &lt;- bet1 + bet2*35\n\nround(cbind.data.frame(pe_18, pe_25, pe_35),2)\n\n        pe_18   pe_25   pe_35\nsmoker -77.04 -197.05 -368.51\n\n\nThis suggests substantially larger effects of smoking on birthweight for older mothers.\n\nreg5 &lt;- lm(birthweight ~ smoker + I(smoker*alcohol) + educ + nprevist + age + alcohol,\n           data=Birthweight_Smoking)\nsummary(reg5)\n\n\nCall:\nlm(formula = birthweight ~ smoker + I(smoker * alcohol) + educ + \n    nprevist + age + alcohol, data = Birthweight_Smoking)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2728.99  -304.16    24.54   359.92  2222.10 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2924.844     74.185  39.426  &lt; 2e-16 ***\nsmoker              -201.852     27.765  -7.270 4.57e-13 ***\nI(smoker * alcohol) -151.860    152.717  -0.994    0.320    \neduc                   5.612      5.532   1.014    0.310    \nnprevist              32.844      2.917  11.260  &lt; 2e-16 ***\nage                    2.403      2.178   1.103    0.270    \nalcohol               39.824    110.440   0.361    0.718    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 570.3 on 2993 degrees of freedom\nMultiple R-squared:  0.07432,   Adjusted R-squared:  0.07247 \nF-statistic: 40.05 on 6 and 2993 DF,  p-value: &lt; 2.2e-16\n\n\nThe point estimate suggests that the effect of smoking is larger for women who consume alcohol and smoke than for women who do not drink alcohol. This seems plausible, but our evidence is not very strong here — the estimates are not statistically significant at any conventional significance level (the p-value is equal to 0.32).\n\n\n\n\nreg6 &lt;- lm(birthweight ~ age + I(age^2), data=Birthweight_Smoking)\nsummary(reg6)\n\n\nCall:\nlm(formula = birthweight ~ age + I(age^2), data = Birthweight_Smoking)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2949.81  -312.81    30.43   371.03  2452.72 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2502.8949   225.6016  11.094  &lt; 2e-16 ***\nage           58.1670    16.9212   3.438 0.000595 ***\nI(age^2)      -0.9099     0.3099  -2.936 0.003353 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 589.6 on 2997 degrees of freedom\nMultiple R-squared:  0.009261,  Adjusted R-squared:  0.0086 \nF-statistic: 14.01 on 2 and 2997 DF,  p-value: 8.813e-07\n\n\n\npreds &lt;- predict(reg6, newdata=data.frame(age=seq(18,40)))\nggplot(data.frame(preds=preds, age=seq(18,40)), aes(x=age, y=preds)) + \n  geom_line() + \n  geom_point(size=3) + \n  theme_bw() + \n  ylab(\"predicted values\")\n\n\n\n\n\n\n\n\nThe figure suggests that predicted birthweight is increasing in mother’s age up until about age 34 and then decreasing after that.\n\n\n\n\nreg7 &lt;- lm(I(log(birthweight)) ~ smoker + educ + nprevist + age + alcohol,\n           data=Birthweight_Smoking)\nsummary(reg7)\n\n\nCall:\nlm(formula = I(log(birthweight)) ~ smoker + educ + nprevist + \n    age + alcohol, data = Birthweight_Smoking)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.96324 -0.07696  0.02435  0.12092  0.50070 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.9402678  0.0270480 293.562  &lt; 2e-16 ***\nsmoker      -0.0635764  0.0099782  -6.372 2.16e-10 ***\neduc         0.0022169  0.0020171   1.099    0.272    \nnprevist     0.0129662  0.0010624  12.205  &lt; 2e-16 ***\nage          0.0003059  0.0007941   0.385    0.700    \nalcohol     -0.0181053  0.0278428  -0.650    0.516    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2079 on 2994 degrees of freedom\nMultiple R-squared:  0.07322,   Adjusted R-squared:  0.07167 \nF-statistic: 47.31 on 5 and 2994 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated coefficient on \\(smoker\\) says that smoking during pregnancy decreases a baby’s birthweight by 6.3%, on average, holding education, number of pre-natal visits, age of the mother, and whether or not the mother consumed alcohol during the pregnancy constant.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10-regression_inference.html#coding-questions",
    "href": "10-regression_inference.html#coding-questions",
    "title": "11  Inference",
    "section": "11.4 Coding Questions",
    "text": "11.4 Coding Questions\n\nFor this problem, we will use the data Caschool. This data contains information about test scores for schools from California from the 1998-1999 academic year. For this problem, we will use the variables testscr (average test score in the school), str (student teacher ratio in the school), avginc (the average income in the school district), and elpct (the percent of English learners in the school).\n\nRun a regression of test scores on student teacher ratio, average income, and English learners percentage. Report your results. Which regressors are statistically significant? How do you know?\nWhat is the average test score across all schools in the data?\nWhat is the predicted average test score for a school with a student teacher ratio of 20, average income of $30,000, and 10% English learners? How does this compare to the overall average test score from part (b)?\nWhat is the predicted average test score for a school with a student teacher ratio of 15, average income of $30,000, and 10% English learners? How does this compare to your answer from part (c)?\n\nFor this problem, we will use the data intergenerational_mobility.\n\nRun a regression of child family income (\\(child\\_fincome\\)) on parents’ family income (\\(parent\\_fincome\\)). How should you interpret the estimated coefficient on parents’ family income? What is the p-value for the coefficient on parents’ family income?\nRun a regression of \\(\\log(child\\_fincome)\\) on \\(parent\\_fincome\\). How should you interpret the estimated cofficient on \\(parent\\_fincome\\)?\nRun a regression of \\(child\\_fincome\\) on \\(\\log(parent\\_fincome)\\). How should you interpret the estimated coefficient on \\(\\log(parent\\_fincome)\\)?\nRun a regression of \\(\\log(child\\_fincome)\\) on \\(\\log(parent\\_fincome)\\). How should you interpret the estimated coefficient on \\(\\log(parent\\_fincome)\\)?\n\nFor this question, we’ll use the fertilizer_2000 data.\n\nRun a regression of \\(\\log(avyield)\\) on \\(\\log(avfert)\\). How do you interpret the estimated coefficient on \\(\\log(avfert)\\)?\nNow suppose that you additionally want to control for precipitation and the region that a country is located in. How would you do this? Estimate the model that you propose here, report the results, and interpret the coefficient on \\(\\log(avfert)\\).\nNow suppose that you are interested in whether the effect of fertilizer varies by region that a country is located in (while still controlling for the same covariates as in part (b)). Propose a model that can be used for this purpose. Estimate the model that you proposed, report the results, and discuss whether the effect of fertilizer appears to vary by region or not.\n\nFor this question, we will use the data mutual_funds. We’ll be interested in whether mutual funds that have higher expense ratios (these are typically actively managed funds) have higher returns relative to mutual funds that have lower expense ratios (e.g., index funds). For this problem, we will use the variables fund_return_3years, investment_type, risk_rating, size_type, fund_net_annual_expense_ratio, asset_cash, asset_stocks, asset_bonds.\n\nCalculate the median fund_net_annual_expense_ratio.\nUse the datasummary_balance function from the modelsummary package to report summary statistics for fund_return_3year, fund_net_annual_expense_ratio, risk_rating, asset_cash, asset_stocks, asset_bonds based on whether their expense ratio is above or below the median. Do you notice any interesting patterns?\nRun a regression of fund_return_3years on fund_net_annual_expense_ratio. How do you interpret the results?\nNow, additionally control for investment_type, risk_rating, and size_type Hint: think carefully about what type of variables each of these are and how they should enter the model. How do these results compare to the ones from part c?\nNow, add the variables assets_cash, assets_stocks, and assets_bonds to the model from part d. How do you interpret these results? Compare and interpret the differences between parts c, d, and e.\n\nFor this question, we’ll use the data Lead_Mortality to study the effect of lead pipes on infant mortality in 1900.\n\nRun a regression of infant mortality (infrate) on whether or not a city had lead pipes (lead) and interpret/discuss the results.\nIt turns out that the amount of lead in drinking water depends on how acidic the water is, with more acidic water leaching more of the lead (so that there is more exposure to lead with more acidic water). To measure acidity, we’ll use the pH of the water in a particular city (ph); recall that, a lower value of pH indicates higher acidity. Run a regression of infant mortality on whether or not a city has lead pipes, the pH of its water, and the interaction between having lead pipes and pH. Report your results. What is the estimated partial effect of having lead pipes from this model?\nGiven the results in part b, calculate an estimate of the average partial effect of having lead pipes on infant mortality.\nGiven the results in part b, how much does the partial effect of having lead pipes differ for cities that have a pH of 6.5 relative to a pH of 7.5?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10-regression_inference.html#extra-questions",
    "href": "10-regression_inference.html#extra-questions",
    "title": "11  Inference",
    "section": "11.5 Extra Questions",
    "text": "11.5 Extra Questions\n\nSuppose you run the following regression \\[\\begin{align*}\n  Earnings = \\beta_0 + \\beta_1 Education + U\n\\end{align*}\\] with \\(\\E[U|Education] = 0\\). How do you interpret \\(\\beta_1\\) here?\nSuppose you run the following regression \\[\\begin{align*}\n  Earnings = \\beta_0 + \\beta_1 Education + \\beta_2 Experience + \\beta_3 Female + U\n\\end{align*}\\] with \\(\\E[U|Education, Experience, Female] = 0\\). How do you interpret \\(\\beta_1\\) here?\nSuppose you are interested in testing whether an extra year of education increases earnings by the same amount for men and women.\n\nPropose a regression and strategy for this sort of test.\nSuppose you also want to control for experience in conducting this test, how would do it?\n\nSuppose you run the following regression \\[\\begin{align*}\n  \\log(Earnings) = \\beta_0 + \\beta_1 Education + \\beta_2 Experience + \\beta_3 Female + U\n\\end{align*}\\] with \\(\\E[U|Education, Experience, Female] = 0\\). How do you interpret \\(\\beta_1\\) here?\nA common extra condition (though somewhat old-fashioned) is to impose homoskedasticity. Homoskedasticity says that \\(\\E[U^2|X] = \\sigma^2\\) (i.e., the variance of the error term does not change across different values of \\(X\\)).\n\nUnder homoskedasticity, the expression for \\(V\\) in Equation 11.4 simplifies. Provide a new expression for \\(V\\) under homoskedasticity. Hint: you will need to use the law of iterated expectations.\nUsing this expression for \\(V\\), explain how to calculate standard errors for an estimate of \\(\\beta_1\\) in a simple linear regression.\nExplain how to construct a t-statistic for testing \\(H_0: \\beta_1=0\\) under homoskedasticity.\nExplain how to contruct a p-value for \\(\\beta_1\\) under homoskedasticity.\nExplain how to construct a 95% confidence interval for \\(\\beta_1\\) under homoskedasticity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html",
    "href": "11-binary_outcomes.html",
    "title": "12  Binary Outcome Models",
    "section": "",
    "text": "12.1 Linear Probability Model\nSW 11.1\nLet’s continue to consider\n\\[\n  \\E[Y|X_1,X_2,X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n\\]\nwhen \\(Y\\) is binary. Of course, you can still run this regression.\nOne thing that is helpful to notice before we really get started here is that when \\(Y\\) is binary (so that either \\(Y=0\\) or \\(Y=1\\))\n\\[\n  \\begin{aligned}\n  \\E[Y] &= \\sum_{y \\in \\mathcal{Y}} y \\P(Y=y) \\\\\n  &= 0 \\P(Y=0) + 1 \\P(Y=1) \\\\\n  &= \\P(Y=1)\n  \\end{aligned}\n\\] And exactly the same sort of argument implies that, when \\(Y\\) is binary, \\(\\E[Y|X] = \\P(Y=1|X)\\). Thus, if we believe the model in the first part of this section, this result implies that\n\\[\n  \\P(Y=1|X_1,X_2,X_3) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n\\] For this reason, the model in this section is called the linear probabilty model. Moreover, this further implies that we should interpret\n\\[\n  \\beta_1 = \\frac{\\partial \\, \\P(Y=1|X_1,X_2,X_3)}{\\partial \\, X_1}\n\\] as a partial effect. That is, \\(\\beta_1\\) is how much the probability that \\(Y=1\\) changes when \\(X_1\\) increases by one unit, holding \\(X_2\\) and \\(X_3\\) constant. This is good (and simple), but there are some drawbacks:\nWe can circumvent both of the main problems with the linear probability model by consider nonlinear models for binary outcomes. By far the most common are probit and logit. We will discuss these next.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html#linear-probability-model",
    "href": "11-binary_outcomes.html#linear-probability-model",
    "title": "12  Binary Outcome Models",
    "section": "",
    "text": "It’s possible to get non-sensical predictions (predicted probabilities that are less than 0 or greater than 1) with a linear probability model.\nA related problem is that the linear probability model implies constant partial effects. That is, the effect of a change in one regressor always changes the probability of \\(Y=1\\) (holding other regressors constant) by the same amount. It may not be obvious that this is a disadvantage, but it is.\n\n\nExample: Let \\(Y=1\\) if an individual participates in the labor force. Further let \\(X_1=1\\) if an individual is male and 0 otherwise, \\(X_2\\) denote an individual’s age, and \\(X_3=1\\) for college graduates and 0 otherwise.\nAdditionally, suppose that \\(\\beta_0=0.4, \\beta_1=0.2, \\beta_2=0.01, \\beta_3=0.1\\).\nLet’s calculate the probability of being in the labor force for a 40 year old woman who is not a college graduate. This is given by\n\\[\n  \\P(Y=1 | X_1=0, X_2=40, X_3=0) = 0.4 + (0.01)(40) = 0.8\n\\] In other words, we’d predict that, given these characteristics, the probability of being in the labor force is 0.8.\nNow, let’s calculate the probability of being in the labor force for a 40 year old man who is a college graduate. This is given by\n\\[\n  \\P(Y=1|X_1=1, X_2=40, X_3=1) = 0.4 + 0.2 + (0.01)(40) + 0.1 = 1.1\n\\] We have calculated that the predicted probability of being in the labor force, given these characteristics, is 1.1 — this makes no sense! Our maximum predicted probabilty should be 1.\nThe problem of constant partial effect is closely related. Here, labor force participation is increasing in age, but with a binary outcome (by construction) the effect has to die off — for those who are already very likely to participate in the labor force (in this example, older men with a college education, the partial effect of age has to be low because they are already very likely to participate in the labor force).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html#probit-and-logit",
    "href": "11-binary_outcomes.html#probit-and-logit",
    "title": "12  Binary Outcome Models",
    "section": "12.2 Probit and Logit",
    "text": "12.2 Probit and Logit\nSW 11.2, 11.3\nLet’s start this section with probit. A probit model arises from setting\n\\[\n  \\P(Y=1|X_1,X_2,X_3) = \\Phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3)\n\\] where \\(\\Phi\\) is the cdf of a standard normal random variable. This is a nonlinear model due to \\(\\Phi\\) making the model nonlinear in parameters.\nUsing \\(\\Phi\\) (or any cdf) here has a useful property that no matter what value the “index” \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\) takes on, the cdf is always between 0 and 1. This implies that we cannot get predicted probabilities outside of 0 and 1.\nThus, this circumvents the problems with the linear probability model. That said, there are some things we have to be careful about. First, as usual, we are interested in partial effects rather than the parameters themselves. But partial effects are more complicated here. Notice that\n\\[\n  \\begin{aligned}\n  \\frac{ \\partial \\, P(Y=1|X_1,X_2,X_3)}{\\partial \\, X_1} &= \\frac{\\partial \\, \\Phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3)}{\\partial \\, X_1} \\\\\n  &= \\phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3) \\beta_1\n  \\end{aligned}\n\\] where \\(\\phi\\) is the pdf of a standard normal random variable. And the second equality requires using the chain rule — take the derivative of the “outside” (i.e., \\(\\Phi\\)) and then the derivative of the “inside” with respect to \\(X_1\\). Notice that this partial effect is more complicated that in the case of the linear models that we have mainly considered — it involves \\(\\phi\\), but more importantly it also depends on the values of all the covariates. In other words, the partial effect of \\(X_1\\) can vary across different values of \\(X_1\\), \\(X_2\\), and \\(X_3\\).\nLogit is conceptually similar to probit, but instead of using \\(\\Phi\\), Logit uses the logistic function \\(\\Lambda(z) = \\frac{\\exp(z)}{1+\\exp(z)}\\). The logistic function has the same important properties as \\(\\Phi\\): (i) \\(\\Lambda(z)\\) is increasing in \\(z\\), (ii) \\(\\Lambda(z) \\rightarrow 1\\) as \\(z \\rightarrow \\infty\\), and (iii) \\(\\Lambda(z) \\rightarrow 0\\) as \\(z \\rightarrow -\\infty\\). Thus, in a logit model,\n\\[\n  \\begin{aligned}\n  \\P(Y=1 | X_1, X_2, X_3) &= \\Lambda(\\beta_0 + \\beta_1 X_1 + \\beta_2 + \\beta_3 X_3) \\\\\n  &= \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3)}{1+\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3)}\n  \\end{aligned}\n\\]\nEstimation\nBecause probit and logit models are nonlinear, estimation is more complicated than for the linear regression models that we were studying before. In particular, we cannot write down a formula like \\(\\hat{\\beta}_1 = \\textrm{something}\\).\nInstead, probit and logit models are typically esetimated through an approach called maximum likelihood estimation. Basically, the computer will solve an optimization problem trying to choose the “most likely” values of the parameters given the data that you have. It turns out that this particular optimization problem is actually quite easy for the computer to solve — even though estimating the parameters is more complicated than for linear regression, it will still feel like R can estimate a probit or logit model pretty much instantly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html#average-partial-effects",
    "href": "11-binary_outcomes.html#average-partial-effects",
    "title": "12  Binary Outcome Models",
    "section": "12.3 Average Partial Effects",
    "text": "12.3 Average Partial Effects\nOne of the complications with Probit and Logit is that it is not so simple to interpret the estimated parameters.\nRemember we are generally interested in partial effects, not the parameters themselves. It just so happens that in many of the linear models that we have considered so far the \\(\\beta\\)’s correspond to the partial effect — this means that it is sometimes easy to forget that they are not what we are typically most interested in.\nThis is helpful framing for thinking about how to interpret the results from a Probit or Logit model.\nLet’s focus on the Probit model. In that case, \\[\\begin{align*}\n  \\P(Y=1|X_1,X_2,X_3) = \\Phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3)\n\\end{align*}\\] where \\(\\Phi\\) is the cdf of standard normal random variable.\nContinuous Case: When \\(X_1\\) is continuous, the partial effect of \\(X_1\\) is given by \\[\\begin{align*}\n  \\frac{\\partial \\P(Y=1|X_1,X_2,X_3)}{\\partial X_1} = \\phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3) \\beta_1\n\\end{align*}\\] where \\(\\phi\\) is the pdf of a standard normal random variable. This is more complicated than the partial effect in the context of a linear model. It depends on \\(\\phi\\) (which looks complicated, but you can just use R’s dnorm command to handle that part). More importantly, the partial effect depends on the values of \\(X_1,X_2,\\) and \\(X_3\\). [As discussed above, this is likely a good thing in the context of a binary outcome model]. Thus, in order to get a partial effect, we need to put in some values for these. If you have particular values of the covariates that you are interested in, you can definitely do that, but my general suggestion is to report the Average Partial Effect: \\[\\begin{align*}\n  APE &= \\E\\left[ \\frac{\\partial \\P(Y=1|X_1,X_2,X_3)}{\\partial X_1} \\right] \\\\\n  &= \\E\\left[ \\phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3) \\beta_1 \\right]\n\\end{align*}\\] which you can estimate by \\[\\begin{align*}\n  \\widehat{APE} &= \\frac{1}{n} \\sum_{i=1}^n \\phi(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\hat{\\beta}_3 X_3) \\hat{\\beta}_1\n\\end{align*}\\] which amounts to just computing the partial effect at each value of the covariates in your data and then averaging these partial effects together. This can be a bit cumbersome to do in practice, and it is often convenient to use the R package mfx to compute these sorts of average partial effects for you.\nDiscrete/Binary Case: When \\(X_1\\) is discrete (let’s say binary, but extention to discrete is straightforward), the partial effect of \\(X_1\\) is \\[\\begin{align*}\n  & \\P(Y=1|X_1=1, X_2, X_3) - \\P(Y=1|X_1=0, X_2, X_3) \\\\\n  &\\hspace{100pt} = \\Phi(\\beta_0 + \\beta_1 + \\beta_2 X_2 + \\beta_3 X_3) - \\Phi(\\beta_0 + \\beta_2 X_2 + \\beta_3 X_3)\n\\end{align*}\\] Notice that \\(\\beta_1\\) does not show up in the last term. As above, the partial effect depends on the values of \\(X_2\\) and \\(X_3\\) which suggests reporting an \\(APE\\) as above (follows the same steps, just replacing the partial effect, as in the continuous case above)\n\nExtensions to Logit are virtually identical, just replace \\(\\Phi\\) with \\(\\Lambda\\) and \\(\\phi\\) with \\(\\lambda\\).\n\n\nSide-Comment: The parameters from LPM, Probit, and Logit could be quite different (in fact, they are quite different by construction), but APE’s are often very similar.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html#lab-5-estimating-binary-outcome-models",
    "href": "11-binary_outcomes.html#lab-5-estimating-binary-outcome-models",
    "title": "12  Binary Outcome Models",
    "section": "12.4 Lab 5: Estimating Binary Outcome Models",
    "text": "12.4 Lab 5: Estimating Binary Outcome Models\nIn this lab, I’ll demonstrate how to estimate and interpret binary outcome models using the titanic_training data. The outcome for this data is Survived which is a binary variable indicating whether a particular passenger survived the Titanic wreck. We’ll estimate models that also include passenger class (Pclass), passenger’s sex, and passenger’s age.\nThe main R function for estimating binary outcome models is the glm function (this stands for “generalized linear model”). The syntax is very similar to the syntax for the lm command, so much of what we do below will feel feel familiar. We’ll also use the probitmfx and logitmfx functions from the mfx package to compute partial effects.\nLinear Probability Model\nWe’ll start by estimating a linear probability model.\n\n# load the data\ntitanic_train &lt;- read.csv(\"data/titanic_training.csv\")\n\n# linear probability model\nlpm &lt;- lm(Survived ~ as.factor(Pclass) + \n            Sex + Age, \n          data=titanic_train)\nsummary(lpm)\n\n\nCall:\nlm(formula = Survived ~ as.factor(Pclass) + Sex + Age, data = titanic_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05638 -0.26294 -0.07656  0.21103  0.98057 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.065516   0.061481  17.331  &lt; 2e-16 ***\nas.factor(Pclass)2 -0.148575   0.050593  -2.937 0.003472 ** \nas.factor(Pclass)3 -0.349809   0.046462  -7.529 2.44e-13 ***\nSexmale            -0.490605   0.037330 -13.143  &lt; 2e-16 ***\nAge                -0.004570   0.001317  -3.471 0.000564 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3915 on 495 degrees of freedom\nMultiple R-squared:  0.3747,    Adjusted R-squared:  0.3696 \nF-statistic: 74.14 on 4 and 495 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s quickly interpret a couple of these parameters. Recall that these can all be directly interpreted as partial effects on the probability of surviving the Titanic wreck. For example, these estimates indicate that third class passengers were about 33% less likely to survive on average than first class passengers controlling for passenger’s sex and age.\nThe other thing that jumps out is passenger’s sex. These estimates indicate that men were, on average, 49% less likely to survive the Titanic wreck than women after controlling for passenger class, and age.\nBefore we move on, let’s compute a couple of predicted probabilities.\n\n# young, first class, female\npred_df1 &lt;- data.frame(Pclass=1, Sex=\"female\", Age=25, Embarked=\"S\")\n\n# old, third class, male\npred_df2 &lt;- data.frame(Pclass=3, Sex=\"male\", Age=55, Embarked=\"S\")\npred_df &lt;- rbind.data.frame(pred_df1, pred_df2)\nround(predict(lpm, newdata=pred_df), 3)\n\n     1      2 \n 0.951 -0.026 \n\n\nThis illustrates that there were very large differences in survival probabilities. It also demonstrates that the linear probability model can deliver non-sensical predictions — we predict the survival probability of a 55 year old, male, third-class passenger to be -3%.\nEstimating Probit and Logit Models\nLet’s estimate Probit and Logit models using the same specifications. First, Probit:\n\n# probit\nprobit &lt;- glm(Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), \n              family=binomial(link=\"probit\"), \n              data=titanic_train)\nsummary(probit)\n\n\nCall:\nglm(formula = Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), \n    family = binomial(link = \"probit\"), data = titanic_train)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            5.418219 146.954198   0.037  0.97059    \nas.factor(Pclass)2    -0.445235   0.197398  -2.256  0.02410 *  \nas.factor(Pclass)3    -1.145788   0.188508  -6.078 1.22e-09 ***\nSexmale               -1.464201   0.136512 -10.726  &lt; 2e-16 ***\nAge                   -0.015764   0.005009  -3.147  0.00165 ** \nas.factor(Embarked)C  -3.443295 146.954199  -0.023  0.98131    \nas.factor(Embarked)Q  -3.464431 146.954544  -0.024  0.98119    \nas.factor(Embarked)S  -3.662911 146.954180  -0.025  0.98011    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 678.28  on 499  degrees of freedom\nResidual deviance: 468.38  on 492  degrees of freedom\nAIC: 484.38\n\nNumber of Fisher Scoring iterations: 12\n\n\nNow, Logit:\n\n# logit\nlogit &lt;- glm(Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), \n              family=binomial(link=\"logit\"), \n              data=titanic_train)\nsummary(logit)\n\n\nCall:\nglm(formula = Survived ~ as.factor(Pclass) + Sex + Age + as.factor(Embarked), \n    family = binomial(link = \"logit\"), data = titanic_train)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           14.646649 535.411272   0.027  0.97818    \nas.factor(Pclass)2    -0.793003   0.342058  -2.318  0.02043 *  \nas.factor(Pclass)3    -2.055828   0.335386  -6.130  8.8e-10 ***\nSexmale               -2.461563   0.241113 -10.209  &lt; 2e-16 ***\nAge                   -0.028436   0.008764  -3.245  0.00118 ** \nas.factor(Embarked)C -11.262004 535.411276  -0.021  0.98322    \nas.factor(Embarked)Q -11.229232 535.411568  -0.021  0.98327    \nas.factor(Embarked)S -11.593053 535.411259  -0.022  0.98273    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 678.28  on 499  degrees of freedom\nResidual deviance: 467.01  on 492  degrees of freedom\nAIC: 483.01\n\nNumber of Fisher Scoring iterations: 12\n\n\nIt’s rather hard to interpret the parameters in both of these models (let’s defer this to the next section). But it is worth mentioning that all of the estimated coefficients have the same sign for the linear probability model, the probit model, and the logit model, and the same set of regressors have statistically significant effects across models (and the t-statistics/p-values are very similar across models).\nNow, let’s calculate the same predicted probabilities as we did for the linear probability model above:\n\n# for probit\npredict(probit, newdata=pred_df, type=\"response\")\n\n         1          2 \n0.91327666 0.04256272 \n\n# for logit\npredict(logit, newdata=pred_df, type=\"response\")\n\n         1          2 \n0.91235117 0.04618584 \n\n\nBefore we interpret the result, notice that we add the argument type=response (this indicates that we want to get a predicted probability).\nHere (for both models), we estimate that a 25 year old woman traveling first class has a 91% probability of survival (this is slightly smaller than the prediction from the linear probability model). On the other hand, we estimate that a 55 year old man traveling 3rd class has a 4.3% (from probit) or 4.6% (from logit) probability of survival. While these probabilities are still quite low, unlike the estimates from the linear probability model, they are at least positive.\nAverage Partial Effects\nTo conclude this section, let’s calculate average partial effects for each model. First, for probit:\n\nlibrary(mfx)\nprobit_ape &lt;- probitmfx(Survived ~ as.factor(Pclass) + Sex + Age, \n                        data=titanic_train, \n                        atmean=FALSE)\nprobit_ape\n\nCall:\nprobitmfx(formula = Survived ~ as.factor(Pclass) + Sex + Age, \n    data = titanic_train, atmean = FALSE)\n\nMarginal Effects:\n                        dF/dx  Std. Err.        z     P&gt;|z|    \nas.factor(Pclass)2 -0.1303971  0.0424994  -3.0682 0.0021534 ** \nas.factor(Pclass)3 -0.3438262  0.0470404  -7.3092 2.688e-13 ***\nSexmale            -0.4895845  0.0412840 -11.8590 &lt; 2.2e-16 ***\nAge                -0.0042614  0.0012934  -3.2948 0.0009851 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"as.factor(Pclass)2\" \"as.factor(Pclass)3\" \"Sexmale\"           \n\n\nNow, for logit:\n\nlogit_ape &lt;- logitmfx(Survived ~ as.factor(Pclass) + Sex + Age, \n                        data=titanic_train, \n                        atmean=FALSE)\nlogit_ape\n\nCall:\nlogitmfx(formula = Survived ~ as.factor(Pclass) + Sex + Age, \n    data = titanic_train, atmean = FALSE)\n\nMarginal Effects:\n                        dF/dx  Std. Err.        z     P&gt;|z|    \nas.factor(Pclass)2 -0.1304383  0.0420444  -3.1024  0.001920 ** \nas.factor(Pclass)3 -0.3542499  0.0474154  -7.4712 7.947e-14 ***\nSexmale            -0.4859890  0.0413167 -11.7625 &lt; 2.2e-16 ***\nAge                -0.0044098  0.0014205  -3.1045  0.001906 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"as.factor(Pclass)2\" \"as.factor(Pclass)3\" \"Sexmale\"           \n\n\nA couple of things to notice:\n\nThe average partial effects are extremely similar across models. For example, across all three models, the average partial effect of being male is to reduce the probability of survival by 49% controlling for the other variables in the model. The other average partial effects are quite similar across models as well.\nBoth probitmfx and logitmfx functions took in an argument for atmean. We set it equal to FALSE. If you set it equal to TRUE, you will compute a different kind of partial effect. You can check ?probitmfx or ?logitmfx for more details.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html#coding-questions",
    "href": "11-binary_outcomes.html#coding-questions",
    "title": "12  Binary Outcome Models",
    "section": "12.5 Coding Questions",
    "text": "12.5 Coding Questions\n\nFor this problem, we will use the data mroz.\n\nEstimate a probit model where the outcome is whether or not the wife is in the labor force (inlf) using the the number of kids less than 6 (kidslt6) and the number of kids who are 6 or older living in the household (kidsge6). Calculate the average partial effect of each variable. What do you notice?\nNow, add the variables age, educ, and city to the model. Calculate the average partial effects of kidslt6 and kidsge6. How do you interpret these? How do they compare to the answers from part a?\nEstimate a linear probability model and logit model using the same specification as in part b. For each one, how do the estimated coefficients compare to the ones from part b? Compute average partial effects for each model. How do these compare to the ones from part b?\n\nFor this problem, we will use the Fair data.\n\nThe variable nbaffairs contains the number of self-reported affairs that an individual has had in the previous year. Create a variable had_affair that is equal to 1 if an individual had any affair in the past year and that is equal to 0 otherwise. What fraction of individuals in the data have had an affair in the past year?\nEstimate a logit model where the outcome is had_affair and the regressor is whether or not the person has a child (child). Calculate the average partial effect of having a child on having an affair. How do you interpret the results?\nNow add sex, age, education, and occupation to the model. Calculate the average partial effect of each variable. How do you interpret the results? Hint: Make sure to treat the categorical variables in the model as categorical rather than as numeric.\nIn addition to the variables in part c, add the variables years married (ym) and religious to the mode. Calculate the average partial effect of each variable. How do you interpret the results? Hint: Make sure to treat the categorical variables in the model as categorical rather than as numeric.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "11-binary_outcomes.html#extra-questions",
    "href": "11-binary_outcomes.html#extra-questions",
    "title": "12  Binary Outcome Models",
    "section": "12.6 Extra Questions",
    "text": "12.6 Extra Questions\n\nSuppose you try to estimate a linear probability model, probit model, and logit model using the same specifications. You notice that the estimated coefficients are substantially different from each other. Does this mean that something has gone wrong? Explain.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Binary Outcome Models</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html",
    "href": "12-model_selection.html",
    "title": "13  Model Selection",
    "section": "",
    "text": "13.1 Measures of Regression Fit\nWe’ll start this chapter by learning about measures of how well a regression fits the data. Consider the following figure\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nThese are exactly the same two regression lines. But we probably have the sense that the regression line in the second figure “fits better” than in the first figure, and, furthermore, that this is likely to result in better predictions of whatever the second outcome is relative to the first one. We’ll formalize this below.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#measures-of-regression-fit",
    "href": "12-model_selection.html#measures-of-regression-fit",
    "title": "13  Model Selection",
    "section": "",
    "text": "13.1.1 TSS, ESS, SSR\nSW 6.4\nLet’s start by defining some quantities. These will be useful for quantifying how well the model fits the data.\n\nTotal Sum of Squares (TSS) — measures total variation in the data\n\\[\n    TSS = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n  \\]\nExplained Sum of Squares (ESS) — measures variation explained by the model\n\\[\n    ESS = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\n  \\]\nSum of Squared Residuals (SSR) — measures “leftover” variation in the data that is not explained by the model\n\\[\n    SSR = \\sum_{i=1}^n \\hat{U}_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\n  \\]\n\nProperties\n\nA first useful property is\n\\[\n    TSS = ESS + SSR\n  \\]\nWe will not prove this property though it is a useful exercise and not actually that challenging.\nAnother useful property is that \\(TSS\\), \\(ESS\\), and \\(SSR\\) are all positive — this holds because they all involve sums of squared quantities.\n\n\n\n13.1.2 \\(R^2\\)\nSW 6.4\n\\(R^2\\) is probably the most common measure of regression fit. It is defined as\n\\[\n  R^2 := \\frac{ESS}{TSS}\n\\]\nso that \\(R^2\\) is the fraction of the variation in \\(Y\\) explained by the model. Notice that \\(R^2\\) is always between 0 and 1 which holds by the two properties of \\(TSS\\), \\(ESS\\), and \\(SSR\\) listed in the previous section.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#model-selection",
    "href": "12-model_selection.html#model-selection",
    "title": "13  Model Selection",
    "section": "13.2 Model Selection",
    "text": "13.2 Model Selection\nSW 7.5 (note: we cover substantially more details than the textbook about model selection)\nOften, when we want to use data to make predictions, there are multiple possible models that we could estimate to make the predictions. For example, suppose that we are interested in predicting house prices and we have access to data about the number of square feet, whether or not the house has a basement, the number of bedrooms, and the number of bathrooms. All of these are probably good variables to include in a model to predict house prices (though it is less clear if we should include quadratic terms, interaction terms, or other higher order terms). But suppose we also observe some variables that are probably irrelevant (e.g., whether the street number is odd or even) or not clear whether they matter or not (e.g., number of ceiling fans or paint color). It is not clear which variables we should include in the model. An alternative way to think about this is that there would be a large number of possible models that we could estimate here, and it is not immediately obvious which one will predict the best.\nFrom the previous example, it seems that in many realistic applications, there are a large number of possible models that we could estimate to make predictions. How should we choose which one to use?\nOne idea is to just throw all the data that we have into the model. In many applications, this may not be a good idea. I’ll provide a specific example below.\nBefore doing that, I want to distinguish between two concepts. Typically, what we mean we say that a model is good at making predictions is that it is a good at making out-of-sample predictions. In other words, some new observation shows up (e.g., a new house comes on the market) with certain characteristics — we would like to be using a model that makes good predictions for this house. This is a distinct concept from in-sample fit of the model — how well a model predicts for the data that it is estimated on. In the context of prediction, one very common problem is over-fitting. Over-fitting is the problem of getting predictions that are too specific to the particular data that you have, but then that don’t work well for new data.\n\nExample: Suppose that you are interested in predicting height of students at UGA. You consider estimating three models\n\\[\n  \\begin{aligned}\n    Height &= \\beta_0 + \\beta_1 Male + U \\\\\n    Height &= \\beta_0 + \\beta_1 Male + \\beta_2 Year + U \\\\\n    Height &= \\beta_0 + \\beta_1 Male + \\beta_2 Year + \\beta_3 Birthday + U\n  \\end{aligned}\n\\] where \\(Year\\) is what year in school a student is in (e.g., Freshman, Sophomore, etc.), and \\(Birthday\\) is what day of the year a student was born on.\nYou also notice that \\(R^2\\) is greatest for the third model, in the middle for the second model, and smallest for the first model.\nThe third model likely suffers from over-fitting. In particular, knowing a student’s birthday may really help you predict height in-sample. For example, suppose that the center on the basketball team is in your sample, and his birthday is July 15. Knowing this, you will likely be able to make a much better prediction for this observation in your data using Model 3 which \\(\\implies\\) that the \\(R^2\\) will be much higher for this model. But this won’t help you predict well out-of-sample. If a new person shows up whose birthday is July 15, they are unlikely to also be a center on the basketball team which further implies that your prediction of their height is likely to be very poor.\n\nThe previous example is about over-fitting with the idea that better in-sample predictions can actually lead to worse out-of-sample predictions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#limitations-of-r2",
    "href": "12-model_selection.html#limitations-of-r2",
    "title": "13  Model Selection",
    "section": "13.3 Limitations of \\(R^2\\)",
    "text": "13.3 Limitations of \\(R^2\\)\nSW 6.4\n\\(R^2\\) is a measure of in-sample fit. In fact, you can always increase \\(R^2\\) by adding new variables into a model. To see this, notice that\n\\[\n  \\begin{aligned}\n    R^2 &= \\frac{ESS}{TSS} \\\\\n    &= 1 - \\frac{SSR}{TSS}\n  \\end{aligned}\n\\] When you add a new variable to a model, \\(SSR\\) cannot increase (see explanation below). Since \\(SSR\\) cannot increase, it means that \\(R^2\\) can only increase (or at a minimum remain unchanged) when a new regressor is added to the model.\nThis means that, if you were to choose a model by \\(R^2\\), it would always choose the “kitchen-sink” (include everything) model. But, as we discussed above, this is likely to lead to severe over-fitting problems. This suggests using alternative approaches to choose a model for the purposed of prediction.\n\nSide-Comment: It might not be obvious why \\(SSR\\) necessarily decreases when a covariate is added to the model. Here is an explanation. Suppose that we estimate two models (I’ll include an \\(i\\) subscript here and just use \\(\\beta\\) and \\(\\delta\\) to remind you that the parameters are not equal to each other across models)\n\\[\n  \\begin{aligned}\n  Y_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1i} + \\hat{U}_{1i} \\\\\n  Y_i &= \\hat{\\delta}_0 + \\hat{\\delta}_1 X_{1i} + \\hat{\\delta}_2 X_{2i} + \\hat{U}_{2i}\n  \\end{aligned}\n\\] Recall that, for both models, we estimate the parameters by minimizing the sum of squared residuals (\\(SSR\\)). Notice that the second model is equivalent to the first model when \\(\\hat{\\delta}_2=0\\). In that case, \\(\\hat{\\delta}_0 = \\hat{\\beta}_0\\), \\(\\hat{\\delta}_1 = \\hat{\\beta}_1\\), the residuals would be exactly the same which implies that \\(SSR\\) would be exactly the same across each model. Now, for the second model, if we estimate some any value of \\(\\hat{\\delta}_2 \\neq 0\\), the reason why we would estimate that is because it makes the \\(SSR\\) of the second model even lower.\nThis discussion means that, if we estimate \\(\\hat{\\delta}_2=0\\), then \\(SSR\\) wil be the same across the first and second model, and that if we estimate any other value for \\(\\hat{\\delta}_2\\), then \\(SSR\\) for the second model will be lower than for the first model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#adjusted-r2",
    "href": "12-model_selection.html#adjusted-r2",
    "title": "13  Model Selection",
    "section": "13.4 Adjusted \\(R^2\\)",
    "text": "13.4 Adjusted \\(R^2\\)\nSW 6.4\nGiven the above discussion, we would like to have a way to choose a model that doesn’t necessarily always select the most complicated model.\nA main way to do this is to add a penalty to adding more regressors to the model. One version of this is called adjusted \\(R^2\\), which we will write as \\(\\bar{R}^2\\). It is defined as\n\\[\n  \\bar{R}^2 := 1 - \\underbrace{\\frac{(n-1)}{(n-k)}}_{\\textrm{penalty}} \\frac{SSR}{TSS}\n\\] where \\(k\\) is the number of regressors included in the model (note: we also count the intercept as a regressor here; so for example, in the regression \\(\\E[Y|X_1,X_2] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\), \\(k=3\\)). Notice, if the penalty term were equal to 1, then this would just be \\(R^2\\).\nNow, let’s think about what happens to \\(\\bar{R}^2\\) when you add a new regressor to the model.\n\n\\(SSR \\downarrow \\implies \\bar{R}^2 \\uparrow\\)\n\\((n-k-1) \\downarrow \\implies \\bar{R}^2 \\downarrow\\)\n\nThus, there is a “benefit” and a “cost” to adding a new regressor. If you are choosing among several models based on \\(\\bar{R}^2\\), you would choose the model that has the highest \\(\\bar{R}^2\\). And the logic is this: if the regressor greatly decreases \\(SSR\\), then the “benefit” of adding that regressor will tend to outweigh the “cost”. On the other hand, if the regressor has little effect on \\(SSR\\), then the “benefit” of adding that regressor will tend to be smaller than the “cost”.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#aic-bic",
    "href": "12-model_selection.html#aic-bic",
    "title": "13  Model Selection",
    "section": "13.5 AIC, BIC",
    "text": "13.5 AIC, BIC\nThere are other approaches to model selection that follow a similar idea. Two of the most common ones are the Akaike Information Criteria (AIC) and the Bayesian Information Criteria (BIC). These tend to be somewhat better choices for model selection than \\(\\bar{R}^2\\).\nThese are given by\n\n\\(AIC = 2k + n \\log(SSR)\\)\n\\(BIC = k \\log(n) + n \\log(SSR)\\)\n\nFor both AIC and BIC, you would choose the model that minimizes these.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#cross-validation",
    "href": "12-model_selection.html#cross-validation",
    "title": "13  Model Selection",
    "section": "13.6 Cross-Validation",
    "text": "13.6 Cross-Validation\nAnother common way to choose a model is called cross-validation. The idea is to mimic the out-of-sample prediction problem using the data that we have access to.\nIn particular, one simple version of this is to split your data into two parts: these are usually called the training sample and the testing sample. Then, estimate all models under consideration using the training sample. Given the estimated values of the parameters, then predict the outcomes for observations in the testing sample; and compare these predictions to the actual outcomes in the testing sample. Choose the model that minimizes the squared prediction error in the testing sample.\nCross-validation is a somewhat more complicated version of the same idea. Basically, we will repeatedly split the data into a training sample and a testing sample, and get predictions for all observations in our data (instead of just for a held-out testing sample).\nHere is an algorithm for cross-validation:\n\nAlgorithm:\n\nSplit the data into J folds\nFor the jth fold, do the following:\n\nEstimate the model using all observations not in the Jth fold (\\(\\implies\\) we obtain estimates \\(\\hat{\\beta}_0^j, \\hat{\\beta}_1^j, \\ldots, \\hat{\\beta}_k^j\\))\nPredict outcomes for observations in the Jth fold using the estimated model from part (1): \\[\\begin{align*}\n    \\tilde{Y}_{ij} = \\hat{\\beta}_0^j + \\hat{\\beta}_1^j X_{1ij} + \\cdots + \\hat{\\beta_k^j} X_{kij}\n  \\end{align*}\\]\nCompute the prediction error: \\[\\begin{align*}\n    \\tilde{U}_{ij} = Y_{ij} - \\tilde{Y}_{ij}\n  \\end{align*}\\] (this is the difference between actual outcomes for individuals in the Jth fold and their predicted outcome based on the model from part (1))\n\nDo steps 1-3 for all \\(J\\) folds. This gives a prediction error \\(\\tilde{U}_i\\) for each observation in the data\ncompute the cross validation criteria (mean squared prediction error): \\[\\begin{align*}\nCV = \\frac{1}{n} \\sum_{i=1}^n \\tilde{U}_{i}^2\n  \\end{align*}\\]\nchoose the model that produces the smallest value of \\(CV\\).\n\n\nCross-validation is a good way to choose a model, but it can sometimes be computationally challenging (because you are estimating lots of models) — particularly, if the models under consideration are very complicated or there a large number of observations.\nAbove, we split the data into \\(J\\) folds. This introduces some randomness into our model selection criteria. In particular, if you split the data in a different way from how I split the data, then we could choose different models. This is a somewhat undesirable feature of a model selection criteria. One way to get around this is to set \\(J=n\\). This makes it so that every observation has its own fold. This is called leave-one-out cross-validation. In this case, there is no randomness in the model selection criteria. The drawback is that it is more computational — in this case, you need to estimate the model \\(n\\) times rather than \\(J\\) times.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#model-averaging",
    "href": "12-model_selection.html#model-averaging",
    "title": "13  Model Selection",
    "section": "13.7 Model Averaging",
    "text": "13.7 Model Averaging\nIn the case where you are considering a large number of possible models, it is pretty common that a number of models will, by any of the above model selection criteria, be expected to perform very similarly when making predictions.\nIn this case, one strategy that usually does well in terms of making out-of-sample predictions is model averaging.\nSuppose you have \\(M\\) different models, and that each model can produce a predicted value for \\(Y_i\\) — let’s call the predicted value from model \\(m\\), \\(\\hat{Y}_i^m\\). Model averaging would involve obtaining a new predicted value, call it \\(\\hat{Y}_i\\) by computing \\[\\begin{align*}\n  \\hat{Y}_i = \\frac{1}{M} \\sum_{m=1}^M \\hat{Y}_i^m\n\\end{align*}\\]\n\nUsually, you would throw out models that you know predict poorly and only average together ones that perform reasonably well.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "12-model_selection.html#computation",
    "href": "12-model_selection.html#computation",
    "title": "13  Model Selection",
    "section": "13.8 Computation",
    "text": "13.8 Computation\n\\(R^2\\) and \\(\\bar{R}^2\\) are both reported as output from R’s lm command.\nIt is straightforward (and a useful exercise) to compute \\(TSS, ESS,\\) and \\(SSR\\) directly. It is also straightforward to calculate \\(AIC\\) and \\(BIC\\) — there are probably packages that will do this for you, but they are so easy that I suggest just calculating on your own.\nThe same applies to cross validation. I suspect that there are packages available that will do this for you, but I think these are useful coding exercises and not all that difficult. Also, if you do this yourself, it removes any kinds of “black box” issues from downloading R code where it may not be clear exactly what it is doing.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "13-machine_learning.html",
    "href": "13-machine_learning.html",
    "title": "14  Machine Learning",
    "section": "",
    "text": "14.1 Machine Learning\nSW 14.1, 14.2, 14.6\nSome extra resources on estimating Lasso and Ridge regressions in R:\n“Big” Data typically means one of two things:\n\\(n\\) being large is more of a computer science problem. That said, there are economists who think about these issues, but I think it is still the case that it is relatively uncommon for an economist to have so much data that they have trouble computing their estimators.\nWe’ll focus on the second issue — where \\(k\\) is large. A main reason that this may occur in applications is that we may be unsure of the functional form for \\(\\E[Y|X_1,X_2,X_3]\\). Should it include higher order terms like \\(X_1^2\\) or \\(X_1^3\\)? Should it include interactions like \\(X_1 X_2\\). Once you start proceeding this way, even if you only have 5-10 actual covariates, \\(k\\) can become quite large.\nAs in the case of the mean, perhaps we can improve predictions by introducing some bias while simultaneously decreasing the variance of our predictions.\nLet’s suppose that we have some function that can take in regressors and makes predictions of the outcome; we’ll call this function \\(\\hat{f}\\) (where the “hat” indicates that we’ll typically estimate this function). An example would just be a regression where, for example, \\(\\hat{f}(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\hat{\\beta}_3 X_3\\). One way to evaluate how well \\(\\hat{f}\\) makes predictions is by considering its mean squared prediction error:\n\\[\n  MSPE := \\E\\left[ (Y - \\hat{f}(X))^2 \\right]\n\\] \\(MSPE\\) quantifies the mean “distance” between our predictions and actual outcomes.\nRecall that, if we could just pick any function to minimize \\(MSPE\\), we would set \\(\\hat{f}(X) = \\E[Y|X]\\), but generally we do not just know what \\(\\E[Y|X]\\) is. We can “decompose” MSPE into several underlying conditions\n\\[\n  \\begin{aligned}\n  MSPE &= \\E\\left[ \\left( (Y - \\E[Y|X]) - (\\hat{f}(X) - \\E[\\hat{f}(X)|X]) - (\\E[\\hat{f}(X)|X] - \\E[Y|X]) \\right)^2 \\right] \\\\\n  &= \\E\\left[ (Y-\\E[Y|X])^2 \\right] + \\E\\left[ (\\hat{f}(X) - \\E[\\hat{f}(X)|X])^2\\right] + \\E\\left[ (\\E[\\hat{f}(X)|X] - \\E[Y|X])^2 \\right] \\\\\n  &= \\Var(U) + \\E[\\Var(\\hat{f}(X)|X)] + \\E\\left[\\textrm{Bias}(\\hat{f}(X))^2\\right]\n  \\end{aligned}\n\\] where, to understand this decomposition, the first equality just adds and subtracts \\(\\E[Y|X]\\) and \\(\\E[\\hat{f}(X)|X]\\). The second equality squares the long expression from the first equality and cancels some terms. The third equality holds where \\(U := Y-\\E[Y|X]\\), by the definition of (conditional) variance, and we call the last bias because it is the difference between the mean of our prediction function \\(\\hat{f}\\) and \\(\\E[Y|X]\\).\nYou can think of the term \\(\\Var(U)\\) as being irreducible prediction error — even if we knew \\(\\E[Y|X]\\), we wouldn’t get every prediction exactly right. But the other two terms come from having to estimate \\(\\hat{f}\\). The term \\(\\E[\\Var(\\hat{f}(X)X)]\\) is the average variance of our predictions across different values of \\(X\\). The term \\(\\E\\left[\\textrm{Bias}(\\hat{f}(X))^2\\right]\\) is the squared bias of our predictions averaged across different values of \\(X\\). Many machine learning estimators have the property of being biased (so that the last term is not equal to 0), but having reduced variance relative to OLS estimators.\nTo do this, we’ll estimate the parameters of the model in a similar way to what we have done before except that we’ll add a penalty term that gets larger as parameter estimates move further away from 0. In particular, we consider estimates of the form\n\\[\n  (\\hat{\\beta}_1, \\hat{\\beta}_2, \\ldots, \\hat{\\beta}_k) = \\underset{b_1,\\ldots,b_k}{\\textrm{argmin}} \\sum_{i=1}^n  (Y_i - b_1 X_{1i} - \\cdots - b_k X_{ki})^2 + \\textrm{penalty}\n\\]\nLasso and ridge regression, which are the two approaches to machine learning that we will talk about below, amount to different choices of the penalty term.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "13-machine_learning.html#machine-learning",
    "href": "13-machine_learning.html#machine-learning",
    "title": "14  Machine Learning",
    "section": "",
    "text": "glmnet tutorial\nglmnetUtils vignette\n\n\n\n\\(n\\) — the number of observations is extremely large\n\\(k\\) — the number of regressors is very large\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide-Comment: Generally, you should “standardize” the regressors before implementing Lasso or Ridge regression. The glmnet package does this for you by default.\n\n\n14.1.1 Lasso\nSW 14.3\nFor Lasso, the penalty term is given by\n\\[\n  \\lambda \\sum_{j=1}^k |b_j|\n\\] The absolute value on each of the \\(b_j\\) terms means that the penalty function gets larger for larger values of any \\(b_j\\). Since we are trying to minimize the objective function, this means that there is a “cost” to choosing a larger value of \\(b_j\\) relative to OLS. Thus, Lasso estimates tend to be “shrunk” towards 0.\n\\(\\lambda\\) is called a tuning parameter — larger values of \\(\\lambda\\) imply that the penalty term is more important. Notice that, if you send \\(\\lambda \\rightarrow \\infty\\), then the penalty term will be so large that you would set all the parameters to be equal to 0. On the other hand, if you set \\(\\lambda=0\\), then you will get the OLS estimates (because there will be no penalty in this case). In general, it is hard to know what is the “right” value for \\(\\lambda\\), and it is typically chosen using cross validation (this will be done automatically for you using the glmnet package).\n\n\n14.1.2 Ridge Regression\nSW 14.4\nFor Ridge, the penalty term is given by\n\\[\n  \\lambda \\sum_{j=1}^k b_j^2\n\\] Much of the discussion from Lasso applies here. The only difference is that the form of the penalty is different here: \\(b_j^2\\) instead of \\(|b_j|\\). Relative to the Lasso penalty, the Ridge penalty “dislikes more” very large values of \\(b_j\\).\nComparing Lasso and Ridge\n\nBoth shrink the estimated parameters towards 0. This tends to introduce bias into our predictions but comes with the benefit of reducing the variance of the predictions.\nInterestingly, both Lasso and Ridge can be implemented when \\(k &gt; n\\) (i.e., if you have more regressors than observations). This is in contrast to to OLS, where the parameters cannot be estimated in this case.\nBoth are generally not very computationally intensive. For ridge regression, we can in fact derive an explicit expression for the estimated \\(\\beta\\)’s. We cannot do this with Lasso; a full discussion of how Lasso actually estimates the parameters is beyond the scope of our course, but, suffice it to say, that you can generally compute Lasso estimates quickly.\nLasso also performs “model selection” — that is, if you use the Lasso, many of the estimated parameters will be set equal to 0. This can sometimes be an advantage. Ridge (like OLS) will generally produce non-zero estimates of all parameters in the model.\n\n\n\n14.1.3 Computation\nComputing Lasso and Ridge regressions is somewhat more complicated than most other things that we have computed this semester. The main R package for Lasso and Ridge regressions is the glmnet package. For some reason, the syntax of the package is somewhat different from, for example, the lm command. In my view, it is often easier to use the glmnetUtils package, which seems to be just a wrapper for glmnet but with functions that are analogous to lm.\nI’m just going to sketch here how you would use these functions to estimate a Lasso or Ridge regression. In the lab later on in this chapter, we’ll do a more concrete example. Suppose that you have access to a training dataset called train and a testing dataset called test, you can use the glmnetUtils package in the following way:\n\nlibrary(glmnetUtils)\nlasso_model &lt;- cv.glmnet(Y ~ X1 + X2 + X3, data=train, use.model.frame=TRUE) # or whatever formula you want to use\ncoef(lasso_model) # if you are interested in estimated coefficients\npredict(lasso_model, newdata=test) # get predictions\n\nridge_model &lt;- cv.glmnet(Y ~ X1 + X2 + X3, \n                         data=train, \n                         alpha=0,\n                         use.model.frame=TRUE)\ncoef(ridge_model)\npredict(ridge_model, newdata=test)\n\nIt’s worth making a couple of comments about this\n• In terms of code, the only difference between the Lasso and Ridge, is that for Ridge, we added the extra argument alpha=0. The default value of alpha is 1, and that leads to a Lasso regression (which is why didn’t need to specify it for our Lasso estimates). If you are interested, you can read more details about this in the documentation for glmnet using ?glmnet.\n• Generally, if you are going to use Lasso or Ridge, then you would have many more regressors to include than just X1 + X2 + X3. One common way that you get more regressors is when you start to thinking about including higher order terms or interaction terms. One way to do this quickly is to use the poly function inside the formula. For example,\n\nlasso_model2 &lt;- cv.glmnet(Y ~ poly(X1, X2, X3, degree=2, raw=TRUE), \n                          data=train, \n                          use.model.frame=TRUE)\n\nwould include all interactions between \\(X_1\\), \\(X_2\\) and \\(X_3\\) as well as squared terms for each; if you wanted to include more interactions and cubic terms, you could specify degree=3.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "13-machine_learning.html#lab-6-predicting-diamond-prices",
    "href": "13-machine_learning.html#lab-6-predicting-diamond-prices",
    "title": "14  Machine Learning",
    "section": "14.2 Lab 6: Predicting Diamond Prices",
    "text": "14.2 Lab 6: Predicting Diamond Prices\nFor this lab, we will try our hand at predicted diamond prices. We will use the data set diamond_train (which contains around 40,000 observations) and then see how well we can predict data from the diamond_test data.\n\nEstimate a model for \\(price\\) on \\(carat\\), \\(cut\\), and \\(clarity\\). Report \\(R^2\\), \\(\\bar{R}^2\\), \\(AIC\\), and \\(BIC\\) for this model.\nEstimate a model for \\(price\\) on \\(carat\\), \\(cut\\), \\(clarity\\), \\(depth\\), \\(table\\), \\(x\\), \\(y\\), and \\(z\\). Report \\(R^2\\), \\(\\bar{R}^2\\), \\(AIC\\), and \\(BIC\\) for this model.\nChoose any model that you would like for \\(price\\) and report \\(R^2\\), \\(\\bar{R}^2\\), \\(AIC\\), and \\(BIC\\) for this model. We’ll see if your model can predict better than either of the first two.\nUse 10-fold cross validation to report an estimate of mean squared prediction error for each of the models from 1-3.\nBased on your responses to parts 1-4, which model do you think will predict the best?\nUse diamond_test to calculate (out-of-sample) mean squared prediction error for each of the three models from 1-3. Which model performs the best out-of-sample? How does this compare to your answer from 5.\nUse the Lasso and Ridge regression on diamond_train data. Evaluate the predictions from each of these models by computing (out-of-sample) mean squared prediction error. How well did these models predict relative to each other and relative the models from 1-3.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "13-machine_learning.html#lab-6-solutions",
    "href": "13-machine_learning.html#lab-6-solutions",
    "title": "14  Machine Learning",
    "section": "14.3 Lab 6: Solutions",
    "text": "14.3 Lab 6: Solutions\n\nload(\"data/diamond_train.RData\")\n\n# formulas\nformla1 &lt;- price ~ carat + as.factor(cut) + as.factor(clarity)\nformla2 &lt;- price ~ carat + as.factor(cut) + as.factor(clarity) + depth + table + x + y + x\nformla3 &lt;- price ~ (carat + as.factor(cut) + as.factor(clarity) + depth + table + x + y + x)^2\n\n# estimate each model\nmod1 &lt;- lm(formla1, data=diamond_train)\nmod2 &lt;- lm(formla2, data=diamond_train)\nmod3 &lt;- lm(formla3, data=diamond_train)\n\nmod_sel &lt;- function(formla) {\n  mod &lt;- lm(formla, data=diamond_train)\n  r.squared &lt;- summary(mod)$r.squared\n  adj.r.squared &lt;- summary(mod)$adj.r.squared\n\n  n &lt;- nrow(diamond_train)\n  uhat &lt;- resid(mod)\n  ssr &lt;- sum(uhat^2)\n  k &lt;- length(coef(mod))\n  aic &lt;- 2*k + n*log(ssr)\n  bic &lt;- k*log(n) + n*log(ssr)\n\n  # show results\n  result &lt;- tidyr::tibble(r.squared, adj.r.squared, aic, bic)\n  return(result)\n}\n\nres1 &lt;- mod_sel(formla1)\nres2 &lt;- mod_sel(formla2)\nres3 &lt;- mod_sel(formla3)\n\nround(rbind.data.frame(res1, res2, res3),3)\n\n# A tibble: 3 × 4\n  r.squared adj.r.squared      aic      bic\n      &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     0.898         0.898 1104188. 1104301.\n2     0.9           0.9   1103499. 1103647.\n3     0.928         0.928 1089372. 1090328.\n\n# k-fold cross validation\n\n# setup data\nk &lt;- 10\nn &lt;- nrow(diamond_train)\ndiamond_train$fold &lt;- sample(1:k, size=n, replace=TRUE)\ndiamond_train$id &lt;- 1:n\n\ncv_mod_sel &lt;- function(formla) {\n  u.squared &lt;- rep(NA, n)\n  for (i in 1:k) {\n    this.train &lt;- subset(diamond_train, fold != i)\n    this.test &lt;- subset(diamond_train, fold == i)\n    this.id &lt;- this.test$id\n    cv_reg &lt;- lm(formla,\n              data=this.train)\n    pred &lt;- predict(cv_reg, newdata=this.test)\n    u &lt;- this.test$price - pred\n    u.squared[this.id] &lt;- u^2\n  }\n  cv &lt;- sqrt(mean(u.squared))\n  return(cv)\n}\n\ncv_res1 &lt;- cv_mod_sel(formla1)\ncv_res2 &lt;- cv_mod_sel(formla2)\ncv_res3 &lt;- cv_mod_sel(formla3)\n\nres1 &lt;- cbind.data.frame(res1, cv=cv_res1)\nres2 &lt;- cbind.data.frame(res2, cv=cv_res2)\nres3 &lt;- cbind.data.frame(res3, cv=cv_res3)\n\nround(rbind.data.frame(res1, res2, res3),3)\n\n  r.squared adj.r.squared     aic     bic       cv\n1     0.898         0.898 1104188 1104301 1365.962\n2     0.900         0.900 1103499 1103647 1373.381\n3     0.928         0.928 1089372 1090328 2263.520\n\n# lasso and ridge\nlibrary(glmnet)\nlibrary(glmnetUtils)\n\nlasso_res &lt;- cv.glmnet(formla3, data=diamond_train, alpha=1)\nridge_res &lt;- cv.glmnet(formla3, data=diamond_train, alpha=0)\n\n# out of sample predictions\n\nload(\"data/diamond_test.RData\")\n\n# compute prediction errors with test data\nu1 &lt;- diamond_test$price - predict(mod1, newdata=diamond_test)\nu2 &lt;- diamond_test$price - predict(mod2, newdata=diamond_test)\nu3 &lt;- diamond_test$price - predict(mod3, newdata=diamond_test)\nu_lasso &lt;- diamond_test$price - predict(lasso_res, newdata=diamond_test)\nu_ridge &lt;- diamond_test$price - predict(ridge_res, newdata=diamond_test)\n\n# compute root mean squared prediction errors\nrmspe1 &lt;- sqrt(mean(u1^2))\nrmspe2 &lt;- sqrt(mean(u2^2))\nrmspe3 &lt;- sqrt(mean(u3^2))\nrmspe_lasso &lt;- sqrt(mean(u_lasso^2))\nrmspe_ridge &lt;- sqrt(mean(u_ridge^2))\n\n# report results\nrmspe &lt;- data.frame(mod1=rmspe1, mod2=rmspe2, mod3=rmspe3, lasso=rmspe_lasso, ridge=rmspe_ridge)\nround(rmspe,3)\n\n     mod1    mod2    mod3  lasso   ridge\n1 856.014 785.171 616.996 683.36 794.276",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "13-machine_learning.html#extra-questions",
    "href": "13-machine_learning.html#extra-questions",
    "title": "14  Machine Learning",
    "section": "14.4 Extra Questions",
    "text": "14.4 Extra Questions\n\nWhat are some drawbacks of using \\(R^2\\) as a model selection tool?\nFor AIC and BIC, we choose the model that minimizes these (rather than maximizes them). Why?\nDoes AIC or BIC tend to pick “more complicated” models? What is the reason for this?\nSuppose you are interested in predicting some outcome \\(Y\\) and have access to covariates \\(X_1\\), \\(X_2\\), and \\(X_3\\). You estimate the following two models \\[\\begin{align*}\n  Y &= 30 + 4 X_1 - 2 X_2 - 10 X_3, \\qquad R^2=0.5, AIC=3421 \\\\\n  Y &= 9 + 2 X_1 - 3 X_2 - 2 X_3 + 2 X_1^2 + 1 X_2^2 - 4 X_3^2 + 2 X_1 X_2, \\qquad R^2 = 0.75, AIC=4018\n\\end{align*}\\]\n\nWhich model seems to be predicting better? Explain why.\nUsing the model that is predicting better, what would be your prediction for \\(Y\\) when \\(X_1=10, X_2=1, X_3=5\\)?\n\nIn Lasso and Ridge regressions, it is common to “standardize” the regressors before estimating the model (e.g., the glmnet does this automatically for you). What is the reason for doing this?\nIn Lasso and Ridge regressions, the penalty term lead to “shrinking” the estimated parameters in the model towards 0. This tends to introduce bias while reducing variance. Why can introducing bias while reducing variance potentially lead to better predictions? Does this argument always apply or just apply in some cases? Explain.\nIn Lasso and Ridge regressions, the penalty term depends on the tuning parameter \\(\\lambda\\).\n\nHow is this tuning parameter often chosen in practice? Why does it make sense to choose it in this way?\nWhat would happen to the estimated coefficients when \\(\\lambda=0\\)?\nWhat would happen to the estimated coefficients as \\(\\lambda \\rightarrow \\infty\\)?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "13-machine_learning.html#answers-to-some-extra-questions",
    "href": "13-machine_learning.html#answers-to-some-extra-questions",
    "title": "14  Machine Learning",
    "section": "14.5 Answers to Some Extra Questions",
    "text": "14.5 Answers to Some Extra Questions\nAnswer to Question 4\n\nThe first model appears to be predicting better because the AIC is lower. [Also, notice that \\(R^2\\) is higher in the second model, but this is by construction because it includes extra terms relative to the first model which implies that it will fit at least as well in-sample as the first model, but may be suffering from over-fitting.]\n\\[\\begin{align*}\n   \\hat{Y} &= 30 + 4 (10) - 2 (1) - 10 (5) \\\\\n   &= 18\n\\end{align*}\\]\n\nAnswer to Question 7\n\nThe tuning parameter is often chosen via cross validation. It makes sense to choose it this way because this is effectively choosing a value of \\(\\lambda\\) that is making good pseudo-out-of-sample predictions. As we will see below, if you make bad choices of \\(\\lambda\\), that could result in very poor predictions.\nWhen \\(\\lambda=0\\), there would effectively be no penalty term and, therefore, the estimated parameters would coincide with the OLS estimates.\nWhen \\(\\lambda \\rightarrow \\infty\\), the penalty term would overwhelm the term corresponding to minimizing SSR. This would result in setting all the estimated parameters to be equal to 0. This extreme approach is likely to lead to very poor predictions.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "14-causal_inference_concepts.html",
    "href": "14-causal_inference_concepts.html",
    "title": "15  Concepts",
    "section": "",
    "text": "15.1 Potential Outcomes\nSW 13.1\nA powerful tool for thinking about causal effects is counterfactual reasoning. For individuals that participate in the treatment, we observe what their outcome is given that they participated in the treatment. But we don’t observe what their outcome would have been if they had not participated in the treatment. For example, among those that went to college, we don’t observe what their earnings would have been if they had not gone to college. For those that don’t participate in the treatment, we face the opposite problem — we don’t observe what their outcome would have been if they had participated in the treatment.\nWe’ll relate causal inference to the problem of trying to figure out what outcomes individuals that participated in the treatment would have experienced if they had not participated in the treatment (at least on average) and/or figuring out what outcomes individuals that did not participated in the treatment would have experienced if they had participated in the treatment.\nTo do this, let’s introduce somewhat more formal notation/terminology.\nTreated potential outcome: \\(Y_i(1)\\), the outcome an individual would experience if they participated in the treatment\nUntreated potential outcome: \\(Y_i(0)\\), the outcome an individual would experience if they did not participate in the treatment\nFor individuals that participate in the treatment, we observe \\(Y_i(1)\\) (but not \\(Y_i(0)\\)). For individuals that do not participate in the treatment, we observe \\(Y_i(0)\\) (but not \\(Y_i(1)\\)). Another way to write this is that the observed outcome, \\(Y_i\\) is given by \\[\\begin{align*}\n  Y_i = D_i Y_i(1) + (1-D_i) Y_i(0)\n\\end{align*}\\]\nWe can think about the individual-level effect of participating in the treatment: \\[\\begin{align*}\n  TE_i = Y_i(1) - Y_i(0)\n\\end{align*}\\]\nConsidering the difference between treated and untreated potential outcomes is a very natural (and, I think, helpful) way to think about causality. The causal effect of the treatment is the difference between the outcome that an individual would experience if they participate in the treatment relative to what they would experience if they did not participate in the treatment.\nThis notation also makes it clear that we are allowing for treatment effect heterogenity — the effect of participating in the treatment can vary across different individuals.\nThat said, most researchers essentially give up on trying to figure out individual level treatment effects. It is not so much that these are not interesting, more it is just that these are very hard to figure out. Take, for example, going to college, and suppose we are interested in the causal effect of going to college on a person’s earnings. I went to college, so I know what my \\(Y(1)\\) is, but I don’t know what my \\(Y(0)\\) is — and, I’d even have a hard time coming with a good guess as to what it might be.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Concepts</span>"
    ]
  },
  {
    "objectID": "14-causal_inference_concepts.html#parameters-of-interest",
    "href": "14-causal_inference_concepts.html#parameters-of-interest",
    "title": "15  Concepts",
    "section": "15.2 Parameters of Interest",
    "text": "15.2 Parameters of Interest\nInstead of going for individual-level effects of participating in the treatment, most researchers instead go for more aggregated parameters. The two most common ones are the Average Treatment Effect (ATE) and Average Treatment Effect on the Treated (ATT).\n\\[\\begin{align*}\n  ATE = \\E[Y(1) - Y(0)] \\qquad \\textrm{and} \\qquad ATT = \\E[Y(1)-Y(0) | D=1]\n\\end{align*}\\] \\(ATE\\) is the difference between treated potential outcomes and untreated potential outcomes, on average, and for the entire population. \\(ATT\\) is the difference between treated and untreated potential outcomes, on average, conditional on being in the treated group.\nWe will mostly focus on \\(ATT\\).\nIt is worth considering the challenges for learning about \\(ATT\\). In particular, notice that we can write \\[\\begin{align*}\n  ATT = \\E[Y(1)|D=1] - \\E[Y(0)|D=1]\n\\end{align*}\\] and consider these term separately\n\n\\(\\E[Y(1)|D=1]\\) is the average treated potential outcome among the treated group. But we observe treated potential outcomes for the treated group \\(\\implies \\E[Y(1)|D=1] = \\E[Y|D=1]\\). In other words, if we want to estimate this component of the \\(ATT\\), we can just look right at the data and compute the average outcome experienced by individuals in the treated group.\n\\(\\E[Y(0)|D=1]\\) is the average untreated potential outcome among the treated group. This is (potentially much) more challenging than the first term because we do not observe untreated potential outcomes among the treated group. But, in order to learn about the \\(ATT\\), we will have to somehow deal with this term. I will provide a number of strategies below, but it is important to remember that this is a major challenge, and their may not be a good solution.\n\n\nSide-Comment: I think it is also worth clearly pointing out that, while I am a big believer in the power/usefulness of using data to try to answer questions in economics, the above discussion suggests that there are a number of questions that we may just not be able to answer. In economics jargon, this amounts to an identification problem — in other words, there may be competing theories of the world which the available data is not able to distinguish among. I probably do not emphasize this issue enough in our class, but it is something that you should remember — there may be a large number of causal questions that we’d be interested in answering, but where it is not possible to answer them (at least given the information that we have available).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Concepts</span>"
    ]
  },
  {
    "objectID": "15-experiments.html",
    "href": "15-experiments.html",
    "title": "16  Experiments",
    "section": "",
    "text": "16.1 Estimating ATT with a Regression\nIt is also often convenient to introduce a regression based estimator of the ATT. This is primarily convenient as it will allow us to leverage all the things we already know about regressions, and, particularly, we it will immediately provide us with standard errors, t-statistics, etc.\nIn order to do this, let’s introduce the following assumption:\nTreatment Effect Homogeneity: \\(Y_i(1) - Y_i(0) = \\alpha\\) (and \\(\\alpha\\) does not vary across individuals).\nThis is a potentially quite unrealistic assumption; I’ll make some additional comments about it below, but, for now, let’s just go with it.\nNotice that we can also write \\[\\begin{align*}\n  Y_i(0) = \\beta_0 + U_i\n\\end{align*}\\] where \\(\\E[U|D=0] = \\E[U|D=1] = 0\\) (this holds under random assignment since random assignment implies that treated and untreated individuals do not have systematically different untreated potential outcomes)\nRecalling the definition of the observed outcome, notice that \\[\\begin{align*}\n  Y_i &= D_i Y_i(1) + (1-D_i) Y_i(0) \\\\\n  &= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\\\\n  &= \\alpha D_i + \\beta_0 + U_i\n\\end{align*}\\] where the second equality just involves re-arranging terms, and the third equality holds under treatment effect homogeneity and using the expression for \\(Y_i(0)\\) in the previous equation.\nThis suggests running a regression of the observed \\(Y_i\\) on \\(D_i\\) and interpreting the estimated version of \\(\\alpha\\) as an estimate of the causal effect of participating in the treatment (and you can pick up standard errors, etc. from the regression output) — this is very convenient.\nThe previous discussion invoked the extra condition of treatment effect homogeneity. I want to point out some things related to this now. In the above regression model, we can alternatively (and equivalently) write it as \\[\\begin{align*}\n  \\E[Y|D] = \\beta_0 + \\alpha D\n\\end{align*}\\] Now plug in particular values for \\(D\\): \\[\\begin{align*}\n  \\E[Y|D=0] = \\beta_0 \\qquad \\textrm{and} \\qquad \\E[Y|D=1] = \\beta_0 + \\alpha\n\\end{align*}\\] Subtracting the second equation from the first implies that \\[\\begin{align*}\n  \\alpha = \\E[Y|D=1] - \\E[Y|D=0]\n\\end{align*}\\] but notice that this is exactly what the \\(ATT\\) is equal to under random assignment. Thus, it is worth pointing out that, although we imposed the assumption of treatment effect homogeneity to arrive at the regression equation, our regression is “robust” to treatment effect heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "15-experiments.html#internal-and-external-validity",
    "href": "15-experiments.html#internal-and-external-validity",
    "title": "16  Experiments",
    "section": "16.2 Internal and External Validity",
    "text": "16.2 Internal and External Validity\nSW 13.2\nAlthough experiments, when they are available, are considered the gold standard of causal inference, there are some ways they can go wrong.\nAn experiment is said to be internally valid if inferences about causal effects are valid for the population being studied.\nAn experiment is said to be externally valid if inferences about causal effects can be generalized from the study setting to other populations/time periods/settings.\nGenerally, its easier for an experiment to be internally valid than externally valid, but there are a number of “threats” to both internal and external validity.\nThreats to internal validity:\n\nFailure to randomize — individuals weren’t actually randomized into the treatment. This seems trivial, but some famous examples include experiments that assigned treatment participation by the first letter of a person’s last name. It turns out that this can be correlated with a number of other things (violating the random assignment assumption)\n\nAs a side-comment, random treatment assignment implies that treatment should be uncorrelated with other covariates (e.g., race, sex, etc.) and this can be used as a way to check if the treatment was really randomly assigned.\n\nFailure to follow treatment protocol — Individuals assigned to participate in the treatment don’t participate or vice versa.\n\nThis is actually a pretty common problem. A leading example would be in the context of an experimental drug where patients who were assigned the placebo somehow to convince the doctor to give them the real drug.\n\nAttrition — subjects non-randomly drop out of the sample\nPlacebo effects — Participating in an experiment can cause changes in behavior/outcomes.\n\nThis is a main reason that medical experiments are often “double-blind”\n\nSmall sample sizes — Experiments can be expensive to run and therefore often include only a small number of observations, and, therefore, may be less able to detect small effects of a treatment\nExperiments can sometimes be unethical\n\nThe 2019 Nobel prize in economics was awarded for experiments in developing countries. Some of these experiments were somewhat controversial (e.g., randomly assigning some schools extra resources and randomly giving glasses to some students but not others).\n\n\nThreats to External Validity:\n\nNonrepresentative samples — The population that is willing to participate in an experiment many be substantially different from the overall population.\n\nA closely related problem is that the economic environment may change across time and/or space which means that effects of some treatment might be different in different time periods or locations\n\nNonrepresentative program/policy — The policy in an experiment may be different from other policies being considered\n\nFor example, the Perry Preschool Project was an experiment that provided intensive pre-school program that seems to have had large and long-lasting effects on participants. It is not clear if these results should apply to less intensive programs like Head Start.\n\nGeneral equilibrium effects — Expanding policies may alter the “state of the world” in a way that an experiment wouldn’t (e.g., the effects of a local minimum wage change could be different from a country-wide minimum wage change)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "15-experiments.html#example-project-star",
    "href": "15-experiments.html#example-project-star",
    "title": "16  Experiments",
    "section": "16.3 Example: Project STAR",
    "text": "16.3 Example: Project STAR\nProject STAR was a an experiment in Tennessee in 1980s on the effects of smaller class sizes on student performance where students and teachers were randomly assigned to be in a small class, a regular class, or a regular class with an aide. In this example, we’ll study the causal effect of being in a small class on reading test scores.\n\ndata(\"Star\", package=\"Ecdat\")\n\n# limit data to small class or regular class\n# this drops the other category of regular class size with an aide\nStar &lt;- subset(Star, classk %in% c(\"small.class\", \"regular\"))\n\n# regression for reading scores\nreading &lt;- lm(log(treadssk) ~ classk, data=Star)\nsummary(reading)\n\n\nCall:\nlm(formula = log(treadssk) ~ classk, data = Star)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31960 -0.04873 -0.00607  0.03929  0.36877 \n\nCoefficients:\n                  Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)       6.072176   0.001568 3871.502  &lt; 2e-16 ***\nclassksmall.class 0.013324   0.002302    5.788  7.7e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07014 on 3731 degrees of freedom\nMultiple R-squared:  0.0089,    Adjusted R-squared:  0.008634 \nF-statistic:  33.5 on 1 and 3731 DF,  p-value: 7.699e-09\n\n\nYou’ll notice that this is a very simple analysis, but that we have access to an experiment means that we do need to do anything complicated — we can just use a regression to compare the differences between test scores of students in small classes relative to large tests. These results say that small class sizes increase reading test scores of students by about 1.3%.\nLet me also make a few comments about internal and external validity.\n\nI don’t have too much to say about internal validity. As far as I know, Project STAR is considered internally valid, but it would be worth reading more about it in order to confirm that.\nThinking about external validity though is quite interesting. For one thing, this experiment is quite old. Are the results still valid now? It is not clear; one could imagine that better teaching technologies now could possibly make having a small class less important. Another thing that is interesting is that this experiment was for very young students (I think students in kindergarten). Do these results imply that, say, high school or college students would also benefit from smaller class sizes? Again, this is not clear. It is a relevant piece of information, but it also would require a seemingly large amount of extrapolation to say that these results should inform policy decisions about class sizes in the present day and for different age groups.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "16-unconfoundedness.html",
    "href": "16-unconfoundedness.html",
    "title": "17  Unconfoundedness",
    "section": "",
    "text": "\\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\renewcommand{\\P}{\\textrm{P}}\n\\let\\L\\relax\n\\newcommand{\\L}{\\textrm{L}} %doesn't work in .qmd, place this command at start of qmd file to use it\n\\newcommand{\\F}{\\textrm{F}}\n\\newcommand{\\var}{\\textrm{var}}\n\\newcommand{\\cov}{\\textrm{cov}}\n\\newcommand{\\corr}{\\textrm{corr}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\Corr}{\\mathrm{Corr}}\n\\newcommand{\\sd}{\\mathrm{sd}}\n\\newcommand{\\se}{\\mathrm{s.e.}}\n\\newcommand{\\T}{T}\n\\newcommand{\\indicator}[1]{\\mathbb{1}\\{#1\\}}\n\\newcommand\\independent{\\perp \\!\\!\\! \\perp}\n\\newcommand{\\N}{\\mathcal{N}}\n\\]\nSW 6.8, SW Ch. 9\nUnfortunately, we rarely have access to experimental data in applications or the ability to run experiments to evaluate causal effects programs/policies that we’d be interested in studying. For example, it’s hard to imagine convincing a large number of countries to randomly assign their interest rate, minimum wage, or immigration policies though these are all policies that economists would be interested in thinking about the causal effect of.\nFor the remainder of this chapter, we’ll discuss common approaches to causal inference when the treatment is not randomly assigned. We’ll start with what is probably the most common approach: unconfoundedness.\nUnconfoundedness Assumption: \\[\\begin{align*}\n  (Y(1),Y(0)) \\independent D | X\n\\end{align*}\\] You can think of this as saying that, among individuals with the same covariates \\(X\\), they have the same distributions of potential outcomes regardless of whether or not they participate in the treatment. Note that the distribution of \\(X\\) is still allowed to be different between the treated and untreated groups. In other words, after you condition on covariates, there is nothing special (in terms of the distributions of potential outcomes) about the group that participates in the treatment relative to the group that doesn’t participate in the treatment.\n\nThis is potentially a strong assumption. In order to believe this assumption, you need to believe that untreated individuals with the same characteristics can deliver, on average, the outcome that individuals in the treated group would have experienced if they had not participated in the treatment. In math, you can write this as \\[\\begin{align*}\n    \\E[Y(0) | X, D=1] = \\E[Y(0) | X, D=0]\n  \\end{align*}\\]\n\nIf you are willing to believe this assumption, then you can recover the \\(ATT\\). There are a few different ways that you could implement this. Probably the most common (and convenient) way is to link this condition to a regression (just like we did in the previous section on experiments).\nTo do this, let’s continue to make the treatment effect homogeneity assumption as above. In addition, let’s assume a linear model for untreated potential outcomes \\[\\begin{align*}\n  Y(0) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U\n\\end{align*}\\] and unconfoundedness implies that \\(\\E[U|X_1,X_2,X_3,D] = 0\\) (the conditioning on \\(D\\) is the unconfoundedness part). Now, recalling the definition of the observed outcome, we can write \\[\\begin{align*}\n  Y_i &= D_i Y_i(1) + (1-D_i) Y_i(0) \\\\\n  &= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\\\\n  &= D_i \\alpha + \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U_i\n\\end{align*}\\] which suggests running the regression of observed \\(Y\\) on \\(X_1,X_2,X_3,\\) and \\(D\\) and interpreting the estimate of \\(\\alpha\\) as the causal effect of participating in the treatment. In practice, this will be very similar to what we have done before — so the process would not be hard, but convincing someone (or even yourself) that unconfoundedness holds will be the bigger issue here.\nAs a final comment, the assumption of treatment effect homogeneity is not quite so innocuous here. It turns out that you can show that, in the presence of treatment effect heterogeneity, \\(\\alpha\\) will be equal to a weighted average of individual treatment effects, but the weights can sometimes be “strange”. There are methods that are robust to treatment effect heterogeneity (they are beyond the scope of the current class, but they are not “way” more difficult than what we are doing here). That said, in my experience, the regression estimators (under treatment effect homogeneity) tend to deliver similar estimates to alternative estimators that are robust to treatment effect heterogeneity at least in the setup considered in this section.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html",
    "href": "17-quasi_experiments.html",
    "title": "18  Quasi-Experiments",
    "section": "",
    "text": "18.1 Panel Data Approaches\nSW All of Ch. 10 and 13.4\nIn the previous section, we invoked the assumption of unconfoundedness and were in the setup where \\(X\\) was fully observed. But suppose instead that you thought this alternative version of unconfoundedness held \\[\\begin{align*}\n  (Y(1),Y(0)) \\independent D | (X,W)\n\\end{align*}\\] where \\(X\\) were observed random variables, but \\(W\\) were not observed. Following exactly the same argument as in the previous section, this would lead to a regression like \\[\\begin{align*}\n  Y_i = \\alpha D_i + \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + U_i\n\\end{align*}\\] (I’m just including one \\(X\\) and one \\(W\\) for simplicity, but you can easily imagine the case where there are more.) If \\(W\\) were observed, then we could just run this regression, but since \\(W\\) is not observed, we run into the problem of omitted variable bias (i.e., if we just ignore \\(W\\), we won’t be estimating the causal effect \\(\\alpha\\))\nIn this section, we’ll consider the case where a researcher has access to a different type of data called panel data. Panel data is data that follows the same individual (or firm, etc.) over time. In this case, it is often helpful to index variables by time. For example, \\(Y_{it}\\) is the outcome for individual \\(i\\) in time period \\(t\\). \\(X_{it}\\) is the value of a regressor for individual \\(i\\) in time period \\(t\\) and \\(D_{it}\\) is the value of the treatment for individual \\(i\\) in time period \\(t\\). If some variable doesn’t vary over time (e.g., a regressor like race), we won’t use a \\(t\\) subscript.\nPanel data potentially gives us a way around the problem of not observing some variables that we would like to condition on in the model. This is particularly likely to be the case when \\(W\\) does not vary over time. Let’s start with there are exactly two time periods of panel data. In that case, we can write \\[\\begin{align*}\n  Y_{it} = \\alpha D_{it} + \\beta_0 + \\beta_1 X_{it} + \\beta_2 W_i + U_{it}\n\\end{align*}\\] where we consider the case where \\(D\\) and \\(X\\) both change over time. Then, defining \\(\\Delta Y_{it} = Y_{it} - Y_{it-1}\\) (and using similar notation for other variables), notice that \\[\\begin{align*}\n  \\Delta Y_{it} = \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\Delta U_{it}\n\\end{align*}\\] which, importantly, no longer involves the unobserved \\(W_i\\) and suggests running the above regression and interpreting the estimated version of \\(\\alpha\\) as an estimate of the causal effect of participating in the treatment.\nOften, there may be many omitted, time invariant variables. In practice, these are usually just lumped into a single individual fixed effect — even if there are many time invariant, unobserved variables, we can difference them all out at the same time \\[\\begin{align*}\n  Y_{it} &= \\alpha D_{it} + \\beta_{0,t} + \\beta_1 X_{it} + \\underbrace{\\beta_2 W_{1i} + \\beta_3 W_{2i} + \\beta_4 W_{3i}} + U_{it} \\\\\n  &= \\alpha D_{it} + \\beta_{0,t} + \\underbrace{\\eta_i} + U_{it}\n\\end{align*}\\] and we can follow the same strategies as above.\nAnother case that is common in practice is when there are more than two time periods. This case is similar to the previous one except there are multiple ways to eliminate the unobserved fixed effect. The two most common are the\nIt’s worth mentioning the cases where a fixed effects strategy can break down:\nAlso, the assumption of treatment effect homogeneity can potentially matter a lot in this context. This will particularly be the case when (i) individuals can become treated at different points in time, and (ii) there are treatment effect dynamics (so that the effect of participating in the treatment can vary over time) — both of these are realistic in many applications. This is a main research area of mine and one I am happy to talk way more about.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#panel-data-approaches",
    "href": "17-quasi_experiments.html#panel-data-approaches",
    "title": "18  Quasi-Experiments",
    "section": "",
    "text": "Time fixed effects — The previous regression did not include an intercept. It is common in applied work to allow for the intercept to vary over time (i.e., so that \\(\\beta_0 = \\beta_{0,t}\\)) which allows for “aggregate shocks” such as recessions or common trends in outcomes over time. In practice, this amounts to just including an intercept in the previous regression, for example,\n\\[\n    \\Delta Y_{it} = \\underbrace{\\theta_t} + \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\Delta U_{it}\n  \\]\n\n\n\n\nWithin estimator\nTo motivate this approach, notice that, if, for each individual, we average their outcomes over time, the we get \\[\\begin{align*}\n    \\bar{Y}_i = \\alpha \\bar{D}_i + \\beta_1 \\bar{X}_i + (\\textrm{time fixed effects}) + \\bar{U}_i\n  \\end{align*}\\] (where I have just written “time fixed effects” to indicate that these are transformed version of original fixed but still show up here.) Subtracting this equation from the expression for \\(Y_{it}\\) gives \\[\\begin{align*}\n    Y_{it} - \\bar{Y}_i = \\alpha (D_{it} - \\bar{D}_i) + \\beta_1 (X_{it} - \\bar{X}_i) + (\\textrm{time fixed effects}) + U_{it} - \\bar{U}_i\n  \\end{align*}\\]\nThis is a feasible regression to estimate (everything is observed here). This is called a within estimator because the terms \\(\\bar{Y}_i\\), \\(\\bar{D}_i\\), and \\(\\bar{X}_i\\) are the within-individual averages-over-time of the corresponding variable.\nFirst differences\nAnother approach to eliminating the unobserved fixed effects is to directly consider \\(\\Delta Y_{it}\\):\n\\[\\begin{align*}\n    \\Delta Y_{it} = \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\Delta U_{it}\n  \\end{align*}\\]\nThis is the same expression as we had before for the two period case. Only here you would include observations from all available time periods on \\(\\Delta Y_{it}, \\Delta D_{it}, \\Delta X_{it}\\) in the regression.\n\n\n\nUnobserved variables vary over time\n\\[\n    Y_{it} = \\alpha D_{it} + \\beta_0 + \\beta_1 X_{it} + \\beta_2 \\underbrace{W_{it}} + U_{it}\n  \\] In this case,\n\\[\n    \\Delta Y_{it} = \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\beta_2 \\underbrace{\\Delta W_{it}} + \\Delta U_{it}\n  \\]\nwhich still involves the unobserved \\(W_{it}\\), and implies that the fixed effects regression will contain omitted variable bias.\nThe effect of unobserved variables varies over time\n\\[\n    Y_{it} = \\alpha D_{it} + \\beta_0 + \\beta_1 X_{it} + \\underbrace{\\beta_{2,t}} W_i + U_{it}\n  \\] In this case,\n\\[\n    \\Delta Y_{it} = \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\underbrace{(\\beta_{2,t} - \\beta_{2,t-1})} W_i + \\Delta U_{it}\n  \\]\nwhich still involves the unobserved \\(W_i\\) (even though it doesn’t vary over time) and, therefore, the fixed effects regressions we have been considering will contain omitted variable bias.\n\n\n\n18.1.1 Difference in differences\nThe panel data approaches that we have been talking about so far are closely related to a natural-experiment type of strategy called difference in differences (DID).\nOne important difference relative to the previous approach is that DID is typically implemented when some units (these are often states or particular locations) implement a policy at some time period while others do not; and, in particular, we observe some periods before any units participate in the treatment.\nLet’s think about the case with exactly two time periods: \\(t\\) and \\(t-1\\). In this case, we’ll suppose that the outcomes that we observe are \\[\\begin{align*}\n  Y_{it} &= D_i Y_{it}(1) + (1-D_i) Y_{it}(0) \\\\\n  Y_{it-1} &= Y_{it-1}(0)\n\\end{align*}\\] In other words, in the second period, we observe treated potential outcomes for treated units and untreated potential outcomes for untreated units (this is just like the cross-sectional case above). But in the first period, we observe untreated potential outcomes for all units — because no one is treated yet.\nDID is often motivated by an assumption called the parallel trends assumption:\nParallel Trends Assumption \\[\\begin{align*}\n\\E[\\Delta Y_t(0) | D=1] = \\E[\\Delta Y_t(0) | D=0]\n\\end{align*}\\] This says that the path of outcomes that individuals in the treated group would have experienced if they had not been treated is the same as the path of outcomes that individual in the untreated group actually experienced.\nAs before, we continue to be interested in \\[\\begin{align*}\n  ATT = \\E[Y_t(1) - Y_t(0) | D=1]\n\\end{align*}\\] Recall that the key identification challenge if for \\(\\E[Y_t(0)|D=1]\\) here, and notice that \\[\\begin{align*}\n  \\E[Y_t(0) | D=1] &= \\E[\\Delta Y_t(0) | D=1] + \\E[Y_{t-1}(0) | D=1] \\\\\n  &= \\E[\\Delta Y_t(0) | D=0] + \\E[Y_{t-1}(0)|D=1] \\\\\n  &= \\E[\\Delta Y_t | D=0] + \\E[Y_{t-1}|D=1]\n\\end{align*}\\] where the first equality adds and subtracts \\(\\E[Y_{t-1}(0)|D=1]\\), the second equality uses the parallel trends assumption, and the last equality holds because all the potential outcomes in the previous line are actually observed outcome. Plugging this expression into the one for \\(ATT\\) yields: \\[\\begin{align*}\n  ATT = \\E[\\Delta Y_t | D=1] - \\E[\\Delta Y_t | D=0]\n\\end{align*}\\] In other words, under parallel trends, the \\(ATT\\) can be recovered by comparing the path of outcomes that treated units experienced relative to the path of outcomes that untreated units experienced (the latter of which is the path of outcomes that treated units would have experienced if they had not participated in the treatment).\nAs above, it is often convenient to estimate \\(ATT\\) using a regression. In fact, you can show that (in the case with two periods), \\(\\alpha\\) in the following regression is equal to the \\(ATT\\): \\[\\begin{align*}\n  Y_{it} = \\alpha D_{it} + \\theta_t + \\eta_i + v_{it}\n\\end{align*}\\] where \\(\\E[v_t | D] = 0\\).\n\n\n18.1.2 Computation\nThe lab in this chapter uses panel data to think about causal effects, so I won’t provide an extended discussion here but rather just mention the syntax for a few panel data estimators. Let’s suppose that you had a data frame that, for the first few rows, looked like (this is totally made up data)\n\n\n  id year         Y       X1 X2\n1  1 2019  87.92934 495.4021  1\n2  1 2020 102.77429 495.6269  1\n3  1 2021 110.84441 495.4844  0\n4  2 2019  76.54302 492.8797  1\n5  2 2020 104.29125 496.1825  1\n6  2 2021 105.06056 492.0129  1\n\n\nThis is what panel data typically looks like — here, we are following a single individual (who is distinguished by their id variable) over three years (from 2019-2021), there is an outcome Y and potential regressors X1 and X2.\nThere are several packages in R for estimating the fixed effects models that we have been considering. I mainly use plm (for “panel linear models”), so I’ll show you that one and then mention one more.\nFor plm, if you want to estimate a fixed effects model in first differences, you would use the plm command with the following sort of syntax\n\nlibrary(plm)\nplm(Y ~ X1 + X2 + as.factor(year), \n    data=name_of_data,\n    effect=\"individual\",\n    model=\"fd\",\n    index=\"id\")\n\nWe include here as.factor(year) to include time fixed effects, effect=\"individual\" means to include an individual fixed effect, model=\"fd\" says to estimate the model in first differences, and index=\"id\" means that the individual identifier is in the column “id”.\nThe code for estimating the model using a within transformation is very similar:\n\nplm(Y ~ X1 + X2 + as.factor(year), \n    data=name_of_data,\n    effect=\"individual\",\n    model=\"within\",\n    index=\"id\")\n\nThe only difference is that model=\"fd\" has been replaced with model=\"within\".\nLet me also just mention that the estimatr package can estimate a fixed effects model using a within transformation. The code for this case would look like\n\nlibrary(estimatr)\n\nlm_robust(Y ~ X1 + X2 + as.factor(year), \n          data=name_of_data,\n          fixed_effects=~id)\n\nI think the advantage of using this approach is that it seems straightforward to get the heteroskedasticity robust standard errors (or cluster-robust standard errors) that are popular in economics (as we have done before for heteroskedasticity robust standard errors for a regression with just cross sectional data). But I am not sure how (or if it is possible) to use estimatr to estimate the fixed effects model in first differences.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#instrumental-variables",
    "href": "17-quasi_experiments.html#instrumental-variables",
    "title": "18  Quasi-Experiments",
    "section": "18.2 Instrumental Variables",
    "text": "18.2 Instrumental Variables\nSW all of chapter 12\nIn the previous section, I used the word natural experiment but didn’t really define it. When an actual experiment is not actually available, a very common strategy used by researchers interested in causal effects is to consider natural experiments — these are not actual experiments, but more like the case where “something weird” happens that makes some individuals more likely to participate in the treatment without otherwise affecting their outcomes. This “something weird” is called an instrumental variable.\nLet me give you some examples:\n\nThis is not as popular of a topic as it used to be, but many economists used to be interested in the causal effect of military service on earnings. This is challenging because individuals “self-select” into the military (i.e., individuals don’t just randomly choose to join the military, and, while there may be many dimensions of choosing to join the military, probably one dimension is what a person expects the effect to be on their future earnings).\n\nA famous example of an instrumental variable in this case is an individual’s Vietname draft lottery number. Here, the idea is that a randomly generated lottery number (by construction) doesn’t have any direct effect on earnings, but it does affect the chances that someone participates in the military. This is therefore a natural experiment and could serve the role of an instrumental variable.\n\nFor studying the effect of education on on earnings, researchers have used the day of birth as an instrument for years of education. The idea is that compulsory school laws are set up so that individuals can leave school when they reach a certain age (e.g., 16). But this means that, among students that want to drop out as early as they can, students who have an “early” birthday (usually around October) will have spent less time in school than students who have a “late” birthday (usually around July) at any particular age. This is a kind of natural experiment — comparing earnings of students who drop out at 16 for those who have early birthdays relative to late birthdays.\n\nLet’s formalize these arguments. Using the same arguments as before, suppose we have a regression that we’d like to run\n\\[\n  Y_i = \\beta_0 + \\alpha D_i + \\underbrace{\\beta_1 W_i + U_i}_{V_i}\n\\] and interpret our estimate of \\(\\alpha\\) as an estimate of the causal effect of participating in the treatment. And where, for simplicity, I am not including any \\(X\\) covariates and where we do not observe \\(W\\). If \\(D\\) is correlated with \\(W\\), then just ignoring \\(W\\) and running a regression of \\(Y\\) on \\(D\\) will result in omitted variable bias so that regression does not recover an estimate of \\(\\alpha\\). To help with the discussion below, we’ll define \\(V_i\\) to be the entire unobservable term, \\(\\beta_1 W_i + U_i\\), in the above equation.\nAn instrumental variable, which we’ll call \\(Z\\), needs to satisfy the following two conditions:\n\n\\(\\Cov(Z,V) = 0\\) — This condition is called the exclusion restriction, and it means that the instrument is uncorrelated with the error term in the above equation. In practice, we’d mainly need to make sure that it is uncorrelated with whatever we think is in \\(W\\).\n\\(\\Cov(Z,D) \\neq 0\\) — This condition is called instrument relevance, and it means that the instrument needs to actually affect whether or not an individual participates in the treatment. We’ll see why this condition is important momentarily.\n\nNext, notice that\n\\[\n  \\begin{aligned}\n  \\Cov(Z,Y) &= \\Cov(Z,\\beta_0 + \\alpha D + V) \\\\\n  &= \\alpha \\Cov(Z,D)\n  \\end{aligned}\n\\] which holds because \\(\\Cov(Z,\\beta_0) = 0\\) (because \\(\\beta_0\\) is a constant) and \\(\\Cov(Z,V)=0\\) by the first condition of \\(Z\\) being a valid instrument. This implies that\n\\[\n  \\alpha = \\frac{\\Cov(Z,Y)}{\\Cov(Z,D)}\n\\] That is, if we have a valid instrument, the above formula gives us a path to recovering the causal effect of \\(D\\) on \\(Y\\). [Now you can also see why we needed the second condition — otherwise, we could divide by 0 here.]\nThe intuition for this is the following: changes in the instrument can cause changes in the outcome but only because they can change whether or not an individual participates in the treatment. These changes show up in the numerator. They are scaled by how much changes in the instrument result in changes in the treatment.\nIf there are other covariates in the model, the formula for \\(\\alpha\\) will become more complicated. But you can use the ivreg function in the ivreg package to make these complications for you.\n\n18.2.1 Example: Return to Education\nIn this example, we’ll estimate the return to education using whether or not an individual lives close to a college as an instrument for attending college. The idea is that (at least after controlling for some other covariates), the distance that a person lives from a college should not directly affect their earnings but it could affect whether or not they attend college due to it being more or less convenient. I think that the papers that use this sort of an idea primarily have in mind that distance-to-college may affect whether or not a student attends a community college rather than a university.\n\nlibrary(ivreg)\nlibrary(modelsummary)\n\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n  options(modelsummary_factory_latex = 'kableExtra')\n  options(modelsummary_factory_html = 'kableExtra')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\n\ndata(\"SchoolingReturns\", package=\"ivreg\")\n\nlm_reg &lt;- lm(log(wage) ~ education + poly(experience, 2, raw = TRUE) + ethnicity + smsa + south,\n  data = SchoolingReturns)\n\niv_reg &lt;- ivreg(log(wage) ~ education + poly(experience, 2, raw = TRUE) + ethnicity + smsa + south, \n  ~ nearcollege + poly(age, 2, raw = TRUE) + ethnicity + smsa + south,\n  data = SchoolingReturns)\n\nreg_list &lt;- list(lm_reg, iv_reg)\n\nmodelsummary(reg_list)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        \n        \n                \n                  (Intercept)                     \n                  4.734    \n                  4.066  \n                \n                \n                                                  \n                  (0.068)  \n                  (0.608)\n                \n                \n                  education                       \n                  0.074    \n                  0.133  \n                \n                \n                                                  \n                  (0.004)  \n                  (0.051)\n                \n                \n                  poly(experience, 2, raw = TRUE)1\n                  0.084    \n                  0.056  \n                \n                \n                                                  \n                  (0.007)  \n                  (0.026)\n                \n                \n                  poly(experience, 2, raw = TRUE)2\n                  -0.002   \n                  -0.001 \n                \n                \n                                                  \n                  (0.000)  \n                  (0.001)\n                \n                \n                  ethnicityafam                   \n                  -0.190   \n                  -0.103 \n                \n                \n                                                  \n                  (0.018)  \n                  (0.077)\n                \n                \n                  smsayes                         \n                  0.161    \n                  0.108  \n                \n                \n                                                  \n                  (0.016)  \n                  (0.050)\n                \n                \n                  southyes                        \n                  -0.125   \n                  -0.098 \n                \n                \n                                                  \n                  (0.015)  \n                  (0.029)\n                \n                \n                  Num.Obs.                        \n                  3010     \n                  3010   \n                \n                \n                  R2                              \n                  0.291    \n                  0.176  \n                \n                \n                  R2 Adj.                         \n                  0.289    \n                  0.175  \n                \n                \n                  AIC                             \n                  40329.6  \n                  40778.6\n                \n                \n                  BIC                             \n                  40377.7  \n                  40826.7\n                \n                \n                  Log.Lik.                        \n                  -1308.702\n                         \n                \n                \n                  F                               \n                  204.932  \n                         \n                \n                \n                  RMSE                            \n                  0.37     \n                  0.40   \n                \n        \n      \n    \n\n\n\nThe main parameter of interest here is the coefficient on education. The IV estimates are noticeably larger than the OLS estimates (0.133 relative to 0.074). [This is actually quite surprising as you would think that OLS would tend to over-estimate the return to education. This is a very famous example, and there are actually quite a few “explanations” from labor economists about why this sort of result arises.]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#regression-discontinuity",
    "href": "17-quasi_experiments.html#regression-discontinuity",
    "title": "18  Quasi-Experiments",
    "section": "18.3 Regression Discontinuity",
    "text": "18.3 Regression Discontinuity\nSW 13.4\nThe final type of natural experiment that we will talk about is called regression discontinuity. The sort of natural experiment is available when there is a running variable with a threshold (i.e., cutoff) where individuals above the threshold are treated while individuals below the threshold are not treated. These sorts of thresholds/cutoffs are fairly common.\nHere are some examples:\n\nCutoffs that make students eligible for a scholarship (e.g., the Hope scholarship)\nRules about maximum numbers of students allowed in a classroom in a particular school district\nVery close political elections\nVery close union elections\nThresholds in tax laws\n\nThen, the idea is to compare outcomes among individuals that “barely” were treated relative to those that “barely” weren’t treated. By construction, this often has properties that are similar to an actual experiment as those that are just above the cutoff should have observed and unobserved characteristics that are the same as those just below the cutoff.\nTypically, regression discontinuity designs are implemented using a regression that includes a binary variable for participating in the treatment, the running variable itself, and the interaction between the running variable and the treatment, using only observations that are “close” to the cutoff. [What should be considered “close” to the cutoff is actually a hard choice and there are tons of papers suggesting various approaches to decide what “close” means — we’ll largely avoid this and just pick what we think is close.] The estimated coefficient on the treatment indicator variable is an estimate of the average effect of participating in the treatment among those individuals who are close to the cutoff.\nThis will become clearer with an example.\n\n18.3.1 Example: Causal effect of Alcohol on Driving Deaths\nIn this section, we’ll be interested in the causal effect of young adult alcohol consumption on the number of deaths in car accidents.\nThe idea here will be to compare the number of deaths in car accidents that involve someone who is 21 or just over to the number of deaths in car accidents that involve someone who is just under 21. The reason to make this comparison is that alcohol consumption markedly increases when individuals turn 21 (due to that being the legal drinking age in the U.S.). If alcohol consumption increases car accident deaths, then we should also be able to detect a jump in the number of car accident deaths involving those who are just over 21.\nThe data that we have consists of age groups by age up to a particular month (agecell) and the number of car accident deaths involving that age group (mva).\n\nlibrary(ggplot2)\ndata(\"mlda\", package=\"masteringmetrics\")\n\n# drop some data with missing observations\nmlda &lt;- mlda[complete.cases(mlda),]\n\n# create treated variable\nmlda$D &lt;- 1*(mlda$agecell &gt;= 21)\n\nIn regression discontinuity designs, it is very common to show a plot of the data. That’s what we’ll do here.\n\nggplot(mlda, aes(x=agecell, y=mva)) + \n  geom_point() + \n  geom_vline(xintercept=21) + \n  xlab(\"age\") + \n  ylab(\"moving vehicle accident deaths\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThis figure at least suggests that the number of car accident deaths does appear to jump at age 21.\nNow, let’s run the regression that we talked about earlier, involving a treatment dummy, age, and age interacted with the treatment.\n\nrd_reg &lt;- lm(mva ~ D + agecell + agecell*D, data=mlda)\nsummary(rd_reg)\n\n\nCall:\nlm(formula = mva ~ D + agecell + agecell * D, data = mlda)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4124 -0.7774 -0.2913  0.8495  3.2378 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  83.8492     9.3328   8.984 1.63e-11 ***\nD            28.9450    13.8638   2.088   0.0426 *  \nagecell      -2.5676     0.4661  -5.508 1.77e-06 ***\nD:agecell    -1.1624     0.6592  -1.763   0.0848 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.299 on 44 degrees of freedom\nMultiple R-squared:  0.7222,    Adjusted R-squared:  0.7032 \nF-statistic: 38.13 on 3 and 44 DF,  p-value: 2.671e-12\n\n\nThese results suggest that alcohol consumption increased car accident deaths (you can see this from the estimated coefficient on D). [The setup in this example is somewhat simplified; if you wanted to be careful about how much alcohol consumption increased car accident deaths, then we would probably need to scale up our estimate by how much alcohol consumption increases on average when people turn 21. Nevertheless, what we have presented above does suggest that alcohol consumption increases car accident deaths.]\nFinally, let me show one more plot that is common to report in a regression discontinuity design.\n\n# get predicted values for plotting\nmlda$preds &lt;- predict(rd_reg)\n\n# make plot\nggplot(mlda, aes(x=agecell, y=mva, color=as.factor(D))) +\n  geom_point() +\n  geom_line(aes(y=preds)) + \n  geom_vline(xintercept=21) +\n  labs(color=\"treated\", x=\"age\", y=\"moving vehicle accident deaths\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThis shows the two lines that we effectively fit with the regression that included the binary variable for the treatment, the running variable, and their interaction. The “jump” between the red line and the blue line at age=21 is our estimated effect of the treatment.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#lab-7-drunk-driving-laws",
    "href": "17-quasi_experiments.html#lab-7-drunk-driving-laws",
    "title": "18  Quasi-Experiments",
    "section": "18.4 Lab 7: Drunk Driving Laws",
    "text": "18.4 Lab 7: Drunk Driving Laws\nFor this lab, we will use the Fatalities data. We will study the causal effect of mandatory jail sentence policies for drunk driving on traffic fatalities. The Fatalities data consists of panel data of traffic fatality death rates, whether or not a state has a mandatory jail sentence policy or not as well as several other variables from 1982-1988. Economic theory suggests that raising the cost of some behavior (in this case, you can think of a mandatory jail sentence as raising the cost of drunk driving) will lead to less of that behavior. That being said, it’s both interesting to test this theory and also consider the magnitude of this effect. That’s what we’ll do in this problem.\n\nThis data comes in a somewhat messier format than some of the data that we have used previously. To start with, create a new column in the data called afatal_per_million that is the number of alcohol involved vehicle fatalities per millions people in a state in a particular year. The variable afatal contains the total number of alcohol involved vehicle fatalities, and the variable pop contains the total population in a state.\nUsing a subset of the data from 1988, run a regression of afatal_per_million on whether or not a state has a mandatory jail sentence policy jail. How do you interpret the results?\nUsing the same subset from part 2, run a regression of afatal_per_million on jail, unemployment rate (unemp), the tax on a case of beer (beertax), the percentage of southern baptists in the state (baptist), the percentage of residents residing in dry counties (dry), the percentage of young drivers in the state, (youngdrivers), and the average miles driven per person in a state (miles). How do you interpret the estimated coefficient on jail? Would you consider this to be a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related fatalities?\nNow, using the full data, let’s estimate a fixed effects model with alcohol related fatalities per million as the outcome and mandatory jail policies as a regressor. Estimate the model using first differences and make sure to include time fixed effects. How do you interpret the results?\nEstimate the same model as in part 4, but using the within estimator instead of first differences. Compare these results to the ones from part 4.\nUsing the same within estimator as in part 5, include the same set of covariates from part 3 and interpret the estimated effect of mandatory jail policies. How do these estimates compare to the earlier ones?\nNow, we’ll switch to using a difference in differences approach to estimating the effect of mandatory jail policies. First, we’ll manipulate the data some.\n\nTo keep things simple, let’s start by limiting the data to the years 1982 and 1988 and drop the in-between periods.\nSecond, let’s calculate the change in alcohol related fatalities per million between 1982 and 1998 and keep the covariates that we have been using from 1982. One way to do this, is to use the pivot_wider function from the tidyr. In the case of panel data, “long format” data means that each row in the data corresponds to a paricular observation and a particular time period. Thus, with long format data, there are \\(n \\times T\\) total rows in the data. On the other hand, “wide format” data means that each row holds all the data (across all time periods) for a particular observation. Converting back and forth between long and wide formats is a common data manipulation task. Hint: This step is probably unfamiliar, so I’d recommend seeing if you can use ?tidyr::pivot_wider to see if you can figure out how to complete this step, but, if not, you can copy this code from the solutions in the next section.\nFinally, drop all states that are already treated in 1982.\n\nUsing the data that you constructed in part 7, implement the difference in differences regression of the change in alcohol related fatalities per million from 1982 to 1988 on the mandatory jail policy. How do you interpret these results and how do they compare to the previous ones? Now, additionally include the set of covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?\nAn alternative to DID, is to include the lagged outcome as a covariate. Using the data constructed in part 7, run a regression of alcohol related fatalities per million in 1988 on the mandatory jail policy and alcohol related fatalities per million in 1982. How do you interpret these results and how do they compare to the previous ones? Now include the additional covariates that we have been using in this model. How do you interpret these results and how do they compare to the previous ones?\nComment on your results from parts 1-9. Which, if any, of these are you most inclined to interpret as a reasonable estimate of the (average) causal effect of mandatory jail policies on alcohol related policies?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#lab-7-solutions",
    "href": "17-quasi_experiments.html#lab-7-solutions",
    "title": "18  Quasi-Experiments",
    "section": "18.5 Lab 7: Solutions",
    "text": "18.5 Lab 7: Solutions\n\n\n\n\nlibrary(tidyr)\nlibrary(plm)\n\ndata(Fatalities, package=\"AER\")\n\nFatalities$afatal_per_million &lt;- 1000000 * (Fatalities$afatal / Fatalities$pop )\n\n\n\n\n\nFatalities88 &lt;- subset(Fatalities, year==1988)\n\nreg88 &lt;- lm(afatal_per_million ~ jail, data=Fatalities88)\nsummary(reg88)\n\n\nCall:\nlm(formula = afatal_per_million ~ jail, data = Fatalities88)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.123 -16.622  -1.469   8.642 112.260 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   59.496      4.273  13.923   &lt;2e-16 ***\njailyes        9.155      7.829   1.169    0.248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.55 on 45 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.02949,   Adjusted R-squared:  0.007921 \nF-statistic: 1.367 on 1 and 45 DF,  p-value: 0.2484\n\n\nThe estimated coefficient on mandatory jail laws is 9.155. We should interpret this as just the difference between alcohol related fatalities per million in states that had mandatory jail laws in 1988 relative to states that did not have them. We cannot reject that there is no difference between states where the policy is in place relative to those that do not have the policy.\n\n\n\n\nreg88_covs &lt;- lm(afatal_per_million ~ jail + unemp + beertax + baptist + dry + youngdrivers + miles, data=Fatalities88)\nsummary(reg88_covs)\n\n\nCall:\nlm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + \n    dry + youngdrivers + miles, data = Fatalities88)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.065  -9.907  -1.690   9.673  82.100 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -29.373536  32.500240  -0.904   0.3717  \njailyes        3.120574   6.849271   0.456   0.6512  \nunemp          4.815081   1.892369   2.544   0.0150 *\nbeertax        2.311850   9.521684   0.243   0.8094  \nbaptist        0.661694   0.527228   1.255   0.2169  \ndry           -0.026675   0.383956  -0.069   0.9450  \nyoungdrivers  -0.092100 142.804244  -0.001   0.9995  \nmiles          0.006802   0.002822   2.411   0.0207 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.41 on 39 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4742,    Adjusted R-squared:  0.3798 \nF-statistic: 5.024 on 7 and 39 DF,  p-value: 0.0003999\n\n\nThe estimated coefficient on jail is 3.12. It is somewhat smaller than the previous estimate, though neither is statistically significant. We should interpret this as the partial effect of the mandatory jail policy, that is, that we estimate that mandatory jail laws increase the number of alcohol related fatalities per million by 3.12 on average controlling for the unemployment rate, beer tax, the fraction of southern baptists in the state, the fraction of residents in dry counties, the fraction of young drivers, and the average miles driven in the state. We cannot reject that the partial effect of mandatory jail policies is equal to 0.\n\n\n\n\nfd_reg &lt;- plm(afatal_per_million ~ jail + as.factor(year),\n              effect=\"individual\",\n              index=\"state\", model=\"fd\",\n              data=Fatalities)\nsummary(fd_reg)\n\nOneway (individual) effect First-Difference Model\n\nCall:\nplm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, \n    effect = \"individual\", model = \"fd\", index = \"state\")\n\nUnbalanced Panel: n = 48, T = 6-7, N = 335\nObservations used in estimation: 287\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-51.66677  -5.09887   0.23801   6.28688 119.08976 \n\nCoefficients: (1 dropped because of singularities)\n                    Estimate Std. Error t-value Pr(&gt;|t|)   \n(Intercept)         -2.15376    0.80673 -2.6697 0.008035 **\njailyes              2.60763    5.28351  0.4935 0.622016   \nas.factor(year)1983 -5.28423    1.82330 -2.8982 0.004050 **\nas.factor(year)1984 -3.58247    2.29451 -1.5613 0.119577   \nas.factor(year)1985 -5.60800    2.43517 -2.3029 0.022017 * \nas.factor(year)1986 -0.74192    2.28988 -0.3240 0.746180   \nas.factor(year)1987 -2.16244    1.80716 -1.1966 0.232476   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    54692\nResidual Sum of Squares: 51620\nR-Squared:      0.056171\nAdj. R-Squared: 0.035946\nF-statistic: 2.77733 on 6 and 280 DF, p-value: 0.012223\n\n\nWe should interpret the estimated coefficient on jail as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after accounting for time invariant variables whose effects do not change over time. Again, we cannot reject that the effect is equal to 0.\n\n\n\n\nwithin_reg &lt;- plm(afatal_per_million ~ jail + as.factor(year),\n              effect=\"individual\",\n              index=\"state\", model=\"within\",\n              data=Fatalities)\nsummary(within_reg)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = afatal_per_million ~ jail + as.factor(year), data = Fatalities, \n    effect = \"individual\", model = \"within\", index = \"state\")\n\nUnbalanced Panel: n = 48, T = 6-7, N = 335\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-95.1937300  -4.9678238   0.0088078   5.1611249  40.6263546 \n\nCoefficients:\n                    Estimate Std. Error t-value  Pr(&gt;|t|)    \njailyes               8.3327     4.9666  1.6777 0.0945164 .  \nas.factor(year)1983  -7.9151     2.6936 -2.9384 0.0035734 ** \nas.factor(year)1984  -8.4863     2.7115 -3.1298 0.0019341 ** \nas.factor(year)1985 -12.7849     2.7331 -4.6778 4.518e-06 ***\nas.factor(year)1986 -10.0726     2.7331 -3.6854 0.0002741 ***\nas.factor(year)1987 -13.5276     2.7115 -4.9890 1.067e-06 ***\nas.factor(year)1988 -13.6296     2.7279 -4.9964 1.030e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    53854\nResidual Sum of Squares: 47607\nR-Squared:      0.116\nAdj. R-Squared: -0.054487\nF-statistic: 5.24882 on 7 and 280 DF, p-value: 1.2051e-05\n\n\nThe estimated coefficient on jail has the same interpretation as in the previous problem. The estimated effect here is marginally statistically significant. 6.\n\nwithin_reg_covs &lt;- plm(afatal_per_million ~ jail + unemp + beertax + baptist + dry + youngdrivers + miles,\n                       effect=\"individual\",\n                       index=\"state\", model=\"within\",\n                       data=Fatalities)\nsummary(within_reg_covs)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = afatal_per_million ~ jail + unemp + beertax + baptist + \n    dry + youngdrivers + miles, data = Fatalities, effect = \"individual\", \n    model = \"within\", index = \"state\")\n\nUnbalanced Panel: n = 48, T = 6-7, N = 335\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-95.62306  -5.69773  -0.56903   4.79219  47.80871 \n\nCoefficients:\n                Estimate  Std. Error t-value  Pr(&gt;|t|)    \njailyes       4.9731e+00  4.9613e+00  1.0024   0.31702    \nunemp        -1.1340e+00  5.6592e-01 -2.0038   0.04605 *  \nbeertax      -2.7456e+01  1.5080e+01 -1.8207   0.06972 .  \nbaptist       2.5083e+00  4.3324e+00  0.5790   0.56308    \ndry           4.3092e-01  1.0870e+00  0.3964   0.69208    \nyoungdrivers  2.6357e+02  5.0169e+01  5.2537 2.957e-07 ***\nmiles        -6.8899e-04  7.3182e-04 -0.9415   0.34727    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    53854\nResidual Sum of Squares: 48083\nR-Squared:      0.10717\nAdj. R-Squared: -0.065023\nF-statistic: 4.80119 on 7 and 280 DF, p-value: 4.0281e-05\n\n\nWe should interpret the estimated coefficient on jail as an estimate of how much alcohol related traffic fatalities per million change on average under mandatory jail policies after controlling for the unemployment rate, beer taxes, the fraction of the state that is southern baptist, the fraction of the state that lives in a dry county, the fraction of young drivers in a state, and the average number of miles driven per person in the stata, and accounting for time invariant variables whose effects do not change over time.\n\n\n\n\n# part a: convert data to two period panel data\ntwo_period &lt;- subset(Fatalities, year==1982 | year==1988)\n# and drop some missing\ntwo_period &lt;- subset(two_period, !is.na(jail))\ntwo_period &lt;- BMisc::makeBalancedPanel(two_period, \"state\", \"year\")\ntwo_period$jail &lt;- 1*(two_period$jail==\"yes\")\n\n# part b: convert into wide format\nwide_df &lt;- pivot_wider(two_period, \n                       id_cols=\"state\", \n                       names_from=\"year\",\n                       values_from=c(\"jail\", \"afatal_per_million\"))\n\n# add back other covariates from 1982\nwide_df &lt;- merge(wide_df, subset(Fatalities, year==1982)[,c(\"unemp\", \"beertax\", \"baptist\", \"dry\", \"youngdrivers\", \"miles\",\"state\")], by=\"state\")\n\n# change in fatal accidents over time\nwide_df$Dafatal_per_million &lt;- wide_df$afatal_per_million_1988 - wide_df$afatal_per_million_1982\n\n# part c: drop already treated states\nwide_df &lt;- subset(wide_df, jail_1982==0)\n\n\n\n\n\ndid &lt;- lm(Dafatal_per_million ~ jail_1988, data=wide_df)\nsummary(did)\n\n\nCall:\nlm(formula = Dafatal_per_million ~ jail_1988, data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.652 -10.993   5.033  10.405  76.822 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -12.585      4.242  -2.966  0.00532 **\njail_1988      5.102     11.695   0.436  0.66526   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.37 on 36 degrees of freedom\nMultiple R-squared:  0.005259,  Adjusted R-squared:  -0.02237 \nF-statistic: 0.1903 on 1 and 36 DF,  p-value: 0.6653\n\ndid_covs &lt;- lm(Dafatal_per_million ~ jail_1988 + unemp + beertax + baptist + dry + youngdrivers + miles, data=wide_df)\nsummary(did_covs)\n\n\nCall:\nlm(formula = Dafatal_per_million ~ jail_1988 + unemp + beertax + \n    baptist + dry + youngdrivers + miles, data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.346 -12.383   1.456   9.092  60.585 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    6.851636  50.643035   0.135   0.8933  \njail_1988     -1.853041  10.834391  -0.171   0.8653  \nunemp          3.725007   1.919862   1.940   0.0618 .\nbeertax        8.300778  10.052007   0.826   0.4154  \nbaptist        0.527893   0.723263   0.730   0.4711  \ndry           -0.955636   0.546475  -1.749   0.0906 .\nyoungdrivers  89.432017 234.379768   0.382   0.7055  \nmiles         -0.010360   0.005823  -1.779   0.0854 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.9 on 30 degrees of freedom\nMultiple R-squared:  0.3303,    Adjusted R-squared:  0.174 \nF-statistic: 2.114 on 7 and 30 DF,  p-value: 0.07276\n\n\nIf we are willing to believe that, in the absence of the policy, that trends in alcohol related fatalities per million people would have followed the same trends over time for treated and untreated states, then we can interpret these as causal effects. These estimates are broadly similar to the previous ones though the second ones (that include additional covariates) are about the only ones where we ever get a negative estimate for the effect of mandatory jail policies. Like the previous estimates, neither of these estimates are statistically different from 0.\n\n\n\n\nlag_reg &lt;- lm(afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982, data=wide_df)\nsummary(lag_reg)\n\n\nCall:\nlm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982, \n    data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.120 -12.663  -0.684   6.873  92.390 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              19.0810    10.1089   1.888 0.067401 .  \njail_1988                 3.4323    10.3171   0.333 0.741363    \nafatal_per_million_1982   0.5607     0.1303   4.303 0.000129 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.47 on 35 degrees of freedom\nMultiple R-squared:  0.3462,    Adjusted R-squared:  0.3088 \nF-statistic: 9.266 on 2 and 35 DF,  p-value: 0.0005896\n\nlag_reg_covs &lt;- lm(afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982 + unemp + beertax + baptist + dry + youngdrivers + miles, data=wide_df)\nsummary(lag_reg_covs)\n\n\nCall:\nlm(formula = afatal_per_million_1988 ~ jail_1988 + afatal_per_million_1982 + \n    unemp + beertax + baptist + dry + youngdrivers + miles, data = wide_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.840  -8.793  -1.364   5.146  71.409 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)             -17.292595  46.318477  -0.373  0.71161   \njail_1988                 0.817453   9.786782   0.084  0.93401   \nafatal_per_million_1982   0.505371   0.173718   2.909  0.00689 **\nunemp                     3.189918   1.736443   1.837  0.07647 . \nbeertax                   2.885796   9.236174   0.312  0.75694   \nbaptist                   0.965785   0.668259   1.445  0.15911   \ndry                      -0.567567   0.509915  -1.113  0.27482   \nyoungdrivers            120.615853 211.026841   0.572  0.57202   \nmiles                    -0.002692   0.005888  -0.457  0.65087   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.7 on 29 degrees of freedom\nMultiple R-squared:  0.5443,    Adjusted R-squared:  0.4186 \nF-statistic: 4.329 on 8 and 29 DF,  p-value: 0.001596\n\n\nThese estimates directly control for alcohol related fatalities per million in the pre-treatment period 1982. These sorts of specifications are less common in economics, but, in my view, it seems like a reasonable approach here. That said, the results are more or less the same as earlier estimates.\n\nWe don’t have very strong evidence that mandatory jail policies reduced the number traffic fatalities. In my view, probably the best specifications for trying to understand the causal effects are the ones in part 7 (particularly, the ones that include covariates there), but I think that the the results in parts 4-9 are also informative. Broadly, these estimates are more or less similar — none of them are statistically significant and most are positive (which is an unexpected sign).\n\nBefore we finish, let me mention a few caveats to these results:\n\nFirst, I would be very hesitant to interpret these results as definitively saying that mandatory jail policies have no effect on alcohol related traffic fatalities. The main reason to be clear about this is that our standard error are quite large. For example, in the second specification in part 7 (the one I like the most), a 95% confidence interval for our estimate is \\([-23.1, 19.4]\\). This is a wide confidence interval — the average number of alcohol related traffic fatalities per million across all states and time periods is only 66. So our estimates are basically still compatible with very large reductions in alcohol related traffic fatalities up to large increases in alcohol related traffic fatalities.\nLet me make one more comment about the sign of our results. Many of our point estimates are positive; as we discussed earlier, it is hard to rationalize harsher punishments increasing alcohol related traffic fatalities. I think the main explanation for these results is just that our estimates are pretty noisy and, therefore, more or less “by chance” we are getting estimates that have an unexpected sign. But there are some other possible explanations that are worth mentioning. For one, there are a number of other policies related to drunk driving that occurred in the 1980s (particularly, related to legal drinking age) but perhaps others. It is not clear how these would interact with our estimates, but they could certainly play some role. Besides that, it seems to me that we have a pretty good set of covariates that enter our models, but there could be important covariates that we are missing. For this reason, some expertise in how to model state-level traffic fatalities is actually a very important skill here (actually probably the key skill here!)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#coding-questions",
    "href": "17-quasi_experiments.html#coding-questions",
    "title": "18  Quasi-Experiments",
    "section": "18.6 Coding Questions",
    "text": "18.6 Coding Questions\n\nFor this problem, we will use the data rand_hie. This is data from the RAND health insurance experiment in the 1980s. In the experiment, participants were randomly assigned to get Catastrophic (the least amount of coverage), insurance that came with a Deductible, insurance that came with Cost Sharing (i.e., co-insurance so that an individual pays part of their medical insurance), and Free (so that there is no cost of medical care).\nFor this problem, we will be interested in whether or not changing the type of health insurance changed the amount of health care utilization and the health status of individuals.\nWe will focus on the difference between the least amount of health insurance (“Catastrophic”) and the most amount of health insurance (“Free”). In particular, you can start this problem by creating a new dataset by running the following command: rand_hie_subset &lt;- subset(rand_hie, plan_type %in% c(\"Catastrophic\", \"Free\"))\nand use this data to answer the questions below.\n\nUse a regression to estimate the average difference between total medical expenditure (total_med_expenditure) by plan type (plan_type) and report your results. Should you interpret these as average causal effects? Explain.\nUse a regression to estimate the average difference between face to face doctors visits (face_to_face_visits) by plan type (plan_type) and report your results. Should you interpret these as average causal effects? Explain.\nUse a regression to estimate the average difference between the overall health index (health_index) by plan type (plan_type) and report your results. Should you interpret these as average causal effects? Explain.\nHow do you interpret the results from parts a-c?\n\nFor this problem, we will study the causal effect of having more children on women’s labor supply using the data Fertility.\n\nLet’s start by running a regression of the number of hours that a woman typically works per week (work) on whether or not she has more than two children (morekids), her age and \\(age^2\\), and race/ethnicity (afam and hispanic). Report your results. How do you feel about interpreting the estimated coefficient on morekids as the causal effect of having more than two children? Explain.\nOne possible instrument in this setup is the sex composition of the first two children (i.e., whether they are both girls, both boys, or a boy and a girl). The thinking here is that, at least in the United States, parents tend to have a preference for having both a girl and a boy and that, therefore, parents whose first two children have the same sex may be more likely to have a third child than they would have been if they have a girl and a boy. Do you think that using a binary variable for whether or not the first two children have the same sex is a reasonable instrument of for morekids from part a?\nCreate a new variable called samesex that is equal to one for families whose first two children have the same sex. Using the same specification as in part a, use samesex as an instrument for morekids and report the results. Provide some discussion about your results.\n\nFor this question, we will use the AJR data. A deep question in development economics is: Why are some countries much richer than other countries? One explanation for this is that richer countries have different institutions (e.g., property rights, democracy, etc.) that are conducive to growth. Its hard to study these questions though because institutions do not arise randomly — there could be reverse causality so that property rights, democracy, etc. are (perhaps partially) caused by being rich rather than the other way around. Alternatively, other factors (say a country’s geography) could cause both of these. We’ll consider one instrumental variables approach to thinking about this question in this problem.\n\nRun a regression of the log of per capita GDP (the log of per capita GDP is stored in the variable GDP) on a measure of the protection against expropriation risk (this is a measure of how “good” a country’s institutions are (a larger number indicates “better” institutions) and it is in the variable Exprop). How do you interpret these results? Do you think it would be reasonable to interpret the estimated coefficient on Exprop as the causal effect of institutions on GDP.\nOne possible instrument for Exprop is settler mortality (we’ll use the log of this which is available in the variable logMort). Settler mortality is a measure of how dangerous it was for early settlers of a particular location. The idea is that places that have high settler mortality may have set up worse (sometimes called “extractive”) institutions than places that had lower settler mortality. But that settler mortality (from a long time ago) does not have any other direct effect on modern GDP. Provide some discussion about whether settler mortality is a valid instrument for institutions.\nEstimate an IV regression of GDP on Exprop using logMort as an instrument for Exprop. How do you interpret the results? How do these results compare to the ones from part a?\n\nFor this question, we’ll use the data house to study the causal effect of incumbency on the probability that a member of the House of Representatives gets re-elected.\n\nOne way to try to estimate the causal effect of incumbency is to just run a regression where the outcome is democratic_vote_share (this is the same outcome we’ll use below) and where the model includes a dummy variable for whether or not the democratic candidate is an incumbent. What are some limitions of this strategy?\nThe house data contains data about the margin of victory (is positive if they won the election and negative if they lost) for Democratic candidates in the current election and data about the Democratic margin of victory in the past election. Explain how you could use this data in a regression discontinuity design to estimate the causal effect of incumbency.\nUse the house data to implement the regression discontinuity design that you proposed in part b. What do you estimate as the causal effect of incumbency?\n\nFor this problem, we will use the data banks. We will study the causal effect of monetary policy on bank closures during the Great Depression. We’ll consider an interesting natural experiment in Mississippi where half the northern half of the state was in St. Louis’s federal reserve district (District 8) and the southern half of the state was in Atlanta’s federal reserve district (District 6). Atlanta had much looser monetary policy (meaning they substantially increased lending) than St. Louis during the early part of the Great Depression and our interest is in whether looser monetary policy made an difference.\n\nPlot the total number of banks separately for District 6 and District 8 across all available time periods in the data.\nAn important event in the South early in the Great Depression was the collapse of Caldwell and Company — the largest banking chain in the South at the time. This happened in November 1930. The Atlanta Fed’s lending markedly increased quickly after this event while St. Louis’s did not. Calculate a DID estimate of the effect of looser monetary policy on the number of banks that are still in business. How do you interpret these results? Hint: You can calculate this by taking the difference between the number of banks in District 6 relative to the number of banks in District 8 across all time periods relative to the difference between the number of banks in District 6 relative to District 8 in the first period (July 1, 1929).",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#extra-questions",
    "href": "17-quasi_experiments.html#extra-questions",
    "title": "18  Quasi-Experiments",
    "section": "18.7 Extra Questions",
    "text": "18.7 Extra Questions\n\nWhat is the difference between treatment effect homogeneity and treatment effect heterogeneity?\nWhy do most researchers give up on trying to estimate the individual-level effect of participating in a treatment?\nExplain what unconfoundedness means.\nWhat is the key condition underlying a difference-in-differences approach to learn about the causal effect of some treatment on some outcome?\nWhat are two key conditions for a valid instrument?\nSuppose you are interested in the causal effect of participating in a union on a person’s income. Consider the following approaches.\n\nSuppose you run the following regression\n\\[\\begin{align*}\n   Earnings_i = \\beta_0 + \\alpha Union_i + \\beta_1 Education_i + U_i\n\\end{align*}\\]\nWould it be reasonable to interpret \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain.\nSuppose you have access to panel data and run the following fixed effects regression \\[\\begin{align*}\n   Earnings_{it} = \\beta_{0,t} + \\alpha Union_{it} + \\beta_1 Education_{it} + \\eta_i + U_{it}\n\\end{align*}\\]\nwhere \\(\\eta_i\\) is an individual fixed effect. Would it be reasonable to interpert \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?\nGoing back to the case with cross-sectional data, consider the regression \\[\\begin{align*}\n   Earnings_i = \\beta_0 + \\alpha Union_i + U_i\n\\end{align*}\\] but using the variable \\(Z_i = 1\\) if birthday is between Jan. 1 and Jun. 30 while \\(Z_i=0\\) otherwise. Would it be reasonable to interpert \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach?\n\nSuppose that you are interested in the effect of lower college costs on the probability of graduating from college. You have access to student-level data from Georgia where students are eligible for the Hope Scholarship if they can keep their GPA above 3.0.\n\nWhat strategy can use to exploit this institional setting to learn about the causal effect of lower college costs on the probability of going to college?\nWhat sort of data would you need in order to implement this strategy?\nCan you think of any ways that the approach that you suggested could go wrong?\nAnother researcher reads the results from the approach you have implemented and complains that your results are only specific to students who have grades right around the 3.0 cutoff. Is this a fair criticism?\n\nSuppose you are willing to believe versions of unconfoundedness, a linear model for untreated potential outcomes, and treatment effect homogeneity so that you could write \\[\\begin{align*}\n  Y_i = \\beta_0 + \\alpha D_i + \\beta_1 X_i + \\beta_2 W_i + U_i\n\\end{align*}\\] with \\(\\E[U|D,X,W] = 0\\) so that you were willing to interpret \\(\\alpha\\) in this regression as the causal effect of \\(D\\) on \\(Y\\). However, suppose that \\(W\\) is not observed so that you cannot operationalize the above regression.\n\nSince you do not observe \\(W\\), you are considering just running a regression of \\(Y\\) on \\(D\\) and \\(X\\) and interpreting the estimated coefficient on \\(D\\) as the causal effect of \\(D\\) on \\(Y\\). Does this seem like a good idea?\nIn part (a), we can write a version of the model that you are thinking about estimating as \\[\\begin{align*}\n   Y_i = \\delta_0 + \\delta_1 D_i + \\delta_2 X_i + \\epsilon_i\n\\end{align*}\\] Suppose that \\(\\E[\\epsilon | D, X] = 0\\) and suppose also that \\[\\begin{align*}\nW_i = \\gamma_0 + \\gamma_1 D_i + \\gamma_2 X_i + V_i\n  \\end{align*}\\] with \\(\\E[V|D,X]=0\\). Provide an expression for \\(\\delta_1\\) in terms of \\(\\alpha\\), \\(\\gamma\\)’s and \\(\\beta\\)’s. Explain what this expression means.\n\nSuppose you have access to an experiment where some participants were randomly assigned to participate in a job training program and others were randomly assigned not to participate. However, some individuals that were assigned to participate in the treatment decided not to actually participate. Let’s use the following notation: \\(D=1\\) for individuals who actually participated and \\(D=0\\) for individuals who did not participate. \\(Z=1\\) for individuals who were assigned to the treatment and \\(Z=0\\) for individuals assigned not to participate (here, \\(D\\) and \\(Z\\) are not exactly the same because some individuals who were assigned to the treatment did not actually participate).\nYou are considering several different approaches to dealing with this issue. Discuss which of the following are good or bad ideas:\n\nEstimating \\(ATT\\) by \\(\\bar{Y}_{D=1} - \\bar{Y}_{D=0}\\).\nRun the regression \\(Y_i = \\beta_0 + \\alpha D_i + U_i\\) using \\(Z_i\\) as an instrument.\n\nSuppose you and a friend have conducted an experiment (things went well so that everyone complied with the treatment that they were assigned to, etc.). You interpret the difference \\(\\bar{Y}_{D=1} - \\bar{Y}_{D=0}\\) as an estimate of the \\(ATT\\), but your friend says that you should interpret it as an estimate of the \\(ATE\\). In fact, according to your friend, random treatment assignment implies that \\(\\E[Y(1)] = \\E[Y(1)|D=1] = \\E[Y|D=1]\\) and \\(\\E[Y(0)] = \\E[Y(0)|D=0] = \\E[Y|D=0]\\) which implies that \\(ATE = \\E[Y|D=1] - \\E[Y|D=0]\\). Who is right?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  },
  {
    "objectID": "17-quasi_experiments.html#answers-to-some-extra-questions",
    "href": "17-quasi_experiments.html#answers-to-some-extra-questions",
    "title": "18  Quasi-Experiments",
    "section": "18.8 Answers to Some Extra Questions",
    "text": "18.8 Answers to Some Extra Questions\nAnswer to Question 4\nThe key condition is the parallel trends assumption that says that, in the absence of participating in the treatment, the path of outcomes that individuals in the treated group is the same, on average, as the path of outcomes that individuals in the untreated group actually experienced.\nAnswer to Question 9\n\nWhen some individuals do not comply with their treatment assignment, this approach is probably not so great. In particular, notice that the comparison in this part of the problem is among individuals who actually participated in the treatment relative to those who didn’t (the latter group includes both those assigned not to participate in the treatment along with those assigned to participate in the treatment, but ultimately didn’t actually participate). This suggests that this approach would generally lead to biased estimates of the \\(ATT\\). In the particular context of job training, you can see this would not be such a good idea if, for example, the people who were assigned to the job training program but who did not participate tended to do this because they were able to find a job before the job training program started.\nThis approach is likely to be better. By construction, \\(Z\\) is not correlated with \\(U\\) (since \\(Z\\) is randomly assigned). \\(Z\\) is also likely to be positively correlated with \\(Z\\) (in particular, this will be the case if being randomly assigned to treatment increases the probability of being treated). This implies that \\(Z\\) is a valid instrument and should be able to deliver a reasonable estimate of the effect of participating in the treatment.\n\nAnswer to Question 10\nWhile your friend’s explanation is not technically wrong, it seems to me that you are more right than your friend. There is an important issue related to external validity here. The group of people that show up to participate in the experiment could be (and likely are) quite different from the general population. Interpreting the results of the experiment as being an \\(ATE\\) (in the sense of across the entire population) is therefore likely to be incorrect — or at least would require extra assumptions and/or justifications. Interpreting them as an \\(ATT\\) (i.e., as the effect among those who participated in the treatment) is still perfectly reasonable though.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Quasi-Experiments</span>"
    ]
  }
]