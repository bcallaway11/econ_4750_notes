[["index.html", "Supplemtary Notes and References for ECON 4750 Topic 1 Introduction 1.1 What is this? 1.2 What is this not? 1.3 Why did I write this? 1.4 Additional References 1.5 Goals for the Course 1.6 Studying for the Class 1.7 First Week of Class", " Supplemtary Notes and References for ECON 4750 Brantly Callaway 2021-04-21 Topic 1 Introduction 1.1 What is this? Short answer: I’m not exactly sure, but you can think of this material as being similar to: Supplementary Notes — there are several topics that we will cover in class in substantially more detail than in the textbook. A big chunk of the material provided here can be seen as additional reference material for topics not covered in depth in the textbook. Quasi-Study Guide — Another main chunk of the material provided here is a course outline (should be close to the order that we cover in class) along with cross-references to the corresponding section in the textbook. In addition, there are additional practice questions (some with answers) provided at the end of each section. 1.2 What is this not? There are a couple of things that I want to explicitly say that this material is not. These include: A substitute for coming to class — I will take attendance anyway, but not attending class and relying on this material is not a good plan. Please do not do this. A substitute for the textbook — A lot of the material provided here is just a reference to the corresponding section in the textbook. This means you still need to use the textbook. Homework questions will also primarily come from the textbook. Sufficient for making good grades on exams — I don’t suspect that it will be a good strategy to rely exclusively on the material provided here in order to do well on the exams. I think this material should be helpful, but not (even close to being) sufficient. 1.3 Why did I write this? I have a strong opinion about the best order to teach the material in ECON 4750. And, although, there are a number of advanced undergraduate textbooks in Econometrics that I like and reference as I teach the course, none of them go in the same order that I would like to teach. Therefore, I think one way that I can both go in the order that I want to during the semester without confusing you (the student) too much is to provide a detailed set of references to material that we are covering throughout the semester. The book that I require for the course is Stock and Watson. By the end of the semester, we will have covered in much detail the first 14 chapters of the textbook (and some topics we will have covered in substantially more detail than in the textbook). If I had more time, the next topic that I would cover in this course would be Time Series Econometrics. I used to cover this material in the current course, but I have found that I am happier with the tradeoff of understanding slightly fewer topics better relative to covering more topics but at a faster pace. Time series is especially important for students who are interested in macroeconomics or finance. Fortunately, we offer a time series econometrics course, and, if you take it, you should be well prepared for it coming out of this course. 1.4 Additional References These are all free to download; they are not main textbooks but I sometimes consult them for the class and could potentially be useful for you to consult in the future: For R programming: Introduction to Econometrics with R, by Cristoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer For prediction/machine learning: An Introduction to Statistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani For causal inference: Causal Inference: The Mixtape, by Scott Cunningham Additional R References: There are tons of free R resources available online. Here are some that seem particularly useful to me. Manageable Introduction: Introduction to R and RStudio, by Stephanie Spielman Full length book: Introduction to Data Science: Data Analysis and Prediction Algorithms with R, by Rafael Irizarry (this is way more than you will need for this course, but I suggest checking out Chapters 1, 2, 3, and 5, and there’s plenty more that you might find interesting). Full length book: STAT 545: Data Wrangling, exploration, and analysis with R, by Jenny Bryan 1.5 Goals for the Course I have three high level goals — that by the end of the semester, students should be able to Run regressions and be able to interpret them, even complex regressions Use data in order to be able to predict outcomes of interest Be able to think clearly about when statistical results can be interpreted causal effects. In order to make progress towards these three goals, we also will need to learn about two additional topics: Statistical Programming Probability and Statistics We’ll start the course off talking about these two topics. Perhaps this will be review material for some of you, but I have found that it is worth it to spend several weeks getting everyone on the same page with respect to these topics. 1.6 Studying for the Class Students ask me all the time “How should I study for your class?” My advice (and I think this applies to most classes, not just my class) is for you to start by studying the notes from class. The things that I have discussed in class are the things that I think are most important for you learn in this sort of class and are the material that will be covered on the exam. That said, it may sometimes be the case that you do not fully understand a lecture or the notes that you took from a lecture when you are studying (this certainly applied to me when I was a student). If there are places that you do not understand what the notes mean, then I think that is the time when you should find the relevant portion of the textbook (or supplementary notes provided here) in order to “supplement” what the notes say. 1.7 First Week of Class SW All of Chapter 1 In the first few classes, we will talk at very high level about the objectives of Econometrics. "],["statistical-programming.html", "Topic 2 Statistical Programming 2.1 What is R? 2.2 Downloading R 2.3 RStudio 2.4 RStudio Development Environment 2.5 Installing R Packages 2.6 Running code 2.7 R Basics 2.8 Workspace 2.9 Solving the quadratic equation 2.10 Functions in R 2.11 Data types 2.12 Vector arithmetic 2.13 Subsetting with logicals 2.14 Logicals 2.15 %in% 2.16 Basic Plots 2.17 Programming basics 2.18 if/else 2.19 Writing functions 2.20 for loops 2.21 Vectorization 2.22 Tidyverse 2.23 Data Visualization 2.24 Reproducible Research 2.25 Technical Writing Tools", " Topic 2 Statistical Programming We will learn a lot more about statistical programming this semester, but we’ll start with a crash course on R with the idea of getting you up-and-running. I listed a few references in the Introduction, but this section will mostly follow the discussion in Introduction to Data Science: Data Analysis and Prediction Algorithms with R, by Rafael Irizarry. I’ll abbreviate this reference as IDS throughout this section. IDS is not specifically geared towards Econometrics, but I think it is a really fantastic book and resource. In this section, I point you to the references for the material that I cover in class, but I would strongly recommend reading all of the first 5 chapters of IDS over the next couple of weeks. We will basically only cover the first 5 chapters in our class, but the course should set you up so that the remaining 35 chapters of the book can serve as helpful reference material throughout the rest of the semester. 2.1 What is R? Statistical programming language =&gt; looks like a “real” programming language =&gt; lots of statistical things are easy to do Popular among statisticians, computer scientists, economists Other popular alternatives Python Stata Easy to share code across platforms: Linux, Windows, Mac Large community; lots of helpful resources on StackOverflow, etc. First place to look if you don’t know how to do something: DuckDuckGo (or…err…Google)! Easy to write and contribute extensions. I have 7 (?) R packages that you can easily download and immediately use 2.2 Downloading R We will use R (https://www.r-project.org/) to analyze data. R is freely available and available across platforms. You should go ahead and download R for your personal computer as soon as possible — this should be relatively straightforward. It is also available at most computer labs on campus. 2.3 RStudio Base R comes with a lightweight development environment (i.e., a place to write and execute code), but most folks prefer RStudio as it has more features. You can download it here: ; choose the free version based on your operating system (Windows, Mac, etc.). 2.4 RStudio Development Environment IDS 1.4 2.5 Installing R Packages IDS 1.5 2.6 Running code IDS 2.1 2.7 R Basics IDS 2.2 2.8 Workspace IDS 2.2 2.9 Solving the quadratic equation IDS 2.2 Write a function to do this 2.10 Functions in R IDS 2.2 Practice: What is the sum of the first 100 positive integers? The formula for the sum of integers 1 through n is n(n+1)/2. Define n=100 and then use R to compute the sum of 1 through 100 using the formula. What is the sum? 2.11 Data types IDS 2.4 2.12 Vector arithmetic IDS 2.11 2.13 Subsetting with logicals IDS 2.13 2.14 Logicals IDS 2.13 2.15 %in% IDS 2.13 2.16 Basic Plots IDS 2.15 2.17 Programming basics 2.18 if/else IDS 3.1 2.19 Writing functions IDS 3.2 Excercise: Write a function for solving quadratic equation 2.20 for loops IDS 3.4 2.21 Vectorization IDS 3.5 2.22 Tidyverse IDS Chapter 4 — strongly recommend that you read this R has very good data cleaning / manipulating tools largely won’t cover in class 2.23 Data Visualization IDS Ch. 6-11 — R has very good data visualization tools; strongly recommend that you read this Another very strong point of R ggplot 538’s graphs produced with ggplot 2.24 Reproducible Research IDS Ch. 40 Rmarkdown is a very useful way to mix code and content 2.25 Technical Writing Tools This is largely beyond the scope of the course, but, especially for students in ECON 6750, I recommend that you look up Latex. This is a markup language mainly for writing technical, academic writing. An easy way to get started here is to use the website Overleaf. This is also closely related to markdown/R-markdown discussed above (Latex tends to be somewhat more complicate which comes with some associated advantages and disadvantages). "],["probability-and-statistics.html", "Topic 3 Probability and Statistics 3.1 Topics in Probability 3.2 Topics in Statistics 3.3 Extra Questions", " Topic 3 Probability and Statistics This section contains a list of topics, cross references to the textbook, and, in some places, additional discussion. I mostly just follow Chapters 2 &amp; 3 in the book for this part of the course, so this is primarily a list of references to the corresponding places in the textbook. 3.1 Topics in Probability 3.1.1 Random Variables SW 2.1 3.1.2 pdfs, pmfs, and cdfs SW 2.1 3.1.3 Summation operator course notes 3.1.4 Continuous Random Variables SW 2.1 3.1.5 Expected Values SW 2.2 3.1.6 Variance SW 2.2 3.1.7 Mean and Variance of Linear Functions SW 2.2 3.1.8 Properties of Variance SW 2.2 3.1.9 Standardized Random Variables SW 2.2 3.1.10 Multiple Random Variables SW 2.3 3.1.11 Conditional Expectations SW 2.3 3.1.12 Law of Iterated Expectations SW 2.3 3.1.13 Covariance SW 2.3 3.1.14 Correlation SW 2.3 3.1.15 Properties of Expectations SW 2.3 3.1.16 Normal Distribution SW 2.4 3.2 Topics in Statistics 3.2.1 Simple Random Sample SW 2.5 3.2.2 Estimating \\(\\mathbb{E}[Y]\\) SW 2.5, 3.1 3.2.3 Mean of \\(\\bar{Y}\\) SW 2.5, 3.1 3.2.4 Variance of \\(\\bar{Y}\\) SW 2.5, 3.1 3.2.5 Sampling distribution of estimator SW 2.5, 3.1 Bias Sampling Variance 3.2.6 Relative Efficiency SW 3.1 3.2.7 Mean Squared Error Course notes 3.2.8 Large Sample Properties of Estimators SW 2.6 Tools: Law of Large Numbers Central Limit Theorem Properties: Consistency Asymptotic Normality 3.2.9 Inference SW 3.2, 3.3 t-statistics p-values confidence intervals statistical vs. economic significance 3.3 Extra Questions Suppose that \\(\\mathbb{E}[X] = 10\\) and \\(\\mathrm{var}(X) = 2\\). Also, suppose that \\(Y=5 + 9 X\\). What is \\(\\mathbb{E}[Y]\\)? What is \\(\\mathrm{var}(Y)\\)? Suppose you are interested in average height of students at UGA. Let \\(Y\\) denote a student’s height; also let \\(X\\) denote a binary variable that is equal to 1 if a student is female. Suppose that you know that \\(\\mathbb{E}[Y|X=1] = 5\\&#39; \\ 4\\&quot;\\) and that \\(\\mathbb{E}[Y|X=0] = 5\\&#39; \\ 9\\&quot;\\) What is \\(\\mathbb{E}[Y]\\)? Explain how the answer to part (a) is related to the Law of Iterated Expectations. What is the difference between consistency and unbiasedness? Suppose you have an estimator that is unbiased. Will it necessarily be consistent? If not, provide an example of an unbiased estimator that is not consistent. Suppose you have an estimator that is consistent. Will it necessarily be unbiased? If not, provide an example of a consistent estimator that is not unbiased. The Central Limit Theorem says that, \\(\\sqrt{n}\\left(\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\mathbb{E}[Y])\\right) \\rightarrow N(0,V)\\) as \\(n \\rightarrow \\infty\\) where \\(V = \\mathrm{var}(Y)\\). What happens to \\(n \\left(\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\mathbb{E}[Y])\\right)\\) as \\(n \\rightarrow \\infty\\)? Explain. What happens to \\(n^{1/3} \\left(\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\mathbb{E}[Y])\\right)\\) as \\(n \\rightarrow \\infty\\)? Explain. "],["linear-regression.html", "Topic 4 Linear Regression 4.1 Nonparametric Regression / Curse of Dimensionality 4.2 Linear Regression Models 4.3 Partial Effects 4.4 Interpreting Binary Covariate 4.5 Nonlinear Regression Functions 4.6 Interpreting Interaction Terms 4.7 Elasticities 4.8 Omitted Variable Bias 4.9 How to estimate the parameters in a regression model 4.10 Inference 4.11 Extra Questions 4.12 Answers to Some Extra Questions", " Topic 4 Linear Regression This is probably the part of the class where we will jump around in the book the most this semester. The pedagogical approach of the textbook is to introduce the notion of causality very early and to emphasize the requirements on linear regression models in order to deliver causality, while increasing the complexity of the models over several chapters. This is totally reasonable, but I prefer to start by teaching the mechanics of regressions: how to compute them, how to interpret them (even if you are not able to meet the requirements of causality), and how to use them to make predictions. Then, we’ll have a serious discussion about causality over the last few weeks of the semester. In practice, this means we’ll cover parts Chapters 4-8 in the textbook now, and then we’ll circle back to some of the issues covered in these chapters again towards the end of the semester. 4.1 Nonparametric Regression / Curse of Dimensionality If you knew nothing about regressions, it would seem natural to try to estimate \\(\\mathbb{E}[Y|X_1=x_1,X_2=x_2,X_3=x_3]\\) by just calculating the average of \\(Y\\) among observations that have values of the regressors equal to \\(x_1\\), \\(x_2\\), and \\(x_3\\) (if these are discrete) or that are, in some sense, close to \\(x_1\\), \\(x_2\\), and \\(x_3\\) (if these are continuous) This is actually a pretty attractive idea However, you run into the issue that it is practically challenging to do this when the number of regressors starts to get large (i.e., if you have 10 regressors, generally, you would need tons of data to be able to find a suitable number of observations that are ``close’’ to any particular value of the regressors). This issue is called the “curse of dimensionality” We will focus on linear models for \\(\\mathbb{E}[Y|X_1,X_2,X_3]\\) largely to get around the curse of dimensionality 4.2 Linear Regression Models SW 4.1 4.3 Partial Effects In the model, \\[\\begin{align*} \\mathbb{E}[Y | X_1, X_2, X_3] &amp;= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 \\end{align*}\\] If \\(X_1\\) is continuous, \\[\\begin{align*} \\beta_1 = \\frac{\\partial \\mathbb{E}[Y|X_1,X_2,X_3]}{\\partial X_1} \\end{align*}\\] In other words, \\(\\beta_1\\) should be interpreted as how much \\(Y\\) increases, on average, when \\(X_1\\) increases by one unit holding \\(X_2\\) and \\(X_3\\) constant. Make sure to get this interpretation right! If \\(X_1\\) is discrete (let’s say binary): \\[\\begin{align*} \\beta_1 = \\mathbb{E}[Y|X_1=1,X_2,X_3] - \\mathbb{E}[Y|X_1=0,X_2,X_3] \\end{align*}\\] In other words, \\(\\beta_1\\) should be interpreted as how much \\(Y\\) increases, on average, when \\(X_1\\) changes from 0 to 1, hodling \\(X_2\\) and \\(X_3\\) constant. Note: above model can be equivalently written as \\[\\begin{align*} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U \\end{align*}\\] where \\(\\mathbb{E}[U|X_1,X_2,X_3] = 0\\). 4.4 Interpreting Binary Covariate SW 5.3 4.5 Nonlinear Regression Functions SW 8.1, 8.2 Also, please read all of SW Ch. 8 4.6 Interpreting Interaction Terms SW 8.3 4.7 Elasticities SW 8.2 4.8 Omitted Variable Bias SW 6.1 The book talks about omitted variable bias in the context of causality (this is probably the leading case), but we have not talked about causality yet. The same issues arise if we just say that we have some regression of interest but are unable to estimate it because some covariates are unobserved. In this case, we would just not be able to interpret, say \\(\\beta_1\\), as the partial effect of our interest (except under special cases discussed in class). The relationship to causality (which is not so important for now), is that under certain conditions, we may have a particular partial effect that we would be willing to interpret as being the “causal effect,” but if we are unable to control for some variables that would lead to this interpretation, then we get to the issues pointed out in the textbook. 4.9 How to estimate the parameters in a regression model SW 4.2, 6.3 4.10 Inference SW 4.5, 5.1, 5.2, 6.6 Additional (Optional) Material We discussed in class the practical issues of inference in linear regression models. These results rely on arguments building on the Central Limit Theorem (this should not surprise you as it is similar to the case for the asymptotic distribution of \\(\\sqrt{n}(\\bar{Y} - \\mathbb{E}[Y]))\\) that we discussed earlier in the semester. In this section, I sketch these types of arguments for you. This material is advanced/optional, but I suggest that you study this material. In class, we wrote down that, in the simple linear regression model, \\[\\begin{align*} \\sqrt{n}(\\hat{\\beta}_1 - \\beta_1) \\rightarrow N(0,V) \\quad \\textrm{as} \\ n \\rightarrow \\infty \\end{align*}\\] where \\[\\begin{align*} V = \\frac{\\mathbb{E}[(X-\\mathbb{E}[X])^2 U^2]}{\\mathrm{var}(X)^2} \\end{align*}\\] and discussed how to use this result to conduct inference. Now, we want to show why this result holds. To start with, recall that \\[\\begin{align} \\hat{\\beta}_1 = \\frac{\\widehat{\\mathrm{cov}}(X,Y)}{\\widehat{\\mathrm{var}}(X)} \\tag{4.1} \\end{align}\\] Before providing a main result, let’s start with noting the following: Helpful Intermediate Result 1 Notice that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})\\bar{Y}\\Big) &amp;= \\bar{Y} \\frac{1}{n}\\sum_{i=1}^n \\Big( X_i-\\bar{X} \\Big) \\\\ &amp;= \\bar{Y} \\left( \\frac{1}{n}\\sum_{i=1}^n X_i - \\frac{1}{n}\\sum_{i=1}^n \\bar{X} \\right) \\\\ &amp;= \\bar{Y} \\Big(\\bar{X} - \\bar{X} \\Big) \\\\ &amp;= 0 \\end{align*}\\] where the first equality just pull \\(\\bar{Y}\\) out of the summation (it is a constant with respect to the summation), the second equality pushes the summation through the difference, the first part of the third equality holds by the definition of \\(\\bar{X}\\) and the second part holds because it is an average of a constant. This implies that \\[\\begin{align} \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})(Y_i - \\bar{Y})\\Big) = \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})Y_i\\Big) \\tag{4.2} \\end{align}\\] and very similar arguments (basically the same arguments in reverse) also imply that \\[\\begin{align} \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})X_i\\Big) = \\frac{1}{n}\\sum_{i=1}^n \\Big( (X_i - \\bar{X})(X_i - \\bar{X})\\Big) \\tag{4.3} \\end{align}\\] We use both (4.2) and (4.3) below. Next, consider the numerator in (4.1) \\[\\begin{align*} \\widehat{\\mathrm{cov}}(X,Y) &amp;= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})Y_i \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(\\beta_0 + \\beta_1 X_i + U_i) \\\\ &amp;= \\underbrace{\\beta_0 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})}_{(A)} + \\underbrace{\\beta_1 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) X_i}_{(B)} + \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) U_i}_{(C)}) \\\\ \\end{align*}\\] Now, let’s consider each of these in turn. For (A), \\[\\begin{align*} \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} \\qquad \\textrm{and} \\qquad \\frac{1}{n} \\sum_{i=1}^n \\bar{X} = \\bar{X} \\end{align*}\\] which implies that this term is equal to 0. For (B), notice that \\[\\begin{align*} \\beta_1 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) X_i &amp;= \\beta_1 \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) (X_i - \\bar{X}) \\\\ &amp;= \\beta_1 \\widehat{\\mathrm{var}}(X) \\end{align*}\\] For (C), well, we’ll just carry that one around for now. Plugging in the expressions for (A), (B), and (C) back into Equation implies that \\[\\begin{align*} \\hat{\\beta}_1 = \\beta_1 + \\frac{1}{n} \\sum_{i=1}^n \\frac{(X_i - \\bar{X}) U_i}{\\widehat{\\mathrm{var}}(X)} \\end{align*}\\] Next, re-arranging terms and multiplying both sides by \\(\\sqrt{n}\\) implies that \\[\\begin{align*} \\sqrt{n}(\\hat{\\beta}_1 - \\beta_1) &amp;= \\sqrt{n} \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{(X_i - \\bar{X}) U_i}{\\widehat{\\mathrm{var}}(X)}\\right) \\\\ &amp; \\approx \\sqrt{n} \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{(X_i - \\mathbb{E}[X]) U_i}{\\mathrm{var}(X)}\\right) \\end{align*}\\] The last line (the approximately one) is kind of a weak argument, but basically you can replace \\(\\bar{X}\\) and \\(\\widehat{\\mathrm{var}}{X}\\) and the effect of this replacement will converge to 0 in large samples (this is the reason for the approximately) — if you want a more complete explanation, sign up for my graduate econometrics class next semester. Is this helpful? It may not be obvious, but the right hand side of the above equation is actually something that we can apply the Central Limit Theorem to. In particular, maybe it is helpful to define \\(Z_i = \\frac{(X_i - \\mathbb{E}[X]) U_i}{\\mathrm{var}(X)}\\). We know that we could apply a Central Limit Theorem to \\(\\sqrt{n}\\left( \\frac{1}{n} \\sum_{i=1}^n Z_i \\right)\\) if (i) \\(Z_i\\) had mean 0, and (ii) it is iid. That it is iid holds immediately from the random sampling assumption. For mean 0, \\[\\begin{align*} \\mathbb{E}[Z] &amp;= \\mathbb{E}\\left[ \\frac{(X - \\mathbb{E}[X]) U}{\\mathrm{var}(X)}\\right] \\\\ &amp;= \\frac{1}{\\mathrm{var}(X)} \\mathbb{E}[(X - \\mathbb{E}[X]) U] \\\\ &amp;= \\frac{1}{\\mathrm{var}(X)} \\mathbb{E}[(X - \\mathbb{E}[X]) \\underbrace{\\mathbb{E}[U|X]}_{=0}] \\\\ &amp;= 0 \\end{align*}\\] where the only challenging line here is the third one holds from the Law of Iterated Expectations. This means that we can apply the central limit theorem, and in particular, \\(\\sqrt{n} \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i \\right) \\rightarrow N(0,V)\\) where \\(V=\\mathrm{var}(Z) = \\mathbb{E}[Z^2]\\) (where 2nd equality holds because \\(Z\\) has mean 0). Now, just substituting back in for \\(Z\\) implies that \\[\\begin{align*} \\sqrt{n}(\\hat{\\beta}_1 - \\beta_1) \\rightarrow N(0,V) \\end{align*}\\] where \\[\\begin{align} V &amp;= \\mathbb{E}\\left[ \\left( \\frac{(X - \\mathbb{E}[X]) U}{\\mathrm{var}(X)} \\right)^2 \\right] \\nonumber \\\\ &amp;= \\mathbb{E}\\left[ \\frac{(X - \\mathbb{E}[X])^2 U^2}{\\mathrm{var}(X)^2}\\right] \\tag{4.4} \\end{align}\\] which is the expression that we used in class. 4.11 Extra Questions Suppose you run the following regression \\[\\begin{align*} Earnings = \\beta_0 + \\beta_1 Education + U \\end{align*}\\] with \\(\\mathbb{E}[U|Education] = 0\\). How do you interpret \\(\\beta_1\\) here? Suppose you run the following regression \\[\\begin{align*} Earnings = \\beta_0 + \\beta_1 Education + \\beta_2 Experience + \\beta_3 Female + U \\end{align*}\\] with \\(\\mathbb{E}[U|Education, Experience, Female] = 0\\). How do you interpret \\(\\beta_1\\) here? Suppose you are interested in testing whether an extra year of education increases earnings by the same amount for men and women. Propose a regression and strategy for this sort of test. Suppose you also want to control for experience in conducting this test, how would do it? Suppose you run the following regression \\[\\begin{align*} \\log(Earnings) = \\beta_0 + \\beta_1 Education + \\beta_2 Experience + \\beta_3 Female + U \\end{align*}\\] with \\(\\mathbb{E}[U|Education, Experience, Female] = 0\\). How do you interpret \\(\\beta_1\\) here? A common extra condition (though somewhat old-fashioned) is to impose homoskedasticity. Homoskedasticity says that \\(\\mathbb{E}[U^2|X] = \\sigma^2\\) (i.e., the variance of the error term does not change across different values of \\(X\\)). Under homoskedasticity, the expression for \\(V\\) in (4.4) simplifies. Provide a new expression for \\(V\\) under homoskedasticity. Hint: you will need to use the law of iterated expectations. Using this expression for \\(V\\), explain how to calculate standard errors for an estimate of \\(\\beta_1\\) in a simple linear regression. Explain how to construct a t-statistic for testing \\(H_0: \\beta_1=0\\) under homoskedasticity. Explain how to contruct a p-value for \\(\\beta_1\\) under homoskedasticity. Explain how to construct a 95% confidence interval for \\(\\beta_1\\) under homoskedasticity. 4.12 Answers to Some Extra Questions Answer to Question 2 \\(\\beta_1\\) is how much \\(Earnings\\) increase on average when \\(Education\\) increases by one year holding \\(Experience\\) and \\(Female\\) constant. Answer to Question 3 Run the regression \\[\\begin{align*} Earnings &amp;= \\beta_0 + \\beta_1 Education + \\beta_2 Female + \\beta_3 Education \\times Female + U \\end{align*}\\] and test (e.g., calculate a t-statistic and check if it is greater than 1.96 in absolute value) if \\(\\beta_3=0\\). You can run the following regression: \\[\\begin{align*} Earnings &amp;= \\beta_0 + \\beta_1 Education + \\beta_2 Female \\\\ &amp; \\hspace{25pt} + \\beta_3 Education \\times Female + \\beta_4 Experience + U \\end{align*}\\] Here, you would still be interested in \\(\\beta_3\\). If you thought that the return to experience varied for men and women, you might also include an interaction term involving \\(Experience\\) and \\(Female\\). Partial Answer to Question 5 Starting from (4.4) \\[\\begin{align*} V &amp;= \\mathbb{E}\\left[ \\frac{(X - \\mathbb{E}[X])^2 U^2}{\\mathrm{var}(X)^2} \\right] \\\\ &amp;= \\frac{1}{\\mathrm{var}(X)^2} \\mathbb{E}[(X-\\mathbb{E}[X])^2 U^2] \\\\ &amp;= \\frac{1}{\\mathrm{var}(X)^2} \\mathbb{E}\\big[(X-\\mathbb{E}[X])^2 \\mathbb{E}[U^2|X] \\big] \\\\ &amp;= \\frac{1}{\\mathrm{var}(X)^2} \\mathbb{E}[(X-\\mathbb{E}[X])^2 \\sigma^2 ] \\\\ &amp;= \\frac{\\sigma^2}{\\mathrm{var}(X)^2} \\mathbb{E}[(X-\\mathbb{E}[X])^2] \\\\ &amp;= \\frac{\\sigma^2}{\\mathrm{var}(X)^2} \\mathrm{var}(X) \\\\ &amp;= \\frac{\\sigma^2}{\\mathrm{var}(X)} \\end{align*}\\] where the second equality holds because \\(\\mathrm{var}(X)^2\\) is non-random and can come out of the expectation, the third equality uses the law of iterated expectations, the fourth equality holds by the condition of homoskedasticity, the fifth equality holds because \\(\\sigma^2\\) is non-random and can come out of the expectation, the sixth equality holds by the definition of variance, and the last equality holds by cancelling \\(\\mathrm{var}(X)\\) in the numerator with one of the \\(\\mathrm{var}(X)\\)’s in the denominator. "],["prediction.html", "Topic 5 Prediction 5.1 Measures of Regression Fit 5.2 Model Selection 5.3 Machine Learning 5.4 Binary Outcome Models 5.5 Computation 5.6 Extra Questions 5.7 Answers to Some Extra Questions", " Topic 5 Prediction 5.1 Measures of Regression Fit 5.1.1 TSS, ESS, SSR SW 6.4 5.1.2 \\(R^2\\) SW 6.4 5.2 Model Selection SW 7.5 (note: we cover substantially more details than the textbook about model selection) 5.2.1 Limitations of \\(R^2\\) SW 6.4 5.2.2 Adjusted \\(R^2\\) SW 6.4 5.2.3 AIC, BIC Alternative criteria functions: \\(AIC = 2k + n \\log(SSR)\\) \\(BIC = k \\log(n) + n \\log(SSR)\\) Choose model that minimizes these criteria functions. 5.2.4 Cross-Validation Intuition: mimic the out-of-sample prediction problem Algorithm: Split the data into J ``folds’’ For the jth fold, do the following: Estimate the model using all observations not in the Jth fold (\\(\\implies\\) we obtain estimates \\(\\hat{\\beta}_0^j, \\hat{\\beta}_1^j, \\ldots, \\hat{\\beta}_k^j\\)) Predict outcomes for observations in the Jth fold using the estimated model from part (1): \\[\\begin{align*} \\tilde{Y}_{ij} = \\hat{\\beta}_0^j + \\hat{\\beta}_1^j X_{1ij} + \\cdots + \\hat{\\beta_k^j} X_{kij} \\end{align*}\\] Compute the prediction error: \\[\\begin{align*} \\tilde{U}_{ij} = Y_{ij} - \\tilde{Y}_{ij} \\end{align*}\\] (this is the difference between actual outcomes for individuals in the Jth fold and their predicted outcome based on the model from part (1)) Do steps 1-3 for all \\(J\\) folds. This gives a prediction error \\(\\tilde{U}_i\\) for each observation in the data compute the cross validation criteria (mean squared prediction error): \\[\\begin{align*} CV = \\frac{1}{n} \\sum_{i=1}^n \\tilde{U}_{i}^2 \\end{align*}\\] choose the model that produces the smallest value of \\(CV\\). 5.2.5 Model Averaging In the case where you are considering a large number of possible models, it is pretty common that a number of models will, by any of the above model selection criteria, be expected to perform very similarly when making predictions. In this case, one strategy that usually does well in terms of making out-of-sample predictions is model averaging. Suppose you have \\(M\\) different models, and that each model can produce a predicted value for \\(Y_i\\) — let’s call the predicted value from model \\(m\\), \\(\\hat{Y}_i^m\\). Model averaging would involve obtaining a new predicted value, call it \\(\\hat{Y}_i\\) by computing \\[\\begin{align*} \\hat{Y}_i = \\frac{1}{M} \\sum_{m=1}^M \\hat{Y}_i^m \\end{align*}\\] Usually, you would throw out models that you know predict poorly and only average together ones that perform reasonably well. 5.3 Machine Learning SW 14.1, 14.2, 14.6 Some extra resources on estimating Lasso and Ridge regressions in R: glmnet tutorial glmnetUtils vignette 5.3.1 Lasso SW 14.3 5.3.2 Ridge Regression SW 14.4 5.4 Binary Outcome Models Note: we are not necessarily so interested in prediction, but I find this a good spot to teach about binary outcome models before we conclude the course talking about causality In addition to referenced material below, please read all of SW Ch. 11 5.4.1 Linear Probability Model SW 11.1 5.4.2 Probit and Logit SW 11.2, 11.3 5.4.3 Average Partial Effects One of the complications with Probit and Logit is that it is not so simple to interpret the estimated parameters. Remember we are generally interested in partial effects, not the parameters themselves. It just so happens that in many of the linear models that we have considered so far the \\(\\beta\\)’s correspond to the partial effect — this means that it is sometimes easy to forget that they are not what we are typically most interested in. This is helpful framing for thinking about how to interpret the results from a Probit or Logit model. Let’s focus on the Probit model. In that case, \\[\\begin{align*} \\mathrm{P}(Y=1|X_1,X_2,X_3) = \\Phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3) \\end{align*}\\] where \\(\\Phi\\) is the cdf of standard normal random variable. Continuous Case: When \\(X_1\\) is continuous, the partial effect of \\(X_1\\) is given by \\[\\begin{align*} \\frac{\\partial \\mathrm{P}(Y=1|X_1,X_2,X_3)}{\\partial X_1} = \\phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3) \\beta_1 \\end{align*}\\] where \\(\\phi\\) is the pdf of a standard normal random variable. This is more complicated than the partial effect in the context of a linear model. It depends on \\(\\phi\\) (which looks complicated, but you can just use R’s dnorm command to handle that part). More importantly, the partial effect depends on the values of \\(X_1,X_2,\\) and \\(X_3\\). [As discussed above, this is likely a good thing in the context of a binary outcome model]. Thus, in order to get a partial effect, we need to put in some values for these. If you have particular values of the covariates that you are interested in, you can definitely do that, but my general suggestion is to report the Average Partial Effect: \\[\\begin{align*} APE &amp;= \\mathbb{E}\\left[ \\frac{\\partial \\mathrm{P}(Y=1|X_1,X_2,X_3)}{\\partial X_1} \\right] \\\\ &amp;= \\mathbb{E}\\left[ \\phi(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3) \\beta_1 \\right] \\end{align*}\\] which you can estimate by \\[\\begin{align*} \\widehat{APE} &amp;= \\frac{1}{n} \\sum_{i=1}^n \\phi(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\hat{\\beta}_3 X_3) \\hat{\\beta}_1 \\end{align*}\\] which amounts to just computing the partial effect at each value of the covariates in your data and then averaging these partial effects together. This can be a bit cumbersome to do in practice, and it is often convenient to use the R package mfx to compute these sorts of average partial effects for you. Discrete/Binary Case: When \\(X_1\\) is discrete (let’s say binary, but extention to discrete is straightforward), the partial effect of \\(X_1\\) is \\[\\begin{align*} &amp; \\mathrm{P}(Y=1|X_1=1, X_2, X_3) - \\mathrm{P}(Y=1|X_1=0, X_2, X_3) \\\\ &amp;\\hspace{100pt} = \\Phi(\\beta_0 + \\beta_1 + \\beta_2 X_2 + \\beta_3 X_3) - \\Phi(\\beta_0 + \\beta_2 X_2 + \\beta_3 X_3) \\end{align*}\\] Notice that \\(\\beta_1\\) does not show up in the last term. As above, the partial effect depends on the values of \\(X_2\\) and \\(X_3\\) which suggests reporting an \\(APE\\) as above (follows the same steps, just replacing the partial effect, as in the continuous case above) Extensions to Logit are virtually identical, just replace \\(\\Phi\\) with \\(\\Lambda\\) and \\(\\phi\\) with \\(\\lambda\\). Parameters from LPM, Probit, and Logit could be quite different (in fact, they are quite different by construction), but APE’s are often very similar. 5.5 Computation \\(R^2\\) and \\(\\bar{R}^2\\) are both reported as output from R’s lm command. It is straightforward (and a useful excercise) to compute \\(TSS, ESS,\\) and \\(SSR\\) directly. It is also straightforward to calculate \\(AIC\\) and \\(BIC\\) — there are probably packages that will do this for you, but they are so easy that I suggest just calculating on your own. The same applies to cross validation. I suspect that there are packages available that will do this for you, but I think these are useful coding exercises and not all that difficult. Also, if you do this yourself, it removes any kinds of “black box” issues from downloading R code where it may not be clear exactly what it is doing. Computing Lasso and Ridge regressions is substantially more complicated than most other things that we have computed this semester. The main R package for Lasso and Ridge regressions is the glmnet package. For some reason, the syntax of the package is somewhat different from, for example, the lm command. In my view, it is often easier to use the glmnetUtils package, which seems to be just a wrapper for glmnet but with functions that are analogous to lm. Suppose that you have access to a training dataset called train and a testing dataset called test, you can use the glmnetUtils package in the following way: library(glmnetUtils) lasso_model &lt;- cv.glmnet(Y ~ X1 + X2 + X3, data=train, use.model.frame=TRUE) # or whatever formula you want to use coef(lasso_model) # if you are interested in estimated coefficients predict(lasso_model, newdata=test) # get predictions The main R function for estimating binary outcome models is the glm function (this stands for “generalized linear model”). The syntax is very similar to the syntax for the lm command. For example, suppose \\(Y\\) is binary and you are interested in estimating a logit model with regressors \\(X1\\) and \\(X2\\) which are contained in a data.frame called data. In this case, you could use the following code # glm logit bin_reg &lt;- glm(Y ~ X1 + X2, family=binomial(link=&quot;logit&quot;), data=data) summary(bin_reg) # average partial effects library(mfx) bin_ape &lt;- logitmfx(Y ~ X1 + X2, data=data, atmean=FALSE) bin_ape # glm probit pro_reg &lt;- glm(Y ~ X1 + X2, family=binomial(link=&quot;probit&quot;), data=data) summary(pro_reg) # probit average partial effects pro_ape &lt;- probitmfx(Y ~ X1 + X2, data=data, atmean=FALSE) pro_ape 5.6 Extra Questions What are some drawbacks of using \\(R^2\\) as a model selection tool? Does AIC or BIC tend to pick “more complicated” models? What is the reason for this? Suppose you are interested in predicting some outcome \\(Y\\) and have access to covariates \\(X_1\\), \\(X_2\\), and \\(X_3\\). You estimate the following two models \\[\\begin{align*} Y &amp;= 30 + 4 X_1 - 2 X_2 - 10 X_3, \\qquad R^2=0.5, AIC=3421 \\\\ Y &amp;= 9 + 2 X_1 - 3 X_2 - 2 X_3 + 2 X_1^2 + 1 X_2^2 - 4 X_3^2 + 2 X_1 X_2, \\qquad R^2 = 0.75, AIC=4018 \\end{align*}\\] Which model seems to be predicting better? Explain why. Using the model that is predicting better, what would be your prediction for \\(Y\\) when \\(X_1=10, X_2=1, X_3=5\\)? In Lasso and Ridge regressions, it is common to “standardize” the regressors before estimating the model (e.g., the glmnet does this automatically for you). What is the reason for doing this? In Lasso and Ridge regressions, the penalty term lead to “shrinking” the estimated parameters in the model towards 0. This tends to introduce bias while reducing variance. Why can introducing bias while reducing variance potentially lead to better predictions? Does this argument always apply or just apply in some cases? Explain. In Lasso and Ridge regressions, the penalty term depends on the tuning parameter \\(\\lambda\\). How is this tuning parameter often chosen in practice? Why does it make sense to choose it in this way? What would happen to the estimated coefficients when \\(\\lambda=0\\)? What would happen to the estimated coefficients as \\(\\lambda \\rightarrow \\infty\\)? Suppose you try to estimate a linear probability model, probit model, and logit model using the same specifications. You notice that the estimated coefficients are substantially different from each other. Does this mean that something has gone wrong? 5.7 Answers to Some Extra Questions Answer to Question 3 The first model appears to be predicting better because the AIC is lower. [Also, notice that \\(R^2\\) is higher in the second model, but this is by construction because it includes extra terms relative to the first model which implies that it will fit at least as well in-sample as the first model, but may be suffering from over-fitting.] \\[\\begin{align*} \\hat{Y} &amp;= 30 + 4 (10) - 2 (1) - 10 (5) \\\\ &amp;= 18 \\end{align*}\\] Answer to Question 6 The tuning parameter is often chosen via cross validation. It makes sense to choose it this way because this is effectively choosing a value of \\(\\lambda\\) that is making good pseudo-out-of-sample predictions. As we will see below, if you make bad choices of \\(\\lambda\\), that could result in very poor predictions. When \\(\\lambda=0\\), there would effectively be no penalty term and, therefore, the estimated parameters would coincide with the OLS estimates. When \\(\\lambda \\rightarrow \\infty\\), the penalty term would overwhelm the term corresponding to minimizing SSR. This would result in setting all the estimated parameters to be equal to 0. This extreme approach is likely to lead to very poor predictions. "],["causal-inference.html", "Topic 6 Causal Inference 6.1 Potential Outcomes 6.2 Parameters of Interest 6.3 Experiments 6.4 Unconfoundedness 6.5 Panel Data Approaches 6.6 Instrumental Variables 6.7 Regression Discontinuity 6.8 Extra Questions 6.9 Answers to Some Extra Questions", " Topic 6 Causal Inference For simplicity, we will mostly focus on the case where the treatment is binary. We will use \\(D_i\\) to denote the treatment, so that \\(D_i=1\\) if individual \\(i\\) participates in the treatment and \\(D_i=0\\) if individual \\(i\\) does not participate in the treatment. Example: SW 13.3 6.1 Potential Outcomes SW 13.1 Treated potential outcome: \\(Y_i(1)\\), the outcome an individual would experience if they participated in the treatment Untreated potential outcome: \\(Y_i(0)\\), the outcome an individual would experience if they did not participate in the treatment For individuals that participate in the treatment, we observe \\(Y_i(1)\\) (but not \\(Y_i(0)\\)). For individuals that do not participate in the treatment, we observe \\(Y_i(0)\\) (but not \\(Y_i(1)\\)). Another way to write this is that the observed outcome, \\(Y_i\\) is given by \\[\\begin{align*} Y_i = D_i Y_i(1) + (1-D_i) Y_i(0) \\end{align*}\\] We can think about the individual-level effect of participating in the treatment: \\[\\begin{align*} TE_i = Y_i(1) - Y_i(0) \\end{align*}\\] Considering the difference between treated and untreated potential outcomes is a very natural (and, I think, helpful) way to think about causality. The causal effect of the treatment is the difference between the outcome that an individual would experience if they participate in the treatment relative to what they would experience if they did not participate in the treatment. This notation makes it clear that we are allowing for treatment effect heterogenity — the effect of participating in the treatment can vary across different individuals. That said, most researchers essentially give up on trying to figure out individual level treatment effects. It is not so much that these are not interesting, more it is just that these are very hard to figure out. Take, for example, going to college, and suppose we are interested in the causal effect of going to college on a person’s earnings. I went to college, so I know what my \\(Y(1)\\) is, but I don’t know what my \\(Y(0)\\) is — and, I’d even have a hard time coming with a good guess as to what it might be. 6.2 Parameters of Interest Instead of going for individual-level effects of participating in the treatment, most researchers instead go for more aggregated parameters. The two most common ones are the Average Treatment Effect (ATE) and Average Treatment Effect on the Treated (ATT). \\[\\begin{align*} ATE = \\mathbb{E}[Y(1) - Y(0)] \\qquad \\textrm{and} \\qquad ATT = \\mathbb{E}[Y(1)-Y(0) | D=1] \\end{align*}\\] \\(ATE\\) is the difference between treated potential outcomes and untreated potential outcomes, on average, and for the entire population. \\(ATT\\) is the difference between treated and untreated potential outcomes, on average, conditional on being in the treated group. I will mostly focus on \\(ATT\\). It is worth considering the challenges for learning about \\(ATT\\). In particular, notice that we can write \\[\\begin{align*} ATT = \\mathbb{E}[Y(1)|D=1] - \\mathbb{E}[Y(0)|D=1] \\end{align*}\\] and consider these term separately \\(\\mathbb{E}[Y(1)|D=1]\\) is the average treated potential outcome among the treated group. But we observe treated potential outcomes for the treated group \\(\\implies \\mathbb{E}[Y(1)|D=1] = \\mathbb{E}[Y|D=1]\\). In other words, if we want to estimate this component of the \\(ATT\\), we can just look right at the data and compute the average outcome experienced by individuals in the treated group. \\(\\mathbb{E}[Y(0)|D=1]\\) is the average untreated potential outcome among the treated group. This is (potentially much) more challenging than the first term because we do not observe untreated potential outcomes among the treated group. But, in order to learn about the \\(ATT\\), we will have to somehow deal with this term. I will provide a number of strategies below, but it is important to remember that this is a major challenge, and their may not be a good solution. As a side-comment, I’d like to point out that, while I am a big believer in the power/usefulness of using data to try to answer questions in economics, the above discussion suggests that there are a number of questions that we may just not be able to answer. In economics jargon, this amounts to an identification problem — in other words, there may be competing theories of the world which the available data is not able to distinguish among. I probably do not emphasize this issue enough in our class, but it is something that you should remember! 6.3 Experiments An experiment is often called the “gold standard” for causal inference. In particular, here, we are thinking about the case where participation in the treatment is randomly assigned — something like: people who show up to possibly participate in the treatment, someone flips a coin, and if the coin comes up heads then the person participates in the treatment or, if tails, they do not participate in the treatment. Random assignment means that participating in the treatment is independent of potential outcomes, by construction. We can write this in math as \\[\\begin{align*} (Y(1), Y(0)) \\perp D \\end{align*}\\] For our purposes, this also implies that \\[\\begin{align*} \\mathbb{E}[Y(0)|D=1] = \\mathbb{E}[Y(0)|D=0] = \\mathbb{E}[Y|D=0] \\end{align*}\\] In other words, under random assignment, the average untreated potential among the treated group is equal to the average untreated potential outcome among the untreated group (this is the first equality). This is helpful because untreated potential outcomes are observed for those in the untreated group (this is the second equality). Thus, under random assignment, \\[\\begin{align*} ATT = \\mathbb{E}[Y|D=1] - \\mathbb{E}[Y|D=0] \\end{align*}\\] In other words, the \\(ATT\\) is just the difference in (population) average outcomes among the treated group relative to average outcomes among the untreated group. The natural way to estimate the ATT under random assignment is \\[\\begin{align*} \\widehat{ATT} = \\bar{Y}_{D=1} - \\bar{Y}_{D=0} \\end{align*}\\] i.e., as we have done many times before, in order to estimate the parameter of interest, we just replace population averages with sample averages. It is also often convenient to introduce a regression based estimator of the ATT. This is primarily convenient as it will allow us to leverage all the things we already know about regressions, and, particularly, we it will immediately provide us with standard errors, t-statistics, etc. In order to do this, let’s introduce the following assumption: Treatment Effect Homogeneity: \\(Y_i(1) - Y_i(0) = \\alpha\\) (and \\(\\alpha\\) does not vary across individuals). This is a potentially quite unrealistic assumption; I’ll make some additional comments about it below, but, for now, let’s just go with it. Notice that we can also write \\[\\begin{align*} Y_i(0) = \\beta_0 + U_i \\end{align*}\\] where \\(\\mathbb{E}[U|D=0] = \\mathbb{E}[U|D=1] = 0\\) (this holds under random assignment) Recalling the definition of the observed outcome, notice that \\[\\begin{align*} Y_i &amp;= D_i Y_i(1) + (1-D_i) Y_i(0) \\\\ &amp;= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\\\ &amp;= \\alpha D_i + \\beta_0 + U_i \\end{align*}\\] This suggests running a regression of the observed \\(Y_i\\) on \\(D_i\\) and interpreting the estimated version of \\(\\alpha\\) as an estimate of \\(ATT\\) (and you can pick up standard errors, etc. from the regression output) — this is very convenient. The previous discussion invoked the extra condition of treatment effect homogeneity. I want to point out some things related to this now. In the above regression model, we can alternatively (and equivalently) write it as \\[\\begin{align*} \\mathbb{E}[Y|D] = \\beta_0 + \\alpha D \\end{align*}\\] Now plug in particular values for \\(D\\): \\[\\begin{align*} \\mathbb{E}[Y|D=0] = \\beta_0 \\qquad \\textrm{and} \\qquad \\mathbb{E}[Y|D=1] = \\beta_0 + \\alpha \\end{align*}\\] Subtracting the second equation from the first implies that \\[\\begin{align*} \\alpha = \\mathbb{E}[Y|D=1] - \\mathbb{E}[Y|D=0] \\end{align*}\\] but notice that this is exactly what the \\(ATT\\) is equal to under random assignment. Thus, it is worth pointing out that, although we imposed the assumption of treatment effect homogeneity to arrive at the regression equation, our regression is “robust” to treatment effect heterogeneity. Internal Validity: SW 13.2 External Validity: SW 13.2 6.4 Unconfoundedness SW 6.8, SW Ch. 9 Unconfoundedness Assumption: \\[\\begin{align*} (Y(1),Y(0)) \\perp D | X \\end{align*}\\] You can think of this as saying that, among individuals with the same covariates \\(X\\), they have the same distributions of potential outcomes regardless of whether or not they participate in the treatment. Note that the distribution of \\(X\\) is still allowed to be different between the treated and untreated groups. In other words, after you condition on covariates, there is nothing special (in terms of the distributions of potential outcomes) about the group that participates in the treatment relative to the group that doesn’t participate in the treatment. This is potentially a strong assumption. In order to believe this assumption, you need to believe that untreated individuals with the same characteristics can deliver, on average, the outcome that individuals in the treated group would have experienced if they had not participated in the treatment. In math, you can write this as \\[\\begin{align*} \\mathbb{E}[Y(0) | X, D=1] = \\mathbb{E}[Y(0) | X, D=0] \\end{align*}\\] For this case, let’s continue to make the treatment effect heterogeneity assumption as above. In addition, let’s assume a linear model for untreated potential outcomes \\[\\begin{align*} Y(0) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U \\end{align*}\\] and unconfoundedness implies that \\(\\mathbb{E}[U|X_1,X_2,X_3,D] = 0\\) (the conditioning on \\(D\\) is the unconfoundedness part). Now, recalling the definition of the observed outcome, we can write \\[\\begin{align*} Y_i &amp;= D_i Y_i(1) + (1-D_i) Y_i(0) \\\\ &amp;= D_i (Y_i(1) - Y_i(0)) + Y_i(0) \\\\ &amp;= D_i \\alpha + \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + U_i \\end{align*}\\] which suggests running the regression of observed \\(Y\\) on \\(X_1,X_2,X_3,\\) and \\(D\\) and interpreting the estimate of \\(\\alpha\\) as the causal effect of participating in the treatment. In practice, this will be very similar to what we have done before — so the process would not be hard, but convincing someone (or even yourself) that unconfoundedness holds will be the bigger issue here. As a final comment, the assumption of treatment effect homogeneity is not quite so innocuous here. It turns out that you can show that, in the presence of treatment effect heterogeneity, \\(\\alpha\\) will be equal to a weighted average of individual treatment effects, but the weights can sometimes be “strange.” There are methods that are robust to treatment effect heterogeneity (they are beyond the scope of the current class, but they are not “way” more difficult than what we are doing here). That said, in my experience, the regression estimators (under treatment effect homogeneity) tend to deliver similar estimates to alternative estimators that are robust to treatment effect heterogeneity at least in the setup considered in this section. 6.5 Panel Data Approaches SW All of Ch. 10 and 13.4 In the previous section, we invoked the assumption of unconfoundedness and were in the setup where \\(X\\) was fully observed. But suppose instead that you thought this alternative version of unconfoundedness held \\[\\begin{align*} (Y(1),Y(0)) \\perp D | (X,W) \\end{align*}\\] where \\(X\\) were observed random variables, but \\(W\\) were not observed. Following exactly the same argument as in the previous section, this would lead to a regression like \\[\\begin{align*} Y_i = \\alpha D_i + \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + U_i \\end{align*}\\] (I’m just including one \\(X\\) and one \\(W\\) for simplicity, but you can easily imagine the case where there are more.) If \\(W\\) were observed, then we could just run this regression, but since \\(W\\) is not observed, we run into the problem of omitted variable bias (i.e., if we just ignore \\(W\\), we won’t be estimating the causal effect \\(\\alpha\\)) Panel data setup, notation, etc: SW 10.1 Panel data potentially gives us a way around this problem. This is particularly likely to be the case when \\(W\\) does not vary over time. In that case, we can write \\[\\begin{align*} Y_{it} = \\alpha D_{it} + \\beta_0 + \\beta_1 X_{it} + \\beta_2 W_i + U_{it} \\end{align*}\\] where we consider the case where \\(D\\) and \\(X\\) both change over time. Then, defining \\(\\Delta Y_{it} = Y_{it} - Y_{it-1}\\) (and using similar notation for other variables), notice that \\[\\begin{align*} \\Delta Y_{it} = \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\Delta U_{it} \\end{align*}\\] which, importantly, no longer involves the unobserved \\(W_i\\) and suggests running the above regression and interpreting the estimated version of \\(\\alpha\\) as an estimate of the effect of participating in the treatment. Time fixed effects — The previous regression did not include an intercept. It is common in applied work to allow for the intercept to vary over time (i.e., so that \\(\\beta_0 = \\beta_{0,t}\\)) which allows for “aggregate shocks” such as recessions or common trends in outcomes over time. In practice, this amounts to just including an intercept in the previous regression. Often, there may be many omitted, time invariant variables. In practice, these are usually just lumped into a single fixed effect — even if there are many time invariant, unobserved variables, we can difference them all out at the same time \\[\\begin{align*} Y_{it} &amp;= \\alpha D_{it} + \\beta_{0,t} + \\beta_1 X_{it} + \\underbrace{\\beta_2 W_{1i} + \\beta_3 W_{2i} + \\beta_4 W_{3i}} + U_{it} \\\\ &amp;= \\alpha D_{it} + \\beta_{0,t} + \\underbrace{\\eta_i} + U_{it} \\end{align*}\\] and we can follow the same strategies as above. Another case that is common in practice is when there are more than two time periods. This case is similar to the previous one except there are multiple ways to eliminate the unobserved fixed effect. The two most common are the Within estimator To motivate this approach, notice that, if, for each individual, we average their outcomes over time, the we get \\[\\begin{align*} \\bar{Y}_i = \\alpha \\bar{D}_i + \\beta_1 \\bar{X}_i + (\\textrm{time fixed effects}) + \\bar{U}_i \\end{align*}\\] (where I have just written “time fixed effects” to indicate that these are transformed version of original fixed but still show up here.) Subtracting this equation from the expression for \\(Y_{it}\\) gives \\[\\begin{align*} Y_{it} - \\bar{Y}_i = \\alpha (D_{it} - \\bar{D}_i) + \\beta_1 (X_{it} - \\bar{X}_i) + (\\textrm{time fixed effects}) + U_{it} - \\bar{U}_i \\end{align*}\\] This is a feasible regression to estimate (everything is observed here). This is called a within estimator because the terms \\(\\bar{Y}_i\\), \\(\\bar{D}_i\\), and \\(\\bar{X}_i\\) are the within-individual averages-over-time of the corresponding variable. First differences Another approach to eliminating the unobserved fixed effects is to directly consider \\(\\Delta Y_{it}\\): \\[\\begin{align*} \\Delta Y_{it} = \\alpha \\Delta D_{it} + \\beta_1 \\Delta X_{it} + \\Delta U_{it} \\end{align*}\\] This is the same expression as we had before for the two period case. Only here you would include observations from all available time periods on \\(\\Delta Y_{it}, \\Delta D_{it}, \\Delta X_{it}\\) in the regression. Two cases where a fixed effect strategy can break down: unobserved variables vary over time (i.e., \\(\\cdots + \\beta_2 W_{it} + \\cdots\\)) the effect of unobserved variables varies over time (i.e., \\(\\cdots + \\beta_{2,t} W_i + \\cdots\\)) Also, the assumption of treatment effect homogeneity can potentially matter a lot in this context. This will particularly be the case when (i) individuals can become treated at different points in time, and (ii) there are treatment effect dynamics (so that the effect of participating in the treatment can vary over time) — both of these are realistic in many applications. This is a main research area of mine and one I am happy to talk way more about. The case that we have been talking about where a researcher is interested in the effect of a single “regressor of interest” is often called “Difference in Differences” 6.6 Instrumental Variables SW all of chapter 12 6.7 Regression Discontinuity SW 13.4 6.8 Extra Questions What is the difference between treatment effect homogeneity and treatment effect heterogeneity? Why do most researchers give up on trying to estimate the individual-level effect of participating in a treatment? Explain what unconfoundedness means. What is the key condition underlying a difference-in-differences approach to learn about the causal effect of some treatment on some outcome? What are two key conditions for a valid instrument? Suppose you are interested in the causal effect of participating in a union on a person’s income. Consider the following approaches. Suppose you run the following regression \\[\\begin{align*} Earnings_i = \\beta_0 + \\alpha Union_i + \\beta_1 Education_i + U_i \\end{align*}\\] Would it be reasonable to interpret \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Suppose you have access to panel data and run the following fixed effects regression \\[\\begin{align*} Earnings_{it} = \\beta_{0,t} + \\alpha Union_{it} + \\beta_1 Education_{it} + \\eta_i + U_{it} \\end{align*}\\] where \\(\\eta_i\\) is an individual fixed effect. Would it be reasonable to interpert \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach? Going back to the case with cross-sectional data, consider the regression \\[\\begin{align*} Earnings_i = \\beta_0 + \\alpha Union_i + U_i \\end{align*}\\] but using the variable \\(Z_i = 1\\) if birthday is between Jan. 1 and Jun. 30 while \\(Z_i=0\\) otherwise. Would it be reasonable to interpert \\(\\hat{\\alpha}\\) in this regression as an estimate of the causal effect of participating in a union on earnings? Explain. Can you think of any other advantages or disadvantages of this approach? Suppose that you are interested in the effect of lower college costs on the probability of graduating from college. You have access to student-level data from Georgia where students are eligible for the Hope Scholarship if they can keep their GPA above 3.0. What strategy can use to exploit this institional setting to learn about the causal effect of lower college costs on the probability of going to college? What sort of data would you need in order to implement this strategy? Can you think of any ways that the approach that you suggested could go wrong? Another researcher reads the results from the approach you have implemented and complains that your results are only specific to students who have grades right around the 3.0 cutoff. Is this a fair criticism? Suppose you are willing to believe versions of unconfoundedness, a linear model for untreated potential outcomes, and treatment effect homogeneity so that you could write \\[\\begin{align*} Y_i = \\beta_0 + \\alpha D_i + \\beta_1 X_i + \\beta_2 W_i + U_i \\end{align*}\\] with \\(\\mathbb{E}[U|D,X,W] = 0\\) so that you were willing to interpret \\(\\alpha\\) in this regression as the causal effect of \\(D\\) on \\(Y\\). However, suppose that \\(W\\) is not observed so that you cannot operationalize the above regression. Since you do not observe \\(W\\), you are considering just running a regression of \\(Y\\) on \\(D\\) and \\(X\\) and interpreting the estimated coefficient on \\(D\\) as the causal effect of \\(D\\) on \\(Y\\). Does this seem like a good idea? In part (a), we can write a version of the model that you are thinking about estimating as \\[\\begin{align*} Y_i = \\delta_0 + \\delta_1 D_i + \\delta_2 X_i + \\epsilon_i \\end{align*}\\] Suppose that \\(\\mathbb{E}[\\epsilon | D, X] = 0\\) and suppose also that \\[\\begin{align*} W_i = \\gamma_0 + \\gamma_1 D_i + \\gamma_2 X_i + V_i \\end{align*}\\] with \\(\\mathbb{E}[V|D,X]=0\\). Provide an expression for \\(\\delta_1\\) in terms of \\(\\alpha\\), \\(\\gamma\\)’s and \\(\\beta\\)’s. Explain what this expression means. Suppose you have access to an experiment where some participants were randomly assigned to participate in a job training program and others were randomly assigned not to participate. However, some individuals that were assigned to participate in the treatment decided not to actually participate. Let’s use the following notation: \\(D=1\\) for individuals who actually participated and \\(D=0\\) for individuals who did not participate. \\(Z=1\\) for individuals who were assigned to the treatment and \\(Z=0\\) for individuals assigned not to participate (here, \\(D\\) and \\(Z\\) are not exactly the same because some individuals who were assigned to the treatment did not actually participate). You are considering several different approaches to dealing with this issue. Discuss which of the following are good or bad ideas: Estimating \\(ATT\\) by \\(\\bar{Y}_{D=1} - \\bar{Y}_{D=0}\\). Run the regression \\(Y_i = \\beta_0 + \\alpha D_i + U_i\\) using \\(Z_i\\) as an instrument. Suppose you and a friend have conducted an experiment (things went well so that everyone complied with the treatment that they were assigned to, etc.). You interpret the difference \\(\\bar{Y}_{D=1} - \\bar{Y}_{D=0}\\) as an estimate of the \\(ATT\\), but your friend says that you should interpret it as an estimate of the \\(ATE\\). In fact, according to your friend, random treatment assignment implies that \\(\\mathbb{E}[Y(1)] = \\mathbb{E}[Y(1)|D=1] = \\mathbb{E}[Y|D=1]\\) and \\(\\mathbb{E}[Y(0)] = \\mathbb{E}[Y(0)|D=0] = \\mathbb{E}[Y|D=0]\\) which implies that \\(ATE = \\mathbb{E}[Y|D=1] - \\mathbb{E}[Y|D=0]\\). Who is right? 6.9 Answers to Some Extra Questions Answer to Question 4 The key condition is the parallel trends assumption that says that, in the absence of participating in the treatment, the path of outcomes that individuals in the treated group is the same, on average, as the path of outcomes that individuals in the untreated group actually experienced. Answer to Question 9 When some individuals do not comply with their treatment assignment, this approach is probably not so great. In particular, notice that the comparison in this part of the problem is among individuals who actually participated in the treatment relative to those who didn’t (the latter group includes both those assigned not to participate in the treatment along with those assigned to participate in the treatment, but ultimately didn’t actually participate). This suggests that this approach would generally lead to biased estimates of the \\(ATT\\). In the particular context of job training, you can see this would not be such a good idea if, for example, the people who were assigned to the job training program but who did not participate tended to do this because they were able to find a job before the job training program started. This approach is likely to be better. By construction, \\(Z\\) is not correlated with \\(U\\) (since \\(Z\\) is randomly assigned). \\(Z\\) is also likely to be positively correlated with \\(Z\\) (in particular, this will be the case if being randomly assigned to treatment increases the probability of being treated). This implies that \\(Z\\) is a valid instrument and should be able to deliver a reasonable estimate of the effect of participating in the treatment. Answer to Question 10 While your friend’s explanation is not technically wrong, it seems to me that you are more right than your friend. There is an important issue related to external validity here. The group of people that show up to participate in the experiment could be (and likely are) quite different from the general population. Interpreting the results of the experiment as being an \\(ATE\\) (in the sense of across the entire population) is therefore likely to be incorrect — or at least would require extra assumptions and/or justifications. Interpreting them as an \\(ATT\\) (i.e., as the effect among those who participated in the treatment) is still perfectly reasonable though. "]]
